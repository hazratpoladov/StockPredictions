{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9dfde1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import math\n",
    "import matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import yfinance as yf\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "00d41f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 10\n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a793d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3941059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ak = pd.read_csv(\"goog_max.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "acea1903",
   "metadata": {},
   "outputs": [],
   "source": [
    "ak[\"previous\"] = ak.shift(1)[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f771aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-08-19</td>\n",
       "      <td>2.490664</td>\n",
       "      <td>2.591785</td>\n",
       "      <td>2.390042</td>\n",
       "      <td>2.499133</td>\n",
       "      <td>2.499133</td>\n",
       "      <td>897427216</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-08-20</td>\n",
       "      <td>2.515820</td>\n",
       "      <td>2.716817</td>\n",
       "      <td>2.503118</td>\n",
       "      <td>2.697639</td>\n",
       "      <td>2.697639</td>\n",
       "      <td>458857488</td>\n",
       "      <td>2.499133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-08-23</td>\n",
       "      <td>2.758411</td>\n",
       "      <td>2.826406</td>\n",
       "      <td>2.716070</td>\n",
       "      <td>2.724787</td>\n",
       "      <td>2.724787</td>\n",
       "      <td>366857939</td>\n",
       "      <td>2.697639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-08-24</td>\n",
       "      <td>2.770615</td>\n",
       "      <td>2.779581</td>\n",
       "      <td>2.579581</td>\n",
       "      <td>2.611960</td>\n",
       "      <td>2.611960</td>\n",
       "      <td>306396159</td>\n",
       "      <td>2.724787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-08-25</td>\n",
       "      <td>2.614201</td>\n",
       "      <td>2.689918</td>\n",
       "      <td>2.587302</td>\n",
       "      <td>2.640104</td>\n",
       "      <td>2.640104</td>\n",
       "      <td>184645512</td>\n",
       "      <td>2.611960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>86.720001</td>\n",
       "      <td>89.474998</td>\n",
       "      <td>86.699997</td>\n",
       "      <td>89.239998</td>\n",
       "      <td>89.239998</td>\n",
       "      <td>22855600</td>\n",
       "      <td>88.800003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>2023-01-11</td>\n",
       "      <td>90.059998</td>\n",
       "      <td>92.449997</td>\n",
       "      <td>89.739998</td>\n",
       "      <td>92.260002</td>\n",
       "      <td>92.260002</td>\n",
       "      <td>25998800</td>\n",
       "      <td>89.239998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>2023-01-12</td>\n",
       "      <td>92.400002</td>\n",
       "      <td>92.620003</td>\n",
       "      <td>90.570000</td>\n",
       "      <td>91.910004</td>\n",
       "      <td>91.910004</td>\n",
       "      <td>22754200</td>\n",
       "      <td>92.260002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>91.528000</td>\n",
       "      <td>92.980003</td>\n",
       "      <td>90.930000</td>\n",
       "      <td>92.800003</td>\n",
       "      <td>92.800003</td>\n",
       "      <td>18617800</td>\n",
       "      <td>91.910004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>92.779999</td>\n",
       "      <td>92.970001</td>\n",
       "      <td>90.839996</td>\n",
       "      <td>92.160004</td>\n",
       "      <td>92.160004</td>\n",
       "      <td>22924800</td>\n",
       "      <td>92.800003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4635 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "0     2004-08-19   2.490664   2.591785   2.390042   2.499133   2.499133   \n",
       "1     2004-08-20   2.515820   2.716817   2.503118   2.697639   2.697639   \n",
       "2     2004-08-23   2.758411   2.826406   2.716070   2.724787   2.724787   \n",
       "3     2004-08-24   2.770615   2.779581   2.579581   2.611960   2.611960   \n",
       "4     2004-08-25   2.614201   2.689918   2.587302   2.640104   2.640104   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "4630  2023-01-10  86.720001  89.474998  86.699997  89.239998  89.239998   \n",
       "4631  2023-01-11  90.059998  92.449997  89.739998  92.260002  92.260002   \n",
       "4632  2023-01-12  92.400002  92.620003  90.570000  91.910004  91.910004   \n",
       "4633  2023-01-13  91.528000  92.980003  90.930000  92.800003  92.800003   \n",
       "4634  2023-01-17  92.779999  92.970001  90.839996  92.160004  92.160004   \n",
       "\n",
       "         Volume   previous  \n",
       "0     897427216   0.000000  \n",
       "1     458857488   2.499133  \n",
       "2     366857939   2.697639  \n",
       "3     306396159   2.724787  \n",
       "4     184645512   2.611960  \n",
       "...         ...        ...  \n",
       "4630   22855600  88.800003  \n",
       "4631   25998800  89.239998  \n",
       "4632   22754200  92.260002  \n",
       "4633   18617800  91.910004  \n",
       "4634   22924800  92.800003  \n",
       "\n",
       "[4635 rows x 8 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9eb4c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(ak.shape[0]):\n",
    "    row_data= dict(\n",
    "    Open = ak.Open[i],\n",
    "    High = ak.High[i],\n",
    "    #Close = ak.Close[i],\n",
    "    Low = ak.Low[i],\n",
    "    Volume = ak.Volume[i],\n",
    "    Previous = ak.previous[i],\n",
    "    last_difference = ak.Close[i] - ak.previous[i]\n",
    "    )\n",
    "    rows.append(row_data)\n",
    "features = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b0c5db94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4635, 6)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8fdab0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Previous</th>\n",
       "      <th>last_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.490664</td>\n",
       "      <td>2.591785</td>\n",
       "      <td>2.390042</td>\n",
       "      <td>897427216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.499133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.515820</td>\n",
       "      <td>2.716817</td>\n",
       "      <td>2.503118</td>\n",
       "      <td>458857488</td>\n",
       "      <td>2.499133</td>\n",
       "      <td>0.198506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.758411</td>\n",
       "      <td>2.826406</td>\n",
       "      <td>2.716070</td>\n",
       "      <td>366857939</td>\n",
       "      <td>2.697639</td>\n",
       "      <td>0.027148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.770615</td>\n",
       "      <td>2.779581</td>\n",
       "      <td>2.579581</td>\n",
       "      <td>306396159</td>\n",
       "      <td>2.724787</td>\n",
       "      <td>-0.112827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.614201</td>\n",
       "      <td>2.689918</td>\n",
       "      <td>2.587302</td>\n",
       "      <td>184645512</td>\n",
       "      <td>2.611960</td>\n",
       "      <td>0.028144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>86.720001</td>\n",
       "      <td>89.474998</td>\n",
       "      <td>86.699997</td>\n",
       "      <td>22855600</td>\n",
       "      <td>88.800003</td>\n",
       "      <td>0.439995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>90.059998</td>\n",
       "      <td>92.449997</td>\n",
       "      <td>89.739998</td>\n",
       "      <td>25998800</td>\n",
       "      <td>89.239998</td>\n",
       "      <td>3.020004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>92.400002</td>\n",
       "      <td>92.620003</td>\n",
       "      <td>90.570000</td>\n",
       "      <td>22754200</td>\n",
       "      <td>92.260002</td>\n",
       "      <td>-0.349998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>91.528000</td>\n",
       "      <td>92.980003</td>\n",
       "      <td>90.930000</td>\n",
       "      <td>18617800</td>\n",
       "      <td>91.910004</td>\n",
       "      <td>0.889999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>92.779999</td>\n",
       "      <td>92.970001</td>\n",
       "      <td>90.839996</td>\n",
       "      <td>22924800</td>\n",
       "      <td>92.800003</td>\n",
       "      <td>-0.639999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4635 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Open       High        Low     Volume   Previous  last_difference\n",
       "0      2.490664   2.591785   2.390042  897427216   0.000000         2.499133\n",
       "1      2.515820   2.716817   2.503118  458857488   2.499133         0.198506\n",
       "2      2.758411   2.826406   2.716070  366857939   2.697639         0.027148\n",
       "3      2.770615   2.779581   2.579581  306396159   2.724787        -0.112827\n",
       "4      2.614201   2.689918   2.587302  184645512   2.611960         0.028144\n",
       "...         ...        ...        ...        ...        ...              ...\n",
       "4630  86.720001  89.474998  86.699997   22855600  88.800003         0.439995\n",
       "4631  90.059998  92.449997  89.739998   25998800  89.239998         3.020004\n",
       "4632  92.400002  92.620003  90.570000   22754200  92.260002        -0.349998\n",
       "4633  91.528000  92.980003  90.930000   18617800  91.910004         0.889999\n",
       "4634  92.779999  92.970001  90.839996   22924800  92.800003        -0.639999\n",
       "\n",
       "[4635 rows x 6 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d0bcbbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(ak.shape[0]):\n",
    "    row_data= dict(\n",
    "    Close = ak.Close[i]\n",
    "    )\n",
    "    rows.append(row_data)\n",
    "labels = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0608651f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.499133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.697639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.724787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.611960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.640104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>89.239998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>92.260002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>91.910004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>92.800003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>92.160004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4635 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Close\n",
       "0      2.499133\n",
       "1      2.697639\n",
       "2      2.724787\n",
       "3      2.611960\n",
       "4      2.640104\n",
       "...         ...\n",
       "4630  89.239998\n",
       "4631  92.260002\n",
       "4632  91.910004\n",
       "4633  92.800003\n",
       "4634  92.160004\n",
       "\n",
       "[4635 rows x 1 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2353fc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Previous</th>\n",
       "      <th>last_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.490664</td>\n",
       "      <td>2.591785</td>\n",
       "      <td>2.390042</td>\n",
       "      <td>897427216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.499133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.515820</td>\n",
       "      <td>2.716817</td>\n",
       "      <td>2.503118</td>\n",
       "      <td>458857488</td>\n",
       "      <td>2.499133</td>\n",
       "      <td>0.198506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.758411</td>\n",
       "      <td>2.826406</td>\n",
       "      <td>2.716070</td>\n",
       "      <td>366857939</td>\n",
       "      <td>2.697639</td>\n",
       "      <td>0.027148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.770615</td>\n",
       "      <td>2.779581</td>\n",
       "      <td>2.579581</td>\n",
       "      <td>306396159</td>\n",
       "      <td>2.724787</td>\n",
       "      <td>-0.112827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.614201</td>\n",
       "      <td>2.689918</td>\n",
       "      <td>2.587302</td>\n",
       "      <td>184645512</td>\n",
       "      <td>2.611960</td>\n",
       "      <td>0.028144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>86.720001</td>\n",
       "      <td>89.474998</td>\n",
       "      <td>86.699997</td>\n",
       "      <td>22855600</td>\n",
       "      <td>88.800003</td>\n",
       "      <td>0.439995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>90.059998</td>\n",
       "      <td>92.449997</td>\n",
       "      <td>89.739998</td>\n",
       "      <td>25998800</td>\n",
       "      <td>89.239998</td>\n",
       "      <td>3.020004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>92.400002</td>\n",
       "      <td>92.620003</td>\n",
       "      <td>90.570000</td>\n",
       "      <td>22754200</td>\n",
       "      <td>92.260002</td>\n",
       "      <td>-0.349998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>91.528000</td>\n",
       "      <td>92.980003</td>\n",
       "      <td>90.930000</td>\n",
       "      <td>18617800</td>\n",
       "      <td>91.910004</td>\n",
       "      <td>0.889999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>92.779999</td>\n",
       "      <td>92.970001</td>\n",
       "      <td>90.839996</td>\n",
       "      <td>22924800</td>\n",
       "      <td>92.800003</td>\n",
       "      <td>-0.639999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4635 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Open       High        Low     Volume   Previous  last_difference\n",
       "0      2.490664   2.591785   2.390042  897427216   0.000000         2.499133\n",
       "1      2.515820   2.716817   2.503118  458857488   2.499133         0.198506\n",
       "2      2.758411   2.826406   2.716070  366857939   2.697639         0.027148\n",
       "3      2.770615   2.779581   2.579581  306396159   2.724787        -0.112827\n",
       "4      2.614201   2.689918   2.587302  184645512   2.611960         0.028144\n",
       "...         ...        ...        ...        ...        ...              ...\n",
       "4630  86.720001  89.474998  86.699997   22855600  88.800003         0.439995\n",
       "4631  90.059998  92.449997  89.739998   25998800  89.239998         3.020004\n",
       "4632  92.400002  92.620003  90.570000   22754200  92.260002        -0.349998\n",
       "4633  91.528000  92.980003  90.930000   18617800  91.910004         0.889999\n",
       "4634  92.779999  92.970001  90.839996   22924800  92.800003        -0.639999\n",
       "\n",
       "[4635 rows x 6 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "db567f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int((len(features))*.9)\n",
    "test_size = int((len(features))*.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "57c73b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.499133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.697639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.724787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.611960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.640104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>89.239998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>92.260002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>91.910004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>92.800003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>92.160004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4635 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Close\n",
       "0      2.499133\n",
       "1      2.697639\n",
       "2      2.724787\n",
       "3      2.611960\n",
       "4      2.640104\n",
       "...         ...\n",
       "4630  89.239998\n",
       "4631  92.260002\n",
       "4632  91.910004\n",
       "4633  92.800003\n",
       "4634  92.160004\n",
       "\n",
       "[4635 rows x 1 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4ca21c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4171, 463)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size,test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7b7e15a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x = features[:train_size],  features[train_size:]\n",
    "train_y,test_y = labels[:train_size],  labels[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "663f7313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.499133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.697639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.724787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.611960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.640104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4166</th>\n",
       "      <td>102.635002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4167</th>\n",
       "      <td>102.751503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4168</th>\n",
       "      <td>105.738503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>103.096001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4170</th>\n",
       "      <td>103.324501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4171 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Close\n",
       "0       2.499133\n",
       "1       2.697639\n",
       "2       2.724787\n",
       "3       2.611960\n",
       "4       2.640104\n",
       "...          ...\n",
       "4166  102.635002\n",
       "4167  102.751503\n",
       "4168  105.738503\n",
       "4169  103.096001\n",
       "4170  103.324501\n",
       "\n",
       "[4171 rows x 1 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1db25809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler1 = MinMaxScaler(feature_range=(0,1))\n",
    "train_x= pd.DataFrame(\n",
    "        scaler1.fit_transform(train_x),\n",
    "        index = train_x.index,\n",
    "        columns = train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c94d53e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y= pd.DataFrame(\n",
    "        scaler1.fit_transform(train_y),\n",
    "        index = train_y.index,\n",
    "        columns = train_y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0ff70879",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x= pd.DataFrame(\n",
    "        scaler1.fit_transform(test_x),\n",
    "        index = test_x.index,\n",
    "        columns = test_x.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5a6ab0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y= pd.DataFrame(\n",
    "        scaler1.fit_transform(test_y),\n",
    "        index = test_y.index,\n",
    "        columns = test_y.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "68e5e14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Previous</th>\n",
       "      <th>last_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.543577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.277886</td>\n",
       "      <td>0.023485</td>\n",
       "      <td>0.501404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002782</td>\n",
       "      <td>0.002782</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.222151</td>\n",
       "      <td>0.025350</td>\n",
       "      <td>0.489074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.185523</td>\n",
       "      <td>0.025605</td>\n",
       "      <td>0.479002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.111765</td>\n",
       "      <td>0.024545</td>\n",
       "      <td>0.489146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4166</th>\n",
       "      <td>0.976196</td>\n",
       "      <td>0.964491</td>\n",
       "      <td>0.972512</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>0.951069</td>\n",
       "      <td>0.589761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4167</th>\n",
       "      <td>0.977047</td>\n",
       "      <td>0.963045</td>\n",
       "      <td>0.965481</td>\n",
       "      <td>0.015275</td>\n",
       "      <td>0.964474</td>\n",
       "      <td>0.495503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4168</th>\n",
       "      <td>0.978158</td>\n",
       "      <td>0.987165</td>\n",
       "      <td>0.984450</td>\n",
       "      <td>0.014917</td>\n",
       "      <td>0.965569</td>\n",
       "      <td>0.702044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>0.983443</td>\n",
       "      <td>0.970304</td>\n",
       "      <td>0.972376</td>\n",
       "      <td>0.020814</td>\n",
       "      <td>0.993638</td>\n",
       "      <td>0.296985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4170</th>\n",
       "      <td>0.972476</td>\n",
       "      <td>0.959267</td>\n",
       "      <td>0.970412</td>\n",
       "      <td>0.015632</td>\n",
       "      <td>0.968806</td>\n",
       "      <td>0.503562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4171 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open      High       Low    Volume  Previous  last_difference\n",
       "0     0.000195  0.000550  0.000000  0.543577  0.000000         0.666940\n",
       "1     0.000438  0.001739  0.001100  0.277886  0.023485         0.501404\n",
       "2     0.002782  0.002782  0.003171  0.222151  0.025350         0.489074\n",
       "3     0.002900  0.002337  0.001843  0.185523  0.025605         0.479002\n",
       "4     0.001389  0.001484  0.001918  0.111765  0.024545         0.489146\n",
       "...        ...       ...       ...       ...       ...              ...\n",
       "4166  0.976196  0.964491  0.972512  0.020469  0.951069         0.589761\n",
       "4167  0.977047  0.963045  0.965481  0.015275  0.964474         0.495503\n",
       "4168  0.978158  0.987165  0.984450  0.014917  0.965569         0.702044\n",
       "4169  0.983443  0.970304  0.972376  0.020814  0.993638         0.296985\n",
       "4170  0.972476  0.959267  0.970412  0.015632  0.968806         0.503562\n",
       "\n",
       "[4171 rows x 6 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "759afe7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4166</th>\n",
       "      <td>0.963623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4167</th>\n",
       "      <td>0.964744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4168</th>\n",
       "      <td>0.993486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>0.968059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4170</th>\n",
       "      <td>0.970257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4171 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Close\n",
       "0     0.000079\n",
       "1     0.001989\n",
       "2     0.002250\n",
       "3     0.001165\n",
       "4     0.001436\n",
       "...        ...\n",
       "4166  0.963623\n",
       "4167  0.964744\n",
       "4168  0.993486\n",
       "4169  0.968059\n",
       "4170  0.970257\n",
       "\n",
       "[4171 rows x 1 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2bb682bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Previous</th>\n",
       "      <th>last_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4171</th>\n",
       "      <td>0.277898</td>\n",
       "      <td>0.299436</td>\n",
       "      <td>0.301787</td>\n",
       "      <td>0.261932</td>\n",
       "      <td>0.295073</td>\n",
       "      <td>0.563030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>0.275667</td>\n",
       "      <td>0.288924</td>\n",
       "      <td>0.289746</td>\n",
       "      <td>0.214483</td>\n",
       "      <td>0.314435</td>\n",
       "      <td>0.495263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>0.264342</td>\n",
       "      <td>0.262776</td>\n",
       "      <td>0.274356</td>\n",
       "      <td>0.229856</td>\n",
       "      <td>0.313364</td>\n",
       "      <td>0.363479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0.250062</td>\n",
       "      <td>0.245690</td>\n",
       "      <td>0.262412</td>\n",
       "      <td>0.464760</td>\n",
       "      <td>0.272557</td>\n",
       "      <td>0.516035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.249904</td>\n",
       "      <td>0.249420</td>\n",
       "      <td>0.268726</td>\n",
       "      <td>0.376045</td>\n",
       "      <td>0.277749</td>\n",
       "      <td>0.487443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>0.018236</td>\n",
       "      <td>0.044622</td>\n",
       "      <td>0.048918</td>\n",
       "      <td>0.175997</td>\n",
       "      <td>0.078996</td>\n",
       "      <td>0.520525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>0.068572</td>\n",
       "      <td>0.090008</td>\n",
       "      <td>0.094675</td>\n",
       "      <td>0.214715</td>\n",
       "      <td>0.085541</td>\n",
       "      <td>0.647819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>0.103838</td>\n",
       "      <td>0.092601</td>\n",
       "      <td>0.107168</td>\n",
       "      <td>0.174748</td>\n",
       "      <td>0.130469</td>\n",
       "      <td>0.481547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>0.090696</td>\n",
       "      <td>0.098093</td>\n",
       "      <td>0.112587</td>\n",
       "      <td>0.123796</td>\n",
       "      <td>0.125262</td>\n",
       "      <td>0.542727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>0.109565</td>\n",
       "      <td>0.097940</td>\n",
       "      <td>0.111232</td>\n",
       "      <td>0.176849</td>\n",
       "      <td>0.138503</td>\n",
       "      <td>0.467239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>464 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open      High       Low    Volume  Previous  last_difference\n",
       "4171  0.277898  0.299436  0.301787  0.261932  0.295073         0.563030\n",
       "4172  0.275667  0.288924  0.289746  0.214483  0.314435         0.495263\n",
       "4173  0.264342  0.262776  0.274356  0.229856  0.313364         0.363479\n",
       "4174  0.250062  0.245690  0.262412  0.464760  0.272557         0.516035\n",
       "4175  0.249904  0.249420  0.268726  0.376045  0.277749         0.487443\n",
       "...        ...       ...       ...       ...       ...              ...\n",
       "4630  0.018236  0.044622  0.048918  0.175997  0.078996         0.520525\n",
       "4631  0.068572  0.090008  0.094675  0.214715  0.085541         0.647819\n",
       "4632  0.103838  0.092601  0.107168  0.174748  0.130469         0.481547\n",
       "4633  0.090696  0.098093  0.112587  0.123796  0.125262         0.542727\n",
       "4634  0.109565  0.097940  0.111232  0.176849  0.138503         0.467239\n",
       "\n",
       "[464 rows x 6 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7deb0558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4171</th>\n",
       "      <td>0.314435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>0.313364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>0.272557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0.277749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.274320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>0.085541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>0.130469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>0.125262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>0.138503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>0.128981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>464 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Close\n",
       "4171  0.314435\n",
       "4172  0.313364\n",
       "4173  0.272557\n",
       "4174  0.277749\n",
       "4175  0.274320\n",
       "...        ...\n",
       "4630  0.085541\n",
       "4631  0.130469\n",
       "4632  0.125262\n",
       "4633  0.138503\n",
       "4634  0.128981\n",
       "\n",
       "[464 rows x 1 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3d6ec426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(feature_data: pd.DataFrame,label_data: pd.DataFrame,sequence_length):\n",
    "    sequences = []\n",
    "    data_size = len(feature_data)\n",
    "    \n",
    "    for i in range(data_size-sequence_length):\n",
    "        sequence = feature_data[i:i+sequence_length]\n",
    "        label_position = i + sequence_length\n",
    "        label = label_data.iloc[label_position][\"Close\"]\n",
    "        sequences.append((sequence,label))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8ef24df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = create_sequences(train_x,train_y,5)\n",
    "test_sequences = create_sequences(test_x,test_y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "19e069b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4166, 459)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sequences),len(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f4ff6249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(       Open      High       Low    Volume  Previous  last_difference\n",
       "  0  0.000195  0.000550  0.000000  0.543577  0.000000         0.666940\n",
       "  1  0.000438  0.001739  0.001100  0.277886  0.023485         0.501404\n",
       "  2  0.002782  0.002782  0.003171  0.222151  0.025350         0.489074\n",
       "  3  0.002900  0.002337  0.001843  0.185523  0.025605         0.479002\n",
       "  4  0.001389  0.001484  0.001918  0.111765  0.024545         0.489146,\n",
       "  0.0018933248748919732),\n",
       " (       Open      High       Low    Volume  Previous  last_difference\n",
       "  1  0.000438  0.001739  0.001100  0.277886  0.023485         0.501404\n",
       "  2  0.002782  0.002782  0.003171  0.222151  0.025350         0.489074\n",
       "  3  0.002900  0.002337  0.001843  0.185523  0.025605         0.479002\n",
       "  4  0.001389  0.001484  0.001918  0.111765  0.024545         0.489146\n",
       "  5  0.001386  0.001472  0.002107  0.086276  0.024809         0.490543,\n",
       "  0.0014715190007399993),\n",
       " (       Open      High       Low    Volume  Previous  last_difference\n",
       "  2  0.002782  0.002782  0.003171  0.222151  0.025350         0.489074\n",
       "  3  0.002900  0.002337  0.001843  0.185523  0.025605         0.479002\n",
       "  4  0.001389  0.001484  0.001918  0.111765  0.024545         0.489146\n",
       "  5  0.001386  0.001472  0.002107  0.086276  0.024809         0.490543\n",
       "  6  0.002144  0.001630  0.002357  0.075525  0.025256         0.483966,\n",
       "  0.0004793283560317156),\n",
       " (       Open      High       Low    Volume  Previous  last_difference\n",
       "  3  0.002900  0.002337  0.001843  0.185523  0.025605         0.479002\n",
       "  4  0.001389  0.001484  0.001918  0.111765  0.024545         0.489146\n",
       "  5  0.001386  0.001472  0.002107  0.086276  0.024809         0.490543\n",
       "  6  0.002144  0.001630  0.002357  0.075525  0.025256         0.483966\n",
       "  7  0.001466  0.000889  0.001465  0.063169  0.024845         0.479701,\n",
       "  0.0005656024564890255),\n",
       " (       Open      High       Low    Volume  Previous  last_difference\n",
       "  4  0.001389  0.001484  0.001918  0.111765  0.024545         0.489146\n",
       "  5  0.001386  0.001472  0.002107  0.086276  0.024809         0.490543\n",
       "  6  0.002144  0.001630  0.002357  0.075525  0.025256         0.483966\n",
       "  7  0.001466  0.000889  0.001465  0.063169  0.024845         0.479701\n",
       "  8  0.000748  0.000467  0.001502  0.059774  0.023876         0.487766,\n",
       "  5.752248187974526e-05),\n",
       " (       Open      High       Low    Volume  Previous  last_difference\n",
       "  5  0.001386  0.001472  0.002107  0.086276  0.024809         0.490543\n",
       "  6  0.002144  0.001630  0.002357  0.075525  0.025256         0.483966\n",
       "  7  0.001466  0.000889  0.001465  0.063169  0.024845         0.479701\n",
       "  8  0.000748  0.000467  0.001502  0.059774  0.023876         0.487766\n",
       "  9  0.000845  0.000291  0.000899  0.111152  0.023960         0.483321,\n",
       "  0.00035949145584263445),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  6   0.002144  0.001630  0.002357  0.075525  0.025256         0.483966\n",
       "  7   0.001466  0.000889  0.001465  0.063169  0.024845         0.479701\n",
       "  8   0.000748  0.000467  0.001502  0.059774  0.023876         0.487766\n",
       "  9   0.000845  0.000291  0.000899  0.111152  0.023960         0.483321\n",
       "  10  0.000000  0.000149  0.000722  0.183956  0.023464         0.489379,\n",
       "  0.0),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  7   0.001466  0.000889  0.001465  0.063169  0.024845         0.479701\n",
       "  8   0.000748  0.000467  0.001502  0.059774  0.023876         0.487766\n",
       "  9   0.000845  0.000291  0.000899  0.111152  0.023960         0.483321\n",
       "  10  0.000000  0.000149  0.000722  0.183956  0.023464         0.489379\n",
       "  11  0.000424  0.000000  0.000814  0.062629  0.023759         0.484432,\n",
       "  0.0003762728557085218),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  8   0.000748  0.000467  0.001502  0.059774  0.023876         0.487766\n",
       "  9   0.000845  0.000291  0.000899  0.111152  0.023960         0.483321\n",
       "  10  0.000000  0.000149  0.000722  0.183956  0.023464         0.489379\n",
       "  11  0.000424  0.000000  0.000814  0.062629  0.023759         0.484432\n",
       "  12  0.000438  0.000062  0.000884  0.071091  0.023407         0.489934,\n",
       "  0.0005488306789854479),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  9   0.000845  0.000291  0.000899  0.111152  0.023960         0.483321\n",
       "  10  0.000000  0.000149  0.000722  0.183956  0.023464         0.489379\n",
       "  11  0.000424  0.000000  0.000814  0.062629  0.023759         0.484432\n",
       "  12  0.000438  0.000062  0.000884  0.071091  0.023407         0.489934\n",
       "  13  0.000373  0.000306  0.001100  0.060598  0.023775         0.488411,\n",
       "  0.0005512266472002432),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  10  0.000000  0.000149  0.000722  0.183956  0.023464         0.489379\n",
       "  11  0.000424  0.000000  0.000814  0.062629  0.023759         0.484432\n",
       "  12  0.000438  0.000062  0.000884  0.071091  0.023407         0.489934\n",
       "  13  0.000373  0.000306  0.001100  0.060598  0.023775         0.488411\n",
       "  14  0.000804  0.000230  0.001221  0.049351  0.023943         0.487138,\n",
       "  0.0012750014953151027),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  11  0.000424  0.000000  0.000814  0.062629  0.023759         0.484432\n",
       "  12  0.000438  0.000062  0.000884  0.071091  0.023407         0.489934\n",
       "  13  0.000373  0.000306  0.001100  0.060598  0.023775         0.488411\n",
       "  14  0.000804  0.000230  0.001221  0.049351  0.023943         0.487138\n",
       "  15  0.000580  0.001142  0.001293  0.105803  0.023946         0.492533,\n",
       "  0.00179506131099837),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  12  0.000438  0.000062  0.000884  0.071091  0.023407         0.489934\n",
       "  13  0.000373  0.000306  0.001100  0.060598  0.023775         0.488411\n",
       "  14  0.000804  0.000230  0.001221  0.049351  0.023943         0.487138\n",
       "  15  0.000580  0.001142  0.001293  0.105803  0.023946         0.492533\n",
       "  16  0.001791  0.001581  0.002543  0.095397  0.024653         0.491009,\n",
       "  0.0027513124324846927),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  13  0.000373  0.000306  0.001100  0.060598  0.023775         0.488411\n",
       "  14  0.000804  0.000230  0.001221  0.049351  0.023943         0.487138\n",
       "  15  0.000580  0.001142  0.001293  0.105803  0.023946         0.492533\n",
       "  16  0.001791  0.001581  0.002543  0.095397  0.024653         0.491009\n",
       "  17  0.001988  0.002431  0.002623  0.131734  0.025160         0.494271,\n",
       "  0.0028735356785262664),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  14  0.000804  0.000230  0.001221  0.049351  0.023943         0.487138\n",
       "  15  0.000580  0.001142  0.001293  0.105803  0.023946         0.492533\n",
       "  16  0.001791  0.001581  0.002543  0.095397  0.024653         0.491009\n",
       "  17  0.001988  0.002431  0.002623  0.131734  0.025160         0.494271\n",
       "  18  0.002736  0.002960  0.003449  0.130323  0.026094         0.488035,\n",
       "  0.0033456665075512863),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  15  0.000580  0.001142  0.001293  0.105803  0.023946         0.492533\n",
       "  16  0.001791  0.001581  0.002543  0.095397  0.024653         0.491009\n",
       "  17  0.001988  0.002431  0.002623  0.131734  0.025160         0.494271\n",
       "  18  0.002736  0.002960  0.003449  0.130323  0.026094         0.488035\n",
       "  19  0.003165  0.003332  0.003800  0.112711  0.026214         0.490651,\n",
       "  0.004189278255855227),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  16  0.001791  0.001581  0.002543  0.095397  0.024653         0.491009\n",
       "  17  0.001988  0.002431  0.002623  0.131734  0.025160         0.494271\n",
       "  18  0.002736  0.002960  0.003449  0.130323  0.026094         0.488035\n",
       "  19  0.003165  0.003332  0.003800  0.112711  0.026214         0.490651\n",
       "  20  0.003665  0.003732  0.004261  0.115221  0.026675         0.493429,\n",
       "  0.004637439780369963),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  17  0.001988  0.002431  0.002623  0.131734  0.025160         0.494271\n",
       "  18  0.002736  0.002960  0.003449  0.130323  0.026094         0.488035\n",
       "  19  0.003165  0.003332  0.003800  0.112711  0.026214         0.490651\n",
       "  20  0.003665  0.003732  0.004261  0.115221  0.026675         0.493429\n",
       "  21  0.004274  0.004706  0.005041  0.129297  0.027499         0.490472,\n",
       "  0.004273156388097738),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  18  0.002736  0.002960  0.003449  0.130323  0.026094         0.488035\n",
       "  19  0.003165  0.003332  0.003800  0.112711  0.026214         0.490651\n",
       "  20  0.003665  0.003732  0.004261  0.115221  0.026675         0.493429\n",
       "  21  0.004274  0.004706  0.005041  0.129297  0.027499         0.490472\n",
       "  22  0.004962  0.004427  0.005220  0.087906  0.027936         0.484397,\n",
       "  0.004402567538783701),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  19  0.003165  0.003332  0.003800  0.112711  0.026214         0.490651\n",
       "  20  0.003665  0.003732  0.004261  0.115221  0.026675         0.493429\n",
       "  21  0.004274  0.004706  0.005041  0.129297  0.027499         0.490472\n",
       "  22  0.004962  0.004427  0.005220  0.087906  0.027936         0.484397\n",
       "  23  0.004382  0.004249  0.005050  0.092197  0.027581         0.488088,\n",
       "  0.004987347363353413),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  20  0.003665  0.003732  0.004261  0.115221  0.026675         0.493429\n",
       "  21  0.004274  0.004706  0.005041  0.129297  0.027499         0.490472\n",
       "  22  0.004962  0.004427  0.005220  0.087906  0.027936         0.484397\n",
       "  23  0.004382  0.004249  0.005050  0.092197  0.027581         0.488088\n",
       "  24  0.004729  0.004951  0.005101  0.103816  0.027707         0.491493,\n",
       "  0.004750079153552349),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  21  0.004274  0.004706  0.005041  0.129297  0.027499         0.490472\n",
       "  22  0.004962  0.004427  0.005220  0.087906  0.027936         0.484397\n",
       "  23  0.004382  0.004249  0.005050  0.092197  0.027581         0.488088\n",
       "  24  0.004729  0.004951  0.005101  0.103816  0.027707         0.491493\n",
       "  25  0.005234  0.005299  0.005765  0.110972  0.028278         0.485346,\n",
       "  0.00437381592020614),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  22  0.004962  0.004427  0.005220  0.087906  0.027936         0.484397\n",
       "  23  0.004382  0.004249  0.005050  0.092197  0.027581         0.488088\n",
       "  24  0.004729  0.004951  0.005101  0.103816  0.027707         0.491493\n",
       "  25  0.005234  0.005299  0.005765  0.110972  0.028278         0.485346\n",
       "  26  0.004902  0.004536  0.005290  0.085926  0.028046         0.484307,\n",
       "  0.006434897059583132),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  23  0.004382  0.004249  0.005050  0.092197  0.027581         0.488088\n",
       "  24  0.004729  0.004951  0.005101  0.103816  0.027707         0.491493\n",
       "  25  0.005234  0.005299  0.005765  0.110972  0.028278         0.485346\n",
       "  26  0.004902  0.004536  0.005290  0.085926  0.028046         0.484307\n",
       "  27  0.005321  0.006081  0.005874  0.205996  0.027679         0.502533,\n",
       "  0.007446265072372098),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  24  0.004729  0.004951  0.005101  0.103816  0.027707         0.491493\n",
       "  25  0.005234  0.005299  0.005765  0.110972  0.028278         0.485346\n",
       "  26  0.004902  0.004536  0.005290  0.085926  0.028046         0.484307\n",
       "  27  0.005321  0.006081  0.005874  0.205996  0.027679         0.502533\n",
       "  28  0.006621  0.007887  0.007332  0.371407  0.029692         0.494683,\n",
       "  0.007091565552959058),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  25  0.005234  0.005299  0.005765  0.110972  0.028278         0.485346\n",
       "  26  0.004902  0.004536  0.005290  0.085926  0.028046         0.484307\n",
       "  27  0.005321  0.006081  0.005874  0.205996  0.027679         0.502533\n",
       "  28  0.006621  0.007887  0.007332  0.371407  0.029692         0.494683\n",
       "  29  0.007391  0.007242  0.008003  0.167392  0.030679         0.484468,\n",
       "  0.007805756528214729),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  26  0.004902  0.004536  0.005290  0.085926  0.028046         0.484307\n",
       "  27  0.005321  0.006081  0.005874  0.205996  0.027679         0.502533\n",
       "  28  0.006621  0.007887  0.007332  0.371407  0.029692         0.494683\n",
       "  29  0.007391  0.007242  0.008003  0.167392  0.030679         0.484468\n",
       "  30  0.007607  0.007702  0.007979  0.184032  0.030333         0.492461,\n",
       "  0.00840011060328132),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  27  0.005321  0.006081  0.005874  0.205996  0.027679         0.502533\n",
       "  28  0.006621  0.007887  0.007332  0.371407  0.029692         0.494683\n",
       "  29  0.007391  0.007242  0.008003  0.167392  0.030679         0.484468\n",
       "  30  0.007607  0.007702  0.007979  0.184032  0.030333         0.492461\n",
       "  31  0.008678  0.008325  0.009221  0.158441  0.031030         0.491565,\n",
       "  0.009193387774349904),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  28  0.006621  0.007887  0.007332  0.371407  0.029692         0.494683\n",
       "  29  0.007391  0.007242  0.008003  0.167392  0.030679         0.484468\n",
       "  30  0.007607  0.007702  0.007979  0.184032  0.030333         0.492461\n",
       "  31  0.008678  0.008325  0.009221  0.158441  0.031030         0.491565\n",
       "  32  0.008536  0.008719  0.008788  0.182186  0.031611         0.493052,\n",
       "  0.00888423089574263),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  29  0.007391  0.007242  0.008003  0.167392  0.030679         0.484468\n",
       "  30  0.007607  0.007702  0.007979  0.184032  0.030333         0.492461\n",
       "  31  0.008678  0.008325  0.009221  0.158441  0.031030         0.491565\n",
       "  32  0.008536  0.008719  0.008788  0.182186  0.031611         0.493052\n",
       "  33  0.009232  0.008700  0.009698  0.162807  0.032386         0.484809,\n",
       "  0.009308423115747085),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  30  0.007607  0.007702  0.007979  0.184032  0.030333         0.492461\n",
       "  31  0.008678  0.008325  0.009221  0.158441  0.031030         0.491565\n",
       "  32  0.008536  0.008719  0.008788  0.182186  0.031611         0.493052\n",
       "  33  0.009232  0.008700  0.009698  0.162807  0.032386         0.484809\n",
       "  34  0.009080  0.009038  0.009832  0.171739  0.032084         0.490293,\n",
       "  0.009040007319153668),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  31  0.008678  0.008325  0.009221  0.158441  0.031030         0.491565\n",
       "  32  0.008536  0.008719  0.008788  0.182186  0.031611         0.493052\n",
       "  33  0.009232  0.008700  0.009698  0.162807  0.032386         0.484809\n",
       "  34  0.009080  0.009038  0.009832  0.171739  0.032084         0.490293\n",
       "  35  0.009513  0.008991  0.009945  0.134663  0.032498         0.485113,\n",
       "  0.008448049212301873),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  32  0.008536  0.008719  0.008788  0.182186  0.031611         0.493052\n",
       "  33  0.009232  0.008700  0.009698  0.162807  0.032386         0.484809\n",
       "  34  0.009080  0.009038  0.009832  0.171739  0.032084         0.490293\n",
       "  35  0.009513  0.008991  0.009945  0.134663  0.032498         0.485113\n",
       "  36  0.009099  0.008797  0.009178  0.127390  0.032236         0.482694,\n",
       "  0.008960921123340748),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  33  0.009232  0.008700  0.009698  0.162807  0.032386         0.484809\n",
       "  34  0.009080  0.009038  0.009832  0.171739  0.032084         0.490293\n",
       "  35  0.009513  0.008991  0.009945  0.134663  0.032498         0.485113\n",
       "  36  0.009099  0.008797  0.009178  0.127390  0.032236         0.482694\n",
       "  37  0.008483  0.008500  0.009069  0.141918  0.031658         0.490956,\n",
       "  0.009799731312852788),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  34  0.009080  0.009038  0.009832  0.171739  0.032084         0.490293\n",
       "  35  0.009513  0.008991  0.009945  0.134663  0.032498         0.485113\n",
       "  36  0.009099  0.008797  0.009178  0.127390  0.032236         0.482694\n",
       "  37  0.008483  0.008500  0.009069  0.141918  0.031658         0.490956\n",
       "  38  0.010620  0.009908  0.010687  0.240535  0.032159         0.493393,\n",
       "  0.010063355173016614),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  35  0.009513  0.008991  0.009945  0.134663  0.032498         0.485113\n",
       "  36  0.009099  0.008797  0.009178  0.127390  0.032236         0.482694\n",
       "  37  0.008483  0.008500  0.009069  0.141918  0.031658         0.490956\n",
       "  38  0.010620  0.009908  0.010687  0.240535  0.032159         0.493393\n",
       "  39  0.010064  0.009631  0.010318  0.127025  0.032978         0.489092,\n",
       "  0.010569039179411103),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  36  0.009099  0.008797  0.009178  0.127390  0.032236         0.482694\n",
       "  37  0.008483  0.008500  0.009069  0.141918  0.031658         0.490956\n",
       "  38  0.010620  0.009908  0.010687  0.240535  0.032159         0.493393\n",
       "  39  0.010064  0.009631  0.010318  0.127025  0.032978         0.489092\n",
       "  40  0.011008  0.010370  0.011140  0.160535  0.033235         0.490902,\n",
       "  0.011779330288202067),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  37  0.008483  0.008500  0.009069  0.141918  0.031658         0.490956\n",
       "  38  0.010620  0.009908  0.010687  0.240535  0.032159         0.493393\n",
       "  39  0.010064  0.009631  0.010318  0.127025  0.032978         0.489092\n",
       "  40  0.011008  0.010370  0.011140  0.160535  0.033235         0.490902\n",
       "  41  0.010591  0.011247  0.010960  0.170780  0.033729         0.496171,\n",
       "  0.011486945187098366),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  38  0.010620  0.009908  0.010687  0.240535  0.032159         0.493393\n",
       "  39  0.010064  0.009631  0.010318  0.127025  0.032978         0.489092\n",
       "  40  0.011008  0.010370  0.011140  0.160535  0.033235         0.490902\n",
       "  41  0.010591  0.011247  0.010960  0.170780  0.033729         0.496171\n",
       "  42  0.012348  0.012005  0.012448  0.220371  0.034911         0.484934,\n",
       "  0.009701467748959188),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  39  0.010064  0.009631  0.010318  0.127025  0.032978         0.489092\n",
       "  40  0.011008  0.010370  0.011140  0.160535  0.033235         0.490902\n",
       "  41  0.010591  0.011247  0.010960  0.170780  0.033729         0.496171\n",
       "  42  0.012348  0.012005  0.012448  0.220371  0.034911         0.484934\n",
       "  43  0.011754  0.011197  0.010570  0.276526  0.034625         0.473769,\n",
       "  0.011832051211289909),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  40  0.011008  0.010370  0.011140  0.160535  0.033235         0.490902\n",
       "  41  0.010591  0.011247  0.010960  0.170780  0.033729         0.496171\n",
       "  42  0.012348  0.012005  0.012448  0.220371  0.034911         0.484934\n",
       "  43  0.011754  0.011197  0.010570  0.276526  0.034625         0.473769\n",
       "  44  0.010880  0.011467  0.011060  0.354770  0.032882         0.503052,\n",
       "  0.017356239790192473),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  41  0.010591  0.011247  0.010960  0.170780  0.033729         0.496171\n",
       "  42  0.012348  0.012005  0.012448  0.220371  0.034911         0.484934\n",
       "  43  0.011754  0.011197  0.010570  0.276526  0.034625         0.473769\n",
       "  44  0.010880  0.011467  0.011060  0.354770  0.032882         0.503052\n",
       "  45  0.017171  0.018586  0.016500  0.897239  0.034963         0.528429,\n",
       "  0.020943956821612108),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  42  0.012348  0.012005  0.012448  0.220371  0.034911         0.484934\n",
       "  43  0.011754  0.011197  0.010570  0.276526  0.034625         0.473769\n",
       "  44  0.010880  0.011467  0.011060  0.354770  0.032882         0.503052\n",
       "  45  0.017171  0.018586  0.016500  0.897239  0.034963         0.528429\n",
       "  46  0.018581  0.021966  0.018551  0.796839  0.040357         0.513948,\n",
       "  0.019601858593920374),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  43  0.011754  0.011197  0.010570  0.276526  0.034625         0.473769\n",
       "  44  0.010880  0.011467  0.011060  0.354770  0.032882         0.503052\n",
       "  45  0.017171  0.018586  0.016500  0.897239  0.034963         0.528429\n",
       "  46  0.018581  0.021966  0.018551  0.796839  0.040357         0.513948\n",
       "  47  0.020974  0.021542  0.020356  0.542487  0.043861         0.477085,\n",
       "  0.02060124676563536),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  44  0.010880  0.011467  0.011060  0.354770  0.032882         0.503052\n",
       "  45  0.017171  0.018586  0.016500  0.897239  0.034963         0.528429\n",
       "  46  0.018581  0.021966  0.018551  0.796839  0.040357         0.513948\n",
       "  47  0.020974  0.021542  0.020356  0.542487  0.043861         0.477085\n",
       "  48  0.020102  0.020802  0.020785  0.324779  0.042550         0.494594,\n",
       "  0.022357953340472355),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  45  0.017171  0.018586  0.016500  0.897239  0.034963         0.528429\n",
       "  46  0.018581  0.021966  0.018551  0.796839  0.040357         0.513948\n",
       "  47  0.020974  0.021542  0.020356  0.542487  0.043861         0.477085\n",
       "  48  0.020102  0.020802  0.020785  0.324779  0.042550         0.494594\n",
       "  49  0.021055  0.021956  0.021712  0.361028  0.043526         0.500257,\n",
       "  0.02172046221517712),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  46  0.018581  0.021966  0.018551  0.796839  0.040357         0.513948\n",
       "  47  0.020974  0.021542  0.020356  0.542487  0.043861         0.477085\n",
       "  48  0.020102  0.020802  0.020785  0.324779  0.042550         0.494594\n",
       "  49  0.021055  0.021956  0.021712  0.361028  0.043526         0.500257\n",
       "  50  0.023994  0.023274  0.022923  0.514647  0.045242         0.482354,\n",
       "  0.02301222586563349),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  47  0.020974  0.021542  0.020356  0.542487  0.043861         0.477085\n",
       "  48  0.020102  0.020802  0.020785  0.324779  0.042550         0.494594\n",
       "  49  0.021055  0.021956  0.021712  0.361028  0.043526         0.500257\n",
       "  50  0.023994  0.023274  0.022923  0.514647  0.045242         0.482354\n",
       "  51  0.022709  0.022734  0.023086  0.297255  0.044619         0.496780,\n",
       "  0.022734226196180877),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  48  0.020102  0.020802  0.020785  0.324779  0.042550         0.494594\n",
       "  49  0.021055  0.021956  0.021712  0.361028  0.043526         0.500257\n",
       "  50  0.023994  0.023274  0.022923  0.514647  0.045242         0.482354\n",
       "  51  0.022709  0.022734  0.023086  0.297255  0.044619         0.496780\n",
       "  52  0.023967  0.023108  0.023587  0.275884  0.045881         0.485042,\n",
       "  0.02196730467547506),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  49  0.021055  0.021956  0.021712  0.361028  0.043526         0.500257\n",
       "  50  0.023994  0.023274  0.022923  0.514647  0.045242         0.482354\n",
       "  51  0.022709  0.022734  0.023086  0.297255  0.044619         0.496780\n",
       "  52  0.023967  0.023108  0.023587  0.275884  0.045881         0.485042\n",
       "  53  0.023823  0.023665  0.022960  0.337724  0.045610         0.481386,\n",
       "  0.020296872201095367),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  50  0.023994  0.023274  0.022923  0.514647  0.045242         0.482354\n",
       "  51  0.022709  0.022734  0.023086  0.297255  0.044619         0.496780\n",
       "  52  0.023967  0.023108  0.023587  0.275884  0.045881         0.485042\n",
       "  53  0.023823  0.023665  0.022960  0.337724  0.045610         0.481386\n",
       "  54  0.021479  0.021011  0.021167  0.350394  0.044861         0.474630,\n",
       "  0.016618079510426518),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  51  0.022709  0.022734  0.023086  0.297255  0.044619         0.496780\n",
       "  52  0.023967  0.023108  0.023587  0.275884  0.045881         0.485042\n",
       "  53  0.023823  0.023665  0.022960  0.337724  0.045610         0.481386\n",
       "  54  0.021479  0.021011  0.021167  0.350394  0.044861         0.474630\n",
       "  55  0.019924  0.019091  0.017583  0.482311  0.043229         0.459612,\n",
       "  0.01738499140877003),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  52  0.023967  0.023108  0.023587  0.275884  0.045881         0.485042\n",
       "  53  0.023823  0.023665  0.022960  0.337724  0.045610         0.481386\n",
       "  54  0.021479  0.021011  0.021167  0.350394  0.044861         0.474630\n",
       "  55  0.019924  0.019091  0.017583  0.482311  0.043229         0.459612\n",
       "  56  0.017265  0.017465  0.017788  0.272126  0.039637         0.492855,\n",
       "  0.016462303087015486),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  53  0.023823  0.023665  0.022960  0.337724  0.045610         0.481386\n",
       "  54  0.021479  0.021011  0.021167  0.350394  0.044861         0.474630\n",
       "  55  0.019924  0.019091  0.017583  0.482311  0.043229         0.459612\n",
       "  56  0.017265  0.017465  0.017788  0.272126  0.039637         0.492855\n",
       "  57  0.018028  0.017409  0.016788  0.269022  0.040385         0.480221,\n",
       "  0.01626098402279869),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  54  0.021479  0.021011  0.021167  0.350394  0.044861         0.474630\n",
       "  55  0.019924  0.019091  0.017583  0.482311  0.043229         0.459612\n",
       "  56  0.017265  0.017465  0.017788  0.272126  0.039637         0.492855\n",
       "  57  0.018028  0.017409  0.016788  0.269022  0.040385         0.480221\n",
       "  58  0.017202  0.016773  0.017045  0.258802  0.039484         0.485615,\n",
       "  0.019894243695024075),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  55  0.019924  0.019091  0.017583  0.482311  0.043229         0.459612\n",
       "  56  0.017265  0.017465  0.017788  0.272126  0.039637         0.492855\n",
       "  57  0.018028  0.017409  0.016788  0.269022  0.040385         0.480221\n",
       "  58  0.017202  0.016773  0.017045  0.258802  0.039484         0.485615\n",
       "  59  0.016832  0.019435  0.017345  0.364402  0.039288         0.514289,\n",
       "  0.019649787580578625),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  56  0.017265  0.017465  0.017788  0.272126  0.039637         0.492855\n",
       "  57  0.018028  0.017409  0.016788  0.269022  0.040385         0.480221\n",
       "  58  0.017202  0.016773  0.017045  0.258802  0.039484         0.485615\n",
       "  59  0.016832  0.019435  0.017345  0.364402  0.039288         0.514289\n",
       "  60  0.020706  0.020868  0.019726  0.407225  0.042836         0.485293,\n",
       "  0.020337613283109224),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  57  0.018028  0.017409  0.016788  0.269022  0.040385         0.480221\n",
       "  58  0.017202  0.016773  0.017045  0.258802  0.039484         0.485615\n",
       "  59  0.016832  0.019435  0.017345  0.364402  0.039288         0.514289\n",
       "  60  0.020706  0.020868  0.019726  0.407225  0.042836         0.485293\n",
       "  61  0.019556  0.020518  0.020053  0.289388  0.042597         0.492264,\n",
       "  0.017382595440555242),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  58  0.017202  0.016773  0.017045  0.258802  0.039484         0.485615\n",
       "  59  0.016832  0.019435  0.017345  0.364402  0.039288         0.514289\n",
       "  60  0.020706  0.020868  0.019726  0.407225  0.042836         0.485293\n",
       "  61  0.019556  0.020518  0.020053  0.289388  0.042597         0.492264\n",
       "  62  0.018846  0.018420  0.018135  0.508685  0.043269         0.465024,\n",
       "  0.017373011567696047),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  59  0.016832  0.019435  0.017345  0.364402  0.039288         0.514289\n",
       "  60  0.020706  0.020868  0.019726  0.407225  0.042836         0.485293\n",
       "  61  0.019556  0.020518  0.020053  0.289388  0.042597         0.492264\n",
       "  62  0.018846  0.018420  0.018135  0.508685  0.043269         0.465024\n",
       "  63  0.016805  0.017954  0.017692  0.440957  0.040383         0.487049,\n",
       "  0.016184293795200564),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  60  0.020706  0.020868  0.019726  0.407225  0.042836         0.485293\n",
       "  61  0.019556  0.020518  0.020053  0.289388  0.042597         0.492264\n",
       "  62  0.018846  0.018420  0.018135  0.508685  0.043269         0.465024\n",
       "  63  0.016805  0.017954  0.017692  0.440957  0.040383         0.487049\n",
       "  64  0.017111  0.017224  0.016899  0.404392  0.040374         0.478232,\n",
       "  0.016630059351500515),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  61  0.019556  0.020518  0.020053  0.289388  0.042597         0.492264\n",
       "  62  0.018846  0.018420  0.018135  0.508685  0.043269         0.465024\n",
       "  63  0.016805  0.017954  0.017692  0.440957  0.040383         0.487049\n",
       "  64  0.017111  0.017224  0.016899  0.404392  0.040374         0.478232\n",
       "  65  0.016825  0.016172  0.017091  0.213203  0.039213         0.490454,\n",
       "  0.015599523592993162),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  62  0.018846  0.018420  0.018135  0.508685  0.043269         0.465024\n",
       "  63  0.016805  0.017954  0.017692  0.440957  0.040383         0.487049\n",
       "  64  0.017111  0.017224  0.016899  0.404392  0.040374         0.478232\n",
       "  65  0.016825  0.016172  0.017091  0.213203  0.039213         0.490454\n",
       "  66  0.015710  0.016058  0.015829  0.300740  0.039648         0.479415,\n",
       "  0.016179501858770973),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  63  0.016805  0.017954  0.017692  0.440957  0.040383         0.487049\n",
       "  64  0.017111  0.017224  0.016899  0.404392  0.040374         0.478232\n",
       "  65  0.016825  0.016172  0.017091  0.213203  0.039213         0.490454\n",
       "  66  0.015710  0.016058  0.015829  0.300740  0.039648         0.479415\n",
       "  67  0.016553  0.016373  0.017086  0.301837  0.038642         0.491457,\n",
       "  0.017914644719674803),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  64  0.017111  0.017224  0.016899  0.404392  0.040374         0.478232\n",
       "  65  0.016825  0.016172  0.017091  0.213203  0.039213         0.490454\n",
       "  66  0.015710  0.016058  0.015829  0.300740  0.039648         0.479415\n",
       "  67  0.016553  0.016373  0.017086  0.301837  0.038642         0.491457\n",
       "  68  0.018201  0.017885  0.018542  0.371589  0.039208         0.500095,\n",
       "  0.019024276296357372),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  65  0.016825  0.016172  0.017091  0.213203  0.039213         0.490454\n",
       "  66  0.015710  0.016058  0.015829  0.300740  0.039648         0.479415\n",
       "  67  0.016553  0.016373  0.017086  0.301837  0.038642         0.491457\n",
       "  68  0.018201  0.017885  0.018542  0.371589  0.039208         0.500095\n",
       "  69  0.018437  0.018553  0.019222  0.157522  0.040903         0.495418,\n",
       "  0.01942211286599906),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  66  0.015710  0.016058  0.015829  0.300740  0.039648         0.479415\n",
       "  67  0.016553  0.016373  0.017086  0.301837  0.038642         0.491457\n",
       "  68  0.018201  0.017885  0.018542  0.371589  0.039208         0.500095\n",
       "  69  0.018437  0.018553  0.019222  0.157522  0.040903         0.495418\n",
       "  70  0.019534  0.019245  0.019753  0.259351  0.041986         0.490095,\n",
       "  0.019644995644149035),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  67  0.016553  0.016373  0.017086  0.301837  0.038642         0.491457\n",
       "  68  0.018201  0.017885  0.018542  0.371589  0.039208         0.500095\n",
       "  69  0.018437  0.018553  0.019222  0.157522  0.040903         0.495418\n",
       "  70  0.019534  0.019245  0.019753  0.259351  0.041986         0.490095\n",
       "  71  0.019619  0.019257  0.020416  0.187194  0.042375         0.488787,\n",
       "  0.019160884974050028),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  68  0.018201  0.017885  0.018542  0.371589  0.039208         0.500095\n",
       "  69  0.018437  0.018553  0.019222  0.157522  0.040903         0.495418\n",
       "  70  0.019534  0.019245  0.019753  0.259351  0.041986         0.490095\n",
       "  71  0.019619  0.019257  0.020416  0.187194  0.042375         0.488787\n",
       "  72  0.019917  0.019139  0.020247  0.191185  0.042593         0.483501,\n",
       "  0.01902667226457216),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  69  0.018437  0.018553  0.019222  0.157522  0.040903         0.495418\n",
       "  70  0.019534  0.019245  0.019753  0.259351  0.041986         0.490095\n",
       "  71  0.019619  0.019257  0.020416  0.187194  0.042375         0.488787\n",
       "  72  0.019917  0.019139  0.020247  0.191185  0.042593         0.483501\n",
       "  73  0.019424  0.018904  0.020005  0.152190  0.042120         0.486117,\n",
       "  0.019266336442588027),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  70  0.019534  0.019245  0.019753  0.259351  0.041986         0.490095\n",
       "  71  0.019619  0.019257  0.020416  0.187194  0.042375         0.488787\n",
       "  72  0.019917  0.019139  0.020247  0.191185  0.042593         0.483501\n",
       "  73  0.019424  0.018904  0.020005  0.152190  0.042120         0.486117\n",
       "  74  0.019436  0.018797  0.019775  0.142663  0.041989         0.488913,\n",
       "  0.018281324080161827),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  71  0.019619  0.019257  0.020416  0.187194  0.042375         0.488787\n",
       "  72  0.019917  0.019139  0.020247  0.191185  0.042593         0.483501\n",
       "  73  0.019424  0.018904  0.020005  0.152190  0.042120         0.486117\n",
       "  74  0.019436  0.018797  0.019775  0.142663  0.041989         0.488913\n",
       "  75  0.019238  0.018712  0.019392  0.152022  0.042223         0.479755,\n",
       "  0.017116575612176613),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  72  0.019917  0.019139  0.020247  0.191185  0.042593         0.483501\n",
       "  73  0.019424  0.018904  0.020005  0.152190  0.042120         0.486117\n",
       "  74  0.019436  0.018797  0.019775  0.142663  0.041989         0.488913\n",
       "  75  0.019238  0.018712  0.019392  0.152022  0.042223         0.479755\n",
       "  76  0.018485  0.017646  0.018067  0.167027  0.041261         0.478411,\n",
       "  0.016769063997407966),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  73  0.019424  0.018904  0.020005  0.152190  0.042120         0.486117\n",
       "  74  0.019436  0.018797  0.019775  0.142663  0.041989         0.488913\n",
       "  75  0.019238  0.018712  0.019392  0.152022  0.042223         0.479755\n",
       "  76  0.018485  0.017646  0.018067  0.167027  0.041261         0.478411\n",
       "  77  0.017125  0.017048  0.017626  0.183346  0.040123         0.484522,\n",
       "  0.017595894345846023),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  74  0.019436  0.018797  0.019775  0.142663  0.041989         0.488913\n",
       "  75  0.019238  0.018712  0.019392  0.152022  0.042223         0.479755\n",
       "  76  0.018485  0.017646  0.018067  0.167027  0.041261         0.478411\n",
       "  77  0.017125  0.017048  0.017626  0.183346  0.040123         0.484522\n",
       "  78  0.017101  0.017006  0.017563  0.186075  0.039784         0.493303,\n",
       "  0.017169296535264455),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  75  0.019238  0.018712  0.019392  0.152022  0.042223         0.479755\n",
       "  76  0.018485  0.017646  0.018067  0.167027  0.041261         0.478411\n",
       "  77  0.017125  0.017048  0.017626  0.183346  0.040123         0.484522\n",
       "  78  0.017101  0.017006  0.017563  0.186075  0.039784         0.493303\n",
       "  79  0.017867  0.017333  0.018246  0.104913  0.040591         0.483931,\n",
       "  0.01688170337059035),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  76  0.018485  0.017646  0.018067  0.167027  0.041261         0.478411\n",
       "  77  0.017125  0.017048  0.017626  0.183346  0.040123         0.484522\n",
       "  78  0.017101  0.017006  0.017563  0.186075  0.039784         0.493303\n",
       "  79  0.017867  0.017333  0.018246  0.104913  0.040591         0.483931\n",
       "  80  0.017563  0.016930  0.017800  0.117109  0.040175         0.484970,\n",
       "  0.018856510409510033),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  77  0.017125  0.017048  0.017626  0.183346  0.040123         0.484522\n",
       "  78  0.017101  0.017006  0.017563  0.186075  0.039784         0.493303\n",
       "  79  0.017867  0.017333  0.018246  0.104913  0.040591         0.483931\n",
       "  80  0.017563  0.016930  0.017800  0.117109  0.040175         0.484970\n",
       "  81  0.017282  0.018266  0.017837  0.269611  0.039894         0.501888,\n",
       "  0.019117738301459065),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  78  0.017101  0.017006  0.017563  0.186075  0.039784         0.493303\n",
       "  79  0.017867  0.017333  0.018246  0.104913  0.040591         0.483931\n",
       "  80  0.017563  0.016930  0.017800  0.117109  0.040175         0.484970\n",
       "  81  0.017282  0.018266  0.017837  0.269611  0.039894         0.501888\n",
       "  82  0.018964  0.018710  0.019547  0.278917  0.041823         0.489074,\n",
       "  0.018324461130390473),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  79  0.017867  0.017333  0.018246  0.104913  0.040591         0.483931\n",
       "  80  0.017563  0.016930  0.017800  0.117109  0.040175         0.484970\n",
       "  81  0.017282  0.018266  0.017837  0.269611  0.039894         0.501888\n",
       "  82  0.018964  0.018710  0.019547  0.278917  0.041823         0.489074\n",
       "  83  0.018714  0.018662  0.019375  0.208423  0.042078         0.481189,\n",
       "  0.019189636592627592),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  80  0.017563  0.016930  0.017800  0.117109  0.040175         0.484970\n",
       "  81  0.017282  0.018266  0.017837  0.269611  0.039894         0.501888\n",
       "  82  0.018964  0.018710  0.019547  0.278917  0.041823         0.489074\n",
       "  83  0.018714  0.018662  0.019375  0.208423  0.042078         0.481189\n",
       "  84  0.018668  0.018665  0.019520  0.179561  0.041303         0.493590,\n",
       "  0.020373562428693485),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  81  0.017282  0.018266  0.017837  0.269611  0.039894         0.501888\n",
       "  82  0.018964  0.018710  0.019547  0.278917  0.041823         0.489074\n",
       "  83  0.018714  0.018662  0.019375  0.208423  0.042078         0.481189\n",
       "  84  0.018668  0.018665  0.019520  0.179561  0.041303         0.493590\n",
       "  85  0.019929  0.020551  0.020809  0.239112  0.042148         0.495974,\n",
       "  0.0200691974865158),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  82  0.018964  0.018710  0.019547  0.278917  0.041823         0.489074\n",
       "  83  0.018714  0.018662  0.019375  0.208423  0.042078         0.481189\n",
       "  84  0.018668  0.018665  0.019520  0.179561  0.041303         0.493590\n",
       "  85  0.019929  0.020551  0.020809  0.239112  0.042148         0.495974\n",
       "  86  0.020966  0.020413  0.021179  0.134079  0.043304         0.484845,\n",
       "  0.020680332961448274),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  83  0.018714  0.018662  0.019375  0.208423  0.042078         0.481189\n",
       "  84  0.018668  0.018665  0.019520  0.179561  0.041303         0.493590\n",
       "  85  0.019929  0.020551  0.020809  0.239112  0.042148         0.495974\n",
       "  86  0.020966  0.020413  0.021179  0.134079  0.043304         0.484845\n",
       "  87  0.020386  0.020169  0.021085  0.094935  0.043007         0.491690,\n",
       "  0.02106378409943888),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  84  0.018668  0.018665  0.019520  0.179561  0.041303         0.493590\n",
       "  85  0.019929  0.020551  0.020809  0.239112  0.042148         0.495974\n",
       "  86  0.020966  0.020413  0.021179  0.134079  0.043304         0.484845\n",
       "  87  0.020386  0.020169  0.021085  0.094935  0.043007         0.491690\n",
       "  88  0.021241  0.020584  0.021809  0.087823  0.043604         0.489988,\n",
       "  0.022024827157354803),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  85  0.019929  0.020551  0.020809  0.239112  0.042148         0.495974\n",
       "  86  0.020966  0.020413  0.021179  0.134079  0.043304         0.484845\n",
       "  87  0.020386  0.020169  0.021085  0.094935  0.043007         0.491690\n",
       "  88  0.021241  0.020584  0.021809  0.087823  0.043604         0.489988\n",
       "  89  0.021650  0.021698  0.022560  0.148376  0.043978         0.494307,\n",
       "  0.022228542189786395),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  86  0.020966  0.020413  0.021179  0.134079  0.043304         0.484845\n",
       "  87  0.020386  0.020169  0.021085  0.094935  0.043007         0.491690\n",
       "  88  0.021241  0.020584  0.021809  0.087823  0.043604         0.489988\n",
       "  89  0.021650  0.021698  0.022560  0.148376  0.043978         0.494307\n",
       "  90  0.022362  0.021757  0.023023  0.100744  0.044917         0.488644,\n",
       "  0.02226209536715586),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  87  0.020386  0.020169  0.021085  0.094935  0.043007         0.491690\n",
       "  88  0.021241  0.020584  0.021809  0.087823  0.043604         0.489988\n",
       "  89  0.021650  0.021698  0.022560  0.148376  0.043978         0.494307\n",
       "  90  0.022362  0.021757  0.023023  0.100744  0.044917         0.488644\n",
       "  91  0.022283  0.021750  0.023209  0.065044  0.045116         0.487371,\n",
       "  0.02338849872134201),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  88  0.021241  0.020584  0.021809  0.087823  0.043604         0.489988\n",
       "  89  0.021650  0.021698  0.022560  0.148376  0.043978         0.494307\n",
       "  90  0.022362  0.021757  0.023023  0.100744  0.044917         0.488644\n",
       "  91  0.022283  0.021750  0.023209  0.065044  0.045116         0.487371\n",
       "  92  0.022569  0.022866  0.023226  0.143516  0.045148         0.495543,\n",
       "  0.02223573009443078),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  89  0.021650  0.021698  0.022560  0.148376  0.043978         0.494307\n",
       "  90  0.022362  0.021757  0.023023  0.100744  0.044917         0.488644\n",
       "  91  0.022283  0.021750  0.023209  0.065044  0.045116         0.487371\n",
       "  92  0.022569  0.022866  0.023226  0.143516  0.045148         0.495543\n",
       "  93  0.024076  0.023257  0.023398  0.186428  0.046248         0.478501,\n",
       "  0.024613165639421754),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  90  0.022362  0.021757  0.023023  0.100744  0.044917         0.488644\n",
       "  91  0.022283  0.021750  0.023209  0.065044  0.045116         0.487371\n",
       "  92  0.022569  0.022866  0.023226  0.143516  0.045148         0.495543\n",
       "  93  0.024076  0.023257  0.023398  0.186428  0.046248         0.478501\n",
       "  94  0.023635  0.024148  0.024101  0.385288  0.045123         0.504898,\n",
       "  0.022645546505146465),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  91  0.022283  0.021750  0.023209  0.065044  0.045116         0.487371\n",
       "  92  0.022569  0.022866  0.023226  0.143516  0.045148         0.495543\n",
       "  93  0.024076  0.023257  0.023398  0.186428  0.046248         0.478501\n",
       "  94  0.023635  0.024148  0.024101  0.385288  0.045123         0.504898\n",
       "  95  0.024598  0.023980  0.023621  0.334494  0.047444         0.472407,\n",
       "  0.02240828791770771),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  92  0.022569  0.022866  0.023226  0.143516  0.045148         0.495543\n",
       "  93  0.024076  0.023257  0.023398  0.186428  0.046248         0.478501\n",
       "  94  0.023635  0.024148  0.024101  0.385288  0.045123         0.504898\n",
       "  95  0.024598  0.023980  0.023621  0.334494  0.047444         0.472407\n",
       "  96  0.022685  0.022551  0.023318  0.200246  0.045523         0.485346,\n",
       "  0.021219570145212228),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  93  0.024076  0.023257  0.023398  0.186428  0.046248         0.478501\n",
       "  94  0.023635  0.024148  0.024101  0.385288  0.045123         0.504898\n",
       "  95  0.024598  0.023980  0.023621  0.334494  0.047444         0.472407\n",
       "  96  0.022685  0.022551  0.023318  0.200246  0.045523         0.485346\n",
       "  97  0.023077  0.022314  0.022226  0.252553  0.045291         0.478232,\n",
       "  0.022489770081735427),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  94  0.023635  0.024148  0.024101  0.385288  0.045123         0.504898\n",
       "  95  0.024598  0.023980  0.023621  0.334494  0.047444         0.472407\n",
       "  96  0.022685  0.022551  0.023318  0.200246  0.045523         0.485346\n",
       "  97  0.023077  0.022314  0.022226  0.252553  0.045291         0.478232\n",
       "  98  0.022008  0.021923  0.022483  0.234938  0.044130         0.496619,\n",
       "  0.022779759214624332),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  95  0.024598  0.023980  0.023621  0.334494  0.047444         0.472407\n",
       "  96  0.022685  0.022551  0.023318  0.200246  0.045523         0.485346\n",
       "  97  0.023077  0.022314  0.022226  0.252553  0.045291         0.478232\n",
       "  98  0.022008  0.021923  0.022483  0.234938  0.044130         0.496619\n",
       "  99  0.022937  0.022835  0.023221  0.183292  0.045371         0.489289,\n",
       "  0.022415475822352104),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  96   0.022685  0.022551  0.023318  0.200246  0.045523         0.485346\n",
       "  97   0.023077  0.022314  0.022226  0.252553  0.045291         0.478232\n",
       "  98   0.022008  0.021923  0.022483  0.234938  0.044130         0.496619\n",
       "  99   0.022937  0.022835  0.023221  0.183292  0.045371         0.489289\n",
       "  100  0.023207  0.022743  0.023548  0.169163  0.045654         0.484397,\n",
       "  0.022856449442222444),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  97   0.023077  0.022314  0.022226  0.252553  0.045291         0.478232\n",
       "  98   0.022008  0.021923  0.022483  0.234938  0.044130         0.496619\n",
       "  99   0.022937  0.022835  0.023221  0.183292  0.045371         0.489289\n",
       "  100  0.023207  0.022743  0.023548  0.169163  0.045654         0.484397\n",
       "  101  0.022896  0.022321  0.022899  0.198816  0.045298         0.490418,\n",
       "  0.02284446960114846),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  98   0.022008  0.021923  0.022483  0.234938  0.044130         0.496619\n",
       "  99   0.022937  0.022835  0.023221  0.183292  0.045371         0.489289\n",
       "  100  0.023207  0.022743  0.023548  0.169163  0.045654         0.484397\n",
       "  101  0.022896  0.022321  0.022899  0.198816  0.045298         0.490418\n",
       "  102  0.023149  0.022667  0.023759  0.166504  0.045729         0.487031,\n",
       "  0.023956497146045832),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  99   0.022937  0.022835  0.023221  0.183292  0.045371         0.489289\n",
       "  100  0.023207  0.022743  0.023548  0.169163  0.045654         0.484397\n",
       "  101  0.022896  0.022321  0.022899  0.198816  0.045298         0.490418\n",
       "  102  0.023149  0.022667  0.023759  0.166504  0.045729         0.487031\n",
       "  103  0.023298  0.023288  0.023778  0.234388  0.045717         0.495436,\n",
       "  0.024898362835881063),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  100  0.023207  0.022743  0.023548  0.169163  0.045654         0.484397\n",
       "  101  0.022896  0.022321  0.022899  0.198816  0.045298         0.490418\n",
       "  102  0.023149  0.022667  0.023759  0.166504  0.045729         0.487031\n",
       "  103  0.023298  0.023288  0.023778  0.234388  0.045717         0.495436\n",
       "  104  0.024494  0.024475  0.024876  0.320306  0.046803         0.494164,\n",
       "  0.023316600430173483),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  101  0.022896  0.022321  0.022899  0.198816  0.045298         0.490418\n",
       "  102  0.023149  0.022667  0.023759  0.166504  0.045729         0.487031\n",
       "  103  0.023298  0.023288  0.023778  0.234388  0.045717         0.495436\n",
       "  104  0.024494  0.024475  0.024876  0.320306  0.046803         0.494164\n",
       "  105  0.025380  0.024542  0.024403  0.273729  0.047723         0.475293,\n",
       "  0.022506541859239),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  102  0.023149  0.022667  0.023759  0.166504  0.045729         0.487031\n",
       "  103  0.023298  0.023288  0.023778  0.234388  0.045717         0.495436\n",
       "  104  0.024494  0.024475  0.024876  0.320306  0.046803         0.494164\n",
       "  105  0.025380  0.024542  0.024403  0.273729  0.047723         0.475293\n",
       "  106  0.022456  0.022397  0.023262  0.218853  0.046178         0.481063,\n",
       "  0.021154859758688086),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  103  0.023298  0.023288  0.023778  0.234388  0.045717         0.495436\n",
       "  104  0.024494  0.024475  0.024876  0.320306  0.046803         0.494164\n",
       "  105  0.025380  0.024542  0.024403  0.273729  0.047723         0.475293\n",
       "  106  0.022456  0.022397  0.023262  0.218853  0.046178         0.481063\n",
       "  107  0.022947  0.022186  0.022323  0.225099  0.045387         0.477013,\n",
       "  0.01934302667018614),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  104  0.024494  0.024475  0.024876  0.320306  0.046803         0.494164\n",
       "  105  0.025380  0.024542  0.024403  0.273729  0.047723         0.475293\n",
       "  106  0.022456  0.022397  0.023262  0.218853  0.046178         0.481063\n",
       "  107  0.022947  0.022186  0.022323  0.225099  0.045387         0.477013\n",
       "  108  0.021539  0.020757  0.020433  0.340983  0.044067         0.473572,\n",
       "  0.01848024717616382),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  105  0.025380  0.024542  0.024403  0.273729  0.047723         0.475293\n",
       "  106  0.022456  0.022397  0.023262  0.218853  0.046178         0.481063\n",
       "  107  0.022947  0.022186  0.022323  0.225099  0.045387         0.477013\n",
       "  108  0.021539  0.020757  0.020433  0.340983  0.044067         0.473572\n",
       "  109  0.019915  0.019077  0.019457  0.259171  0.042298         0.480669,\n",
       "  0.021384930441482455),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  106  0.022456  0.022397  0.023262  0.218853  0.046178         0.481063\n",
       "  107  0.022947  0.022186  0.022323  0.225099  0.045387         0.477013\n",
       "  108  0.021539  0.020757  0.020433  0.340983  0.044067         0.473572\n",
       "  109  0.019915  0.019077  0.019457  0.259171  0.042298         0.480669\n",
       "  110  0.019272  0.020776  0.020150  0.299273  0.041455         0.508841,\n",
       "  0.021106930772029842),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  107  0.022947  0.022186  0.022323  0.225099  0.045387         0.477013\n",
       "  108  0.021539  0.020757  0.020433  0.340983  0.044067         0.473572\n",
       "  109  0.019915  0.019077  0.019457  0.259171  0.042298         0.480669\n",
       "  110  0.019272  0.020776  0.020150  0.299273  0.041455         0.508841\n",
       "  111  0.021556  0.020646  0.021615  0.161105  0.044292         0.485042,\n",
       "  0.02164856392400859),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  108  0.021539  0.020757  0.020433  0.340983  0.044067         0.473572\n",
       "  109  0.019915  0.019077  0.019457  0.259171  0.042298         0.480669\n",
       "  110  0.019272  0.020776  0.020150  0.299273  0.041455         0.508841\n",
       "  111  0.021556  0.020646  0.021615  0.161105  0.044292         0.485042\n",
       "  112  0.021859  0.022030  0.021892  0.296848  0.044020         0.491171,\n",
       "  0.0229139719241022),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  109  0.019915  0.019077  0.019457  0.259171  0.042298         0.480669\n",
       "  110  0.019272  0.020776  0.020150  0.299273  0.041455         0.508841\n",
       "  111  0.021556  0.020646  0.021615  0.161105  0.044292         0.485042\n",
       "  112  0.021859  0.022030  0.021892  0.296848  0.044020         0.491171\n",
       "  113  0.022742  0.022423  0.023195  0.233328  0.044549         0.496583,\n",
       "  0.02202243118914),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  110  0.019272  0.020776  0.020150  0.299273  0.041455         0.508841\n",
       "  111  0.021556  0.020646  0.021615  0.161105  0.044292         0.485042\n",
       "  112  0.021859  0.022030  0.021892  0.296848  0.044020         0.491171\n",
       "  113  0.022742  0.022423  0.023195  0.233328  0.044549         0.496583\n",
       "  114  0.022908  0.022494  0.022931  0.458132  0.045785         0.480454,\n",
       "  0.025392057378839258),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  111  0.021556  0.020646  0.021615  0.161105  0.044292         0.485042\n",
       "  112  0.021859  0.022030  0.021892  0.296848  0.044020         0.491171\n",
       "  113  0.022742  0.022423  0.023195  0.233328  0.044549         0.496583\n",
       "  114  0.022908  0.022494  0.022931  0.458132  0.045785         0.480454\n",
       "  115  0.028003  0.027267  0.026087  0.797693  0.044914         0.512317,\n",
       "  0.026566399342045955),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  112  0.021859  0.022030  0.021892  0.296848  0.044020         0.491171\n",
       "  113  0.022742  0.022423  0.023195  0.233328  0.044549         0.496583\n",
       "  114  0.022908  0.022494  0.022931  0.458132  0.045785         0.480454\n",
       "  115  0.028003  0.027267  0.026087  0.797693  0.044914         0.512317\n",
       "  116  0.025703  0.026454  0.026607  0.315818  0.048205         0.495902,\n",
       "  0.025008606240848653),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  113  0.022742  0.022423  0.023195  0.233328  0.044549         0.496583\n",
       "  114  0.022908  0.022494  0.022931  0.458132  0.045785         0.480454\n",
       "  115  0.028003  0.027267  0.026087  0.797693  0.044914         0.512317\n",
       "  116  0.025703  0.026454  0.026607  0.315818  0.048205         0.495902\n",
       "  117  0.025818  0.025122  0.025830  0.360359  0.049352         0.475472,\n",
       "  0.02301222586563349),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  114  0.022908  0.022494  0.022931  0.458132  0.045785         0.480454\n",
       "  115  0.028003  0.027267  0.026087  0.797693  0.044914         0.512317\n",
       "  116  0.025703  0.026454  0.026607  0.315818  0.048205         0.495902\n",
       "  117  0.025818  0.025122  0.025830  0.360359  0.049352         0.475472\n",
       "  118  0.025527  0.024802  0.024113  0.315144  0.047831         0.472192,\n",
       "  0.023637746772217052),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  115  0.028003  0.027267  0.026087  0.797693  0.044914         0.512317\n",
       "  116  0.025703  0.026454  0.026607  0.315818  0.048205         0.495902\n",
       "  117  0.025818  0.025122  0.025830  0.360359  0.049352         0.475472\n",
       "  118  0.025527  0.024802  0.024113  0.315144  0.047831         0.472192\n",
       "  119  0.023529  0.023290  0.023875  0.279136  0.045881         0.491798,\n",
       "  0.021945740961541883),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  116  0.025703  0.026454  0.026607  0.315818  0.048205         0.495902\n",
       "  117  0.025818  0.025122  0.025830  0.360359  0.049352         0.475472\n",
       "  118  0.025527  0.024802  0.024113  0.315144  0.047831         0.472192\n",
       "  119  0.023529  0.023290  0.023875  0.279136  0.045881         0.491798\n",
       "  120  0.024444  0.023665  0.022647  0.417572  0.046492         0.474468,\n",
       "  0.02108296146751956),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  117  0.025818  0.025122  0.025830  0.360359  0.049352         0.475472\n",
       "  118  0.025527  0.024802  0.024113  0.315144  0.047831         0.472192\n",
       "  119  0.023529  0.023290  0.023875  0.279136  0.045881         0.491798\n",
       "  120  0.024444  0.023665  0.022647  0.417572  0.046492         0.474468\n",
       "  121  0.022328  0.021440  0.021628  0.461627  0.044839         0.480669,\n",
       "  0.020943956821612108),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  118  0.025527  0.024802  0.024113  0.315144  0.047831         0.472192\n",
       "  119  0.023529  0.023290  0.023875  0.279136  0.045881         0.491798\n",
       "  120  0.024444  0.023665  0.022647  0.417572  0.046492         0.474468\n",
       "  121  0.022328  0.021440  0.021628  0.461627  0.044839         0.480669\n",
       "  122  0.021051  0.021466  0.021826  0.318929  0.043997         0.486081,\n",
       "  0.02228365908108904),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  119  0.023529  0.023290  0.023875  0.279136  0.045881         0.491798\n",
       "  120  0.024444  0.023665  0.022647  0.417572  0.046492         0.474468\n",
       "  121  0.022328  0.021440  0.021628  0.461627  0.044839         0.480669\n",
       "  122  0.021051  0.021466  0.021826  0.318929  0.043997         0.486081\n",
       "  123  0.020134  0.021646  0.020598  0.937866  0.043861         0.497138,\n",
       "  0.022820500296638183),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  120  0.024444  0.023665  0.022647  0.417572  0.046492         0.474468\n",
       "  121  0.022328  0.021440  0.021628  0.461627  0.044839         0.480669\n",
       "  122  0.021051  0.021466  0.021826  0.318929  0.043997         0.486081\n",
       "  123  0.020134  0.021646  0.020598  0.937866  0.043861         0.497138\n",
       "  124  0.022721  0.023248  0.023524  0.627028  0.045169         0.491135,\n",
       "  0.023582620258552105),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  121  0.022328  0.021440  0.021628  0.461627  0.044839         0.480669\n",
       "  122  0.021051  0.021466  0.021826  0.318929  0.043997         0.486081\n",
       "  123  0.020134  0.021646  0.020598  0.937866  0.043861         0.497138\n",
       "  124  0.022721  0.023248  0.023524  0.627028  0.045169         0.491135\n",
       "  125  0.022985  0.023127  0.023820  0.402025  0.045694         0.492819,\n",
       "  0.02346039701251054),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  122  0.021051  0.021466  0.021826  0.318929  0.043997         0.486081\n",
       "  123  0.020134  0.021646  0.020598  0.937866  0.043861         0.497138\n",
       "  124  0.022721  0.023248  0.023524  0.627028  0.045169         0.491135\n",
       "  125  0.022985  0.023127  0.023820  0.402025  0.045694         0.492819\n",
       "  126  0.023739  0.023226  0.024428  0.253217  0.046438         0.486207,\n",
       "  0.023472376853584522),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  123  0.020134  0.021646  0.020598  0.937866  0.043861         0.497138\n",
       "  124  0.022721  0.023248  0.023524  0.627028  0.045169         0.491135\n",
       "  125  0.022985  0.023127  0.023820  0.402025  0.045694         0.492819\n",
       "  126  0.023739  0.023226  0.024428  0.253217  0.046438         0.486207\n",
       "  127  0.023902  0.023011  0.024391  0.206310  0.046319         0.487210,\n",
       "  0.021895406384306534),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  124  0.022721  0.023248  0.023524  0.627028  0.045169         0.491135\n",
       "  125  0.022985  0.023127  0.023820  0.402025  0.045694         0.492819\n",
       "  126  0.023739  0.023226  0.024428  0.253217  0.046438         0.486207\n",
       "  127  0.023902  0.023011  0.024391  0.206310  0.046319         0.487210\n",
       "  128  0.023419  0.023025  0.022873  0.327873  0.046330         0.475328,\n",
       "  0.02251373938624571),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  125  0.022985  0.023127  0.023820  0.402025  0.045694         0.492819\n",
       "  126  0.023739  0.023226  0.024428  0.253217  0.046438         0.486207\n",
       "  127  0.023902  0.023011  0.024391  0.206310  0.046319         0.487210\n",
       "  128  0.023419  0.023025  0.022873  0.327873  0.046330         0.475328\n",
       "  129  0.022649  0.021978  0.022453  0.379008  0.044790         0.491744,\n",
       "  0.021301052309239937),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  126  0.023739  0.023226  0.024428  0.253217  0.046438         0.486207\n",
       "  127  0.023902  0.023011  0.024391  0.206310  0.046319         0.487210\n",
       "  128  0.023419  0.023025  0.022873  0.327873  0.046330         0.475328\n",
       "  129  0.022649  0.021978  0.022453  0.379008  0.044790         0.491744\n",
       "  130  0.020259  0.020880  0.020896  0.627794  0.045394         0.478052,\n",
       "  0.020577277461125084),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  127  0.023902  0.023011  0.024391  0.206310  0.046319         0.487210\n",
       "  128  0.023419  0.023025  0.022873  0.327873  0.046330         0.475328\n",
       "  129  0.022649  0.021978  0.022453  0.379008  0.044790         0.491744\n",
       "  130  0.020259  0.020880  0.020896  0.627794  0.045394         0.478052\n",
       "  131  0.021650  0.020897  0.021690  0.242493  0.044210         0.481708,\n",
       "  0.02108535743573436),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  128  0.023419  0.023025  0.022873  0.327873  0.046330         0.475328\n",
       "  129  0.022649  0.021978  0.022453  0.379008  0.044790         0.491744\n",
       "  130  0.020259  0.020880  0.020896  0.627794  0.045394         0.478052\n",
       "  131  0.021650  0.020897  0.021690  0.242493  0.044210         0.481708\n",
       "  132  0.020892  0.020885  0.021773  0.190074  0.043503         0.490920,\n",
       "  0.020622810479568533),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  129  0.022649  0.021978  0.022453  0.379008  0.044790         0.491744\n",
       "  130  0.020259  0.020880  0.020896  0.627794  0.045394         0.478052\n",
       "  131  0.021650  0.020897  0.021690  0.242493  0.044210         0.481708\n",
       "  132  0.020892  0.020885  0.021773  0.190074  0.043503         0.490920\n",
       "  133  0.021683  0.020857  0.020840  0.226384  0.043999         0.483662,\n",
       "  0.020411907542492547),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  130  0.020259  0.020880  0.020896  0.627794  0.045394         0.478052\n",
       "  131  0.021650  0.020897  0.021690  0.242493  0.044210         0.481708\n",
       "  132  0.020892  0.020885  0.021773  0.190074  0.043503         0.490920\n",
       "  133  0.021683  0.020857  0.020840  0.226384  0.043999         0.483662\n",
       "  134  0.020880  0.020364  0.021412  0.177112  0.043548         0.485543,\n",
       "  0.020850485194148092),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  131  0.021650  0.020897  0.021690  0.242493  0.044210         0.481708\n",
       "  132  0.020892  0.020885  0.021773  0.190074  0.043503         0.490920\n",
       "  133  0.021683  0.020857  0.020840  0.226384  0.043999         0.483662\n",
       "  134  0.020880  0.020364  0.021412  0.177112  0.043548         0.485543\n",
       "  135  0.020923  0.020383  0.021400  0.184971  0.043342         0.490400,\n",
       "  0.02058446536576947),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  132  0.020892  0.020885  0.021773  0.190074  0.043503         0.490920\n",
       "  133  0.021683  0.020857  0.020840  0.226384  0.043999         0.483662\n",
       "  134  0.020880  0.020364  0.021412  0.177112  0.043548         0.485543\n",
       "  135  0.020923  0.020383  0.021400  0.184971  0.043342         0.490400\n",
       "  136  0.021060  0.020264  0.021584  0.164673  0.043770         0.485131,\n",
       "  0.021281874941159257),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  133  0.021683  0.020857  0.020840  0.226384  0.043999         0.483662\n",
       "  134  0.020880  0.020364  0.021412  0.177112  0.043548         0.485543\n",
       "  135  0.020923  0.020383  0.021400  0.184971  0.043342         0.490400\n",
       "  136  0.021060  0.020264  0.021584  0.164673  0.043770         0.485131\n",
       "  137  0.021320  0.020821  0.022059  0.210724  0.043510         0.492336,\n",
       "  0.020416699478922138),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  134  0.020880  0.020364  0.021412  0.177112  0.043548         0.485543\n",
       "  135  0.020923  0.020383  0.021400  0.184971  0.043342         0.490400\n",
       "  136  0.021060  0.020264  0.021584  0.164673  0.043770         0.485131\n",
       "  137  0.021320  0.020821  0.022059  0.210724  0.043510         0.492336\n",
       "  138  0.021638  0.020880  0.021560  0.195612  0.044191         0.480651,\n",
       "  0.019494011157167587),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  135  0.020923  0.020383  0.021400  0.184971  0.043342         0.490400\n",
       "  136  0.021060  0.020264  0.021584  0.164673  0.043770         0.485131\n",
       "  137  0.021320  0.020821  0.022059  0.210724  0.043510         0.492336\n",
       "  138  0.021638  0.020880  0.021560  0.195612  0.044191         0.480651\n",
       "  139  0.020461  0.019648  0.020395  0.276227  0.043346         0.480221,\n",
       "  0.019165676910479625),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  136  0.021060  0.020264  0.021584  0.164673  0.043770         0.485131\n",
       "  137  0.021320  0.020821  0.022059  0.210724  0.043510         0.492336\n",
       "  138  0.021638  0.020880  0.021560  0.195612  0.044191         0.480651\n",
       "  139  0.020461  0.019648  0.020395  0.276227  0.043346         0.480221\n",
       "  140  0.019691  0.018830  0.019726  0.266500  0.042445         0.484665,\n",
       "  0.018643211504219253),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  137  0.021320  0.020821  0.022059  0.210724  0.043510         0.492336\n",
       "  138  0.021638  0.020880  0.021560  0.195612  0.044191         0.480651\n",
       "  139  0.020461  0.019648  0.020395  0.276227  0.043346         0.480221\n",
       "  140  0.019691  0.018830  0.019726  0.266500  0.042445         0.484665\n",
       "  141  0.019554  0.018771  0.019666  0.195179  0.042124         0.483214,\n",
       "  0.017969771233339742),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  138  0.021638  0.020880  0.021560  0.195612  0.044191         0.480651\n",
       "  139  0.020461  0.019648  0.020395  0.276227  0.043346         0.480221\n",
       "  140  0.019691  0.018830  0.019726  0.266500  0.042445         0.484665\n",
       "  141  0.019554  0.018771  0.019666  0.195179  0.042124         0.483214\n",
       "  142  0.019046  0.018167  0.018556  0.271027  0.041614         0.482085,\n",
       "  0.018837342663791657),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  139  0.020461  0.019648  0.020395  0.276227  0.043346         0.480221\n",
       "  140  0.019691  0.018830  0.019726  0.266500  0.042445         0.484665\n",
       "  141  0.019554  0.018771  0.019666  0.195179  0.042124         0.483214\n",
       "  142  0.019046  0.018167  0.018556  0.271027  0.041614         0.482085\n",
       "  143  0.018317  0.018546  0.018953  0.253404  0.040957         0.493608,\n",
       "  0.018115963783891593),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  140  0.019691  0.018830  0.019726  0.266500  0.042445         0.484665\n",
       "  141  0.019554  0.018771  0.019666  0.195179  0.042124         0.483214\n",
       "  142  0.019046  0.018167  0.018556  0.271027  0.041614         0.482085\n",
       "  143  0.018317  0.018546  0.018953  0.253404  0.040957         0.493608\n",
       "  144  0.018654  0.018217  0.019147  0.172753  0.041804         0.481726,\n",
       "  0.019000306991847096),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  141  0.019554  0.018771  0.019666  0.195179  0.042124         0.483214\n",
       "  142  0.019046  0.018167  0.018556  0.271027  0.041614         0.482085\n",
       "  143  0.018317  0.018546  0.018953  0.253404  0.040957         0.493608\n",
       "  144  0.018654  0.018217  0.019147  0.172753  0.041804         0.481726\n",
       "  145  0.018757  0.018461  0.019339  0.200830  0.041099         0.493733,\n",
       "  0.019180052719768404),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  142  0.019046  0.018167  0.018556  0.271027  0.041614         0.482085\n",
       "  143  0.018317  0.018546  0.018953  0.253404  0.040957         0.493608\n",
       "  144  0.018654  0.018217  0.019147  0.172753  0.041804         0.481726\n",
       "  145  0.018757  0.018461  0.019339  0.200830  0.041099         0.493733\n",
       "  146  0.019161  0.018641  0.019947  0.172357  0.041963         0.488465,\n",
       "  0.0193813717839852),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  143  0.018317  0.018546  0.018953  0.253404  0.040957         0.493608\n",
       "  144  0.018654  0.018217  0.019147  0.172753  0.041804         0.481726\n",
       "  145  0.018757  0.018461  0.019339  0.200830  0.041099         0.493733\n",
       "  146  0.019161  0.018641  0.019947  0.172357  0.041963         0.488465\n",
       "  147  0.019272  0.019060  0.019690  0.181933  0.042139         0.488626,\n",
       "  0.01883494669557686),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  144  0.018654  0.018217  0.019147  0.172753  0.041804         0.481726\n",
       "  145  0.018757  0.018461  0.019339  0.200830  0.041099         0.493733\n",
       "  146  0.019161  0.018641  0.019947  0.172357  0.041963         0.488465\n",
       "  147  0.019272  0.019060  0.019690  0.181933  0.042139         0.488626\n",
       "  148  0.019732  0.019006  0.019835  0.136886  0.042335         0.483035,\n",
       "  0.018926012732463766),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  145  0.018757  0.018461  0.019339  0.200830  0.041099         0.493733\n",
       "  146  0.019161  0.018641  0.019947  0.172357  0.041963         0.488465\n",
       "  147  0.019272  0.019060  0.019690  0.181933  0.042139         0.488626\n",
       "  148  0.019732  0.019006  0.019835  0.136886  0.042335         0.483035\n",
       "  149  0.018959  0.018603  0.019864  0.117751  0.041801         0.487802,\n",
       "  0.0189907231189879),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  146  0.019161  0.018641  0.019947  0.172357  0.041963         0.488465\n",
       "  147  0.019272  0.019060  0.019690  0.181933  0.042139         0.488626\n",
       "  148  0.019732  0.019006  0.019835  0.136886  0.042335         0.483035\n",
       "  149  0.018959  0.018603  0.019864  0.117751  0.041801         0.487802\n",
       "  150  0.019616  0.018750  0.020162  0.090027  0.041890         0.487604,\n",
       "  0.019510782934671168),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  147  0.019272  0.019060  0.019690  0.181933  0.042139         0.488626\n",
       "  148  0.019732  0.019006  0.019835  0.136886  0.042335         0.483035\n",
       "  149  0.018959  0.018603  0.019864  0.117751  0.041801         0.487802\n",
       "  150  0.019616  0.018750  0.020162  0.090027  0.041890         0.487604\n",
       "  151  0.019852  0.019684  0.020586  0.212441  0.041954         0.491009,\n",
       "  0.01906741334658602),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  148  0.019732  0.019006  0.019835  0.136886  0.042335         0.483035\n",
       "  149  0.018959  0.018603  0.019864  0.117751  0.041801         0.487802\n",
       "  150  0.019616  0.018750  0.020162  0.090027  0.041890         0.487604\n",
       "  151  0.019852  0.019684  0.020586  0.212441  0.041954         0.491009\n",
       "  152  0.019700  0.019323  0.019888  0.157349  0.042462         0.483805,\n",
       "  0.01927831628366201),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  149  0.018959  0.018603  0.019864  0.117751  0.041801         0.487802\n",
       "  150  0.019616  0.018750  0.020162  0.090027  0.041890         0.487604\n",
       "  151  0.019852  0.019684  0.020586  0.212441  0.041954         0.491009\n",
       "  152  0.019700  0.019323  0.019888  0.157349  0.042462         0.483805\n",
       "  153  0.019602  0.018890  0.020259  0.151587  0.042029         0.488698,\n",
       "  0.01929269209295079),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  150  0.019616  0.018750  0.020162  0.090027  0.041890         0.487604\n",
       "  151  0.019852  0.019684  0.020586  0.212441  0.041954         0.491009\n",
       "  152  0.019700  0.019323  0.019888  0.157349  0.042462         0.483805\n",
       "  153  0.019602  0.018890  0.020259  0.151587  0.042029         0.488698\n",
       "  154  0.018954  0.018875  0.019784  0.164539  0.042234         0.487228,\n",
       "  0.019180052719768404),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  151  0.019852  0.019684  0.020586  0.212441  0.041954         0.491009\n",
       "  152  0.019700  0.019323  0.019888  0.157349  0.042462         0.483805\n",
       "  153  0.019602  0.018890  0.020259  0.151587  0.042029         0.488698\n",
       "  154  0.018954  0.018875  0.019784  0.164539  0.042234         0.487228\n",
       "  155  0.019871  0.019245  0.020353  0.150271  0.042249         0.486278,\n",
       "  0.02043827281521762),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  152  0.019700  0.019323  0.019888  0.157349  0.042462         0.483805\n",
       "  153  0.019602  0.018890  0.020259  0.151587  0.042029         0.488698\n",
       "  154  0.018954  0.018875  0.019784  0.164539  0.042234         0.487228\n",
       "  155  0.019871  0.019245  0.020353  0.150271  0.042249         0.486278\n",
       "  156  0.019436  0.019807  0.020317  0.196349  0.042139         0.496529,\n",
       "  0.021224362081641825),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  153  0.019602  0.018890  0.020259  0.151587  0.042029         0.488698\n",
       "  154  0.018954  0.018875  0.019784  0.164539  0.042234         0.487228\n",
       "  155  0.019871  0.019245  0.020353  0.150271  0.042249         0.486278\n",
       "  156  0.019436  0.019807  0.020317  0.196349  0.042139         0.496529\n",
       "  157  0.021308  0.020978  0.022189  0.212410  0.043367         0.492999,\n",
       "  0.021380138505052857),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  154  0.018954  0.018875  0.019784  0.164539  0.042234         0.487228\n",
       "  155  0.019871  0.019245  0.020353  0.150271  0.042249         0.486278\n",
       "  156  0.019436  0.019807  0.020317  0.196349  0.042139         0.496529\n",
       "  157  0.021308  0.020978  0.022189  0.212410  0.043367         0.492999\n",
       "  158  0.021671  0.020833  0.022192  0.127665  0.044135         0.488285,\n",
       "  0.022468196745439938),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  155  0.019871  0.019245  0.020353  0.150271  0.042249         0.486278\n",
       "  156  0.019436  0.019807  0.020317  0.196349  0.042139         0.496529\n",
       "  157  0.021308  0.020978  0.022189  0.212410  0.043367         0.492999\n",
       "  158  0.021671  0.020833  0.022192  0.127665  0.044135         0.488285\n",
       "  159  0.021561  0.022011  0.022449  0.235651  0.044287         0.495257,\n",
       "  0.022058380334724268),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  156  0.019436  0.019807  0.020317  0.196349  0.042139         0.496529\n",
       "  157  0.021308  0.020978  0.022189  0.212410  0.043367         0.492999\n",
       "  158  0.021671  0.020833  0.022192  0.127665  0.044135         0.488285\n",
       "  159  0.021561  0.022011  0.022449  0.235651  0.044287         0.495257\n",
       "  160  0.022742  0.022124  0.023129  0.124357  0.045350         0.484056,\n",
       "  0.02234118156296878),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  157  0.021308  0.020978  0.022189  0.212410  0.043367         0.492999\n",
       "  158  0.021671  0.020833  0.022192  0.127665  0.044135         0.488285\n",
       "  159  0.021561  0.022011  0.022449  0.235651  0.044287         0.495257\n",
       "  160  0.022742  0.022124  0.023129  0.124357  0.045350         0.484056\n",
       "  161  0.022598  0.022053  0.023340  0.131506  0.044949         0.489235,\n",
       "  0.0225161353544605),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  158  0.021671  0.020833  0.022192  0.127665  0.044135         0.488285\n",
       "  159  0.021561  0.022011  0.022449  0.235651  0.044287         0.495257\n",
       "  160  0.022742  0.022124  0.023129  0.124357  0.045350         0.484056\n",
       "  161  0.022598  0.022053  0.023340  0.131506  0.044949         0.489235\n",
       "  162  0.022576  0.021963  0.022635  0.177941  0.045226         0.488429,\n",
       "  0.022269283271800253),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  159  0.021561  0.022011  0.022449  0.235651  0.044287         0.495257\n",
       "  160  0.022742  0.022124  0.023129  0.124357  0.045350         0.484056\n",
       "  161  0.022598  0.022053  0.023340  0.131506  0.044949         0.489235\n",
       "  162  0.022576  0.021963  0.022635  0.177941  0.045226         0.488429\n",
       "  163  0.022689  0.021940  0.022713  0.159363  0.045397         0.485275,\n",
       "  0.021914583752387213),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  160  0.022742  0.022124  0.023129  0.124357  0.045350         0.484056\n",
       "  161  0.022598  0.022053  0.023340  0.131506  0.044949         0.489235\n",
       "  162  0.022576  0.021963  0.022635  0.177941  0.045226         0.488429\n",
       "  163  0.022689  0.021940  0.022713  0.159363  0.045397         0.485275\n",
       "  164  0.022641  0.021949  0.022802  0.149558  0.045155         0.484468,\n",
       "  0.020368770492263887),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  161  0.022598  0.022053  0.023340  0.131506  0.044949         0.489235\n",
       "  162  0.022576  0.021963  0.022635  0.177941  0.045226         0.488429\n",
       "  163  0.022689  0.021940  0.022713  0.159363  0.045397         0.485275\n",
       "  164  0.022641  0.021949  0.022802  0.149558  0.045155         0.484468\n",
       "  165  0.021878  0.020996  0.021485  0.281505  0.044809         0.475561,\n",
       "  0.020840901321288904),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  162  0.022576  0.021963  0.022635  0.177941  0.045226         0.488429\n",
       "  163  0.022689  0.021940  0.022713  0.159363  0.045397         0.485275\n",
       "  164  0.022641  0.021949  0.022802  0.149558  0.045155         0.484468\n",
       "  165  0.021878  0.020996  0.021485  0.281505  0.044809         0.475561\n",
       "  166  0.020550  0.020413  0.021201  0.159229  0.043299         0.490651,\n",
       "  0.02190260391131323),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  163  0.022689  0.021940  0.022713  0.159363  0.045397         0.485275\n",
       "  164  0.022641  0.021949  0.022802  0.149558  0.045155         0.484468\n",
       "  165  0.021878  0.020996  0.021485  0.281505  0.044809         0.475561\n",
       "  166  0.020550  0.020413  0.021201  0.159229  0.043299         0.490651\n",
       "  167  0.021693  0.021390  0.022301  0.204950  0.043760         0.495060,\n",
       "  0.023508325999168782),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  164  0.022641  0.021949  0.022802  0.149558  0.045155         0.484468\n",
       "  165  0.021878  0.020996  0.021485  0.281505  0.044809         0.475561\n",
       "  166  0.020550  0.020413  0.021201  0.159229  0.043299         0.490651\n",
       "  167  0.021693  0.021390  0.022301  0.204950  0.043760         0.495060\n",
       "  168  0.023919  0.023404  0.024210  0.375736  0.044797         0.499128,\n",
       "  0.024975053063479188),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  165  0.021878  0.020996  0.021485  0.281505  0.044809         0.475561\n",
       "  166  0.020550  0.020413  0.021201  0.159229  0.043299         0.490651\n",
       "  167  0.021693  0.021390  0.022301  0.204950  0.043760         0.495060\n",
       "  168  0.023919  0.023404  0.024210  0.375736  0.044797         0.499128\n",
       "  169  0.024362  0.024471  0.025036  0.431690  0.046365         0.498088,\n",
       "  0.02775272114632665),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  166  0.020550  0.020413  0.021201  0.159229  0.043299         0.490651\n",
       "  167  0.021693  0.021390  0.022301  0.204950  0.043760         0.495060\n",
       "  168  0.023919  0.023404  0.024210  0.375736  0.044797         0.499128\n",
       "  169  0.024362  0.024471  0.025036  0.431690  0.046365         0.498088\n",
       "  170  0.029772  0.028973  0.028654  0.807563  0.047798         0.507891,\n",
       "  0.02960289934862766),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  167  0.021693  0.021390  0.022301  0.204950  0.043760         0.495060\n",
       "  168  0.023919  0.023404  0.024210  0.375736  0.044797         0.499128\n",
       "  169  0.024362  0.024471  0.025036  0.431690  0.046365         0.498088\n",
       "  170  0.029772  0.028973  0.028654  0.807563  0.047798         0.507891\n",
       "  171  0.028550  0.029149  0.029444  0.482479  0.050511         0.500956,\n",
       "  0.028457318626360823),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  168  0.023919  0.023404  0.024210  0.375736  0.044797         0.499128\n",
       "  169  0.024362  0.024471  0.025036  0.431690  0.046365         0.498088\n",
       "  170  0.029772  0.028973  0.028654  0.807563  0.047798         0.507891\n",
       "  171  0.028550  0.029149  0.029444  0.482479  0.050511         0.500956\n",
       "  172  0.029127  0.028499  0.029630  0.420017  0.052317         0.478554,\n",
       "  0.028704170709021076),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  169  0.024362  0.024471  0.025036  0.431690  0.046365         0.498088\n",
       "  170  0.029772  0.028973  0.028654  0.807563  0.047798         0.507891\n",
       "  171  0.028550  0.029149  0.029444  0.482479  0.050511         0.500956\n",
       "  172  0.029127  0.028499  0.029630  0.420017  0.052317         0.478554\n",
       "  173  0.028590  0.028227  0.029255  0.249578  0.051199         0.488966,\n",
       "  0.028625084513208155),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  170  0.029772  0.028973  0.028654  0.807563  0.047798         0.507891\n",
       "  171  0.028550  0.029149  0.029444  0.482479  0.050511         0.500956\n",
       "  172  0.029127  0.028499  0.029630  0.420017  0.052317         0.478554\n",
       "  173  0.028590  0.028227  0.029255  0.249578  0.051199         0.488966\n",
       "  174  0.028954  0.028518  0.029490  0.211099  0.051440         0.486529,\n",
       "  0.02875690125447122),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  171  0.028550  0.029149  0.029444  0.482479  0.050511         0.500956\n",
       "  172  0.029127  0.028499  0.029630  0.420017  0.052317         0.478554\n",
       "  173  0.028590  0.028227  0.029255  0.249578  0.051199         0.488966\n",
       "  174  0.028954  0.028518  0.029490  0.211099  0.051440         0.486529\n",
       "  175  0.029534  0.028559  0.029517  0.222954  0.051362         0.488106,\n",
       "  0.02930572231109436),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  172  0.029127  0.028499  0.029630  0.420017  0.052317         0.478554\n",
       "  173  0.028590  0.028227  0.029255  0.249578  0.051199         0.488966\n",
       "  174  0.028954  0.028518  0.029490  0.211099  0.051440         0.486529\n",
       "  175  0.029534  0.028559  0.029517  0.222954  0.051362         0.488106\n",
       "  176  0.029568  0.028902  0.030095  0.237480  0.051491         0.491224,\n",
       "  0.030240400096285206),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  173  0.028590  0.028227  0.029255  0.249578  0.051199         0.488966\n",
       "  174  0.028954  0.028518  0.029490  0.211099  0.051440         0.486529\n",
       "  175  0.029534  0.028559  0.029517  0.222954  0.051362         0.488106\n",
       "  176  0.029568  0.028902  0.030095  0.237480  0.051491         0.491224\n",
       "  177  0.029519  0.029957  0.030364  0.432378  0.052027         0.494110,\n",
       "  0.03079401308933794),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  174  0.028954  0.028518  0.029490  0.211099  0.051440         0.486529\n",
       "  175  0.029534  0.028559  0.029517  0.222954  0.051362         0.488106\n",
       "  176  0.029568  0.028902  0.030095  0.237480  0.051491         0.491224\n",
       "  177  0.029519  0.029957  0.030364  0.432378  0.052027         0.494110\n",
       "  178  0.030814  0.030367  0.031740  0.293815  0.052940         0.491260,\n",
       "  0.030429729697065717),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  175  0.029534  0.028559  0.029517  0.222954  0.051362         0.488106\n",
       "  176  0.029568  0.028902  0.030095  0.237480  0.051491         0.491224\n",
       "  177  0.029519  0.029957  0.030364  0.432378  0.052027         0.494110\n",
       "  178  0.030814  0.030367  0.031740  0.293815  0.052940         0.491260\n",
       "  179  0.031149  0.030068  0.031469  0.182563  0.053481         0.484397,\n",
       "  0.030678977747940758),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  176  0.029568  0.028902  0.030095  0.237480  0.051491         0.491224\n",
       "  177  0.029519  0.029957  0.030364  0.432378  0.052027         0.494110\n",
       "  178  0.030814  0.030367  0.031740  0.293815  0.052940         0.491260\n",
       "  179  0.031149  0.030068  0.031469  0.182563  0.053481         0.484397\n",
       "  180  0.031096  0.030217  0.031612  0.164425  0.053125         0.488984,\n",
       "  0.030199659014271355),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  177  0.029519  0.029957  0.030364  0.432378  0.052027         0.494110\n",
       "  178  0.030814  0.030367  0.031740  0.293815  0.052940         0.491260\n",
       "  179  0.031149  0.030068  0.031469  0.182563  0.053481         0.484397\n",
       "  180  0.031096  0.030217  0.031612  0.164425  0.053125         0.488984\n",
       "  181  0.030999  0.030040  0.031360  0.134578  0.053368         0.483536,\n",
       "  0.030626247202490613),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  178  0.030814  0.030367  0.031740  0.293815  0.052940         0.491260\n",
       "  179  0.031149  0.030068  0.031469  0.182563  0.053481         0.484397\n",
       "  180  0.031096  0.030217  0.031612  0.164425  0.053125         0.488984\n",
       "  181  0.030999  0.030040  0.031360  0.134578  0.053368         0.483536\n",
       "  182  0.030391  0.029874  0.031188  0.154255  0.052900         0.490310,\n",
       "  0.031462671046150165),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  179  0.031149  0.030068  0.031469  0.182563  0.053481         0.484397\n",
       "  180  0.031096  0.030217  0.031612  0.164425  0.053125         0.488984\n",
       "  181  0.030999  0.030040  0.031360  0.134578  0.053368         0.483536\n",
       "  182  0.030391  0.029874  0.031188  0.154255  0.052900         0.490310\n",
       "  183  0.031233  0.030864  0.031965  0.279107  0.053317         0.493375,\n",
       "  0.03084674363478809),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  180  0.031096  0.030217  0.031612  0.164425  0.053125         0.488984\n",
       "  181  0.030999  0.030040  0.031360  0.134578  0.053368         0.483536\n",
       "  182  0.030391  0.029874  0.031188  0.154255  0.052900         0.490310\n",
       "  183  0.031233  0.030864  0.031965  0.279107  0.053317         0.493375\n",
       "  184  0.031676  0.030924  0.032031  0.217554  0.054134         0.482515,\n",
       "  0.03097136284904446),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  181  0.030999  0.030040  0.031360  0.134578  0.053368         0.483536\n",
       "  182  0.030391  0.029874  0.031188  0.154255  0.052900         0.490310\n",
       "  183  0.031233  0.030864  0.031965  0.279107  0.053317         0.493375\n",
       "  184  0.031676  0.030924  0.032031  0.217554  0.054134         0.482515\n",
       "  185  0.031283  0.030653  0.031818  0.180274  0.053532         0.488052,\n",
       "  0.031405148564270416),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  182  0.030391  0.029874  0.031188  0.154255  0.052900         0.490310\n",
       "  183  0.031233  0.030864  0.031965  0.279107  0.053317         0.493375\n",
       "  184  0.031676  0.030924  0.032031  0.217554  0.054134         0.482515\n",
       "  185  0.031283  0.030653  0.031818  0.180274  0.053532         0.488052\n",
       "  186  0.031404  0.030779  0.032120  0.138095  0.053654         0.490364,\n",
       "  0.0319036446660205),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  183  0.031233  0.030864  0.031965  0.279107  0.053317         0.493375\n",
       "  184  0.031676  0.030924  0.032031  0.217554  0.054134         0.482515\n",
       "  185  0.031283  0.030653  0.031818  0.180274  0.053532         0.488052\n",
       "  186  0.031404  0.030779  0.032120  0.138095  0.053654         0.490364\n",
       "  187  0.031616  0.031213  0.032515  0.189843  0.054077         0.490848,\n",
       "  0.03334879839403543),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  184  0.031676  0.030924  0.032031  0.217554  0.054134         0.482515\n",
       "  185  0.031283  0.030653  0.031818  0.180274  0.053532         0.488052\n",
       "  186  0.031404  0.030779  0.032120  0.138095  0.053654         0.490364\n",
       "  187  0.031616  0.031213  0.032515  0.189843  0.054077         0.490848\n",
       "  188  0.032350  0.032758  0.033319  0.299373  0.054564         0.497927,\n",
       "  0.03335359033046502),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  185  0.031283  0.030653  0.031818  0.180274  0.053532         0.488052\n",
       "  186  0.031404  0.030779  0.032120  0.138095  0.053654         0.490364\n",
       "  187  0.031616  0.031213  0.032515  0.189843  0.054077         0.490848\n",
       "  188  0.032350  0.032758  0.033319  0.299373  0.054564         0.497927\n",
       "  189  0.033969  0.033042  0.034470  0.236242  0.055976         0.487156,\n",
       "  0.03393596456445762),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  186  0.031404  0.030779  0.032120  0.138095  0.053654         0.490364\n",
       "  187  0.031616  0.031213  0.032515  0.189843  0.054077         0.490848\n",
       "  188  0.032350  0.032758  0.033319  0.299373  0.054564         0.497927\n",
       "  189  0.033969  0.033042  0.034470  0.236242  0.055976         0.487156\n",
       "  190  0.034179  0.033161  0.034804  0.198468  0.055980         0.491475,\n",
       "  0.03725286983106904),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  187  0.031616  0.031213  0.032515  0.189843  0.054077         0.490848\n",
       "  188  0.032350  0.032758  0.033319  0.299373  0.054564         0.497927\n",
       "  189  0.033969  0.033042  0.034470  0.236242  0.055976         0.487156\n",
       "  190  0.034179  0.033161  0.034804  0.198468  0.055980         0.491475\n",
       "  191  0.034648  0.037054  0.035545  0.520139  0.056549         0.511923,\n",
       "  0.03738468657233211),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  188  0.032350  0.032758  0.033319  0.299373  0.054564         0.497927\n",
       "  189  0.033969  0.033042  0.034470  0.236242  0.055976         0.487156\n",
       "  190  0.034179  0.033161  0.034804  0.198468  0.055980         0.491475\n",
       "  191  0.034648  0.037054  0.035545  0.520139  0.056549         0.511923\n",
       "  192  0.037969  0.038794  0.038159  0.706330  0.059788         0.488106,\n",
       "  0.03853745519924333),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  189  0.033969  0.033042  0.034470  0.236242  0.055976         0.487156\n",
       "  190  0.034179  0.033161  0.034804  0.198468  0.055980         0.491475\n",
       "  191  0.034648  0.037054  0.035545  0.520139  0.056549         0.511923\n",
       "  192  0.037969  0.038794  0.038159  0.706330  0.059788         0.488106\n",
       "  193  0.036951  0.037737  0.037464  0.439133  0.059917         0.495741,\n",
       "  0.03815159847067562),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  190  0.034179  0.033161  0.034804  0.198468  0.055980         0.491475\n",
       "  191  0.034648  0.037054  0.035545  0.520139  0.056549         0.511923\n",
       "  192  0.037969  0.038794  0.038159  0.706330  0.059788         0.488106\n",
       "  193  0.036951  0.037737  0.037464  0.439133  0.059917         0.495741\n",
       "  194  0.038932  0.038396  0.039321  0.329403  0.061043         0.484235,\n",
       "  0.039781289863041455),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  191  0.034648  0.037054  0.035545  0.520139  0.056549         0.511923\n",
       "  192  0.037969  0.038794  0.038159  0.706330  0.059788         0.488106\n",
       "  193  0.036951  0.037737  0.037464  0.439133  0.059917         0.495741\n",
       "  194  0.038932  0.038396  0.039321  0.329403  0.061043         0.484235\n",
       "  195  0.038811  0.038938  0.039552  0.296262  0.060666         0.499307,\n",
       "  0.04248226771829079),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  192  0.037969  0.038794  0.038159  0.706330  0.059788         0.488106\n",
       "  193  0.036951  0.037737  0.037464  0.439133  0.059917         0.495741\n",
       "  194  0.038932  0.038396  0.039321  0.329403  0.061043         0.484235\n",
       "  195  0.038811  0.038938  0.039552  0.296262  0.060666         0.499307\n",
       "  196  0.040970  0.041865  0.042003  0.540778  0.062258         0.507318,\n",
       "  0.04505382480049186),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  193  0.036951  0.037737  0.037464  0.439133  0.059917         0.495741\n",
       "  194  0.038932  0.038396  0.039321  0.329403  0.061043         0.484235\n",
       "  195  0.038811  0.038938  0.039552  0.296262  0.060666         0.499307\n",
       "  196  0.040970  0.041865  0.042003  0.540778  0.062258         0.507318\n",
       "  197  0.044284  0.045299  0.045067  0.855884  0.064895         0.506350,\n",
       "  0.04502986511834389),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  194  0.038932  0.038396  0.039321  0.329403  0.061043         0.484235\n",
       "  195  0.038811  0.038938  0.039552  0.296262  0.060666         0.499307\n",
       "  196  0.040970  0.041865  0.042003  0.540778  0.062258         0.507318\n",
       "  197  0.044284  0.045299  0.045067  0.855884  0.064895         0.506350\n",
       "  198  0.045615  0.044562  0.045692  0.437094  0.067407         0.486941,\n",
       "  0.043198854661761266),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  195  0.038811  0.038938  0.039552  0.296262  0.060666         0.499307\n",
       "  196  0.040970  0.041865  0.042003  0.540778  0.062258         0.507318\n",
       "  197  0.044284  0.045299  0.045067  0.855884  0.064895         0.506350\n",
       "  198  0.045615  0.044562  0.045692  0.437094  0.067407         0.486941\n",
       "  199  0.045148  0.044448  0.043950  0.456753  0.067383         0.473429,\n",
       "  0.04575843190288835),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  196  0.040970  0.041865  0.042003  0.540778  0.062258         0.507318\n",
       "  197  0.044284  0.045299  0.045067  0.855884  0.064895         0.506350\n",
       "  198  0.045615  0.044562  0.045692  0.437094  0.067407         0.486941\n",
       "  199  0.045148  0.044448  0.043950  0.456753  0.067383         0.473429\n",
       "  200  0.044089  0.045503  0.045021  0.547809  0.065595         0.506260,\n",
       "  0.04628088768678641),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  197  0.044284  0.045299  0.045067  0.855884  0.064895         0.506350\n",
       "  198  0.045615  0.044562  0.045692  0.437094  0.067407         0.486941\n",
       "  199  0.045148  0.044448  0.043950  0.456753  0.067383         0.473429\n",
       "  200  0.044089  0.045503  0.045021  0.547809  0.065595         0.506260\n",
       "  201  0.047629  0.046887  0.047072  0.591521  0.068095         0.491027,\n",
       "  0.04303108877491392),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  198  0.045615  0.044562  0.045692  0.437094  0.067407         0.486941\n",
       "  199  0.045148  0.044448  0.043950  0.456753  0.067383         0.473429\n",
       "  200  0.044089  0.045503  0.045021  0.547809  0.065595         0.506260\n",
       "  201  0.047629  0.046887  0.047072  0.591521  0.068095         0.491027\n",
       "  202  0.046606  0.045370  0.044093  0.625036  0.068605         0.462820,\n",
       "  0.04464880032620577),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  199  0.045148  0.044448  0.043950  0.456753  0.067383         0.473429\n",
       "  200  0.044089  0.045503  0.045021  0.547809  0.065595         0.506260\n",
       "  201  0.047629  0.046887  0.047072  0.591521  0.068095         0.491027\n",
       "  202  0.046606  0.045370  0.044093  0.625036  0.068605         0.462820\n",
       "  203  0.044650  0.044258  0.044713  0.399807  0.065431         0.499217,\n",
       "  0.04373569587731041),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  200  0.044089  0.045503  0.045021  0.547809  0.065595         0.506260\n",
       "  201  0.047629  0.046887  0.047072  0.591521  0.068095         0.491027\n",
       "  202  0.046606  0.045370  0.044093  0.625036  0.068605         0.462820\n",
       "  203  0.044650  0.044258  0.044713  0.399807  0.065431         0.499217\n",
       "  204  0.045196  0.043969  0.044582  0.308728  0.067011         0.480293,\n",
       "  0.04379561432740495),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  201  0.047629  0.046887  0.047072  0.591521  0.068095         0.491027\n",
       "  202  0.046606  0.045370  0.044093  0.625036  0.068605         0.462820\n",
       "  203  0.044650  0.044258  0.044713  0.399807  0.065431         0.499217\n",
       "  204  0.045196  0.043969  0.044582  0.308728  0.067011         0.480293\n",
       "  205  0.043470  0.043237  0.043735  0.311321  0.066119         0.487569,\n",
       "  0.04274109964202502),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  202  0.046606  0.045370  0.044093  0.625036  0.068605         0.462820\n",
       "  203  0.044650  0.044258  0.044713  0.399807  0.065431         0.499217\n",
       "  204  0.045196  0.043969  0.044582  0.308728  0.067011         0.480293\n",
       "  205  0.043470  0.043237  0.043735  0.311321  0.066119         0.487569\n",
       "  206  0.043174  0.042538  0.044033  0.245373  0.066178         0.479235,\n",
       "  0.041890309611439),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  203  0.044650  0.044258  0.044713  0.399807  0.065431         0.499217\n",
       "  204  0.045196  0.043969  0.044582  0.308728  0.067011         0.480293\n",
       "  205  0.043470  0.043237  0.043735  0.311321  0.066119         0.487569\n",
       "  206  0.043174  0.042538  0.044033  0.245373  0.066178         0.479235\n",
       "  207  0.042310  0.041604  0.041533  0.507851  0.065148         0.480759,\n",
       "  0.04252300880030464),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  204  0.045196  0.043969  0.044582  0.308728  0.067011         0.480293\n",
       "  205  0.043470  0.043237  0.043735  0.311321  0.066119         0.487569\n",
       "  206  0.043174  0.042538  0.044033  0.245373  0.066178         0.479235\n",
       "  207  0.042310  0.041604  0.041533  0.507851  0.065148         0.480759\n",
       "  208  0.042132  0.041841  0.042899  0.303031  0.064317         0.491852,\n",
       "  0.04320843853462045),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  205  0.043470  0.043237  0.043735  0.311321  0.066119         0.487569\n",
       "  206  0.043174  0.042538  0.044033  0.245373  0.066178         0.479235\n",
       "  207  0.042310  0.041604  0.041533  0.507851  0.065148         0.480759\n",
       "  208  0.042132  0.041841  0.042899  0.303031  0.064317         0.491852\n",
       "  209  0.043273  0.042315  0.043584  0.253704  0.064935         0.492246,\n",
       "  0.04474227195366978),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  206  0.043174  0.042538  0.044033  0.245373  0.066178         0.479235\n",
       "  207  0.042310  0.041604  0.041533  0.507851  0.065148         0.480759\n",
       "  208  0.042132  0.041841  0.042899  0.303031  0.064317         0.491852\n",
       "  209  0.043273  0.042315  0.043584  0.253704  0.064935         0.492246\n",
       "  210  0.042573  0.044062  0.042574  0.511295  0.065604         0.498590,\n",
       "  0.0450154796866928),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  207  0.042310  0.041604  0.041533  0.507851  0.065148         0.480759\n",
       "  208  0.042132  0.041841  0.042899  0.303031  0.064317         0.491852\n",
       "  209  0.043273  0.042315  0.043584  0.253704  0.064935         0.492246\n",
       "  210  0.042573  0.044062  0.042574  0.511295  0.065604         0.498590\n",
       "  211  0.045456  0.044685  0.045781  0.367972  0.067102         0.489164,\n",
       "  0.04536538726967625),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  208  0.042132  0.041841  0.042899  0.303031  0.064317         0.491852\n",
       "  209  0.043273  0.042315  0.043584  0.253704  0.064935         0.492246\n",
       "  210  0.042573  0.044062  0.042574  0.511295  0.065604         0.498590\n",
       "  211  0.045456  0.044685  0.045781  0.367972  0.067102         0.489164\n",
       "  212  0.045841  0.045164  0.046678  0.254667  0.067369         0.489737,\n",
       "  0.045463650833569846),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  209  0.043273  0.042315  0.043584  0.253704  0.064935         0.492246\n",
       "  210  0.042573  0.044062  0.042574  0.511295  0.065604         0.498590\n",
       "  211  0.045456  0.044685  0.045781  0.367972  0.067102         0.489164\n",
       "  212  0.045841  0.045164  0.046678  0.254667  0.067369         0.489737\n",
       "  213  0.045439  0.045754  0.046152  0.341803  0.067711         0.487855,\n",
       "  0.04727069198564221),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  210  0.042573  0.044062  0.042574  0.511295  0.065604         0.498590\n",
       "  211  0.045456  0.044685  0.045781  0.367972  0.067102         0.489164\n",
       "  212  0.045841  0.045164  0.046678  0.254667  0.067369         0.489737\n",
       "  213  0.045439  0.045754  0.046152  0.341803  0.067711         0.487855\n",
       "  214  0.046137  0.046510  0.046898  0.432159  0.067807         0.500633,\n",
       "  0.04891236321908202),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  211  0.045456  0.044685  0.045781  0.367972  0.067102         0.489164\n",
       "  212  0.045841  0.045164  0.046678  0.254667  0.067369         0.489737\n",
       "  213  0.045439  0.045754  0.046152  0.341803  0.067711         0.487855\n",
       "  214  0.046137  0.046510  0.046898  0.432159  0.067807         0.500633\n",
       "  215  0.048062  0.048043  0.047935  0.432930  0.069572         0.499396,\n",
       "  0.04840907518090233),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  212  0.045841  0.045164  0.046678  0.254667  0.067369         0.489737\n",
       "  213  0.045439  0.045754  0.046152  0.341803  0.067711         0.487855\n",
       "  214  0.046137  0.046510  0.046898  0.432159  0.067807         0.500633\n",
       "  215  0.048062  0.048043  0.047935  0.432930  0.069572         0.499396\n",
       "  216  0.049838  0.049176  0.049906  0.462936  0.071175         0.483357,\n",
       "  0.04618502971346991),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  213  0.045439  0.045754  0.046152  0.341803  0.067711         0.487855\n",
       "  214  0.046137  0.046510  0.046898  0.432159  0.067807         0.500633\n",
       "  215  0.048062  0.048043  0.047935  0.432930  0.069572         0.499396\n",
       "  216  0.049838  0.049176  0.049906  0.462936  0.071175         0.483357\n",
       "  217  0.048929  0.048022  0.047520  0.444990  0.070683         0.470490,\n",
       "  0.04652773976944666),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  214  0.046137  0.046510  0.046898  0.432159  0.067807         0.500633\n",
       "  215  0.048062  0.048043  0.047935  0.432930  0.069572         0.499396\n",
       "  216  0.049838  0.049176  0.049906  0.462936  0.071175         0.483357\n",
       "  217  0.048929  0.048022  0.047520  0.444990  0.070683         0.470490\n",
       "  218  0.046965  0.046730  0.047252  0.367050  0.068511         0.489683,\n",
       "  0.04583272616227167),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  215  0.048062  0.048043  0.047935  0.432930  0.069572         0.499396\n",
       "  216  0.049838  0.049176  0.049906  0.462936  0.071175         0.483357\n",
       "  217  0.048929  0.048022  0.047520  0.444990  0.070683         0.470490\n",
       "  218  0.046965  0.046730  0.047252  0.367050  0.068511         0.489683\n",
       "  219  0.047133  0.046093  0.046811  0.224350  0.068846         0.481923,\n",
       "  0.046901616656940384),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  216  0.049838  0.049176  0.049906  0.462936  0.071175         0.483357\n",
       "  217  0.048929  0.048022  0.047520  0.444990  0.070683         0.470490\n",
       "  218  0.046965  0.046730  0.047252  0.367050  0.068511         0.489683\n",
       "  219  0.047133  0.046093  0.046811  0.224350  0.068846         0.481923\n",
       "  220  0.046426  0.046031  0.047055  0.182183  0.068167         0.495113,\n",
       "  0.045897436548795814),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  217  0.048929  0.048022  0.047520  0.444990  0.070683         0.470490\n",
       "  218  0.046965  0.046730  0.047252  0.367050  0.068511         0.489683\n",
       "  219  0.047133  0.046093  0.046811  0.224350  0.068846         0.481923\n",
       "  220  0.046426  0.046031  0.047055  0.182183  0.068167         0.495113\n",
       "  221  0.047677  0.046415  0.047334  0.194498  0.069211         0.479612,\n",
       "  0.04686087557492652),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  218  0.046965  0.046730  0.047252  0.367050  0.068511         0.489683\n",
       "  219  0.047133  0.046093  0.046811  0.224350  0.068846         0.481923\n",
       "  220  0.046426  0.046031  0.047055  0.182183  0.068167         0.495113\n",
       "  221  0.047677  0.046415  0.047334  0.194498  0.069211         0.479612\n",
       "  222  0.045774  0.045988  0.046639  0.259485  0.068231         0.494325,\n",
       "  0.04702623587119675),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  219  0.047133  0.046093  0.046811  0.224350  0.068846         0.481923\n",
       "  220  0.046426  0.046031  0.047055  0.182183  0.068167         0.495113\n",
       "  221  0.047677  0.046415  0.047334  0.194498  0.069211         0.479612\n",
       "  222  0.045774  0.045988  0.046639  0.259485  0.068231         0.494325\n",
       "  223  0.047424  0.046391  0.047981  0.181298  0.069171         0.488357,\n",
       "  0.04633601420045136),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  220  0.046426  0.046031  0.047055  0.182183  0.068167         0.495113\n",
       "  221  0.047677  0.046415  0.047334  0.194498  0.069211         0.479612\n",
       "  222  0.045774  0.045988  0.046639  0.259485  0.068231         0.494325\n",
       "  223  0.047424  0.046391  0.047981  0.181298  0.069171         0.488357\n",
       "  224  0.047461  0.046178  0.047247  0.203984  0.069333         0.481959,\n",
       "  0.045959750967105154),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  221  0.047677  0.046415  0.047334  0.194498  0.069211         0.479612\n",
       "  222  0.045774  0.045988  0.046639  0.259485  0.068231         0.494325\n",
       "  223  0.047424  0.046391  0.047981  0.181298  0.069171         0.488357\n",
       "  224  0.047461  0.046178  0.047247  0.203984  0.069333         0.481959\n",
       "  225  0.046736  0.045657  0.047225  0.142558  0.068659         0.484307,\n",
       "  0.047656548714209906),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  222  0.045774  0.045988  0.046639  0.259485  0.068231         0.494325\n",
       "  223  0.047424  0.046391  0.047981  0.181298  0.069171         0.488357\n",
       "  224  0.047461  0.046178  0.047247  0.203984  0.069333         0.481959\n",
       "  225  0.046736  0.045657  0.047225  0.142558  0.068659         0.484307\n",
       "  226  0.046524  0.046804  0.047508  0.278112  0.068291         0.499809,\n",
       "  0.04814305535252371),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  223  0.047424  0.046391  0.047981  0.181298  0.069171         0.488357\n",
       "  224  0.047461  0.046178  0.047247  0.203984  0.069333         0.481959\n",
       "  225  0.046736  0.045657  0.047225  0.142558  0.068659         0.484307\n",
       "  226  0.046524  0.046804  0.047508  0.278112  0.068291         0.499809\n",
       "  227  0.049612  0.048583  0.049439  0.259378  0.069948         0.490758,\n",
       "  0.04821495364369223),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  224  0.047461  0.046178  0.047247  0.203984  0.069333         0.481959\n",
       "  225  0.046736  0.045657  0.047225  0.142558  0.068659         0.484307\n",
       "  226  0.046524  0.046804  0.047508  0.278112  0.068291         0.499809\n",
       "  227  0.049612  0.048583  0.049439  0.259378  0.069948         0.490758\n",
       "  228  0.048625  0.047789  0.049369  0.205154  0.070424         0.487658,\n",
       "  0.04781951304226534),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  225  0.046736  0.045657  0.047225  0.142558  0.068659         0.484307\n",
       "  226  0.046524  0.046804  0.047508  0.278112  0.068291         0.499809\n",
       "  227  0.049612  0.048583  0.049439  0.259378  0.069948         0.490758\n",
       "  228  0.048625  0.047789  0.049369  0.205154  0.070424         0.487658\n",
       "  229  0.048327  0.047434  0.048877  0.150899  0.070494         0.484164,\n",
       "  0.0503024000557943),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  226  0.046524  0.046804  0.047508  0.278112  0.068291         0.499809\n",
       "  227  0.049612  0.048583  0.049439  0.259378  0.069948         0.490758\n",
       "  228  0.048625  0.047789  0.049369  0.205154  0.070424         0.487658\n",
       "  229  0.048327  0.047434  0.048877  0.150899  0.070494         0.484164\n",
       "  230  0.048832  0.049436  0.049858  0.306899  0.070108         0.505687,\n",
       "  0.05080568809397398),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  227  0.049612  0.048583  0.049439  0.259378  0.069948         0.490758\n",
       "  228  0.048625  0.047789  0.049369  0.205154  0.070424         0.487658\n",
       "  229  0.048327  0.047434  0.048877  0.150899  0.070494         0.484164\n",
       "  230  0.048832  0.049436  0.049858  0.306899  0.070108         0.505687\n",
       "  231  0.049667  0.049972  0.049858  0.347981  0.072532         0.490884,\n",
       "  0.05127063101835462),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  228  0.048625  0.047789  0.049369  0.205154  0.070424         0.487658\n",
       "  229  0.048327  0.047434  0.048877  0.150899  0.070494         0.484164\n",
       "  230  0.048832  0.049436  0.049858  0.306899  0.070108         0.505687\n",
       "  231  0.049667  0.049972  0.049858  0.347981  0.072532         0.490884\n",
       "  232  0.051708  0.051202  0.052137  0.481249  0.073024         0.490597,\n",
       "  0.04850494277658114),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  229  0.048327  0.047434  0.048877  0.150899  0.070494         0.484164\n",
       "  230  0.048832  0.049436  0.049858  0.306899  0.070108         0.505687\n",
       "  231  0.049667  0.049972  0.049858  0.347981  0.072532         0.490884\n",
       "  232  0.051708  0.051202  0.052137  0.481249  0.073024         0.490597\n",
       "  233  0.049860  0.049176  0.048533  0.568749  0.073478         0.466440,\n",
       "  0.04693516983430984),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  230  0.048832  0.049436  0.049858  0.306899  0.070108         0.505687\n",
       "  231  0.049667  0.049972  0.049858  0.347981  0.072532         0.490884\n",
       "  232  0.051708  0.051202  0.052137  0.481249  0.073024         0.490597\n",
       "  233  0.049860  0.049176  0.048533  0.568749  0.073478         0.466440\n",
       "  234  0.048902  0.047763  0.048201  0.234838  0.070777         0.475382,\n",
       "  0.04699268269382728),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  231  0.049667  0.049972  0.049858  0.347981  0.072532         0.490884\n",
       "  232  0.051708  0.051202  0.052137  0.481249  0.073024         0.490597\n",
       "  233  0.049860  0.049176  0.048533  0.568749  0.073478         0.466440\n",
       "  234  0.048902  0.047763  0.048201  0.234838  0.070777         0.475382\n",
       "  235  0.047126  0.046510  0.047506  0.238684  0.069244         0.487551,\n",
       "  0.047194001758044085),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  232  0.051708  0.051202  0.052137  0.481249  0.073024         0.490597\n",
       "  233  0.049860  0.049176  0.048533  0.568749  0.073478         0.466440\n",
       "  234  0.048902  0.047763  0.048201  0.234838  0.070777         0.475382\n",
       "  235  0.047126  0.046510  0.047506  0.238684  0.069244         0.487551\n",
       "  236  0.047783  0.046564  0.047581  0.175468  0.069300         0.488626,\n",
       "  0.04637196334603562),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  233  0.049860  0.049176  0.048533  0.568749  0.073478         0.466440\n",
       "  234  0.048902  0.047763  0.048201  0.234838  0.070777         0.475382\n",
       "  235  0.047126  0.046510  0.047506  0.238684  0.069244         0.487551\n",
       "  236  0.047783  0.046564  0.047581  0.175468  0.069300         0.488626\n",
       "  237  0.047704  0.046370  0.047794  0.144034  0.069497         0.480974,\n",
       "  0.044996311940974434),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  234  0.048902  0.047763  0.048201  0.234838  0.070777         0.475382\n",
       "  235  0.047126  0.046510  0.047506  0.238684  0.069244         0.487551\n",
       "  236  0.047783  0.046564  0.047581  0.175468  0.069300         0.488626\n",
       "  237  0.047704  0.046370  0.047794  0.144034  0.069497         0.480974\n",
       "  238  0.046435  0.045287  0.046271  0.203328  0.068694         0.476834,\n",
       "  0.04591900026272898),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  235  0.047126  0.046510  0.047506  0.238684  0.069244         0.487551\n",
       "  236  0.047783  0.046564  0.047581  0.175468  0.069300         0.488626\n",
       "  237  0.047704  0.046370  0.047794  0.144034  0.069497         0.480974\n",
       "  238  0.046435  0.045287  0.046271  0.203328  0.068694         0.476834\n",
       "  239  0.045468  0.045206  0.046540  0.137633  0.067350         0.494020,\n",
       "  0.047735634910022834),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  236  0.047783  0.046564  0.047581  0.175468  0.069300         0.488626\n",
       "  237  0.047704  0.046370  0.047794  0.144034  0.069497         0.480974\n",
       "  238  0.046435  0.045287  0.046271  0.203328  0.068694         0.476834\n",
       "  239  0.045468  0.045206  0.046540  0.137633  0.067350         0.494020\n",
       "  240  0.046305  0.046870  0.047271  0.177226  0.068252         0.500705,\n",
       "  0.047282671826716194),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  237  0.047704  0.046370  0.047794  0.144034  0.069497         0.480974\n",
       "  238  0.046435  0.045287  0.046271  0.203328  0.068694         0.476834\n",
       "  239  0.045468  0.045206  0.046540  0.137633  0.067350         0.494020\n",
       "  240  0.046305  0.046870  0.047271  0.177226  0.068252         0.500705\n",
       "  241  0.047846  0.046917  0.048356  0.144156  0.070026         0.483733,\n",
       "  0.047385727327039384),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  238  0.046435  0.045287  0.046271  0.203328  0.068694         0.476834\n",
       "  239  0.045468  0.045206  0.046540  0.137633  0.067350         0.494020\n",
       "  240  0.046305  0.046870  0.047271  0.177226  0.068252         0.500705\n",
       "  241  0.047846  0.046917  0.048356  0.144156  0.070026         0.483733\n",
       "  242  0.047256  0.046747  0.048271  0.127273  0.069583         0.487891,\n",
       "  0.04609635002243549),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  239  0.045468  0.045206  0.046540  0.137633  0.067350         0.494020\n",
       "  240  0.046305  0.046870  0.047271  0.177226  0.068252         0.500705\n",
       "  241  0.047846  0.046917  0.048356  0.144156  0.070026         0.483733\n",
       "  242  0.047256  0.046747  0.048271  0.127273  0.069583         0.487891\n",
       "  243  0.047725  0.046631  0.047317  0.144377  0.069684         0.477479,\n",
       "  0.04583272616227167),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  240  0.046305  0.046870  0.047271  0.177226  0.068252         0.500705\n",
       "  241  0.047846  0.046917  0.048356  0.144156  0.070026         0.483733\n",
       "  242  0.047256  0.046747  0.048271  0.127273  0.069583         0.487891\n",
       "  243  0.047725  0.046631  0.047317  0.144377  0.069684         0.477479\n",
       "  244  0.046787  0.045953  0.047118  0.108916  0.068425         0.485149,\n",
       "  0.0459094163898698),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  241  0.047846  0.046917  0.048356  0.144156  0.070026         0.483733\n",
       "  242  0.047256  0.046747  0.048271  0.127273  0.069583         0.487891\n",
       "  243  0.047725  0.046631  0.047317  0.144377  0.069684         0.477479\n",
       "  244  0.046787  0.045953  0.047118  0.108916  0.068425         0.485149\n",
       "  245  0.046392  0.045249  0.046639  0.140476  0.068167         0.487694,\n",
       "  0.04449781583922434),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  242  0.047256  0.046747  0.048271  0.127273  0.069583         0.487891\n",
       "  243  0.047725  0.046631  0.047317  0.144377  0.069684         0.477479\n",
       "  244  0.046787  0.045953  0.047118  0.108916  0.068425         0.485149\n",
       "  245  0.046392  0.045249  0.046639  0.140476  0.068167         0.487694\n",
       "  246  0.046233  0.045166  0.045760  0.167224  0.068242         0.476565,\n",
       "  0.04410716717422704),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  243  0.047725  0.046631  0.047317  0.144377  0.069684         0.477479\n",
       "  244  0.046787  0.045953  0.047118  0.108916  0.068425         0.485149\n",
       "  245  0.046392  0.045249  0.046639  0.140476  0.068167         0.487694\n",
       "  246  0.046233  0.045166  0.045760  0.167224  0.068242         0.476565\n",
       "  247  0.044931  0.043803  0.044728  0.182692  0.066864         0.484199,\n",
       "  0.04546604680178465),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  244  0.046787  0.045953  0.047118  0.108916  0.068425         0.485149\n",
       "  245  0.046392  0.045249  0.046639  0.140476  0.068167         0.487694\n",
       "  246  0.046233  0.045166  0.045760  0.167224  0.068242         0.476565\n",
       "  247  0.044931  0.043803  0.044728  0.182692  0.066864         0.484199\n",
       "  248  0.044322  0.044661  0.044975  0.160095  0.066482         0.497282,\n",
       "  0.04409518733315304),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  245  0.046392  0.045249  0.046639  0.140476  0.068167         0.487694\n",
       "  246  0.046233  0.045166  0.045760  0.167224  0.068242         0.476565\n",
       "  247  0.044931  0.043803  0.044728  0.182692  0.066864         0.484199\n",
       "  248  0.044322  0.044661  0.044975  0.160095  0.066482         0.497282\n",
       "  249  0.045872  0.045270  0.045491  0.198740  0.067809         0.476870,\n",
       "  0.044490627934579946),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  246  0.046233  0.045166  0.045760  0.167224  0.068242         0.476565\n",
       "  247  0.044931  0.043803  0.044728  0.182692  0.066864         0.484199\n",
       "  248  0.044322  0.044661  0.044975  0.160095  0.066482         0.497282\n",
       "  249  0.045872  0.045270  0.045491  0.198740  0.067809         0.476870\n",
       "  250  0.044688  0.044090  0.045387  0.172824  0.066470         0.490078,\n",
       "  0.044358811193316874),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  247  0.044931  0.043803  0.044728  0.182692  0.066864         0.484199\n",
       "  248  0.044322  0.044661  0.044975  0.160095  0.066482         0.497282\n",
       "  249  0.045872  0.045270  0.045491  0.198740  0.067809         0.476870\n",
       "  250  0.044688  0.044090  0.045387  0.172824  0.066470         0.490078\n",
       "  251  0.044840  0.043801  0.045546  0.094359  0.066857         0.486135,\n",
       "  0.043134144275237124),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  248  0.044322  0.044661  0.044975  0.160095  0.066482         0.497282\n",
       "  249  0.045872  0.045270  0.045491  0.198740  0.067809         0.476870\n",
       "  250  0.044688  0.044090  0.045387  0.172824  0.066470         0.490078\n",
       "  251  0.044840  0.043801  0.045546  0.094359  0.066857         0.486135\n",
       "  252  0.042529  0.042363  0.043366  0.288690  0.066728         0.477963,\n",
       "  0.043136540243451926),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  249  0.045872  0.045270  0.045491  0.198740  0.067809         0.476870\n",
       "  250  0.044688  0.044090  0.045387  0.172824  0.066470         0.490078\n",
       "  251  0.044840  0.043801  0.045546  0.094359  0.066857         0.486135\n",
       "  252  0.042529  0.042363  0.043366  0.288690  0.066728         0.477963\n",
       "  253  0.043752  0.042588  0.044486  0.134726  0.065532         0.487138,\n",
       "  0.04170097038829619),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  250  0.044688  0.044090  0.045387  0.172824  0.066470         0.490078\n",
       "  251  0.044840  0.043801  0.045546  0.094359  0.066857         0.486135\n",
       "  252  0.042529  0.042363  0.043366  0.288690  0.066728         0.477963\n",
       "  253  0.043752  0.042588  0.044486  0.134726  0.065532         0.487138\n",
       "  254  0.043812  0.042592  0.042967  0.165619  0.065534         0.476386,\n",
       "  0.043035880711343524),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  251  0.044840  0.043801  0.045546  0.094359  0.066857         0.486135\n",
       "  252  0.042529  0.042363  0.043366  0.288690  0.066728         0.477963\n",
       "  253  0.043752  0.042588  0.044486  0.134726  0.065532         0.487138\n",
       "  254  0.043812  0.042592  0.042967  0.165619  0.065534         0.476386\n",
       "  255  0.042590  0.042183  0.043153  0.141507  0.064132         0.497103,\n",
       "  0.043752467654814),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  252  0.042529  0.042363  0.043366  0.288690  0.066728         0.477963\n",
       "  253  0.043752  0.042588  0.044486  0.134726  0.065532         0.487138\n",
       "  254  0.043812  0.042592  0.042967  0.165619  0.065534         0.476386\n",
       "  255  0.042590  0.042183  0.043153  0.141507  0.064132         0.497103\n",
       "  256  0.042929  0.043370  0.043718  0.208917  0.065436         0.492479,\n",
       "  0.04375725959124359),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  253  0.043752  0.042588  0.044486  0.134726  0.065532         0.487138\n",
       "  254  0.043812  0.042592  0.042967  0.165619  0.065534         0.476386\n",
       "  255  0.042590  0.042183  0.043153  0.141507  0.064132         0.497103\n",
       "  256  0.042929  0.043370  0.043718  0.208917  0.065436         0.492479\n",
       "  257  0.044127  0.043192  0.044570  0.106358  0.066136         0.487156,\n",
       "  0.04399452780104465),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  254  0.043812  0.042592  0.042967  0.165619  0.065534         0.476386\n",
       "  255  0.042590  0.042183  0.043153  0.141507  0.064132         0.497103\n",
       "  256  0.042929  0.043370  0.043718  0.208917  0.065436         0.492479\n",
       "  257  0.044127  0.043192  0.044570  0.106358  0.066136         0.487156\n",
       "  258  0.044351  0.043434  0.045222  0.091246  0.066140         0.488895,\n",
       "  0.045161672237244654),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  255  0.042590  0.042183  0.043153  0.141507  0.064132         0.497103\n",
       "  256  0.042929  0.043370  0.043718  0.208917  0.065436         0.492479\n",
       "  257  0.044127  0.043192  0.044570  0.106358  0.066136         0.487156\n",
       "  258  0.044351  0.043434  0.045222  0.091246  0.066140         0.488895\n",
       "  259  0.044053  0.044405  0.045120  0.143485  0.066372         0.495848,\n",
       "  0.044878880631362444),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  256  0.042929  0.043370  0.043718  0.208917  0.065436         0.492479\n",
       "  257  0.044127  0.043192  0.044570  0.106358  0.066136         0.487156\n",
       "  258  0.044351  0.043434  0.045222  0.091246  0.066140         0.488895\n",
       "  259  0.044053  0.044405  0.045120  0.143485  0.066372         0.495848\n",
       "  260  0.045292  0.044498  0.046002  0.116462  0.067512         0.485006,\n",
       "  0.044574506066822464),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  257  0.044127  0.043192  0.044570  0.106358  0.066136         0.487156\n",
       "  258  0.044351  0.043434  0.045222  0.091246  0.066140         0.488895\n",
       "  259  0.044053  0.044405  0.045120  0.143485  0.066372         0.495848\n",
       "  260  0.045292  0.044498  0.046002  0.116462  0.067512         0.485006\n",
       "  261  0.045494  0.044258  0.045634  0.122348  0.067236         0.484845,\n",
       "  0.044634424516917),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  258  0.044351  0.043434  0.045222  0.091246  0.066140         0.488895\n",
       "  259  0.044053  0.044405  0.045120  0.143485  0.066372         0.495848\n",
       "  260  0.045292  0.044498  0.046002  0.116462  0.067512         0.485006\n",
       "  261  0.045494  0.044258  0.045634  0.122348  0.067236         0.484845\n",
       "  262  0.044936  0.044021  0.045789  0.066601  0.066939         0.487569,\n",
       "  0.045161672237244654),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  259  0.044053  0.044405  0.045120  0.143485  0.066372         0.495848\n",
       "  260  0.045292  0.044498  0.046002  0.116462  0.067512         0.485006\n",
       "  261  0.045494  0.044258  0.045634  0.122348  0.067236         0.484845\n",
       "  262  0.044936  0.044021  0.045789  0.066601  0.066939         0.487569\n",
       "  263  0.045080  0.044612  0.046137  0.083443  0.066997         0.491063,\n",
       "  0.044840525895201086),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  260  0.045292  0.044498  0.046002  0.116462  0.067512         0.485006\n",
       "  261  0.045494  0.044258  0.045634  0.122348  0.067236         0.484845\n",
       "  262  0.044936  0.044021  0.045789  0.066601  0.066939         0.487569\n",
       "  263  0.045080  0.044612  0.046137  0.083443  0.066997         0.491063\n",
       "  264  0.045680  0.044469  0.046225  0.102361  0.067512         0.484719,\n",
       "  0.04670029759272358),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  261  0.045494  0.044258  0.045634  0.122348  0.067236         0.484845\n",
       "  262  0.044936  0.044021  0.045789  0.066601  0.066939         0.487569\n",
       "  263  0.045080  0.044612  0.046137  0.083443  0.066997         0.491063\n",
       "  264  0.045680  0.044469  0.046225  0.102361  0.067512         0.484719\n",
       "  265  0.044931  0.045917  0.045856  0.182317  0.067198         0.501027,\n",
       "  0.04682492642934226),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  262  0.044936  0.044021  0.045789  0.066601  0.066939         0.487569\n",
       "  263  0.045080  0.044612  0.046137  0.083443  0.066997         0.491063\n",
       "  264  0.045680  0.044469  0.046225  0.102361  0.067512         0.484719\n",
       "  265  0.044931  0.045917  0.045856  0.182317  0.067198         0.501027\n",
       "  266  0.047083  0.046813  0.047814  0.160762  0.069015         0.488052,\n",
       "  0.04771166560551256),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  263  0.045080  0.044612  0.046137  0.083443  0.066997         0.491063\n",
       "  264  0.045680  0.044469  0.046225  0.102361  0.067512         0.484719\n",
       "  265  0.044931  0.045917  0.045856  0.182317  0.067198         0.501027\n",
       "  266  0.047083  0.046813  0.047814  0.160762  0.069015         0.488052\n",
       "  267  0.047672  0.046770  0.048589  0.106696  0.069136         0.493751,\n",
       "  0.05026405494199525),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  264  0.045680  0.044469  0.046225  0.102361  0.067512         0.484719\n",
       "  265  0.044931  0.045917  0.045856  0.182317  0.067198         0.501027\n",
       "  266  0.047083  0.046813  0.047814  0.160762  0.069015         0.488052\n",
       "  267  0.047672  0.046770  0.048589  0.106696  0.069136         0.493751\n",
       "  268  0.048748  0.049690  0.049664  0.252539  0.070002         0.506206,\n",
       "  0.05072899786637587),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  265  0.044931  0.045917  0.045856  0.182317  0.067198         0.501027\n",
       "  266  0.047083  0.046813  0.047814  0.160762  0.069015         0.488052\n",
       "  267  0.047672  0.046770  0.048589  0.106696  0.069136         0.493751\n",
       "  268  0.048748  0.049690  0.049664  0.252539  0.070002         0.506206\n",
       "  269  0.050493  0.050664  0.050916  0.250432  0.072495         0.490597,\n",
       "  0.04864873935891818),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  266  0.047083  0.046813  0.047814  0.160762  0.069015         0.488052\n",
       "  267  0.047672  0.046770  0.048589  0.106696  0.069136         0.493751\n",
       "  268  0.048748  0.049690  0.049664  0.252539  0.070002         0.506206\n",
       "  269  0.050493  0.050664  0.050916  0.250432  0.072495         0.490597\n",
       "  270  0.050428  0.050131  0.049495  0.274169  0.072949         0.471565,\n",
       "  0.048557673322031286),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  267  0.047672  0.046770  0.048589  0.106696  0.069136         0.493751\n",
       "  268  0.048748  0.049690  0.049664  0.252539  0.070002         0.506206\n",
       "  269  0.050493  0.050664  0.050916  0.250432  0.072495         0.490597\n",
       "  270  0.050428  0.050131  0.049495  0.274169  0.072949         0.471565\n",
       "  271  0.048211  0.048583  0.048916  0.376094  0.070917         0.486440,\n",
       "  0.04797768543389118),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  268  0.048748  0.049690  0.049664  0.252539  0.070002         0.506206\n",
       "  269  0.050493  0.050664  0.050916  0.250432  0.072495         0.490597\n",
       "  270  0.050428  0.050131  0.049495  0.274169  0.072949         0.471565\n",
       "  271  0.048211  0.048583  0.048916  0.376094  0.070917         0.486440\n",
       "  272  0.049294  0.048050  0.049390  0.184270  0.070828         0.482784,\n",
       "  0.048838068959698694),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  269  0.050493  0.050664  0.050916  0.250432  0.072495         0.490597\n",
       "  270  0.050428  0.050131  0.049495  0.274169  0.072949         0.471565\n",
       "  271  0.048211  0.048583  0.048916  0.376094  0.070917         0.486440\n",
       "  272  0.049294  0.048050  0.049390  0.184270  0.070828         0.482784\n",
       "  273  0.048568  0.048406  0.049594  0.140053  0.070262         0.493554,\n",
       "  0.04982547729033969),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  270  0.050428  0.050131  0.049495  0.274169  0.072949         0.471565\n",
       "  271  0.048211  0.048583  0.048916  0.376094  0.070917         0.486440\n",
       "  272  0.049294  0.048050  0.049390  0.184270  0.070828         0.482784\n",
       "  273  0.048568  0.048406  0.049594  0.140053  0.070262         0.493554\n",
       "  274  0.049807  0.049662  0.050689  0.227352  0.071102         0.494504,\n",
       "  0.050781718789463706),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  271  0.048211  0.048583  0.048916  0.376094  0.070917         0.486440\n",
       "  272  0.049294  0.048050  0.049390  0.184270  0.070828         0.482784\n",
       "  273  0.048568  0.048406  0.049594  0.140053  0.070262         0.493554\n",
       "  274  0.049807  0.049662  0.050689  0.227352  0.071102         0.494504\n",
       "  275  0.050351  0.050245  0.050865  0.246049  0.072067         0.494271,\n",
       "  0.050654703606992535),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  272  0.049294  0.048050  0.049390  0.184270  0.070828         0.482784\n",
       "  273  0.048568  0.048406  0.049594  0.140053  0.070262         0.493554\n",
       "  274  0.049807  0.049662  0.050689  0.227352  0.071102         0.494504\n",
       "  275  0.050351  0.050245  0.050865  0.246049  0.072067         0.494271\n",
       "  276  0.051095  0.051539  0.051885  0.316263  0.073000         0.486171,\n",
       "  0.051610945106116565),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  273  0.048568  0.048406  0.049594  0.140053  0.070262         0.493554\n",
       "  274  0.049807  0.049662  0.050689  0.227352  0.071102         0.494504\n",
       "  275  0.050351  0.050245  0.050865  0.246049  0.072067         0.494271\n",
       "  276  0.051095  0.051539  0.051885  0.316263  0.073000         0.486171\n",
       "  277  0.051456  0.051062  0.052471  0.206258  0.072876         0.494271,\n",
       "  0.05135211318238232),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  274  0.049807  0.049662  0.050689  0.227352  0.071102         0.494504\n",
       "  275  0.050351  0.050245  0.050865  0.246049  0.072067         0.494271\n",
       "  276  0.051095  0.051539  0.051885  0.316263  0.073000         0.486171\n",
       "  277  0.051456  0.051062  0.052471  0.206258  0.072876         0.494271\n",
       "  278  0.053020  0.051948  0.052464  0.240569  0.073810         0.485185,\n",
       "  0.05127063101835462),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  275  0.050351  0.050245  0.050865  0.246049  0.072067         0.494271\n",
       "  276  0.051095  0.051539  0.051885  0.316263  0.073000         0.486171\n",
       "  277  0.051456  0.051062  0.052471  0.206258  0.072876         0.494271\n",
       "  278  0.053020  0.051948  0.052464  0.240569  0.073810         0.485185\n",
       "  279  0.051925  0.051347  0.052663  0.167081  0.073558         0.486511,\n",
       "  0.04936772227060346),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  276  0.051095  0.051539  0.051885  0.316263  0.073000         0.486171\n",
       "  277  0.051456  0.051062  0.052471  0.206258  0.072876         0.494271\n",
       "  278  0.053020  0.051948  0.052464  0.240569  0.073810         0.485185\n",
       "  279  0.051925  0.051347  0.052663  0.167081  0.073558         0.486511\n",
       "  280  0.051749  0.050562  0.050778  0.194428  0.073478         0.472891,\n",
       "  0.05023529370105537),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  277  0.051456  0.051062  0.052471  0.206258  0.072876         0.494271\n",
       "  278  0.053020  0.051948  0.052464  0.240569  0.073810         0.485185\n",
       "  279  0.051925  0.051347  0.052663  0.167081  0.073558         0.486511\n",
       "  280  0.051749  0.050562  0.050778  0.194428  0.073478         0.472891\n",
       "  281  0.049935  0.049524  0.050895  0.136450  0.071620         0.493608,\n",
       "  0.051874568966280385),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  278  0.053020  0.051948  0.052464  0.240569  0.073810         0.485185\n",
       "  279  0.051925  0.051347  0.052663  0.167081  0.073558         0.486511\n",
       "  280  0.051749  0.050562  0.050778  0.194428  0.073478         0.472891\n",
       "  281  0.049935  0.049524  0.050895  0.136450  0.071620         0.493608\n",
       "  282  0.051749  0.051131  0.052399  0.222494  0.072467         0.499379,\n",
       "  0.05240661824539995),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  279  0.051925  0.051347  0.052663  0.167081  0.073558         0.486511\n",
       "  280  0.051749  0.050562  0.050778  0.194428  0.073478         0.472891\n",
       "  281  0.049935  0.049524  0.050895  0.136450  0.071620         0.493608\n",
       "  282  0.051749  0.051131  0.052399  0.222494  0.072467         0.499379\n",
       "  283  0.051607  0.051749  0.052520  0.222713  0.074068         0.491099,\n",
       "  0.05056602391595813),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  280  0.051749  0.050562  0.050778  0.194428  0.073478         0.472891\n",
       "  281  0.049935  0.049524  0.050895  0.136450  0.071620         0.493608\n",
       "  282  0.051749  0.051131  0.052399  0.222494  0.072467         0.499379\n",
       "  283  0.051607  0.051749  0.052520  0.222713  0.074068         0.491099\n",
       "  284  0.053128  0.052027  0.052023  0.222324  0.074587         0.473357,\n",
       "  0.05049652159300441),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  281  0.049935  0.049524  0.050895  0.136450  0.071620         0.493608\n",
       "  282  0.051749  0.051131  0.052399  0.222494  0.072467         0.499379\n",
       "  283  0.051607  0.051749  0.052520  0.222713  0.074068         0.491099\n",
       "  284  0.053128  0.052027  0.052023  0.222324  0.074587         0.473357\n",
       "  285  0.051381  0.050515  0.051360  0.202479  0.072790         0.486601,\n",
       "  0.0509854338218953),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  282  0.051749  0.051131  0.052399  0.222494  0.072467         0.499379\n",
       "  283  0.051607  0.051749  0.052520  0.222713  0.074068         0.491099\n",
       "  284  0.053128  0.052027  0.052023  0.222324  0.074587         0.473357\n",
       "  285  0.051381  0.050515  0.051360  0.202479  0.072790         0.486601\n",
       "  286  0.051730  0.050415  0.051866  0.194340  0.072722         0.490776,\n",
       "  0.05104294668141275),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  283  0.051607  0.051749  0.052520  0.222713  0.074068         0.491099\n",
       "  284  0.053128  0.052027  0.052023  0.222324  0.074587         0.473357\n",
       "  285  0.051381  0.050515  0.051360  0.202479  0.072790         0.486601\n",
       "  286  0.051730  0.050415  0.051866  0.194340  0.072722         0.490776\n",
       "  287  0.051886  0.050934  0.051975  0.164580  0.073199         0.487551,\n",
       "  0.050482145783715626),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  284  0.053128  0.052027  0.052023  0.222324  0.074587         0.473357\n",
       "  285  0.051381  0.050515  0.051360  0.202479  0.072790         0.486601\n",
       "  286  0.051730  0.050415  0.051866  0.194340  0.072722         0.490776\n",
       "  287  0.051886  0.050934  0.051975  0.164580  0.073199         0.487551\n",
       "  288  0.051530  0.050496  0.051638  0.135439  0.073256         0.482927,\n",
       "  0.049391691575113736),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  285  0.051381  0.050515  0.051360  0.202479  0.072790         0.486601\n",
       "  286  0.051730  0.050415  0.051866  0.194340  0.072722         0.490776\n",
       "  287  0.051886  0.050934  0.051975  0.164580  0.073199         0.487551\n",
       "  288  0.051530  0.050496  0.051638  0.135439  0.073256         0.482927\n",
       "  289  0.050880  0.049982  0.050599  0.207689  0.072708         0.478967,\n",
       "  0.048162232720604395),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  286  0.051730  0.050415  0.051866  0.194340  0.072722         0.490776\n",
       "  287  0.051886  0.050934  0.051975  0.164580  0.073199         0.487551\n",
       "  288  0.051530  0.050496  0.051638  0.135439  0.073256         0.482927\n",
       "  289  0.050880  0.049982  0.050599  0.207689  0.072708         0.478967\n",
       "  290  0.049578  0.048688  0.049180  0.226262  0.071643         0.477927,\n",
       "  0.04731622500408565),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  287  0.051886  0.050934  0.051975  0.164580  0.073199         0.487551\n",
       "  288  0.051530  0.050496  0.051638  0.135439  0.073256         0.482927\n",
       "  289  0.050880  0.049982  0.050599  0.207689  0.072708         0.478967\n",
       "  290  0.049578  0.048688  0.049180  0.226262  0.071643         0.477927\n",
       "  291  0.048808  0.047458  0.047164  0.256946  0.070442         0.480794,\n",
       "  0.047004662534901265),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  288  0.051530  0.050496  0.051638  0.135439  0.073256         0.482927\n",
       "  289  0.050880  0.049982  0.050599  0.207689  0.072708         0.478967\n",
       "  290  0.049578  0.048688  0.049180  0.226262  0.071643         0.477927\n",
       "  291  0.048808  0.047458  0.047164  0.256946  0.070442         0.480794\n",
       "  292  0.048303  0.047038  0.047615  0.207117  0.069616         0.484791,\n",
       "  0.04912805809258759),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  289  0.050880  0.049982  0.050599  0.207689  0.072708         0.478967\n",
       "  290  0.049578  0.048688  0.049180  0.226262  0.071643         0.477927\n",
       "  291  0.048808  0.047458  0.047164  0.256946  0.070442         0.480794\n",
       "  292  0.048303  0.047038  0.047615  0.207117  0.069616         0.484791\n",
       "  293  0.047725  0.048216  0.048104  0.183952  0.069312         0.502999,\n",
       "  0.04871584571365713),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  290  0.049578  0.048688  0.049180  0.226262  0.071643         0.477927\n",
       "  291  0.048808  0.047458  0.047164  0.256946  0.070442         0.480794\n",
       "  292  0.048303  0.047038  0.047615  0.207117  0.069616         0.484791\n",
       "  293  0.047725  0.048216  0.048104  0.183952  0.069312         0.502999\n",
       "  294  0.049521  0.048870  0.050086  0.172060  0.071386         0.484038,\n",
       "  0.0500148068911202),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  291  0.048808  0.047458  0.047164  0.256946  0.070442         0.480794\n",
       "  292  0.048303  0.047038  0.047615  0.207117  0.069616         0.484791\n",
       "  293  0.047725  0.048216  0.048104  0.183952  0.069312         0.502999\n",
       "  294  0.049521  0.048870  0.050086  0.172060  0.071386         0.484038\n",
       "  295  0.049290  0.049323  0.050381  0.170428  0.070983         0.496834,\n",
       "  0.04869666834557644),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  292  0.048303  0.047038  0.047615  0.207117  0.069616         0.484791\n",
       "  293  0.047725  0.048216  0.048104  0.183952  0.069312         0.502999\n",
       "  294  0.049521  0.048870  0.050086  0.172060  0.071386         0.484038\n",
       "  295  0.049290  0.049323  0.050381  0.170428  0.070983         0.496834\n",
       "  296  0.050731  0.049621  0.049715  0.338283  0.072252         0.477264,\n",
       "  0.05749221955028466),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  293  0.047725  0.048216  0.048104  0.183952  0.069312         0.502999\n",
       "  294  0.049521  0.048870  0.050086  0.172060  0.071386         0.484038\n",
       "  295  0.049290  0.049323  0.050381  0.170428  0.070983         0.496834\n",
       "  296  0.050731  0.049621  0.049715  0.338283  0.072252         0.477264\n",
       "  297  0.059349  0.057987  0.057415  0.556724  0.070964         0.552891,\n",
       "  0.0595892498352459),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  294  0.049521  0.048870  0.050086  0.172060  0.071386         0.484038\n",
       "  295  0.049290  0.049323  0.050381  0.170428  0.070983         0.496834\n",
       "  296  0.050731  0.049621  0.049715  0.338283  0.072252         0.477264\n",
       "  297  0.059349  0.057987  0.057415  0.556724  0.070964         0.552891\n",
       "  298  0.058764  0.058667  0.059641  0.229315  0.079554         0.502801,\n",
       "  0.05917224551988584),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  295  0.049290  0.049323  0.050381  0.170428  0.070983         0.496834\n",
       "  296  0.050731  0.049621  0.049715  0.338283  0.072252         0.477264\n",
       "  297  0.059349  0.057987  0.057415  0.556724  0.070964         0.552891\n",
       "  298  0.058764  0.058667  0.059641  0.229315  0.079554         0.502801\n",
       "  299  0.059344  0.058217  0.059803  0.167207  0.081602         0.484002,\n",
       "  0.061216545259396946),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  296  0.050731  0.049621  0.049715  0.338283  0.072252         0.477264\n",
       "  297  0.059349  0.057987  0.057415  0.556724  0.070964         0.552891\n",
       "  298  0.058764  0.058667  0.059641  0.229315  0.079554         0.502801\n",
       "  299  0.059344  0.058217  0.059803  0.167207  0.081602         0.484002\n",
       "  300  0.059465  0.060255  0.060610  0.216564  0.081195         0.502407,\n",
       "  0.06064616048884064),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  297  0.059349  0.057987  0.057415  0.556724  0.070964         0.552891\n",
       "  298  0.058764  0.058667  0.059641  0.229315  0.079554         0.502801\n",
       "  299  0.059344  0.058217  0.059803  0.167207  0.081602         0.484002\n",
       "  300  0.059465  0.060255  0.060610  0.216564  0.081195         0.502407\n",
       "  301  0.061948  0.060513  0.061940  0.124790  0.083191         0.482855,\n",
       "  0.06187082740692039),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  298  0.058764  0.058667  0.059641  0.229315  0.079554         0.502801\n",
       "  299  0.059344  0.058217  0.059803  0.167207  0.081602         0.484002\n",
       "  300  0.059465  0.060255  0.060610  0.216564  0.081195         0.502407\n",
       "  301  0.061948  0.060513  0.061940  0.124790  0.083191         0.482855\n",
       "  302  0.061628  0.060954  0.062749  0.143497  0.082634         0.496278,\n",
       "  0.06521888026032416),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  299  0.059344  0.058217  0.059803  0.167207  0.081602         0.484002\n",
       "  300  0.059465  0.060255  0.060610  0.216564  0.081195         0.502407\n",
       "  301  0.061948  0.060513  0.061940  0.124790  0.083191         0.482855\n",
       "  302  0.061628  0.060954  0.062749  0.143497  0.082634         0.496278\n",
       "  303  0.062824  0.064698  0.063836  0.348771  0.083830         0.512156,\n",
       "  0.0669540327435903),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  300  0.059465  0.060255  0.060610  0.216564  0.081195         0.502407\n",
       "  301  0.061948  0.060513  0.061940  0.124790  0.083191         0.482855\n",
       "  302  0.061628  0.060954  0.062749  0.143497  0.082634         0.496278\n",
       "  303  0.062824  0.064698  0.063836  0.348771  0.083830         0.512156\n",
       "  304  0.065621  0.066866  0.066137  0.397739  0.087100         0.500095,\n",
       "  0.06702593103475882),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  301  0.061948  0.060513  0.061940  0.124790  0.083191         0.482855\n",
       "  302  0.061628  0.060954  0.062749  0.143497  0.082634         0.496278\n",
       "  303  0.062824  0.064698  0.063836  0.348771  0.083830         0.512156\n",
       "  304  0.065621  0.066866  0.066137  0.397739  0.087100         0.500095\n",
       "  305  0.067989  0.067127  0.068114  0.256890  0.088794         0.487658,\n",
       "  0.06852859762229119),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  302  0.061628  0.060954  0.062749  0.143497  0.082634         0.496278\n",
       "  303  0.062824  0.064698  0.063836  0.348771  0.083830         0.512156\n",
       "  304  0.065621  0.066866  0.066137  0.397739  0.087100         0.500095\n",
       "  305  0.067989  0.067127  0.068114  0.256890  0.088794         0.487658\n",
       "  306  0.068160  0.067502  0.069133  0.181074  0.088864         0.498357,\n",
       "  0.06960228005338949),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  303  0.062824  0.064698  0.063836  0.348771  0.083830         0.512156\n",
       "  304  0.065621  0.066866  0.066137  0.397739  0.087100         0.500095\n",
       "  305  0.067989  0.067127  0.068114  0.256890  0.088794         0.487658\n",
       "  306  0.068160  0.067502  0.069133  0.181074  0.088864         0.498357\n",
       "  307  0.069982  0.068736  0.070119  0.214555  0.090332         0.495149,\n",
       "  0.07070471410306535),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  304  0.065621  0.066866  0.066137  0.397739  0.087100         0.500095\n",
       "  305  0.067989  0.067127  0.068114  0.256890  0.088794         0.487658\n",
       "  306  0.068160  0.067502  0.069133  0.181074  0.088864         0.498357\n",
       "  307  0.069982  0.068736  0.070119  0.214555  0.090332         0.495149\n",
       "  308  0.071214  0.070082  0.071742  0.233201  0.091380         0.495364,\n",
       "  0.06947525524855601),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  305  0.067989  0.067127  0.068114  0.256890  0.088794         0.487658\n",
       "  306  0.068160  0.067502  0.069133  0.181074  0.088864         0.498357\n",
       "  307  0.069982  0.068736  0.070119  0.214555  0.090332         0.495149\n",
       "  308  0.071214  0.070082  0.071742  0.233201  0.091380         0.495364\n",
       "  309  0.071009  0.069637  0.070877  0.191998  0.092457         0.477927,\n",
       "  0.06689890622992534),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  306  0.068160  0.067502  0.069133  0.181074  0.088864         0.498357\n",
       "  307  0.069982  0.068736  0.070119  0.214555  0.090332         0.495149\n",
       "  308  0.071214  0.070082  0.071742  0.233201  0.091380         0.495364\n",
       "  309  0.071009  0.069637  0.070877  0.191998  0.092457         0.477927\n",
       "  310  0.069185  0.067907  0.068322  0.254494  0.091256         0.467855,\n",
       "  0.06976284841323012),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  307  0.069982  0.068736  0.070119  0.214555  0.090332         0.495149\n",
       "  308  0.071214  0.070082  0.071742  0.233201  0.091380         0.495364\n",
       "  309  0.071009  0.069637  0.070877  0.191998  0.092457         0.477927\n",
       "  310  0.069185  0.067907  0.068322  0.254494  0.091256         0.467855\n",
       "  311  0.067185  0.068632  0.068177  0.221945  0.088740         0.508536,\n",
       "  0.0695950921487451),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  308  0.071214  0.070082  0.071742  0.233201  0.091380         0.495364\n",
       "  309  0.071009  0.069637  0.070877  0.191998  0.092457         0.477927\n",
       "  310  0.069185  0.067907  0.068322  0.254494  0.091256         0.467855\n",
       "  311  0.067185  0.068632  0.068177  0.221945  0.088740         0.508536\n",
       "  312  0.071219  0.069947  0.070943  0.171722  0.091537         0.485866,\n",
       "  0.07116965702744599),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  309  0.071009  0.069637  0.070877  0.191998  0.092457         0.477927\n",
       "  310  0.069185  0.067907  0.068322  0.254494  0.091256         0.467855\n",
       "  311  0.067185  0.068632  0.068177  0.221945  0.088740         0.508536\n",
       "  312  0.071219  0.069947  0.070943  0.171722  0.091537         0.485866\n",
       "  313  0.070497  0.070260  0.071592  0.189818  0.091373         0.498895,\n",
       "  0.0701702784780933),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  310  0.069185  0.067907  0.068322  0.254494  0.091256         0.467855\n",
       "  311  0.067185  0.068632  0.068177  0.221945  0.088740         0.508536\n",
       "  312  0.071219  0.069947  0.070943  0.171722  0.091537         0.485866\n",
       "  313  0.070497  0.070260  0.071592  0.189818  0.091373         0.498895\n",
       "  314  0.071040  0.069971  0.071451  0.209691  0.092911         0.479648,\n",
       "  0.07145245825569049),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  311  0.067185  0.068632  0.068177  0.221945  0.088740         0.508536\n",
       "  312  0.071219  0.069947  0.070943  0.171722  0.091537         0.485866\n",
       "  313  0.070497  0.070260  0.071592  0.189818  0.091373         0.498895\n",
       "  314  0.071040  0.069971  0.071451  0.209691  0.092911         0.479648\n",
       "  315  0.071478  0.070409  0.072217  0.211400  0.091935         0.496708,\n",
       "  0.07272265819221368),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  312  0.071219  0.069947  0.070943  0.171722  0.091537         0.485866\n",
       "  313  0.070497  0.070260  0.071592  0.189818  0.091373         0.498895\n",
       "  314  0.071040  0.069971  0.071451  0.209691  0.092911         0.479648\n",
       "  315  0.071478  0.070409  0.072217  0.211400  0.091935         0.496708\n",
       "  316  0.072826  0.071585  0.073530  0.223976  0.093187         0.496619,\n",
       "  0.071946162421011),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  313  0.070497  0.070260  0.071592  0.189818  0.091373         0.498895\n",
       "  314  0.071040  0.069971  0.071451  0.209691  0.092911         0.479648\n",
       "  315  0.071478  0.070409  0.072217  0.211400  0.091935         0.496708\n",
       "  316  0.072826  0.071585  0.073530  0.223976  0.093187         0.496619\n",
       "  317  0.073233  0.071748  0.073607  0.170790  0.094428         0.481314,\n",
       "  0.07413906030165104),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  314  0.071040  0.069971  0.071451  0.209691  0.092911         0.479648\n",
       "  315  0.071478  0.070409  0.072217  0.211400  0.091935         0.496708\n",
       "  316  0.072826  0.071585  0.073530  0.223976  0.093187         0.496619\n",
       "  317  0.073233  0.071748  0.073607  0.170790  0.094428         0.481314\n",
       "  318  0.072193  0.073047  0.072067  0.251288  0.093670         0.503518,\n",
       "  0.07584304595340022),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  315  0.071478  0.070409  0.072217  0.211400  0.091935         0.496708\n",
       "  316  0.072826  0.071585  0.073530  0.223976  0.093187         0.496619\n",
       "  317  0.073233  0.071748  0.073607  0.170790  0.094428         0.481314\n",
       "  318  0.072193  0.073047  0.072067  0.251288  0.093670         0.503518\n",
       "  319  0.074475  0.074784  0.075153  0.233311  0.095811         0.499862,\n",
       "  0.07737447378187244),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  316  0.072826  0.071585  0.073530  0.223976  0.093187         0.496619\n",
       "  317  0.073233  0.071748  0.073607  0.170790  0.094428         0.481314\n",
       "  318  0.072193  0.073047  0.072067  0.251288  0.093670         0.503518\n",
       "  319  0.074475  0.074784  0.075153  0.233311  0.095811         0.499862\n",
       "  320  0.076494  0.076540  0.077466  0.245205  0.097475         0.498572,\n",
       "  0.07875492674572553),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  317  0.073233  0.071748  0.073607  0.170790  0.094428         0.481314\n",
       "  318  0.072193  0.073047  0.072067  0.251288  0.093670         0.503518\n",
       "  319  0.074475  0.074784  0.075153  0.233311  0.095811         0.499862\n",
       "  320  0.076494  0.076540  0.077466  0.245205  0.097475         0.498572\n",
       "  321  0.078597  0.077495  0.079772  0.117631  0.098971         0.497443,\n",
       "  0.0775230719230014),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  318  0.072193  0.073047  0.072067  0.251288  0.093670         0.503518\n",
       "  319  0.074475  0.074784  0.075153  0.233311  0.095811         0.499862\n",
       "  320  0.076494  0.076540  0.077466  0.245205  0.097475         0.498572\n",
       "  321  0.078597  0.077495  0.079772  0.117631  0.098971         0.497443\n",
       "  322  0.079569  0.078085  0.079079  0.267665  0.100319         0.477909,\n",
       "  0.07274423152850917),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  319  0.074475  0.074784  0.075153  0.233311  0.095811         0.499862\n",
       "  320  0.076494  0.076540  0.077466  0.245205  0.097475         0.498572\n",
       "  321  0.078597  0.077495  0.079772  0.117631  0.098971         0.497443\n",
       "  322  0.079569  0.078085  0.079079  0.267665  0.100319         0.477909\n",
       "  323  0.078280  0.076938  0.074162  0.522754  0.099116         0.451386,\n",
       "  0.07307256577519713),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  320  0.076494  0.076540  0.077466  0.245205  0.097475         0.498572\n",
       "  321  0.078597  0.077495  0.079772  0.117631  0.098971         0.497443\n",
       "  322  0.079569  0.078085  0.079079  0.267665  0.100319         0.477909\n",
       "  323  0.078280  0.076938  0.074162  0.522754  0.099116         0.451386\n",
       "  324  0.073418  0.072684  0.072568  0.379266  0.094449         0.489576,\n",
       "  0.07527265156048159),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  321  0.078597  0.077495  0.079772  0.117631  0.098971         0.497443\n",
       "  322  0.079569  0.078085  0.079079  0.267665  0.100319         0.477909\n",
       "  323  0.078280  0.076938  0.074162  0.522754  0.099116         0.451386\n",
       "  324  0.073418  0.072684  0.072568  0.379266  0.094449         0.489576\n",
       "  325  0.074607  0.074341  0.075652  0.236933  0.094770         0.503572,\n",
       "  0.0761378270227187),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  322  0.079569  0.078085  0.079079  0.267665  0.100319         0.477909\n",
       "  323  0.078280  0.076938  0.074162  0.522754  0.099116         0.451386\n",
       "  324  0.073418  0.072684  0.072568  0.379266  0.094449         0.489576\n",
       "  325  0.074607  0.074341  0.075652  0.236933  0.094770         0.503572\n",
       "  326  0.076470  0.075310  0.077001  0.183387  0.096918         0.493590,\n",
       "  0.0732978445215619),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  323  0.078280  0.076938  0.074162  0.522754  0.099116         0.451386\n",
       "  324  0.073418  0.072684  0.072568  0.379266  0.094449         0.489576\n",
       "  325  0.074607  0.074341  0.075652  0.236933  0.094770         0.503572\n",
       "  326  0.076470  0.075310  0.077001  0.183387  0.096918         0.493590\n",
       "  327  0.076484  0.074829  0.074680  0.250177  0.097763         0.465884,\n",
       "  0.07298389570652504),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  324  0.073418  0.072684  0.072568  0.379266  0.094449         0.489576\n",
       "  325  0.074607  0.074341  0.075652  0.236933  0.094770         0.503572\n",
       "  326  0.076470  0.075310  0.077001  0.183387  0.096918         0.493590\n",
       "  327  0.076484  0.074829  0.074680  0.250177  0.097763         0.465884\n",
       "  328  0.074487  0.074571  0.074055  0.367802  0.094990         0.484773,\n",
       "  0.07290720547892691),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  325  0.074607  0.074341  0.075652  0.236933  0.094770         0.503572\n",
       "  326  0.076470  0.075310  0.077001  0.183387  0.096918         0.493590\n",
       "  327  0.076484  0.074829  0.074680  0.250177  0.097763         0.465884\n",
       "  328  0.074487  0.074571  0.074055  0.367802  0.094990         0.484773\n",
       "  329  0.073875  0.072270  0.073404  0.283658  0.094683         0.486547,\n",
       "  0.07444821718025832),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  326  0.076470  0.075310  0.077001  0.183387  0.096918         0.493590\n",
       "  327  0.076484  0.074829  0.074680  0.250177  0.097763         0.465884\n",
       "  328  0.074487  0.074571  0.074055  0.367802  0.094990         0.484773\n",
       "  329  0.073875  0.072270  0.073404  0.283658  0.094683         0.486547\n",
       "  330  0.073668  0.073206  0.074283  0.216625  0.094608         0.498644,\n",
       "  0.07410071518785198),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  327  0.076484  0.074829  0.074680  0.250177  0.097763         0.465884\n",
       "  328  0.074487  0.074571  0.074055  0.367802  0.094990         0.484773\n",
       "  329  0.073875  0.072270  0.073404  0.283658  0.094683         0.486547\n",
       "  330  0.073668  0.073206  0.074283  0.216625  0.094608         0.498644\n",
       "  331  0.076003  0.074421  0.075717  0.185817  0.096113         0.484522,\n",
       "  0.07491795204106856),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  328  0.074487  0.074571  0.074055  0.367802  0.094990         0.484773\n",
       "  329  0.073875  0.072270  0.073404  0.283658  0.094683         0.486547\n",
       "  330  0.073668  0.073206  0.074283  0.216625  0.094608         0.498644\n",
       "  331  0.076003  0.074421  0.075717  0.185817  0.096113         0.484522\n",
       "  332  0.075914  0.074286  0.076054  0.168954  0.095774         0.493232,\n",
       "  0.07608750206784566),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  329  0.073875  0.072270  0.073404  0.283658  0.094683         0.486547\n",
       "  330  0.073668  0.073206  0.074283  0.216625  0.094608         0.498644\n",
       "  331  0.076003  0.074421  0.075717  0.185817  0.096113         0.484522\n",
       "  332  0.075914  0.074286  0.076054  0.168954  0.095774         0.493232\n",
       "  333  0.075401  0.074947  0.076463  0.198475  0.096572         0.495866,\n",
       "  0.07643979599668159),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  330  0.073668  0.073206  0.074283  0.216625  0.094608         0.498644\n",
       "  331  0.076003  0.074421  0.075717  0.185817  0.096113         0.484522\n",
       "  332  0.075914  0.074286  0.076054  0.168954  0.095774         0.493232\n",
       "  333  0.075401  0.074947  0.076463  0.198475  0.096572         0.495866\n",
       "  334  0.076494  0.075357  0.077395  0.161178  0.097714         0.489755,\n",
       "  0.07730017952248912),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  331  0.076003  0.074421  0.075717  0.185817  0.096113         0.484522\n",
       "  332  0.075914  0.074286  0.076054  0.168954  0.095774         0.493232\n",
       "  333  0.075401  0.074947  0.076463  0.198475  0.096572         0.495866\n",
       "  334  0.076494  0.075357  0.077395  0.161178  0.097714         0.489755\n",
       "  335  0.076992  0.076166  0.077640  0.146958  0.098058         0.493554,\n",
       "  0.07912160610621255),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  332  0.075914  0.074286  0.076054  0.168954  0.095774         0.493232\n",
       "  333  0.075401  0.074947  0.076463  0.198475  0.096572         0.495866\n",
       "  334  0.076494  0.075357  0.077395  0.161178  0.097714         0.489755\n",
       "  335  0.076992  0.076166  0.077640  0.146958  0.098058         0.493554\n",
       "  336  0.078491  0.078384  0.079154  0.397117  0.098898         0.500741,\n",
       "  0.0777914877195948),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  333  0.075401  0.074947  0.076463  0.198475  0.096572         0.495866\n",
       "  334  0.076494  0.075357  0.077395  0.161178  0.097714         0.489755\n",
       "  335  0.076992  0.076166  0.077640  0.146958  0.098058         0.493554\n",
       "  336  0.078491  0.078384  0.079154  0.397117  0.098898         0.500741\n",
       "  337  0.080142  0.081633  0.078515  0.533481  0.100677         0.477174,\n",
       "  0.07902334254231896),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  334  0.076494  0.075357  0.077395  0.161178  0.097714         0.489755\n",
       "  335  0.076992  0.076166  0.077640  0.146958  0.098058         0.493554\n",
       "  336  0.078491  0.078384  0.079154  0.397117  0.098898         0.500741\n",
       "  337  0.080142  0.081633  0.078515  0.533481  0.100677         0.477174\n",
       "  338  0.079098  0.078313  0.079619  0.245198  0.099378         0.496332,\n",
       "  0.07820609606674009),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  335  0.076992  0.076166  0.077640  0.146958  0.098058         0.493554\n",
       "  336  0.078491  0.078384  0.079154  0.397117  0.098898         0.500741\n",
       "  337  0.080142  0.081633  0.078515  0.533481  0.100677         0.477174\n",
       "  338  0.079098  0.078313  0.079619  0.245198  0.099378         0.496332\n",
       "  339  0.080467  0.079417  0.078660  0.272858  0.100581         0.481009,\n",
       "  0.0795745595671569),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  336  0.078491  0.078384  0.079154  0.397117  0.098898         0.500741\n",
       "  337  0.080142  0.081633  0.078515  0.533481  0.100677         0.477174\n",
       "  338  0.079098  0.078313  0.079619  0.245198  0.099378         0.496332\n",
       "  339  0.080467  0.079417  0.078660  0.272858  0.100581         0.481009\n",
       "  340  0.080039  0.078469  0.079924  0.183463  0.099783         0.497353,\n",
       "  0.07930853973877826),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  337  0.080142  0.081633  0.078515  0.533481  0.100677         0.477174\n",
       "  338  0.079098  0.078313  0.079619  0.245198  0.099378         0.496332\n",
       "  339  0.080467  0.079417  0.078660  0.272858  0.100581         0.481009\n",
       "  340  0.080039  0.078469  0.079924  0.183463  0.099783         0.497353\n",
       "  341  0.080130  0.078384  0.080615  0.111672  0.101119         0.485131,\n",
       "  0.077801071592454),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  338  0.079098  0.078313  0.079619  0.245198  0.099378         0.496332\n",
       "  339  0.080467  0.079417  0.078660  0.272858  0.100581         0.481009\n",
       "  340  0.080039  0.078469  0.079924  0.183463  0.099783         0.497353\n",
       "  341  0.080130  0.078384  0.080615  0.111672  0.101119         0.485131\n",
       "  342  0.080060  0.078232  0.079156  0.162939  0.100860         0.475848,\n",
       "  0.0782923797895597),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  339  0.080467  0.079417  0.078660  0.272858  0.100581         0.481009\n",
       "  340  0.080039  0.078469  0.079924  0.183463  0.099783         0.497353\n",
       "  341  0.080130  0.078384  0.080615  0.111672  0.101119         0.485131\n",
       "  342  0.080060  0.078232  0.079156  0.162939  0.100860         0.475848\n",
       "  343  0.078251  0.077265  0.078793  0.173035  0.099387         0.490794,\n",
       "  0.0767249931931409),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  340  0.080039  0.078469  0.079924  0.183463  0.099783         0.497353\n",
       "  341  0.080130  0.078384  0.080615  0.111672  0.101119         0.485131\n",
       "  342  0.080060  0.078232  0.079156  0.162939  0.100860         0.475848\n",
       "  343  0.078251  0.077265  0.078793  0.173035  0.099387         0.490794\n",
       "  344  0.079127  0.077490  0.078287  0.168849  0.099867         0.475400,\n",
       "  0.0754571892248325),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  341  0.080130  0.078384  0.080615  0.111672  0.101119         0.485131\n",
       "  342  0.080060  0.078232  0.079156  0.162939  0.100860         0.475848\n",
       "  343  0.078251  0.077265  0.078793  0.173035  0.099387         0.490794\n",
       "  344  0.079127  0.077490  0.078287  0.168849  0.099867         0.475400\n",
       "  345  0.076549  0.074997  0.076972  0.184448  0.098336         0.477640,\n",
       "  0.08033908511964791),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  342  0.080060  0.078232  0.079156  0.162939  0.100860         0.475848\n",
       "  343  0.078251  0.077265  0.078793  0.173035  0.099387         0.490794\n",
       "  344  0.079127  0.077490  0.078287  0.168849  0.099867         0.475400\n",
       "  345  0.076549  0.074997  0.076972  0.184448  0.098336         0.477640\n",
       "  346  0.077813  0.079135  0.078057  0.319056  0.097098         0.523626,\n",
       "  0.08273808437857207),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  343  0.078251  0.077265  0.078793  0.173035  0.099387         0.490794\n",
       "  344  0.079127  0.077490  0.078287  0.168849  0.099867         0.475400\n",
       "  345  0.076549  0.074997  0.076972  0.184448  0.098336         0.477640\n",
       "  346  0.077813  0.079135  0.078057  0.319056  0.097098         0.523626\n",
       "  347  0.082958  0.082284  0.083272  0.371721  0.101866         0.505059,\n",
       "  0.08417605020194259),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  344  0.079127  0.077490  0.078287  0.168849  0.099867         0.475400\n",
       "  345  0.076549  0.074997  0.076972  0.184448  0.098336         0.477640\n",
       "  346  0.077813  0.079135  0.078057  0.319056  0.097098         0.523626\n",
       "  347  0.082958  0.082284  0.083272  0.371721  0.101866         0.505059\n",
       "  348  0.083463  0.082898  0.083696  0.262798  0.104209         0.497873,\n",
       "  0.08763196011446146),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  345  0.076549  0.074997  0.076972  0.184448  0.098336         0.477640\n",
       "  346  0.077813  0.079135  0.078057  0.319056  0.097098         0.523626\n",
       "  347  0.082958  0.082284  0.083272  0.371721  0.101866         0.505059\n",
       "  348  0.083463  0.082898  0.083696  0.262798  0.104209         0.497873\n",
       "  349  0.086079  0.087389  0.086539  0.431811  0.105613         0.512963,\n",
       "  0.08792913715199475),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  346  0.077813  0.079135  0.078057  0.319056  0.097098         0.523626\n",
       "  347  0.082958  0.082284  0.083272  0.371721  0.101866         0.505059\n",
       "  348  0.083463  0.082898  0.083696  0.262798  0.104209         0.497873\n",
       "  349  0.086079  0.087389  0.086539  0.431811  0.105613         0.512963\n",
       "  350  0.088375  0.088076  0.088404  0.311046  0.108988         0.489343,\n",
       "  0.08861456688631054),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  347  0.082958  0.082284  0.083272  0.371721  0.101866         0.505059\n",
       "  348  0.083463  0.082898  0.083696  0.262798  0.104209         0.497873\n",
       "  349  0.086079  0.087389  0.086539  0.431811  0.105613         0.512963\n",
       "  350  0.088375  0.088076  0.088404  0.311046  0.108988         0.489343\n",
       "  351  0.087896  0.087330  0.088671  0.221176  0.109278         0.492246,\n",
       "  0.0890627284108253),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  348  0.083463  0.082898  0.083696  0.262798  0.104209         0.497873\n",
       "  349  0.086079  0.087389  0.086539  0.431811  0.105613         0.512963\n",
       "  350  0.088375  0.088076  0.088404  0.311046  0.108988         0.489343\n",
       "  351  0.087896  0.087330  0.088671  0.221176  0.109278         0.492246\n",
       "  352  0.089545  0.088481  0.090400  0.218994  0.109948         0.490472,\n",
       "  0.08714544385378535),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  349  0.086079  0.087389  0.086539  0.431811  0.105613         0.512963\n",
       "  350  0.088375  0.088076  0.088404  0.311046  0.108988         0.489343\n",
       "  351  0.087896  0.087330  0.088671  0.221176  0.109278         0.492246\n",
       "  352  0.089545  0.088481  0.090400  0.218994  0.109948         0.490472\n",
       "  353  0.090134  0.088453  0.088540  0.246185  0.110385         0.472784,\n",
       "  0.08777336072858373),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  350  0.088375  0.088076  0.088404  0.311046  0.108988         0.489343\n",
       "  351  0.087896  0.087330  0.088671  0.221176  0.109278         0.492246\n",
       "  352  0.089545  0.088481  0.090400  0.218994  0.109948         0.490472\n",
       "  353  0.090134  0.088453  0.088540  0.246185  0.110385         0.472784\n",
       "  354  0.087870  0.086533  0.088567  0.186138  0.108513         0.491816,\n",
       "  0.08797946210686781),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  351  0.087896  0.087330  0.088671  0.221176  0.109278         0.492246\n",
       "  352  0.089545  0.088481  0.090400  0.218994  0.109948         0.490472\n",
       "  353  0.090134  0.088453  0.088540  0.246185  0.110385         0.472784\n",
       "  354  0.087870  0.086533  0.088567  0.186138  0.108513         0.491816\n",
       "  355  0.087569  0.087247  0.088789  0.201065  0.109126         0.488662,\n",
       "  0.08265899818275915),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  352  0.089545  0.088481  0.090400  0.218994  0.109948         0.490472\n",
       "  353  0.090134  0.088453  0.088540  0.246185  0.110385         0.472784\n",
       "  354  0.087870  0.086533  0.088567  0.186138  0.108513         0.491816\n",
       "  355  0.087569  0.087247  0.088789  0.201065  0.109126         0.488662\n",
       "  356  0.083776  0.084275  0.084119  0.498185  0.109328         0.447336,\n",
       "  0.08063147022075161),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  353  0.090134  0.088453  0.088540  0.246185  0.110385         0.472784\n",
       "  354  0.087870  0.086533  0.088567  0.186138  0.108513         0.491816\n",
       "  355  0.087569  0.087247  0.088789  0.201065  0.109126         0.488662\n",
       "  356  0.083776  0.084275  0.084119  0.498185  0.109328         0.447336\n",
       "  357  0.084708  0.083358  0.081637  0.353500  0.104132         0.471959,\n",
       "  0.07176641669308968),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  354  0.087870  0.086533  0.088567  0.186138  0.108513         0.491816\n",
       "  355  0.087569  0.087247  0.088789  0.201065  0.109126         0.488662\n",
       "  356  0.083776  0.084275  0.084119  0.498185  0.109328         0.447336\n",
       "  357  0.084708  0.083358  0.081637  0.353500  0.104132         0.471959\n",
       "  358  0.081707  0.080168  0.072369  1.000000  0.102152         0.420831,\n",
       "  0.07848650132676979),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  355  0.087569  0.087247  0.088789  0.201065  0.109126         0.488662\n",
       "  356  0.083776  0.084275  0.084119  0.498185  0.109328         0.447336\n",
       "  357  0.084708  0.083358  0.081637  0.353500  0.104132         0.471959\n",
       "  358  0.081707  0.080168  0.072369  1.000000  0.102152         0.420831\n",
       "  359  0.074169  0.077410  0.075031  0.553051  0.093494         0.537371,\n",
       "  0.0822084310676673),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  356  0.083776  0.084275  0.084119  0.498185  0.109328         0.447336\n",
       "  357  0.084708  0.083358  0.081637  0.353500  0.104132         0.471959\n",
       "  358  0.081707  0.080168  0.072369  1.000000  0.102152         0.420831\n",
       "  359  0.074169  0.077410  0.075031  0.553051  0.093494         0.537371\n",
       "  360  0.081064  0.081334  0.081995  0.376055  0.100057         0.514952,\n",
       "  0.07980463987231357),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  357  0.084708  0.083358  0.081637  0.353500  0.104132         0.471959\n",
       "  358  0.081707  0.080168  0.072369  1.000000  0.102152         0.420831\n",
       "  359  0.074169  0.077410  0.075031  0.553051  0.093494         0.537371\n",
       "  360  0.081064  0.081334  0.081995  0.376055  0.100057         0.514952\n",
       "  361  0.084729  0.083533  0.080721  0.455719  0.103692         0.469146,\n",
       "  0.08010900481449124),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  358  0.081707  0.080168  0.072369  1.000000  0.102152         0.420831\n",
       "  359  0.074169  0.077410  0.075031  0.553051  0.093494         0.537371\n",
       "  360  0.081064  0.081334  0.081995  0.376055  0.100057         0.514952\n",
       "  361  0.084729  0.083533  0.080721  0.455719  0.103692         0.469146\n",
       "  362  0.081909  0.080159  0.079350  0.314310  0.101344         0.489396,\n",
       "  0.07992207118192553),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  359  0.074169  0.077410  0.075031  0.553051  0.093494         0.537371\n",
       "  360  0.081064  0.081334  0.081995  0.376055  0.100057         0.514952\n",
       "  361  0.084729  0.083533  0.080721  0.455719  0.103692         0.469146\n",
       "  362  0.081909  0.080159  0.079350  0.314310  0.101344         0.489396\n",
       "  363  0.080816  0.079739  0.080663  0.205490  0.101641         0.485723,\n",
       "  0.07832353699871437),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  360  0.081064  0.081334  0.081995  0.376055  0.100057         0.514952\n",
       "  361  0.084729  0.083533  0.080721  0.455719  0.103692         0.469146\n",
       "  362  0.081909  0.080159  0.079350  0.314310  0.101344         0.489396\n",
       "  363  0.080816  0.079739  0.080663  0.205490  0.101641         0.485723\n",
       "  364  0.079427  0.078568  0.079699  0.208815  0.101459         0.475167,\n",
       "  0.07972314808592354),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  361  0.084729  0.083533  0.080721  0.455719  0.103692         0.469146\n",
       "  362  0.081909  0.080159  0.079350  0.314310  0.101344         0.489396\n",
       "  363  0.080816  0.079739  0.080663  0.205490  0.101641         0.485723\n",
       "  364  0.079427  0.078568  0.079699  0.208815  0.101459         0.475167\n",
       "  365  0.079750  0.080066  0.079449  0.536623  0.099898         0.497586,\n",
       "  0.07232242565435719),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  362  0.081909  0.080159  0.079350  0.314310  0.101344         0.489396\n",
       "  363  0.080816  0.079739  0.080663  0.205490  0.101641         0.485723\n",
       "  364  0.079427  0.078568  0.079699  0.208815  0.101459         0.475167\n",
       "  365  0.079750  0.080066  0.079449  0.536623  0.099898         0.497586\n",
       "  366  0.069753  0.071156  0.070621  0.659614  0.101264         0.431780,\n",
       "  0.070946774249296),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  363  0.080816  0.079739  0.080663  0.205490  0.101641         0.485723\n",
       "  364  0.079427  0.078568  0.079699  0.208815  0.101459         0.475167\n",
       "  365  0.079750  0.080066  0.079449  0.536623  0.099898         0.497586\n",
       "  366  0.069753  0.071156  0.070621  0.659614  0.101264         0.431780\n",
       "  367  0.073312  0.072222  0.072670  0.287107  0.094037         0.476834,\n",
       "  0.06747409255927356),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  364  0.079427  0.078568  0.079699  0.208815  0.101459         0.475167\n",
       "  365  0.079750  0.080066  0.079449  0.536623  0.099898         0.497586\n",
       "  366  0.069753  0.071156  0.070621  0.659614  0.101264         0.431780\n",
       "  367  0.073312  0.072222  0.072670  0.287107  0.094037         0.476834\n",
       "  368  0.070858  0.069236  0.067000  0.444579  0.092694         0.461153,\n",
       "  0.06832489221222189),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  365  0.079750  0.080066  0.079449  0.536623  0.099898         0.497586\n",
       "  366  0.069753  0.071156  0.070621  0.659614  0.101264         0.431780\n",
       "  367  0.073312  0.072222  0.072670  0.287107  0.094037         0.476834\n",
       "  368  0.070858  0.069236  0.067000  0.444579  0.092694         0.461153\n",
       "  369  0.068858  0.068288  0.068693  0.217365  0.089302         0.493483,\n",
       "  0.0642075122475352),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  366  0.069753  0.071156  0.070621  0.659614  0.101264         0.431780\n",
       "  367  0.073312  0.072222  0.072670  0.287107  0.094037         0.476834\n",
       "  368  0.070858  0.069236  0.067000  0.444579  0.092694         0.461153\n",
       "  369  0.068858  0.068288  0.068693  0.217365  0.089302         0.493483\n",
       "  370  0.068299  0.066819  0.064766  0.404406  0.090133         0.456332,\n",
       "  0.06448552153935011),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  367  0.073312  0.072222  0.072670  0.287107  0.094037         0.476834\n",
       "  368  0.070858  0.069236  0.067000  0.444579  0.092694         0.461153\n",
       "  369  0.068858  0.068288  0.068693  0.217365  0.089302         0.493483\n",
       "  370  0.068299  0.066819  0.064766  0.404406  0.090133         0.456332\n",
       "  371  0.064807  0.063736  0.062664  0.505929  0.086112         0.489199,\n",
       "  0.062014623989257445),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  368  0.070858  0.069236  0.067000  0.444579  0.092694         0.461153\n",
       "  369  0.068858  0.068288  0.068693  0.217365  0.089302         0.493483\n",
       "  370  0.068299  0.066819  0.064766  0.404406  0.090133         0.456332\n",
       "  371  0.064807  0.063736  0.062664  0.505929  0.086112         0.489199\n",
       "  372  0.065462  0.064615  0.063013  0.289654  0.086384         0.468644,\n",
       "  0.06293491634279719),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  369  0.068858  0.068288  0.068693  0.217365  0.089302         0.493483\n",
       "  370  0.068299  0.066819  0.064766  0.404406  0.090133         0.456332\n",
       "  371  0.064807  0.063736  0.062664  0.505929  0.086112         0.489199\n",
       "  372  0.065462  0.064615  0.063013  0.289654  0.086384         0.468644\n",
       "  373  0.063236  0.062269  0.062293  0.370191  0.083970         0.494002,\n",
       "  0.05888225638699694),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  370  0.068299  0.066819  0.064766  0.404406  0.090133         0.456332\n",
       "  371  0.064807  0.063736  0.062664  0.505929  0.086112         0.489199\n",
       "  372  0.065462  0.064615  0.063013  0.289654  0.086384         0.468644\n",
       "  373  0.063236  0.062269  0.062293  0.370191  0.083970         0.494002\n",
       "  374  0.059551  0.058975  0.059568  0.479507  0.084869         0.456816,\n",
       "  0.05831186199407831),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  371  0.064807  0.063736  0.062664  0.505929  0.086112         0.489199\n",
       "  372  0.065462  0.064615  0.063013  0.289654  0.086384         0.468644\n",
       "  373  0.063236  0.062269  0.062293  0.370191  0.083970         0.494002\n",
       "  374  0.059551  0.058975  0.059568  0.479507  0.084869         0.456816\n",
       "  375  0.059236  0.059233  0.059692  0.356338  0.080911         0.482855,\n",
       "  0.058086583247713555),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  372  0.065462  0.064615  0.063013  0.289654  0.086384         0.468644\n",
       "  373  0.063236  0.062269  0.062293  0.370191  0.083970         0.494002\n",
       "  374  0.059551  0.058975  0.059568  0.479507  0.084869         0.456816\n",
       "  375  0.059236  0.059233  0.059692  0.356338  0.080911         0.482855\n",
       "  376  0.058259  0.057885  0.058585  0.314818  0.080354         0.485436,\n",
       "  0.06385761428691406),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  373  0.063236  0.062269  0.062293  0.370191  0.083970         0.494002\n",
       "  374  0.059551  0.058975  0.059568  0.479507  0.084869         0.456816\n",
       "  375  0.059236  0.059233  0.059692  0.356338  0.080911         0.482855\n",
       "  376  0.058259  0.057885  0.058585  0.314818  0.080354         0.485436\n",
       "  377  0.059318  0.062861  0.060198  0.518368  0.080134         0.530274,\n",
       "  0.06440643534353718),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  374  0.059551  0.058975  0.059568  0.479507  0.084869         0.456816\n",
       "  375  0.059236  0.059233  0.059692  0.356338  0.080911         0.482855\n",
       "  376  0.058259  0.057885  0.058585  0.314818  0.080354         0.485436\n",
       "  377  0.059318  0.062861  0.060198  0.518368  0.080134         0.530274\n",
       "  378  0.065139  0.064080  0.064832  0.348219  0.085770         0.491224,\n",
       "  0.06388877149606874),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  375  0.059236  0.059233  0.059692  0.356338  0.080911         0.482855\n",
       "  376  0.058259  0.057885  0.058585  0.314818  0.080354         0.485436\n",
       "  377  0.059318  0.062861  0.060198  0.518368  0.080134         0.530274\n",
       "  378  0.065139  0.064080  0.064832  0.348219  0.085770         0.491224\n",
       "  379  0.064316  0.064411  0.065193  0.211177  0.086306         0.483250,\n",
       "  0.06362513801354258),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  376  0.058259  0.057885  0.058585  0.314818  0.080354         0.485436\n",
       "  377  0.059318  0.062861  0.060198  0.518368  0.080134         0.530274\n",
       "  378  0.065139  0.064080  0.064832  0.348219  0.085770         0.491224\n",
       "  379  0.064316  0.064411  0.065193  0.211177  0.086306         0.483250\n",
       "  380  0.064487  0.063324  0.064890  0.157427  0.085801         0.485149,\n",
       "  0.06664007430619111),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  377  0.059318  0.062861  0.060198  0.518368  0.080134         0.530274\n",
       "  378  0.065139  0.064080  0.064832  0.348219  0.085770         0.491224\n",
       "  379  0.064316  0.064411  0.065193  0.211177  0.086306         0.483250\n",
       "  380  0.064487  0.063324  0.064890  0.157427  0.085801         0.485149\n",
       "  381  0.064117  0.066236  0.065260  0.305201  0.085543         0.509665,\n",
       "  0.06647949632398818),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  378  0.065139  0.064080  0.064832  0.348219  0.085770         0.491224\n",
       "  379  0.064316  0.064411  0.065193  0.211177  0.086306         0.483250\n",
       "  380  0.064487  0.063324  0.064890  0.157427  0.085801         0.485149\n",
       "  381  0.064117  0.066236  0.065260  0.305201  0.085543         0.509665\n",
       "  382  0.066930  0.065959  0.067222  0.157624  0.088488         0.485920,\n",
       "  0.06959030021231549),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  379  0.064316  0.064411  0.065193  0.211177  0.086306         0.483250\n",
       "  380  0.064487  0.063324  0.064890  0.157427  0.085801         0.485149\n",
       "  381  0.064117  0.066236  0.065260  0.305201  0.085543         0.509665\n",
       "  382  0.066930  0.065959  0.067222  0.157624  0.088488         0.485920\n",
       "  383  0.067885  0.068715  0.068867  0.248299  0.088331         0.510382,\n",
       "  0.06293731231101199),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  380  0.064487  0.063324  0.064890  0.157427  0.085801         0.485149\n",
       "  381  0.064117  0.066236  0.065260  0.305201  0.085543         0.509665\n",
       "  382  0.066930  0.065959  0.067222  0.157624  0.088488         0.485920\n",
       "  383  0.067885  0.068715  0.068867  0.248299  0.088331         0.510382\n",
       "  384  0.070756  0.070099  0.058750  0.959159  0.091369         0.437372,\n",
       "  0.06345977771727236),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  381  0.064117  0.066236  0.065260  0.305201  0.085543         0.509665\n",
       "  382  0.066930  0.065959  0.067222  0.157624  0.088488         0.485920\n",
       "  383  0.067885  0.068715  0.068867  0.248299  0.088331         0.510382\n",
       "  384  0.070756  0.070099  0.058750  0.959159  0.091369         0.437372\n",
       "  385  0.064827  0.063442  0.064270  0.293273  0.084872         0.491027,\n",
       "  0.06625182160940861),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  382  0.066930  0.065959  0.067222  0.157624  0.088488         0.485920\n",
       "  383  0.067885  0.068715  0.068867  0.248299  0.088331         0.510382\n",
       "  384  0.070756  0.070099  0.058750  0.959159  0.091369         0.437372\n",
       "  385  0.064827  0.063442  0.064270  0.293273  0.084872         0.491027\n",
       "  386  0.063797  0.066203  0.064488  0.445758  0.085382         0.507999,\n",
       "  0.06666643957891619),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  383  0.067885  0.068715  0.068867  0.248299  0.088331         0.510382\n",
       "  384  0.070756  0.070099  0.058750  0.959159  0.091369         0.437372\n",
       "  385  0.064827  0.063442  0.064270  0.293273  0.084872         0.491027\n",
       "  386  0.063797  0.066203  0.064488  0.445758  0.085382         0.507999\n",
       "  387  0.068615  0.067658  0.067772  0.290860  0.088108         0.490221,\n",
       "  0.06425065892012616),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  384  0.070756  0.070099  0.058750  0.959159  0.091369         0.437372\n",
       "  385  0.064827  0.063442  0.064270  0.293273  0.084872         0.491027\n",
       "  386  0.063797  0.066203  0.064488  0.445758  0.085382         0.507999\n",
       "  387  0.068615  0.067658  0.067772  0.290860  0.088108         0.490221\n",
       "  388  0.067799  0.066748  0.065684  0.217347  0.088513         0.469056,\n",
       "  0.06337588996266753),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  385  0.064827  0.063442  0.064270  0.293273  0.084872         0.491027\n",
       "  386  0.063797  0.066203  0.064488  0.445758  0.085382         0.507999\n",
       "  387  0.068615  0.067658  0.067772  0.290860  0.088108         0.490221\n",
       "  388  0.067799  0.066748  0.065684  0.217347  0.088513         0.469056\n",
       "  389  0.063975  0.063205  0.063507  0.252351  0.086154         0.480579,\n",
       "  0.06084267799426553),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  386  0.063797  0.066203  0.064488  0.445758  0.085382         0.507999\n",
       "  387  0.068615  0.067658  0.067772  0.290860  0.088108         0.490221\n",
       "  388  0.067799  0.066748  0.065684  0.217347  0.088513         0.469056\n",
       "  389  0.063975  0.063205  0.063507  0.252351  0.086154         0.480579\n",
       "  390  0.061306  0.061210  0.061664  0.285596  0.085300         0.468178,\n",
       "  0.0582351717664802),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  387  0.068615  0.067658  0.067772  0.290860  0.088108         0.490221\n",
       "  388  0.067799  0.066748  0.065684  0.217347  0.088513         0.469056\n",
       "  389  0.063975  0.063205  0.063507  0.252351  0.086154         0.480579\n",
       "  390  0.061306  0.061210  0.061664  0.285596  0.085300         0.468178\n",
       "  391  0.061657  0.060854  0.059474  0.338252  0.082826         0.467622,\n",
       "  0.05691703322093644),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  388  0.067799  0.066748  0.065684  0.217347  0.088513         0.469056\n",
       "  389  0.063975  0.063205  0.063507  0.252351  0.086154         0.480579\n",
       "  390  0.061306  0.061210  0.061664  0.285596  0.085300         0.468178\n",
       "  391  0.061657  0.060854  0.059474  0.338252  0.082826         0.467622\n",
       "  392  0.058796  0.057529  0.057064  0.469967  0.080279         0.477264,\n",
       "  0.05681158175239845),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  389  0.063975  0.063205  0.063507  0.252351  0.086154         0.480579\n",
       "  390  0.061306  0.061210  0.061664  0.285596  0.085300         0.468178\n",
       "  391  0.061657  0.060854  0.059474  0.338252  0.082826         0.467622\n",
       "  392  0.058796  0.057529  0.057064  0.469967  0.080279         0.477264\n",
       "  393  0.058177  0.057909  0.058008  0.331733  0.078992         0.486332,\n",
       "  0.0601908014373192),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  390  0.061306  0.061210  0.061664  0.285596  0.085300         0.468178\n",
       "  391  0.061657  0.060854  0.059474  0.338252  0.082826         0.467622\n",
       "  392  0.058796  0.057529  0.057064  0.469967  0.080279         0.477264\n",
       "  393  0.058177  0.057909  0.058008  0.331733  0.078992         0.486332\n",
       "  394  0.057265  0.059394  0.057323  0.448687  0.078889         0.512389,\n",
       "  0.05859466322232283),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  391  0.061657  0.060854  0.059474  0.338252  0.082826         0.467622\n",
       "  392  0.058796  0.057529  0.057064  0.469967  0.080279         0.477264\n",
       "  393  0.058177  0.057909  0.058008  0.331733  0.078992         0.486332\n",
       "  394  0.057265  0.059394  0.057323  0.448687  0.078889         0.512389\n",
       "  395  0.060545  0.059378  0.059239  0.310484  0.082189         0.475185,\n",
       "  0.057221407785476436),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  392  0.058796  0.057529  0.057064  0.469967  0.080279         0.477264\n",
       "  393  0.058177  0.057909  0.058008  0.331733  0.078992         0.486332\n",
       "  394  0.057265  0.059394  0.057323  0.448687  0.078889         0.512389\n",
       "  395  0.060545  0.059378  0.059239  0.310484  0.082189         0.475185\n",
       "  396  0.060025  0.058537  0.058602  0.243544  0.080631         0.476852,\n",
       "  0.05746585427755957),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  393  0.058177  0.057909  0.058008  0.331733  0.078992         0.486332\n",
       "  394  0.057265  0.059394  0.057323  0.448687  0.078889         0.512389\n",
       "  395  0.060545  0.059378  0.059239  0.310484  0.082189         0.475185\n",
       "  396  0.060025  0.058537  0.058602  0.243544  0.080631         0.476852\n",
       "  397  0.057665  0.056885  0.057883  0.207910  0.079289         0.488948,\n",
       "  0.059479006430278314),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  394  0.057265  0.059394  0.057323  0.448687  0.078889         0.512389\n",
       "  395  0.060545  0.059378  0.059239  0.310484  0.082189         0.475185\n",
       "  396  0.060025  0.058537  0.058602  0.243544  0.080631         0.476852\n",
       "  397  0.057665  0.056885  0.057883  0.207910  0.079289         0.488948\n",
       "  398  0.058517  0.058854  0.059484  0.253052  0.079528         0.502174,\n",
       "  0.05749701148671424),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  395  0.060545  0.059378  0.059239  0.310484  0.082189         0.475185\n",
       "  396  0.060025  0.058537  0.058602  0.243544  0.080631         0.476852\n",
       "  397  0.057665  0.056885  0.057883  0.207910  0.079289         0.488948\n",
       "  398  0.058517  0.058854  0.059484  0.253052  0.079528         0.502174\n",
       "  399  0.060362  0.059226  0.058888  0.239029  0.081494         0.472300,\n",
       "  0.05756890977788279),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  396  0.060025  0.058537  0.058602  0.243544  0.080631         0.476852\n",
       "  397  0.057665  0.056885  0.057883  0.207910  0.079289         0.488948\n",
       "  398  0.058517  0.058854  0.059484  0.253052  0.079528         0.502174\n",
       "  399  0.060362  0.059226  0.058888  0.239029  0.081494         0.472300\n",
       "  400  0.057893  0.057435  0.058505  0.184664  0.079559         0.487658,\n",
       "  0.05796914231573927),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  397  0.057665  0.056885  0.057883  0.207910  0.079289         0.488948\n",
       "  398  0.058517  0.058854  0.059484  0.253052  0.079528         0.502174\n",
       "  399  0.060362  0.059226  0.058888  0.239029  0.081494         0.472300\n",
       "  400  0.057893  0.057435  0.058505  0.184664  0.079559         0.487658\n",
       "  401  0.058519  0.057826  0.059159  0.180741  0.079629         0.490113,\n",
       "  0.0636994322729259),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  398  0.058517  0.058854  0.059484  0.253052  0.079528         0.502174\n",
       "  399  0.060362  0.059226  0.058888  0.239029  0.081494         0.472300\n",
       "  400  0.057893  0.057435  0.058505  0.184664  0.079559         0.487658\n",
       "  401  0.058519  0.057826  0.059159  0.180741  0.079629         0.490113\n",
       "  402  0.064841  0.063594  0.064563  0.369147  0.080020         0.529970,\n",
       "  0.06463171408990195),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  399  0.060362  0.059226  0.058888  0.239029  0.081494         0.472300\n",
       "  400  0.057893  0.057435  0.058505  0.184664  0.079559         0.487658\n",
       "  401  0.058519  0.057826  0.059159  0.180741  0.079629         0.490113\n",
       "  402  0.064841  0.063594  0.064563  0.369147  0.080020         0.529970\n",
       "  403  0.064473  0.063978  0.065166  0.170744  0.085616         0.494092,\n",
       "  0.06643156733732992),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  400  0.057893  0.057435  0.058505  0.184664  0.079559         0.487658\n",
       "  401  0.058519  0.057826  0.059159  0.180741  0.079629         0.490113\n",
       "  402  0.064841  0.063594  0.064563  0.369147  0.080020         0.529970\n",
       "  403  0.064473  0.063978  0.065166  0.170744  0.085616         0.494092\n",
       "  404  0.065585  0.065435  0.066660  0.217496  0.086526         0.500579,\n",
       "  0.07069273426199137),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  401  0.058519  0.057826  0.059159  0.180741  0.079629         0.490113\n",
       "  402  0.064841  0.063594  0.064563  0.369147  0.080020         0.529970\n",
       "  403  0.064473  0.063978  0.065166  0.170744  0.085616         0.494092\n",
       "  404  0.065585  0.065435  0.066660  0.217496  0.086526         0.500579\n",
       "  405  0.067565  0.070445  0.068681  0.462717  0.088284         0.518984,\n",
       "  0.06912535728793488),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  402  0.064841  0.063594  0.064563  0.369147  0.080020         0.529970\n",
       "  403  0.064473  0.063978  0.065166  0.170744  0.085616         0.494092\n",
       "  404  0.065585  0.065435  0.066660  0.217496  0.086526         0.500579\n",
       "  405  0.067565  0.070445  0.068681  0.462717  0.088284         0.518984\n",
       "  406  0.069791  0.069141  0.069674  0.357742  0.092445         0.475400,\n",
       "  0.06949922455306629),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  403  0.064473  0.063978  0.065166  0.170744  0.085616         0.494092\n",
       "  404  0.065585  0.065435  0.066660  0.217496  0.086526         0.500579\n",
       "  405  0.067565  0.070445  0.068681  0.462717  0.088284         0.518984\n",
       "  406  0.069791  0.069141  0.069674  0.357742  0.092445         0.475400\n",
       "  407  0.069683  0.068755  0.069775  0.888227  0.090915         0.489916,\n",
       "  0.06942732626189775),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  404  0.065585  0.065435  0.066660  0.217496  0.086526         0.500579\n",
       "  405  0.067565  0.070445  0.068681  0.462717  0.088284         0.518984\n",
       "  406  0.069791  0.069141  0.069674  0.357742  0.092445         0.475400\n",
       "  407  0.069683  0.068755  0.069775  0.888227  0.090915         0.489916\n",
       "  408  0.069873  0.068897  0.070720  0.197475  0.091280         0.486583,\n",
       "  0.07293595709750449),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  405  0.067565  0.070445  0.068681  0.462717  0.088284         0.518984\n",
       "  406  0.069791  0.069141  0.069674  0.357742  0.092445         0.475400\n",
       "  407  0.069683  0.068755  0.069775  0.888227  0.090915         0.489916\n",
       "  408  0.069873  0.068897  0.070720  0.197475  0.091280         0.486583\n",
       "  409  0.069962  0.071843  0.070771  0.382163  0.091210         0.513357,\n",
       "  0.07381072605496308),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  406  0.069791  0.069141  0.069674  0.357742  0.092445         0.475400\n",
       "  407  0.069683  0.068755  0.069775  0.888227  0.090915         0.489916\n",
       "  408  0.069873  0.068897  0.070720  0.197475  0.091280         0.486583\n",
       "  409  0.069962  0.071843  0.070771  0.382163  0.091210         0.513357\n",
       "  410  0.074366  0.074135  0.074327  0.326092  0.094636         0.493662,\n",
       "  0.0745752419850918),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  407  0.069683  0.068755  0.069775  0.888227  0.090915         0.489916\n",
       "  408  0.069873  0.068897  0.070720  0.197475  0.091280         0.486583\n",
       "  409  0.069962  0.071843  0.070771  0.382163  0.091210         0.513357\n",
       "  410  0.074366  0.074135  0.074327  0.326092  0.094636         0.493662\n",
       "  411  0.073955  0.073973  0.074959  0.209048  0.095490         0.492837,\n",
       "  0.07337213878094523),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  408  0.069873  0.068897  0.070720  0.197475  0.091280         0.486583\n",
       "  409  0.069962  0.071843  0.070771  0.382163  0.091210         0.513357\n",
       "  410  0.074366  0.074135  0.074327  0.326092  0.094636         0.493662\n",
       "  411  0.073955  0.073973  0.074959  0.209048  0.095490         0.492837\n",
       "  412  0.075380  0.073727  0.074617  0.170797  0.096237         0.478124,\n",
       "  0.07582147261710473),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  409  0.069962  0.071843  0.070771  0.382163  0.091210         0.513357\n",
       "  410  0.074366  0.074135  0.074327  0.326092  0.094636         0.493662\n",
       "  411  0.073955  0.073973  0.074959  0.209048  0.095490         0.492837\n",
       "  412  0.075380  0.073727  0.074617  0.170797  0.096237         0.478124\n",
       "  413  0.074097  0.074751  0.074915  0.226600  0.095062         0.505436,\n",
       "  0.07421095859281958),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  410  0.074366  0.074135  0.074327  0.326092  0.094636         0.493662\n",
       "  411  0.073955  0.073973  0.074959  0.209048  0.095490         0.492837\n",
       "  412  0.075380  0.073727  0.074617  0.170797  0.096237         0.478124\n",
       "  413  0.074097  0.074751  0.074915  0.226600  0.095062         0.505436\n",
       "  414  0.076345  0.075208  0.075150  0.270068  0.097454         0.475078,\n",
       "  0.07404079673775746),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  411  0.073955  0.073973  0.074959  0.209048  0.095490         0.492837\n",
       "  412  0.075380  0.073727  0.074617  0.170797  0.096237         0.478124\n",
       "  413  0.074097  0.074751  0.074915  0.226600  0.095062         0.505436\n",
       "  414  0.076345  0.075208  0.075150  0.270068  0.097454         0.475078\n",
       "  415  0.074559  0.073367  0.074901  0.146258  0.095881         0.485848,\n",
       "  0.07241350131360641),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  412  0.075380  0.073727  0.074617  0.170797  0.096237         0.478124\n",
       "  413  0.074097  0.074751  0.074915  0.226600  0.095062         0.505436\n",
       "  414  0.076345  0.075208  0.075150  0.270068  0.097454         0.475078\n",
       "  415  0.074559  0.073367  0.074901  0.146258  0.095881         0.485848\n",
       "  416  0.074470  0.072995  0.073765  0.159293  0.095715         0.474952,\n",
       "  0.07353032079493338),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  413  0.074097  0.074751  0.074915  0.226600  0.095062         0.505436\n",
       "  414  0.076345  0.075208  0.075150  0.270068  0.097454         0.475078\n",
       "  415  0.074559  0.073367  0.074901  0.146258  0.095881         0.485848\n",
       "  416  0.074470  0.072995  0.073765  0.159293  0.095715         0.474952\n",
       "  417  0.073223  0.073644  0.073847  0.200803  0.094126         0.495472,\n",
       "  0.0729119974153565),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  414  0.076345  0.075208  0.075150  0.270068  0.097454         0.475078\n",
       "  415  0.074559  0.073367  0.074901  0.146258  0.095881         0.485848\n",
       "  416  0.074470  0.072995  0.073765  0.159293  0.095715         0.474952\n",
       "  417  0.073223  0.073644  0.073847  0.200803  0.094126         0.495472\n",
       "  418  0.074301  0.073011  0.074007  0.197838  0.095217         0.482497,\n",
       "  0.07441226803467406),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  415  0.074559  0.073367  0.074901  0.146258  0.095881         0.485848\n",
       "  416  0.074470  0.072995  0.073765  0.159293  0.095715         0.474952\n",
       "  417  0.073223  0.073644  0.073847  0.200803  0.094126         0.495472\n",
       "  418  0.074301  0.073011  0.074007  0.197838  0.095217         0.482497\n",
       "  419  0.075418  0.073914  0.075274  0.164858  0.094613         0.498339,\n",
       "  0.07549074240220197),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  416  0.074470  0.072995  0.073765  0.159293  0.095715         0.474952\n",
       "  417  0.073223  0.073644  0.073847  0.200803  0.094126         0.495472\n",
       "  418  0.074301  0.073011  0.074007  0.197838  0.095217         0.482497\n",
       "  419  0.075418  0.073914  0.075274  0.164858  0.094613         0.498339\n",
       "  420  0.075043  0.074473  0.075630  0.298388  0.096078         0.495185,\n",
       "  0.08078724664416266),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  417  0.073223  0.073644  0.073847  0.200803  0.094126         0.495472\n",
       "  418  0.074301  0.073011  0.074007  0.197838  0.095217         0.482497\n",
       "  419  0.075418  0.073914  0.075274  0.164858  0.094613         0.498339\n",
       "  420  0.075043  0.074473  0.075630  0.298388  0.096078         0.495185\n",
       "  421  0.084161  0.082701  0.082405  0.548427  0.097131         0.526726,\n",
       "  0.08160209715152673),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  418  0.074301  0.073011  0.074007  0.197838  0.095217         0.482497\n",
       "  419  0.075418  0.073914  0.075274  0.164858  0.094613         0.498339\n",
       "  420  0.075043  0.074473  0.075630  0.298388  0.096078         0.495185\n",
       "  421  0.084161  0.082701  0.082405  0.548427  0.097131         0.526726\n",
       "  422  0.081875  0.081275  0.082489  0.214835  0.102304         0.493214,\n",
       "  0.07840501916274209),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  419  0.075418  0.073914  0.075274  0.164858  0.094613         0.498339\n",
       "  420  0.075043  0.074473  0.075630  0.298388  0.096078         0.495185\n",
       "  421  0.084161  0.082701  0.082405  0.548427  0.097131         0.526726\n",
       "  422  0.081875  0.081275  0.082489  0.214835  0.102304         0.493214\n",
       "  423  0.081930  0.080407  0.079941  0.232654  0.103099         0.463214,\n",
       "  0.07811982196628278),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  420  0.075043  0.074473  0.075630  0.298388  0.096078         0.495185\n",
       "  421  0.084161  0.082701  0.082405  0.548427  0.097131         0.526726\n",
       "  422  0.081875  0.081275  0.082489  0.214835  0.102304         0.493214\n",
       "  423  0.081930  0.080407  0.079941  0.232654  0.103099         0.463214\n",
       "  424  0.079069  0.077801  0.079343  0.176925  0.099977         0.484988,\n",
       "  0.07669624157456333),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  421  0.084161  0.082701  0.082405  0.548427  0.097131         0.526726\n",
       "  422  0.081875  0.081275  0.082489  0.214835  0.102304         0.493214\n",
       "  423  0.081930  0.080407  0.079941  0.232654  0.103099         0.463214\n",
       "  424  0.079069  0.077801  0.079343  0.176925  0.099977         0.484988\n",
       "  425  0.077907  0.077059  0.078340  0.202710  0.099699         0.476475,\n",
       "  0.07619534950459844),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  422  0.081875  0.081275  0.082489  0.214835  0.102304         0.493214\n",
       "  423  0.081930  0.080407  0.079941  0.232654  0.103099         0.463214\n",
       "  424  0.079069  0.077801  0.079343  0.176925  0.099977         0.484988\n",
       "  425  0.077907  0.077059  0.078340  0.202710  0.099699         0.476475\n",
       "  426  0.076876  0.076779  0.077592  0.180415  0.098308         0.483375,\n",
       "  0.07163220398361181),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  423  0.081930  0.080407  0.079941  0.232654  0.103099         0.463214\n",
       "  424  0.079069  0.077801  0.079343  0.176925  0.099977         0.484988\n",
       "  425  0.077907  0.077059  0.078340  0.202710  0.099699         0.476475\n",
       "  426  0.076876  0.076779  0.077592  0.180415  0.098308         0.483375\n",
       "  427  0.076838  0.075289  0.073292  0.251923  0.097819         0.452999,\n",
       "  0.07064959721176271),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  424  0.079069  0.077801  0.079343  0.176925  0.099977         0.484988\n",
       "  425  0.077907  0.077059  0.078340  0.202710  0.099699         0.476475\n",
       "  426  0.076876  0.076779  0.077592  0.180415  0.098308         0.483375\n",
       "  427  0.076838  0.075289  0.073292  0.251923  0.097819         0.452999\n",
       "  428  0.072653  0.071272  0.070834  0.318645  0.093363         0.479773,\n",
       "  0.07049861272478126),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  425  0.077907  0.077059  0.078340  0.202710  0.099699         0.476475\n",
       "  426  0.076876  0.076779  0.077592  0.180415  0.098308         0.483375\n",
       "  427  0.076838  0.075289  0.073292  0.251923  0.097819         0.452999\n",
       "  428  0.072653  0.071272  0.070834  0.318645  0.093363         0.479773\n",
       "  429  0.071515  0.071037  0.071435  0.196247  0.092403         0.485992,\n",
       "  0.07063761737068873),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  426  0.076876  0.076779  0.077592  0.180415  0.098308         0.483375\n",
       "  427  0.076838  0.075289  0.073292  0.251923  0.097819         0.452999\n",
       "  428  0.072653  0.071272  0.070834  0.318645  0.093363         0.479773\n",
       "  429  0.071515  0.071037  0.071435  0.196247  0.092403         0.485992\n",
       "  430  0.071197  0.070414  0.071757  0.113056  0.092256         0.488160,\n",
       "  0.07052976993393595),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  427  0.076838  0.075289  0.073292  0.251923  0.097819         0.452999\n",
       "  428  0.072653  0.071272  0.070834  0.318645  0.093363         0.479773\n",
       "  429  0.071515  0.071037  0.071435  0.196247  0.092403         0.485992\n",
       "  430  0.071197  0.070414  0.071757  0.113056  0.092256         0.488160\n",
       "  431  0.071815  0.070843  0.071653  0.147425  0.092392         0.486314,\n",
       "  0.07064480527533311),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  428  0.072653  0.071272  0.070834  0.318645  0.093363         0.479773\n",
       "  429  0.071515  0.071037  0.071435  0.196247  0.092403         0.485992\n",
       "  430  0.071197  0.070414  0.071757  0.113056  0.092256         0.488160\n",
       "  431  0.071815  0.070843  0.071653  0.147425  0.092392         0.486314\n",
       "  432  0.071216  0.069999  0.071233  0.124406  0.092286         0.487981,\n",
       "  0.07400484759217318),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  429  0.071515  0.071037  0.071435  0.196247  0.092403         0.485992\n",
       "  430  0.071197  0.070414  0.071757  0.113056  0.092256         0.488160\n",
       "  431  0.071815  0.070843  0.071653  0.147425  0.092392         0.486314\n",
       "  432  0.071216  0.069999  0.071233  0.124406  0.092286         0.487981\n",
       "  433  0.071358  0.072815  0.072130  0.222234  0.092399         0.512246,\n",
       "  0.07261001881903131),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  430  0.071197  0.070414  0.071757  0.113056  0.092256         0.488160\n",
       "  431  0.071815  0.070843  0.071653  0.147425  0.092392         0.486314\n",
       "  432  0.071216  0.069999  0.071233  0.124406  0.092286         0.487981\n",
       "  433  0.071358  0.072815  0.072130  0.222234  0.092399         0.512246\n",
       "  434  0.074393  0.073457  0.074094  0.150397  0.095680         0.476690,\n",
       "  0.06878024164138102),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  431  0.071815  0.070843  0.071653  0.147425  0.092392         0.486314\n",
       "  432  0.071216  0.069999  0.071233  0.124406  0.092286         0.487981\n",
       "  433  0.071358  0.072815  0.072130  0.222234  0.092399         0.512246\n",
       "  434  0.074393  0.073457  0.074094  0.150397  0.095680         0.476690\n",
       "  435  0.073216  0.071798  0.070005  0.216207  0.094318         0.458483,\n",
       "  0.06569581264814107),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  432  0.071216  0.069999  0.071233  0.124406  0.092286         0.487981\n",
       "  433  0.071358  0.072815  0.072130  0.222234  0.092399         0.512246\n",
       "  434  0.074393  0.073457  0.074094  0.150397  0.095680         0.476690\n",
       "  435  0.073216  0.071798  0.070005  0.216207  0.094318         0.458483\n",
       "  436  0.068432  0.067096  0.067237  0.245268  0.090578         0.464056,\n",
       "  0.06619190315931406),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  433  0.071358  0.072815  0.072130  0.222234  0.092399         0.512246\n",
       "  434  0.074393  0.073457  0.074094  0.150397  0.095680         0.476690\n",
       "  435  0.073216  0.071798  0.070005  0.216207  0.094318         0.458483\n",
       "  436  0.068432  0.067096  0.067237  0.245268  0.090578         0.464056\n",
       "  437  0.066600  0.065978  0.065953  0.208844  0.087565         0.490830,\n",
       "  0.06501757081846968),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  434  0.074393  0.073457  0.074094  0.150397  0.095680         0.476690\n",
       "  435  0.073216  0.071798  0.070005  0.216207  0.094318         0.458483\n",
       "  436  0.068432  0.067096  0.067237  0.245268  0.090578         0.464056\n",
       "  437  0.066600  0.065978  0.065953  0.208844  0.087565         0.490830\n",
       "  438  0.066615  0.065198  0.066350  0.157789  0.088050         0.478339,\n",
       "  0.06578448271681318),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  435  0.073216  0.071798  0.070005  0.216207  0.094318         0.458483\n",
       "  436  0.068432  0.067096  0.067237  0.245268  0.090578         0.464056\n",
       "  437  0.066600  0.065978  0.065953  0.208844  0.087565         0.490830\n",
       "  438  0.066615  0.065198  0.066350  0.157789  0.088050         0.478339\n",
       "  439  0.065320  0.065904  0.066430  0.258797  0.086903         0.492855,\n",
       "  0.06494327655908635),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  436  0.068432  0.067096  0.067237  0.245268  0.090578         0.464056\n",
       "  437  0.066600  0.065978  0.065953  0.208844  0.087565         0.490830\n",
       "  438  0.066615  0.065198  0.066350  0.157789  0.088050         0.478339\n",
       "  439  0.065320  0.065904  0.066430  0.258797  0.086903         0.492855\n",
       "  440  0.067286  0.066371  0.066549  0.141831  0.087652         0.480830,\n",
       "  0.06471080028571488),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  437  0.066600  0.065978  0.065953  0.208844  0.087565         0.490830\n",
       "  438  0.066615  0.065198  0.066350  0.157789  0.088050         0.478339\n",
       "  439  0.065320  0.065904  0.066430  0.258797  0.086903         0.492855\n",
       "  440  0.067286  0.066371  0.066549  0.141831  0.087652         0.480830\n",
       "  441  0.065963  0.064639  0.064093  0.277146  0.086831         0.485382,\n",
       "  0.06493369268622716),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  438  0.066615  0.065198  0.066350  0.157789  0.088050         0.478339\n",
       "  439  0.065320  0.065904  0.066430  0.258797  0.086903         0.492855\n",
       "  440  0.067286  0.066371  0.066549  0.141831  0.087652         0.480830\n",
       "  441  0.065963  0.064639  0.064093  0.277146  0.086831         0.485382\n",
       "  442  0.064656  0.064290  0.065227  0.209192  0.086604         0.488787,\n",
       "  0.06604331464054743),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  439  0.065320  0.065904  0.066430  0.258797  0.086903         0.492855\n",
       "  440  0.067286  0.066371  0.066549  0.141831  0.087652         0.480830\n",
       "  441  0.065963  0.064639  0.064093  0.277146  0.086831         0.485382\n",
       "  442  0.064656  0.064290  0.065227  0.209192  0.086604         0.488787\n",
       "  443  0.066186  0.066862  0.067239  0.218401  0.086821         0.495418,\n",
       "  0.06740219426810504),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  440  0.067286  0.066371  0.066549  0.141831  0.087652         0.480830\n",
       "  441  0.065963  0.064639  0.064093  0.277146  0.086831         0.485382\n",
       "  442  0.064656  0.064290  0.065227  0.209192  0.086604         0.488787\n",
       "  443  0.066186  0.066862  0.067239  0.218401  0.086821         0.495418\n",
       "  444  0.066942  0.066757  0.066767  0.232284  0.087905         0.497282,\n",
       "  0.0678192082058274),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  441  0.065963  0.064639  0.064093  0.277146  0.086831         0.485382\n",
       "  442  0.064656  0.064290  0.065227  0.209192  0.086604         0.488787\n",
       "  443  0.066186  0.066862  0.067239  0.218401  0.086821         0.495418\n",
       "  444  0.066942  0.066757  0.066767  0.232284  0.087905         0.497282\n",
       "  445  0.067358  0.066653  0.066937  0.199224  0.089232         0.490239,\n",
       "  0.06742616357261531),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  442  0.064656  0.064290  0.065227  0.209192  0.086604         0.488787\n",
       "  443  0.066186  0.066862  0.067239  0.218401  0.086821         0.495418\n",
       "  444  0.066942  0.066757  0.066767  0.232284  0.087905         0.497282\n",
       "  445  0.067358  0.066653  0.066937  0.199224  0.089232         0.490239\n",
       "  446  0.068675  0.067336  0.068806  0.089098  0.089639         0.484181,\n",
       "  0.0651709512736659),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  443  0.066186  0.066862  0.067239  0.218401  0.086821         0.495418\n",
       "  444  0.066942  0.066757  0.066767  0.232284  0.087905         0.497282\n",
       "  445  0.067358  0.066653  0.066937  0.199224  0.089232         0.490239\n",
       "  446  0.068675  0.067336  0.068806  0.089098  0.089639         0.484181\n",
       "  447  0.067166  0.066179  0.066728  0.104884  0.089255         0.470257,\n",
       "  0.06514219003272603),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  444  0.066942  0.066757  0.066767  0.232284  0.087905         0.497282\n",
       "  445  0.067358  0.066653  0.066937  0.199224  0.089232         0.490239\n",
       "  446  0.068675  0.067336  0.068806  0.089098  0.089639         0.484181\n",
       "  447  0.067166  0.066179  0.066728  0.104884  0.089255         0.470257\n",
       "  448  0.066088  0.065527  0.065597  0.194036  0.087053         0.486905,\n",
       "  0.067730528514793),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  445  0.067358  0.066653  0.066937  0.199224  0.089232         0.490239\n",
       "  446  0.068675  0.067336  0.068806  0.089098  0.089639         0.484181\n",
       "  447  0.067166  0.066179  0.066728  0.104884  0.089255         0.470257\n",
       "  448  0.066088  0.065527  0.065597  0.194036  0.087053         0.486905\n",
       "  449  0.066025  0.066651  0.066765  0.152606  0.087025         0.506475,\n",
       "  0.06696840855287907),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  446  0.068675  0.067336  0.068806  0.089098  0.089639         0.484181\n",
       "  447  0.067166  0.066179  0.066728  0.104884  0.089255         0.470257\n",
       "  448  0.066088  0.065527  0.065597  0.194036  0.087053         0.486905\n",
       "  449  0.066025  0.066651  0.066765  0.152606  0.087025         0.506475\n",
       "  450  0.069226  0.067620  0.068182  0.155243  0.089553         0.481422,\n",
       "  0.0657701069075244),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  447  0.067166  0.066179  0.066728  0.104884  0.089255         0.470257\n",
       "  448  0.066088  0.065527  0.065597  0.194036  0.087053         0.486905\n",
       "  449  0.066025  0.066651  0.066765  0.152606  0.087025         0.506475\n",
       "  450  0.069226  0.067620  0.068182  0.155243  0.089553         0.481422\n",
       "  451  0.066660  0.066286  0.067382  0.135105  0.088808         0.478160,\n",
       "  0.0694968285848515),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  448  0.066088  0.065527  0.065597  0.194036  0.087053         0.486905\n",
       "  449  0.066025  0.066651  0.066765  0.152606  0.087025         0.506475\n",
       "  450  0.069226  0.067620  0.068182  0.155243  0.089553         0.481422\n",
       "  451  0.066660  0.066286  0.067382  0.135105  0.088808         0.478160\n",
       "  452  0.066757  0.068312  0.067903  0.249457  0.087638         0.514988,\n",
       "  0.06866281033176903),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  449  0.066025  0.066651  0.066765  0.152606  0.087025         0.506475\n",
       "  450  0.069226  0.067620  0.068182  0.155243  0.089553         0.481422\n",
       "  451  0.066660  0.066286  0.067382  0.135105  0.088808         0.478160\n",
       "  452  0.066757  0.068312  0.067903  0.249457  0.087638         0.514988\n",
       "  453  0.070766  0.069464  0.070374  0.216657  0.091278         0.480884,\n",
       "  0.07029010575592008),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  450  0.069226  0.067620  0.068182  0.155243  0.089553         0.481422\n",
       "  451  0.066660  0.066286  0.067382  0.135105  0.088808         0.478160\n",
       "  452  0.066757  0.068312  0.067903  0.249457  0.087638         0.514988\n",
       "  453  0.070766  0.069464  0.070374  0.216657  0.091278         0.480884\n",
       "  454  0.069445  0.069324  0.068458  0.251882  0.090463         0.499289,\n",
       "  0.06867718614105783),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  451  0.066660  0.066286  0.067382  0.135105  0.088808         0.478160\n",
       "  452  0.066757  0.068312  0.067903  0.249457  0.087638         0.514988\n",
       "  453  0.070766  0.069464  0.070374  0.216657  0.091278         0.480884\n",
       "  454  0.069445  0.069324  0.068458  0.251882  0.090463         0.499289\n",
       "  455  0.070513  0.069599  0.070095  0.149675  0.092052         0.475060,\n",
       "  0.06747169659105877),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  452  0.066757  0.068312  0.067903  0.249457  0.087638         0.514988\n",
       "  453  0.070766  0.069464  0.070374  0.216657  0.091278         0.480884\n",
       "  454  0.069445  0.069324  0.068458  0.251882  0.090463         0.499289\n",
       "  455  0.070513  0.069599  0.070095  0.149675  0.092052         0.475060\n",
       "  456  0.069587  0.068428  0.069041  0.121985  0.090477         0.478106,\n",
       "  0.06866520629998384),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  453  0.070766  0.069464  0.070374  0.216657  0.091278         0.480884\n",
       "  454  0.069445  0.069324  0.068458  0.251882  0.090463         0.499289\n",
       "  455  0.070513  0.069599  0.070095  0.149675  0.092052         0.475060\n",
       "  456  0.069587  0.068428  0.069041  0.121985  0.090477         0.478106\n",
       "  457  0.067796  0.067601  0.068344  0.186199  0.089300         0.496045,\n",
       "  0.06815473035715976),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  454  0.069445  0.069324  0.068458  0.251882  0.090463         0.499289\n",
       "  455  0.070513  0.069599  0.070095  0.149675  0.092052         0.475060\n",
       "  456  0.069587  0.068428  0.069041  0.121985  0.090477         0.478106\n",
       "  457  0.067796  0.067601  0.068344  0.186199  0.089300         0.496045\n",
       "  458  0.069945  0.068573  0.068441  0.188945  0.090465         0.483303,\n",
       "  0.06973888873108214),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  455  0.070513  0.069599  0.070095  0.149675  0.092052         0.475060\n",
       "  456  0.069587  0.068428  0.069041  0.121985  0.090477         0.478106\n",
       "  457  0.067796  0.067601  0.068344  0.186199  0.089300         0.496045\n",
       "  458  0.069945  0.068573  0.068441  0.188945  0.090465         0.483303\n",
       "  459  0.069173  0.068845  0.069526  0.164955  0.089967         0.498966,\n",
       "  0.06966699043991362),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  456  0.069587  0.068428  0.069041  0.121985  0.090477         0.478106\n",
       "  457  0.067796  0.067601  0.068344  0.186199  0.089300         0.496045\n",
       "  458  0.069945  0.068573  0.068441  0.188945  0.090465         0.483303\n",
       "  459  0.069173  0.068845  0.069526  0.164955  0.089967         0.498966\n",
       "  460  0.069770  0.068532  0.070737  0.128930  0.091514         0.486583,\n",
       "  0.06905345899676635),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  457  0.067796  0.067601  0.068344  0.186199  0.089300         0.496045\n",
       "  458  0.069945  0.068573  0.068441  0.188945  0.090465         0.483303\n",
       "  459  0.069173  0.068845  0.069526  0.164955  0.089967         0.498966\n",
       "  460  0.069770  0.068532  0.070737  0.128930  0.091514         0.486583\n",
       "  461  0.070191  0.069449  0.070490  0.185567  0.091444         0.482533,\n",
       "  0.06882098272339489),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  458  0.069945  0.068573  0.068441  0.188945  0.090465         0.483303\n",
       "  459  0.069173  0.068845  0.069526  0.164955  0.089967         0.498966\n",
       "  460  0.069770  0.068532  0.070737  0.128930  0.091514         0.486583\n",
       "  461  0.070191  0.069449  0.070490  0.185567  0.091444         0.482533\n",
       "  462  0.069512  0.068755  0.070376  0.098168  0.090845         0.485382,\n",
       "  0.07240631340896202),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  459  0.069173  0.068845  0.069526  0.164955  0.089967         0.498966\n",
       "  460  0.069770  0.068532  0.070737  0.128930  0.091514         0.486583\n",
       "  461  0.070191  0.069449  0.070490  0.185567  0.091444         0.482533\n",
       "  462  0.069512  0.068755  0.070376  0.098168  0.090845         0.485382\n",
       "  463  0.070241  0.071630  0.071161  0.212597  0.090617         0.513930,\n",
       "  0.07188384800270164),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  460  0.069770  0.068532  0.070737  0.128930  0.091514         0.486583\n",
       "  461  0.070191  0.069449  0.070490  0.185567  0.091444         0.482533\n",
       "  462  0.069512  0.068755  0.070376  0.098168  0.090845         0.485382\n",
       "  463  0.070241  0.071630  0.071161  0.212597  0.090617         0.513930\n",
       "  464  0.072773  0.072104  0.070737  0.143701  0.094119         0.483214,\n",
       "  0.07306058593412314),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  461  0.070191  0.069449  0.070490  0.185567  0.091444         0.482533\n",
       "  462  0.069512  0.068755  0.070376  0.098168  0.090845         0.485382\n",
       "  463  0.070241  0.071630  0.071161  0.212597  0.090617         0.513930\n",
       "  464  0.072773  0.072104  0.070737  0.143701  0.094119         0.483214\n",
       "  465  0.073057  0.072992  0.073823  0.129178  0.093609         0.495920,\n",
       "  0.07290720547892691),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  462  0.069512  0.068755  0.070376  0.098168  0.090845         0.485382\n",
       "  463  0.070241  0.071630  0.071161  0.212597  0.090617         0.513930\n",
       "  464  0.072773  0.072104  0.070737  0.143701  0.094119         0.483214\n",
       "  465  0.073057  0.072992  0.073823  0.129178  0.093609         0.495920\n",
       "  466  0.074017  0.072649  0.074431  0.086281  0.094758         0.485974,\n",
       "  0.07245184642740547),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  463  0.070241  0.071630  0.071161  0.212597  0.090617         0.513930\n",
       "  464  0.072773  0.072104  0.070737  0.143701  0.094119         0.483214\n",
       "  465  0.073057  0.072992  0.073823  0.129178  0.093609         0.495920\n",
       "  466  0.074017  0.072649  0.074431  0.086281  0.094758         0.485974\n",
       "  467  0.073767  0.072578  0.073888  0.099802  0.094608         0.483716,\n",
       "  0.07336015893987125),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  464  0.072773  0.072104  0.070737  0.143701  0.094119         0.483214\n",
       "  465  0.073057  0.072992  0.073823  0.129178  0.093609         0.495920\n",
       "  466  0.074017  0.072649  0.074431  0.086281  0.094758         0.485974\n",
       "  467  0.073767  0.072578  0.073888  0.099802  0.094608         0.483716\n",
       "  468  0.073358  0.072217  0.073917  0.090156  0.094163         0.493913,\n",
       "  0.07616419229544379),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  465  0.073057  0.072992  0.073823  0.129178  0.093609         0.495920\n",
       "  466  0.074017  0.072649  0.074431  0.086281  0.094758         0.485974\n",
       "  467  0.073767  0.072578  0.073888  0.099802  0.094608         0.483716\n",
       "  468  0.073358  0.072217  0.073917  0.090156  0.094163         0.493913\n",
       "  469  0.074316  0.074995  0.075053  0.161854  0.095050         0.508088,\n",
       "  0.07652847568771601),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  466  0.074017  0.072649  0.074431  0.086281  0.094758         0.485974\n",
       "  467  0.073767  0.072578  0.073888  0.099802  0.094608         0.483716\n",
       "  468  0.073358  0.072217  0.073917  0.090156  0.094163         0.493913\n",
       "  469  0.074316  0.074995  0.075053  0.161854  0.095050         0.508088\n",
       "  470  0.076147  0.075263  0.076630  0.152120  0.097789         0.489845,\n",
       "  0.07745596556826247),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  467  0.073767  0.072578  0.073888  0.099802  0.094608         0.483716\n",
       "  468  0.073358  0.072217  0.073917  0.090156  0.094163         0.493913\n",
       "  469  0.074316  0.074995  0.075053  0.161854  0.095050         0.508088\n",
       "  470  0.076147  0.075263  0.076630  0.152120  0.097789         0.489845\n",
       "  471  0.077216  0.076315  0.078355  0.052362  0.098145         0.494056,\n",
       "  0.07703895163054009),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  468  0.073358  0.072217  0.073917  0.090156  0.094163         0.493913\n",
       "  469  0.074316  0.074995  0.075053  0.161854  0.095050         0.508088\n",
       "  470  0.076147  0.075263  0.076630  0.152120  0.097789         0.489845\n",
       "  471  0.077216  0.076315  0.078355  0.052362  0.098145         0.494056\n",
       "  472  0.077572  0.076085  0.077432  0.121171  0.099050         0.484002,\n",
       "  0.07745356960004766),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  469  0.074316  0.074995  0.075053  0.161854  0.095050         0.508088\n",
       "  470  0.076147  0.075263  0.076630  0.152120  0.097789         0.489845\n",
       "  471  0.077216  0.076315  0.078355  0.052362  0.098145         0.494056\n",
       "  472  0.077572  0.076085  0.077432  0.121171  0.099050         0.484002\n",
       "  473  0.078020  0.076696  0.078967  0.089587  0.098643         0.490221,\n",
       "  0.07679689148430942),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  470  0.076147  0.075263  0.076630  0.152120  0.097789         0.489845\n",
       "  471  0.077216  0.076315  0.078355  0.052362  0.098145         0.494056\n",
       "  472  0.077572  0.076085  0.077432  0.121171  0.099050         0.484002\n",
       "  473  0.078020  0.076696  0.078967  0.089587  0.098643         0.490221\n",
       "  474  0.078662  0.077291  0.077490  0.146863  0.099048         0.482210,\n",
       "  0.07625765430054547),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  471  0.077216  0.076315  0.078355  0.052362  0.098145         0.494056\n",
       "  472  0.077572  0.076085  0.077432  0.121171  0.099050         0.484002\n",
       "  473  0.078020  0.076696  0.078967  0.089587  0.098643         0.490221\n",
       "  474  0.078662  0.077291  0.077490  0.146863  0.099048         0.482210\n",
       "  475  0.078034  0.076661  0.077611  0.107812  0.098407         0.483088,\n",
       "  0.07778190384673563),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  472  0.077572  0.076085  0.077432  0.121171  0.099050         0.484002\n",
       "  473  0.078020  0.076696  0.078967  0.089587  0.098643         0.490221\n",
       "  474  0.078662  0.077291  0.077490  0.146863  0.099048         0.482210\n",
       "  475  0.078034  0.076661  0.077611  0.107812  0.098407         0.483088\n",
       "  476  0.076848  0.076618  0.076800  0.145146  0.097880         0.498518,\n",
       "  0.07602997958596591),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  473  0.078020  0.076696  0.078967  0.089587  0.098643         0.490221\n",
       "  474  0.078662  0.077291  0.077490  0.146863  0.099048         0.482210\n",
       "  475  0.078034  0.076661  0.077611  0.107812  0.098407         0.483088\n",
       "  476  0.076848  0.076618  0.076800  0.145146  0.097880         0.498518\n",
       "  477  0.077709  0.076071  0.077696  0.119251  0.099369         0.474020,\n",
       "  0.07401203549681756),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  474  0.078662  0.077291  0.077490  0.146863  0.099048         0.482210\n",
       "  475  0.078034  0.076661  0.077611  0.107812  0.098407         0.483088\n",
       "  476  0.076848  0.076618  0.076800  0.145146  0.097880         0.498518\n",
       "  477  0.077709  0.076071  0.077696  0.119251  0.099369         0.474020\n",
       "  478  0.075762  0.075028  0.075298  0.168331  0.097658         0.472031,\n",
       "  0.07273464765564998),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  475  0.078034  0.076661  0.077611  0.107812  0.098407         0.483088\n",
       "  476  0.076848  0.076618  0.076800  0.145146  0.097880         0.498518\n",
       "  477  0.077709  0.076071  0.077696  0.119251  0.099369         0.474020\n",
       "  478  0.075762  0.075028  0.075298  0.168331  0.097658         0.472031\n",
       "  479  0.074879  0.073405  0.073307  0.183596  0.095687         0.477569,\n",
       "  0.07378675675045282),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  476  0.076848  0.076618  0.076800  0.145146  0.097880         0.498518\n",
       "  477  0.077709  0.076071  0.077696  0.119251  0.099369         0.474020\n",
       "  478  0.075762  0.075028  0.075298  0.168331  0.097658         0.472031\n",
       "  479  0.074879  0.073405  0.073307  0.183596  0.095687         0.477569\n",
       "  480  0.073507  0.073289  0.074545  0.141269  0.094440         0.494988,\n",
       "  0.07262680021889718),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  477  0.077709  0.076071  0.077696  0.119251  0.099369         0.474020\n",
       "  478  0.075762  0.075028  0.075298  0.168331  0.097658         0.472031\n",
       "  479  0.074879  0.073405  0.073307  0.183596  0.095687         0.477569\n",
       "  480  0.073507  0.073289  0.074545  0.141269  0.094440         0.494988\n",
       "  481  0.074739  0.073187  0.073096  0.207548  0.095467         0.478447,\n",
       "  0.07165617328812209),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  478  0.075762  0.075028  0.075298  0.168331  0.097658         0.472031\n",
       "  479  0.074879  0.073405  0.073307  0.183596  0.095687         0.477569\n",
       "  480  0.073507  0.073289  0.074545  0.141269  0.094440         0.494988\n",
       "  481  0.074739  0.073187  0.073096  0.207548  0.095467         0.478447\n",
       "  482  0.071192  0.070952  0.072350  0.207102  0.094334         0.479863,\n",
       "  0.06880900288232089),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  479  0.074879  0.073405  0.073307  0.183596  0.095687         0.477569\n",
       "  480  0.073507  0.073289  0.074545  0.141269  0.094440         0.494988\n",
       "  481  0.074739  0.073187  0.073096  0.207548  0.095467         0.478447\n",
       "  482  0.071192  0.070952  0.072350  0.207102  0.094334         0.479863\n",
       "  483  0.073423  0.071734  0.070170  0.304887  0.093386         0.465830,\n",
       "  0.06952558982579136),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  480  0.073507  0.073289  0.074545  0.141269  0.094440         0.494988\n",
       "  481  0.074739  0.073187  0.073096  0.207548  0.095467         0.478447\n",
       "  482  0.071192  0.070952  0.072350  0.207102  0.094334         0.479863\n",
       "  483  0.073423  0.071734  0.070170  0.304887  0.093386         0.465830\n",
       "  484  0.069057  0.068727  0.068240  0.285815  0.090606         0.492479,\n",
       "  0.06971491942657188),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  481  0.074739  0.073187  0.073096  0.207548  0.095467         0.478447\n",
       "  482  0.071192  0.070952  0.072350  0.207102  0.094334         0.479863\n",
       "  483  0.073423  0.071734  0.070170  0.304887  0.093386         0.465830\n",
       "  484  0.069057  0.068727  0.068240  0.285815  0.090606         0.492479\n",
       "  485  0.070665  0.069234  0.069092  0.196585  0.091306         0.488536,\n",
       "  0.06934584409787005),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  482  0.071192  0.070952  0.072350  0.207102  0.094334         0.479863\n",
       "  483  0.073423  0.071734  0.070170  0.304887  0.093386         0.465830\n",
       "  484  0.069057  0.068727  0.068240  0.285815  0.090606         0.492479\n",
       "  485  0.070665  0.069234  0.069092  0.196585  0.091306         0.488536\n",
       "  486  0.068788  0.068622  0.069720  0.140033  0.091490         0.484361,\n",
       "  0.06842075018553839),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  483  0.073423  0.071734  0.070170  0.304887  0.093386         0.465830\n",
       "  484  0.069057  0.068727  0.068240  0.285815  0.090606         0.492479\n",
       "  485  0.070665  0.069234  0.069092  0.196585  0.091306         0.488536\n",
       "  486  0.068788  0.068622  0.069720  0.140033  0.091490         0.484361\n",
       "  487  0.069553  0.068765  0.069526  0.134458  0.091130         0.480203,\n",
       "  0.06767780759170515),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  484  0.069057  0.068727  0.068240  0.285815  0.090606         0.492479\n",
       "  485  0.070665  0.069234  0.069092  0.196585  0.091306         0.488536\n",
       "  486  0.068788  0.068622  0.069720  0.140033  0.091490         0.484361\n",
       "  487  0.069553  0.068765  0.069526  0.134458  0.091130         0.480203\n",
       "  488  0.069353  0.067717  0.068303  0.137114  0.090227         0.481565,\n",
       "  0.06904866706033676),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  485  0.070665  0.069234  0.069092  0.196585  0.091306         0.488536\n",
       "  486  0.068788  0.068622  0.069720  0.140033  0.091490         0.484361\n",
       "  487  0.069553  0.068765  0.069526  0.134458  0.091130         0.480203\n",
       "  488  0.069353  0.067717  0.068303  0.137114  0.090227         0.481565\n",
       "  489  0.068061  0.068208  0.069218  0.099231  0.089501         0.497371,\n",
       "  0.06868437404570221),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  486  0.068788  0.068622  0.069720  0.140033  0.091490         0.484361\n",
       "  487  0.069553  0.068765  0.069526  0.134458  0.091130         0.480203\n",
       "  488  0.069353  0.067717  0.068303  0.137114  0.090227         0.481565\n",
       "  489  0.068061  0.068208  0.069218  0.099231  0.089501         0.497371\n",
       "  490  0.069505  0.068115  0.069601  0.111677  0.090840         0.484396,\n",
       "  0.06602654286304384),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  487  0.069553  0.068765  0.069526  0.134458  0.091130         0.480203\n",
       "  488  0.069353  0.067717  0.068303  0.137114  0.090227         0.481565\n",
       "  489  0.068061  0.068208  0.069218  0.099231  0.089501         0.497371\n",
       "  490  0.069505  0.068115  0.069601  0.111677  0.090840         0.484396\n",
       "  491  0.068810  0.067310  0.067712  0.132787  0.090484         0.467246,\n",
       "  0.06404215195126496),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  488  0.069353  0.067717  0.068303  0.137114  0.090227         0.481565\n",
       "  489  0.068061  0.068208  0.069218  0.099231  0.089501         0.497371\n",
       "  490  0.069505  0.068115  0.069601  0.111677  0.090840         0.484396\n",
       "  491  0.068810  0.067310  0.067712  0.132787  0.090484         0.467246\n",
       "  492  0.066521  0.065272  0.065214  0.172546  0.087888         0.472282,\n",
       "  0.06599778162210397),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  489  0.068061  0.068208  0.069218  0.099231  0.089501         0.497371\n",
       "  490  0.069505  0.068115  0.069601  0.111677  0.090840         0.484396\n",
       "  491  0.068810  0.067310  0.067712  0.132787  0.090484         0.467246\n",
       "  492  0.066521  0.065272  0.065214  0.172546  0.087888         0.472282\n",
       "  493  0.063965  0.065447  0.064769  0.153798  0.085951         0.501744,\n",
       "  0.06562870629340214),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  490  0.069505  0.068115  0.069601  0.111677  0.090840         0.484396\n",
       "  491  0.068810  0.067310  0.067712  0.132787  0.090484         0.467246\n",
       "  492  0.066521  0.065272  0.065214  0.172546  0.087888         0.472282\n",
       "  493  0.063965  0.065447  0.064769  0.153798  0.085951         0.501744\n",
       "  494  0.067474  0.066103  0.066801  0.123836  0.087860         0.484361,\n",
       "  0.06661131306525124),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  491  0.068810  0.067310  0.067712  0.132787  0.090484         0.467246\n",
       "  492  0.066521  0.065272  0.065214  0.172546  0.087888         0.472282\n",
       "  493  0.063965  0.065447  0.064769  0.153798  0.085951         0.501744\n",
       "  494  0.067474  0.066103  0.066801  0.123836  0.087860         0.484361\n",
       "  495  0.065534  0.065878  0.066656  0.095906  0.087500         0.494468,\n",
       "  0.06734227581801049),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  492  0.066521  0.065272  0.065214  0.172546  0.087888         0.472282\n",
       "  493  0.063965  0.065447  0.064769  0.153798  0.085951         0.501744\n",
       "  494  0.067474  0.066103  0.066801  0.123836  0.087860         0.484361\n",
       "  495  0.065534  0.065878  0.066656  0.095906  0.087500         0.494468\n",
       "  496  0.068258  0.067009  0.068579  0.139598  0.088460         0.492586,\n",
       "  0.06636925291902059),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  493  0.063965  0.065447  0.064769  0.153798  0.085951         0.501744\n",
       "  494  0.067474  0.066103  0.066801  0.123836  0.087860         0.484361\n",
       "  495  0.065534  0.065878  0.066656  0.095906  0.087500         0.494468\n",
       "  496  0.068258  0.067009  0.068579  0.139598  0.088460         0.492586\n",
       "  497  0.068254  0.067051  0.067918  0.104762  0.089173         0.479845,\n",
       "  0.06571258442564466),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  494  0.067474  0.066103  0.066801  0.123836  0.087860         0.484361\n",
       "  495  0.065534  0.065878  0.066656  0.095906  0.087500         0.494468\n",
       "  496  0.068258  0.067009  0.068579  0.139598  0.088460         0.492586\n",
       "  497  0.068254  0.067051  0.067918  0.104762  0.089173         0.479845\n",
       "  498  0.066107  0.065390  0.066973  0.103568  0.088223         0.482210,\n",
       "  0.06434651689344266),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  495  0.065534  0.065878  0.066656  0.095906  0.087500         0.494468\n",
       "  496  0.068258  0.067009  0.068579  0.139598  0.088460         0.492586\n",
       "  497  0.068254  0.067051  0.067918  0.104762  0.089173         0.479845\n",
       "  498  0.066107  0.065390  0.066973  0.103568  0.088223         0.482210\n",
       "  499  0.066232  0.064824  0.065893  0.091518  0.087582         0.476906,\n",
       "  0.06456939967159263),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  496  0.068258  0.067009  0.068579  0.139598  0.088460         0.492586\n",
       "  497  0.068254  0.067051  0.067918  0.104762  0.089173         0.479845\n",
       "  498  0.066107  0.065390  0.066973  0.103568  0.088223         0.482210\n",
       "  499  0.066232  0.064824  0.065893  0.091518  0.087582         0.476906\n",
       "  500  0.065534  0.064788  0.066055  0.120750  0.086248         0.488787,\n",
       "  0.06733508791336609),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  497  0.068254  0.067051  0.067918  0.104762  0.089173         0.479845\n",
       "  498  0.066107  0.065390  0.066973  0.103568  0.088223         0.482210\n",
       "  499  0.066232  0.064824  0.065893  0.091518  0.087582         0.476906\n",
       "  500  0.065534  0.064788  0.066055  0.120750  0.086248         0.488787\n",
       "  501  0.066162  0.066338  0.067007  0.162827  0.086465         0.507801,\n",
       "  0.06895279946465795),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  498  0.066107  0.065390  0.066973  0.103568  0.088223         0.482210\n",
       "  499  0.066232  0.064824  0.065893  0.091518  0.087582         0.476906\n",
       "  500  0.065534  0.064788  0.066055  0.120750  0.086248         0.488787\n",
       "  501  0.066162  0.066338  0.067007  0.162827  0.086465         0.507801\n",
       "  502  0.068417  0.067945  0.069313  0.142273  0.089166         0.499217,\n",
       "  0.06849264847670691),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  499  0.066232  0.064824  0.065893  0.091518  0.087582         0.476906\n",
       "  500  0.065534  0.064788  0.066055  0.120750  0.086248         0.488787\n",
       "  501  0.066162  0.066338  0.067007  0.162827  0.086465         0.507801\n",
       "  502  0.068417  0.067945  0.069313  0.142273  0.089166         0.499217\n",
       "  503  0.069118  0.068312  0.069749  0.123472  0.090746         0.483680,\n",
       "  0.06790787827449951),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  500  0.065534  0.064788  0.066055  0.120750  0.086248         0.488787\n",
       "  501  0.066162  0.066338  0.067007  0.162827  0.086465         0.507801\n",
       "  502  0.068417  0.067945  0.069313  0.142273  0.089166         0.499217\n",
       "  503  0.069118  0.068312  0.069749  0.123472  0.090746         0.483680\n",
       "  504  0.069098  0.067622  0.068981  0.120358  0.090297         0.482748,\n",
       "  0.0664555366418402),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  501  0.066162  0.066338  0.067007  0.162827  0.086465         0.507801\n",
       "  502  0.068417  0.067945  0.069313  0.142273  0.089166         0.499217\n",
       "  503  0.069118  0.068312  0.069749  0.123472  0.090746         0.483680\n",
       "  504  0.069098  0.067622  0.068981  0.120358  0.090297         0.482748\n",
       "  505  0.067123  0.065705  0.067641  0.097764  0.089726         0.476260,\n",
       "  0.06669279522927896),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  502  0.068417  0.067945  0.069313  0.142273  0.089166         0.499217\n",
       "  503  0.069118  0.068312  0.069749  0.123472  0.090746         0.483680\n",
       "  504  0.069098  0.067622  0.068981  0.120358  0.090297         0.482748\n",
       "  505  0.067123  0.065705  0.067641  0.097764  0.089726         0.476260\n",
       "  506  0.067033  0.065767  0.067549  0.101189  0.088307         0.488895,\n",
       "  0.06552804676129376),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  503  0.069118  0.068312  0.069749  0.123472  0.090746         0.483680\n",
       "  504  0.069098  0.067622  0.068981  0.120358  0.090297         0.482748\n",
       "  505  0.067123  0.065705  0.067641  0.097764  0.089726         0.476260\n",
       "  506  0.067033  0.065767  0.067549  0.101189  0.088307         0.488895\n",
       "  507  0.067012  0.065532  0.067021  0.088497  0.088539         0.478411,\n",
       "  0.06559994505246228),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  504  0.069098  0.067622  0.068981  0.120358  0.090297         0.482748\n",
       "  505  0.067123  0.065705  0.067641  0.097764  0.089726         0.476260\n",
       "  506  0.067033  0.065767  0.067549  0.101189  0.088307         0.488895\n",
       "  507  0.067012  0.065532  0.067021  0.088497  0.088539         0.478411\n",
       "  508  0.066242  0.065089  0.066924  0.084610  0.087402         0.487658,\n",
       "  0.06548730567927989),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  505  0.067123  0.065705  0.067641  0.097764  0.089726         0.476260\n",
       "  506  0.067033  0.065767  0.067549  0.101189  0.088307         0.488895\n",
       "  507  0.067012  0.065532  0.067021  0.088497  0.088539         0.478411\n",
       "  508  0.066242  0.065089  0.066924  0.084610  0.087402         0.487658\n",
       "  509  0.065914  0.064833  0.066983  0.059902  0.087472         0.486278,\n",
       "  0.06733029597693652),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  506  0.067033  0.065767  0.067549  0.101189  0.088307         0.488895\n",
       "  507  0.067012  0.065532  0.067021  0.088497  0.088539         0.478411\n",
       "  508  0.066242  0.065089  0.066924  0.084610  0.087402         0.487658\n",
       "  509  0.065914  0.064833  0.066983  0.059902  0.087472         0.486278\n",
       "  510  0.066523  0.066167  0.067588  0.101186  0.087362         0.500902,\n",
       "  0.0668509772432671),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  507  0.067012  0.065532  0.067021  0.088497  0.088539         0.478411\n",
       "  508  0.066242  0.065089  0.066924  0.084610  0.087402         0.487658\n",
       "  509  0.065914  0.064833  0.066983  0.059902  0.087472         0.486278\n",
       "  510  0.066523  0.066167  0.067588  0.101186  0.087362         0.500902\n",
       "  511  0.067767  0.066492  0.068121  0.108386  0.089162         0.483536,\n",
       "  0.06728236699027826),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  508  0.066242  0.065089  0.066924  0.084610  0.087402         0.487658\n",
       "  509  0.065914  0.064833  0.066983  0.059902  0.087472         0.486278\n",
       "  510  0.066523  0.066167  0.067588  0.101186  0.087362         0.500902\n",
       "  511  0.067767  0.066492  0.068121  0.108386  0.089162         0.483536\n",
       "  512  0.067390  0.067044  0.068438  0.098277  0.088694         0.490346,\n",
       "  0.06675031771115869),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  509  0.065914  0.064833  0.066983  0.059902  0.087472         0.486278\n",
       "  510  0.066523  0.066167  0.067588  0.101186  0.087362         0.500902\n",
       "  511  0.067767  0.066492  0.068121  0.108386  0.089162         0.483536\n",
       "  512  0.067390  0.067044  0.068438  0.098277  0.088694         0.490346\n",
       "  513  0.067938  0.066452  0.068363  0.071899  0.089115         0.483142,\n",
       "  0.06676708948866228),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  510  0.066523  0.066167  0.067588  0.101186  0.087362         0.500902\n",
       "  511  0.067767  0.066492  0.068121  0.108386  0.089162         0.483536\n",
       "  512  0.067390  0.067044  0.068438  0.098277  0.088694         0.490346\n",
       "  513  0.067938  0.066452  0.068363  0.071899  0.089115         0.483142\n",
       "  514  0.067818  0.066246  0.068119  0.064918  0.088595         0.487246,\n",
       "  0.06814754245251536),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  511  0.067767  0.066492  0.068121  0.108386  0.089162         0.483536\n",
       "  512  0.067390  0.067044  0.068438  0.098277  0.088694         0.490346\n",
       "  513  0.067938  0.066452  0.068363  0.071899  0.089115         0.483142\n",
       "  514  0.067818  0.066246  0.068119  0.064918  0.088595         0.487246\n",
       "  515  0.067548  0.067222  0.068179  0.099005  0.088612         0.497443,\n",
       "  0.0671361744397264),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  512  0.067390  0.067044  0.068438  0.098277  0.088694         0.490346\n",
       "  513  0.067938  0.066452  0.068363  0.071899  0.089115         0.483142\n",
       "  514  0.067818  0.066246  0.068119  0.064918  0.088595         0.487246\n",
       "  515  0.067548  0.067222  0.068179  0.099005  0.088612         0.497443\n",
       "  516  0.068085  0.066698  0.068717  0.090487  0.089960         0.479558,\n",
       "  0.06674073383829951),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  513  0.067938  0.066452  0.068363  0.071899  0.089115         0.483142\n",
       "  514  0.067818  0.066246  0.068119  0.064918  0.088595         0.487246\n",
       "  515  0.067548  0.067222  0.068179  0.099005  0.088612         0.497443\n",
       "  516  0.068085  0.066698  0.068717  0.090487  0.089960         0.479558\n",
       "  517  0.067433  0.066357  0.068169  0.093354  0.088972         0.484164,\n",
       "  0.06658734376074096),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  514  0.067818  0.066246  0.068119  0.064918  0.088595         0.487246\n",
       "  515  0.067548  0.067222  0.068179  0.099005  0.088612         0.497443\n",
       "  516  0.068085  0.066698  0.068717  0.090487  0.089960         0.479558\n",
       "  517  0.067433  0.066357  0.068169  0.093354  0.088972         0.484164\n",
       "  518  0.066790  0.066129  0.068005  0.074903  0.088586         0.485974,\n",
       "  0.06808283206599124),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  515  0.067548  0.067222  0.068179  0.099005  0.088612         0.497443\n",
       "  516  0.068085  0.066698  0.068717  0.090487  0.089960         0.479558\n",
       "  517  0.067433  0.066357  0.068169  0.093354  0.088972         0.484164\n",
       "  518  0.066790  0.066129  0.068005  0.074903  0.088586         0.485974\n",
       "  519  0.067161  0.067054  0.068259  0.110069  0.088436         0.498303,\n",
       "  0.06995458360458773),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  516  0.068085  0.066698  0.068717  0.090487  0.089960         0.479558\n",
       "  517  0.067433  0.066357  0.068169  0.093354  0.088972         0.484164\n",
       "  518  0.066790  0.066129  0.068005  0.074903  0.088586         0.485974\n",
       "  519  0.067161  0.067054  0.068259  0.110069  0.088436         0.498303\n",
       "  520  0.068783  0.068959  0.069981  0.132277  0.089897         0.501117,\n",
       "  0.07347040234483883),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  517  0.067433  0.066357  0.068169  0.093354  0.088972         0.484164\n",
       "  518  0.066790  0.066129  0.068005  0.074903  0.088586         0.485974\n",
       "  519  0.067161  0.067054  0.068259  0.110069  0.088436         0.498303\n",
       "  520  0.068783  0.068959  0.069981  0.132277  0.089897         0.501117\n",
       "  521  0.071226  0.072284  0.072457  0.237499  0.091725         0.513411,\n",
       "  0.07284968299704717),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  518  0.066790  0.066129  0.068005  0.074903  0.088586         0.485974\n",
       "  519  0.067161  0.067054  0.068259  0.110069  0.088436         0.498303\n",
       "  520  0.068783  0.068959  0.069981  0.132277  0.089897         0.501117\n",
       "  521  0.071226  0.072284  0.072457  0.237499  0.091725         0.513411\n",
       "  522  0.073428  0.072170  0.074111  0.130426  0.095158         0.482479,\n",
       "  0.07426367951590743),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  519  0.067161  0.067054  0.068259  0.110069  0.088436         0.498303\n",
       "  520  0.068783  0.068959  0.069981  0.132277  0.089897         0.501117\n",
       "  521  0.071226  0.072284  0.072457  0.237499  0.091725         0.513411\n",
       "  522  0.073428  0.072170  0.074111  0.130426  0.095158         0.482479\n",
       "  523  0.074193  0.073063  0.075276  0.190555  0.094552         0.497694,\n",
       "  0.07541644814281864),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  520  0.068783  0.068959  0.069981  0.132277  0.089897         0.501117\n",
       "  521  0.071226  0.072284  0.072457  0.237499  0.091725         0.513411\n",
       "  522  0.073428  0.072170  0.074111  0.130426  0.095158         0.482479\n",
       "  523  0.074193  0.073063  0.075276  0.190555  0.094552         0.497694\n",
       "  524  0.074800  0.075111  0.075937  0.172763  0.095933         0.495741,\n",
       "  0.0728089419150333),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  521  0.071226  0.072284  0.072457  0.237499  0.091725         0.513411\n",
       "  522  0.073428  0.072170  0.074111  0.130426  0.095158         0.482479\n",
       "  523  0.074193  0.073063  0.075276  0.190555  0.094552         0.497694\n",
       "  524  0.074800  0.075111  0.075937  0.172763  0.095933         0.495741\n",
       "  525  0.076114  0.074353  0.071885  0.347555  0.097059         0.467622,\n",
       "  0.07117685455445268),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  522  0.073428  0.072170  0.074111  0.130426  0.095158         0.482479\n",
       "  523  0.074193  0.073063  0.075276  0.190555  0.094552         0.497694\n",
       "  524  0.074800  0.075111  0.075937  0.172763  0.095933         0.495741\n",
       "  525  0.076114  0.074353  0.071885  0.347555  0.097059         0.467622\n",
       "  526  0.074102  0.072433  0.072340  0.222409  0.094512         0.474916,\n",
       "  0.07353750869957776),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  523  0.074193  0.073063  0.075276  0.190555  0.094552         0.497694\n",
       "  524  0.074800  0.075111  0.075937  0.172763  0.095933         0.495741\n",
       "  525  0.076114  0.074353  0.071885  0.347555  0.097059         0.467622\n",
       "  526  0.074102  0.072433  0.072340  0.222409  0.094512         0.474916\n",
       "  527  0.072465  0.072684  0.073610  0.259972  0.092918         0.504773,\n",
       "  0.07280175401038892),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  524  0.074800  0.075111  0.075937  0.172763  0.095933         0.495741\n",
       "  525  0.076114  0.074353  0.071885  0.347555  0.097059         0.467622\n",
       "  526  0.074102  0.072433  0.072340  0.222409  0.094512         0.474916\n",
       "  527  0.072465  0.072684  0.073610  0.259972  0.092918         0.504773\n",
       "  528  0.073591  0.072447  0.073973  0.112998  0.095224         0.481619,\n",
       "  0.07284968299704717),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  525  0.076114  0.074353  0.071885  0.347555  0.097059         0.467622\n",
       "  526  0.074102  0.072433  0.072340  0.222409  0.094512         0.474916\n",
       "  527  0.072465  0.072684  0.073610  0.259972  0.092918         0.504773\n",
       "  528  0.073591  0.072447  0.073973  0.112998  0.095224         0.481619\n",
       "  529  0.073736  0.072921  0.074249  0.139454  0.094505         0.487479,\n",
       "  0.07354230063600735),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  526  0.074102  0.072433  0.072340  0.222409  0.094512         0.474916\n",
       "  527  0.072465  0.072684  0.073610  0.259972  0.092918         0.504773\n",
       "  528  0.073591  0.072447  0.073973  0.112998  0.095224         0.481619\n",
       "  529  0.073736  0.072921  0.074249  0.139454  0.094505         0.487479\n",
       "  530  0.073717  0.072502  0.074072  0.128560  0.094552         0.492300,\n",
       "  0.07259564300974253),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  527  0.072465  0.072684  0.073610  0.259972  0.092918         0.504773\n",
       "  528  0.073591  0.072447  0.073973  0.112998  0.095224         0.481619\n",
       "  529  0.073736  0.072921  0.074249  0.139454  0.094505         0.487479\n",
       "  530  0.073717  0.072502  0.074072  0.128560  0.094552         0.492300\n",
       "  531  0.073909  0.073341  0.074218  0.142845  0.095228         0.480042,\n",
       "  0.07275381540136837),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  528  0.073591  0.072447  0.073973  0.112998  0.095224         0.481619\n",
       "  529  0.073736  0.072921  0.074249  0.139454  0.094505         0.487479\n",
       "  530  0.073717  0.072502  0.074072  0.128560  0.094552         0.492300\n",
       "  531  0.073909  0.073341  0.074218  0.142845  0.095228         0.480042\n",
       "  532  0.073375  0.072336  0.073774  0.124133  0.094304         0.488303,\n",
       "  0.07235118689529708),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  529  0.073736  0.072921  0.074249  0.139454  0.094505         0.487479\n",
       "  530  0.073717  0.072502  0.074072  0.128560  0.094552         0.492300\n",
       "  531  0.073909  0.073341  0.074218  0.142845  0.095228         0.480042\n",
       "  532  0.073375  0.072336  0.073774  0.124133  0.094304         0.488303\n",
       "  533  0.073628  0.072014  0.073985  0.080436  0.094458         0.484110,\n",
       "  0.07224094349032949),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  530  0.073717  0.072502  0.074072  0.128560  0.094552         0.492300\n",
       "  531  0.073909  0.073341  0.074218  0.142845  0.095228         0.480042\n",
       "  532  0.073375  0.072336  0.073774  0.124133  0.094304         0.488303\n",
       "  533  0.073628  0.072014  0.073985  0.080436  0.094458         0.484110\n",
       "  534  0.072850  0.072104  0.073837  0.088730  0.094065         0.486296,\n",
       "  0.07286405880633595),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  531  0.073909  0.073341  0.074218  0.142845  0.095228         0.480042\n",
       "  532  0.073375  0.072336  0.073774  0.124133  0.094304         0.488303\n",
       "  533  0.073628  0.072014  0.073985  0.080436  0.094458         0.484110\n",
       "  534  0.072850  0.072104  0.073837  0.088730  0.094065         0.486296\n",
       "  535  0.072703  0.072213  0.073205  0.132824  0.093957         0.491780,\n",
       "  0.0756585082890493),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  532  0.073375  0.072336  0.073774  0.124133  0.094304         0.488303\n",
       "  533  0.073628  0.072014  0.073985  0.080436  0.094458         0.484110\n",
       "  534  0.072850  0.072104  0.073837  0.088730  0.094065         0.486296\n",
       "  535  0.072703  0.072213  0.073205  0.132824  0.093957         0.491780\n",
       "  536  0.073589  0.074419  0.074382  0.161941  0.094566         0.508017,\n",
       "  0.07472622647207325),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  533  0.073628  0.072014  0.073985  0.080436  0.094458         0.484110\n",
       "  534  0.072850  0.072104  0.073837  0.088730  0.094065         0.486296\n",
       "  535  0.072703  0.072213  0.073205  0.132824  0.093957         0.491780\n",
       "  536  0.073589  0.074419  0.074382  0.161941  0.094566         0.508017\n",
       "  537  0.075931  0.075004  0.076274  0.140731  0.097295         0.480149,\n",
       "  0.07680888094774571),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  534  0.072850  0.072104  0.073837  0.088730  0.094065         0.486296\n",
       "  535  0.072703  0.072213  0.073205  0.132824  0.093957         0.491780\n",
       "  536  0.073589  0.074419  0.074382  0.161941  0.094566         0.508017\n",
       "  537  0.075931  0.075004  0.076274  0.140731  0.097295         0.480149\n",
       "  538  0.074853  0.075874  0.076005  0.178352  0.096385         0.502694,\n",
       "  0.07884599278261244),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  535  0.072703  0.072213  0.073205  0.132824  0.093957         0.491780\n",
       "  536  0.073589  0.074419  0.074382  0.161941  0.094566         0.508017\n",
       "  537  0.075931  0.075004  0.076274  0.140731  0.097295         0.480149\n",
       "  538  0.074853  0.075874  0.076005  0.178352  0.096385         0.502694\n",
       "  539  0.078361  0.078253  0.079316  0.184355  0.098418         0.502353,\n",
       "  0.07828279591670051),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  536  0.073589  0.074419  0.074382  0.161941  0.094566         0.508017\n",
       "  537  0.075931  0.075004  0.076274  0.140731  0.097295         0.480149\n",
       "  538  0.074853  0.075874  0.076005  0.178352  0.096385         0.502694\n",
       "  539  0.078361  0.078253  0.079316  0.184355  0.098418         0.502353\n",
       "  540  0.079988  0.079651  0.079067  0.237996  0.100408         0.482909,\n",
       "  0.07824684677111625),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  537  0.075931  0.075004  0.076274  0.140731  0.097295         0.480149\n",
       "  538  0.074853  0.075874  0.076005  0.178352  0.096385         0.502694\n",
       "  539  0.078361  0.078253  0.079316  0.184355  0.098418         0.502353\n",
       "  540  0.079988  0.079651  0.079067  0.237996  0.100408         0.482909\n",
       "  541  0.078414  0.077770  0.079399  0.136976  0.099858         0.486852,\n",
       "  0.07847212551748102),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  538  0.074853  0.075874  0.076005  0.178352  0.096385         0.502694\n",
       "  539  0.078361  0.078253  0.079316  0.184355  0.098418         0.502353\n",
       "  540  0.079988  0.079651  0.079067  0.237996  0.100408         0.482909\n",
       "  541  0.078414  0.077770  0.079399  0.136976  0.099858         0.486852\n",
       "  542  0.079266  0.077715  0.079457  0.117726  0.099823         0.488805,\n",
       "  0.07843857234011156),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  539  0.078361  0.078253  0.079316  0.184355  0.098418         0.502353\n",
       "  540  0.079988  0.079651  0.079067  0.237996  0.100408         0.482909\n",
       "  541  0.078414  0.077770  0.079399  0.136976  0.099858         0.486852\n",
       "  542  0.079266  0.077715  0.079457  0.117726  0.099823         0.488805\n",
       "  543  0.079074  0.077673  0.079835  0.088015  0.100043         0.486870,\n",
       "  0.0771084539534938),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  540  0.079988  0.079651  0.079067  0.237996  0.100408         0.482909\n",
       "  541  0.078414  0.077770  0.079399  0.136976  0.099858         0.486852\n",
       "  542  0.079266  0.077715  0.079457  0.117726  0.099823         0.488805\n",
       "  543  0.079074  0.077673  0.079835  0.088015  0.100043         0.486870\n",
       "  544  0.079059  0.077602  0.078812  0.104966  0.100010         0.477174,\n",
       "  0.07684243412511518),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  541  0.078414  0.077770  0.079399  0.136976  0.099858         0.486852\n",
       "  542  0.079266  0.077715  0.079457  0.117726  0.099823         0.488805\n",
       "  543  0.079074  0.077673  0.079835  0.088015  0.100043         0.486870\n",
       "  544  0.079059  0.077602  0.078812  0.104966  0.100010         0.477174\n",
       "  545  0.077278  0.076310  0.077689  0.126653  0.098711         0.485131,\n",
       "  0.07652368375128642),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  542  0.079266  0.077715  0.079457  0.117726  0.099823         0.488805\n",
       "  543  0.079074  0.077673  0.079835  0.088015  0.100043         0.486870\n",
       "  544  0.079059  0.077602  0.078812  0.104966  0.100010         0.477174\n",
       "  545  0.077278  0.076310  0.077689  0.126653  0.098711         0.485131\n",
       "  546  0.077926  0.076547  0.077882  0.146265  0.098451         0.484737,\n",
       "  0.07814139530257826),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  543  0.079074  0.077673  0.079835  0.088015  0.100043         0.486870\n",
       "  544  0.079059  0.077602  0.078812  0.104966  0.100010         0.477174\n",
       "  545  0.077278  0.076310  0.077689  0.126653  0.098711         0.485131\n",
       "  546  0.077926  0.076547  0.077882  0.146265  0.098451         0.484737\n",
       "  547  0.077262  0.077673  0.078384  0.279708  0.098140         0.499217,\n",
       "  0.08619639025930573),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  544  0.079059  0.077602  0.078812  0.104966  0.100010         0.477174\n",
       "  545  0.077278  0.076310  0.077689  0.126653  0.098711         0.485131\n",
       "  546  0.077926  0.076547  0.077882  0.146265  0.098451         0.484737\n",
       "  547  0.077262  0.077673  0.078384  0.279708  0.098140         0.499217\n",
       "  548  0.086590  0.084924  0.086624  0.283220  0.099720         0.547353,\n",
       "  0.09125562629146534),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  545  0.077278  0.076310  0.077689  0.126653  0.098711         0.485131\n",
       "  546  0.077926  0.076547  0.077882  0.146265  0.098451         0.484737\n",
       "  547  0.077262  0.077673  0.078384  0.279708  0.098140         0.499217\n",
       "  548  0.086590  0.084924  0.086624  0.283220  0.099720         0.547353\n",
       "  549  0.087381  0.090740  0.088266  0.367296  0.107586         0.524952,\n",
       "  0.08946536653925888),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  546  0.077926  0.076547  0.077882  0.146265  0.098451         0.484737\n",
       "  547  0.077262  0.077673  0.078384  0.279708  0.098140         0.499217\n",
       "  548  0.086590  0.084924  0.086624  0.283220  0.099720         0.547353\n",
       "  549  0.087381  0.090740  0.088266  0.367296  0.107586         0.524952\n",
       "  550  0.090751  0.089133  0.090940  0.210549  0.112527         0.473734,\n",
       "  0.09265045506460724),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  547  0.077262  0.077673  0.078384  0.279708  0.098140         0.499217\n",
       "  548  0.086590  0.084924  0.086624  0.283220  0.099720         0.547353\n",
       "  549  0.087381  0.090740  0.088266  0.367296  0.107586         0.524952\n",
       "  550  0.090751  0.089133  0.090940  0.210549  0.112527         0.473734\n",
       "  551  0.091042  0.091655  0.091836  0.223375  0.110779         0.510938,\n",
       "  0.0922909636087646),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  548  0.086590  0.084924  0.086624  0.283220  0.099720         0.547353\n",
       "  549  0.087381  0.090740  0.088266  0.367296  0.107586         0.524952\n",
       "  550  0.090751  0.089133  0.090940  0.210549  0.112527         0.473734\n",
       "  551  0.091042  0.091655  0.091836  0.223375  0.110779         0.510938\n",
       "  552  0.093494  0.092474  0.094038  0.170939  0.113889         0.484432,\n",
       "  0.08991832000020322),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  549  0.087381  0.090740  0.088266  0.367296  0.107586         0.524952\n",
       "  550  0.090751  0.089133  0.090940  0.210549  0.112527         0.473734\n",
       "  551  0.091042  0.091655  0.091836  0.223375  0.110779         0.510938\n",
       "  552  0.093494  0.092474  0.094038  0.170939  0.113889         0.484432\n",
       "  553  0.092584  0.090882  0.091202  0.160535  0.113538         0.469379,\n",
       "  0.09024665424689118),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  550  0.090751  0.089133  0.090940  0.210549  0.112527         0.473734\n",
       "  551  0.091042  0.091655  0.091836  0.223375  0.110779         0.510938\n",
       "  552  0.093494  0.092474  0.094038  0.170939  0.113889         0.484432\n",
       "  553  0.092584  0.090882  0.091202  0.160535  0.113538         0.469379\n",
       "  554  0.090399  0.089749  0.090601  0.159541  0.111221         0.489576,\n",
       "  0.09020351719666253),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  551  0.091042  0.091655  0.091836  0.223375  0.110779         0.510938\n",
       "  552  0.093494  0.092474  0.094038  0.170939  0.113889         0.484432\n",
       "  553  0.092584  0.090882  0.091202  0.160535  0.113538         0.469379\n",
       "  554  0.090399  0.089749  0.090601  0.159541  0.111221         0.489576\n",
       "  555  0.091179  0.090152  0.091529  0.152786  0.111542         0.486798,\n",
       "  0.08807293373433181),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  552  0.093494  0.092474  0.094038  0.170939  0.113889         0.484432\n",
       "  553  0.092584  0.090882  0.091202  0.160535  0.113538         0.469379\n",
       "  554  0.090399  0.089749  0.090601  0.159541  0.111221         0.489576\n",
       "  555  0.091179  0.090152  0.091529  0.152786  0.111542         0.486798\n",
       "  556  0.091347  0.089434  0.089451  0.131890  0.111500         0.471189,\n",
       "  0.08865051603189483),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  553  0.092584  0.090882  0.091202  0.160535  0.113538         0.469379\n",
       "  554  0.090399  0.089749  0.090601  0.159541  0.111221         0.489576\n",
       "  555  0.091179  0.090152  0.091529  0.152786  0.111542         0.486798\n",
       "  556  0.091347  0.089434  0.089451  0.131890  0.111500         0.471189\n",
       "  557  0.088638  0.088154  0.089722  0.127278  0.109419         0.491440,\n",
       "  0.08910347911520146),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  554  0.090399  0.089749  0.090601  0.159541  0.111221         0.489576\n",
       "  555  0.091179  0.090152  0.091529  0.152786  0.111542         0.486798\n",
       "  556  0.091347  0.089434  0.089451  0.131890  0.111500         0.471189\n",
       "  557  0.088638  0.088154  0.089722  0.127278  0.109419         0.491440\n",
       "  558  0.089776  0.088159  0.089402  0.119276  0.109983         0.490508,\n",
       "  0.0903377299061404),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  555  0.091179  0.090152  0.091529  0.152786  0.111542         0.486798\n",
       "  556  0.091347  0.089434  0.089451  0.131890  0.111500         0.471189\n",
       "  557  0.088638  0.088154  0.089722  0.127278  0.109419         0.491440\n",
       "  558  0.089776  0.088159  0.089402  0.119276  0.109983         0.490508\n",
       "  559  0.090146  0.089560  0.091163  0.121314  0.110425         0.496350,\n",
       "  0.08928801677955236),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  556  0.091347  0.089434  0.089451  0.131890  0.111500         0.471189\n",
       "  557  0.088638  0.088154  0.089722  0.127278  0.109419         0.491440\n",
       "  558  0.089776  0.088159  0.089402  0.119276  0.109983         0.490508\n",
       "  559  0.090146  0.089560  0.091163  0.121314  0.110425         0.496350\n",
       "  560  0.090912  0.089408  0.091027  0.119018  0.111631         0.479271,\n",
       "  0.08987039101354496),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  557  0.088638  0.088154  0.089722  0.127278  0.109419         0.491440\n",
       "  558  0.089776  0.088159  0.089402  0.119276  0.109983         0.490508\n",
       "  559  0.090146  0.089560  0.091163  0.121314  0.110425         0.496350\n",
       "  560  0.090912  0.089408  0.091027  0.119018  0.111631         0.479271\n",
       "  561  0.089323  0.090053  0.090260  0.193640  0.110605         0.491475,\n",
       "  0.08930239258884116),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  558  0.089776  0.088159  0.089402  0.119276  0.109983         0.490508\n",
       "  559  0.090146  0.089560  0.091163  0.121314  0.110425         0.496350\n",
       "  560  0.090912  0.089408  0.091027  0.119018  0.111631         0.479271\n",
       "  561  0.089323  0.090053  0.090260  0.193640  0.110605         0.491475\n",
       "  562  0.090803  0.089519  0.091049  0.118583  0.111174         0.482873,\n",
       "  0.08952287939877633),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  559  0.090146  0.089560  0.091163  0.121314  0.110425         0.496350\n",
       "  560  0.090912  0.089408  0.091027  0.119018  0.111631         0.479271\n",
       "  561  0.089323  0.090053  0.090260  0.193640  0.110605         0.491475\n",
       "  562  0.090803  0.089519  0.091049  0.118583  0.111174         0.482873\n",
       "  563  0.090149  0.088389  0.090669  0.067929  0.110619         0.488769,\n",
       "  0.0913155447415599),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  560  0.090912  0.089408  0.091027  0.119018  0.111631         0.479271\n",
       "  561  0.089323  0.090053  0.090260  0.193640  0.110605         0.491475\n",
       "  562  0.090803  0.089519  0.091049  0.118583  0.111174         0.482873\n",
       "  563  0.090149  0.088389  0.090669  0.067929  0.110619         0.488769\n",
       "  564  0.090418  0.089917  0.091601  0.105514  0.110835         0.500526,\n",
       "  0.09329753968512397),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  561  0.089323  0.090053  0.090260  0.193640  0.110605         0.491475\n",
       "  562  0.090803  0.089519  0.091049  0.118583  0.111174         0.482873\n",
       "  563  0.090149  0.088389  0.090669  0.067929  0.110619         0.488769\n",
       "  564  0.090418  0.089917  0.091601  0.105514  0.110835         0.500526\n",
       "  565  0.091814  0.091998  0.093142  0.175601  0.112586         0.501941,\n",
       "  0.09392784290577483),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  562  0.090803  0.089519  0.091049  0.118583  0.111174         0.482873\n",
       "  563  0.090149  0.088389  0.090669  0.067929  0.110619         0.488769\n",
       "  564  0.090418  0.089917  0.091601  0.105514  0.110835         0.500526\n",
       "  565  0.091814  0.091998  0.093142  0.175601  0.112586         0.501941\n",
       "  566  0.094878  0.094344  0.095911  0.203508  0.114521         0.491834,\n",
       "  0.09487930209083156),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  563  0.090149  0.088389  0.090669  0.067929  0.110619         0.488769\n",
       "  564  0.090418  0.089917  0.091601  0.105514  0.110835         0.500526\n",
       "  565  0.091814  0.091998  0.093142  0.175601  0.112586         0.501941\n",
       "  566  0.094878  0.094344  0.095911  0.203508  0.114521         0.491834\n",
       "  567  0.095256  0.093830  0.096063  0.123773  0.115137         0.494235,\n",
       "  0.09557191972979175),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  564  0.090418  0.089917  0.091601  0.105514  0.110835         0.500526\n",
       "  565  0.091814  0.091998  0.093142  0.175601  0.112586         0.501941\n",
       "  566  0.094878  0.094344  0.095911  0.203508  0.114521         0.491834\n",
       "  567  0.095256  0.093830  0.096063  0.123773  0.115137         0.494235\n",
       "  568  0.094835  0.094299  0.096170  0.133950  0.116066         0.492300,\n",
       "  0.09467558705839996),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  565  0.091814  0.091998  0.093142  0.175601  0.112586         0.501941\n",
       "  566  0.094878  0.094344  0.095911  0.203508  0.114521         0.491834\n",
       "  567  0.095256  0.093830  0.096063  0.123773  0.115137         0.494235\n",
       "  568  0.094835  0.094299  0.096170  0.133950  0.116066         0.492300\n",
       "  569  0.096074  0.094001  0.096085  0.124549  0.116742         0.480418,\n",
       "  0.09817463402114748),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  566  0.094878  0.094344  0.095911  0.203508  0.114521         0.491834\n",
       "  567  0.095256  0.093830  0.096063  0.123773  0.115137         0.494235\n",
       "  568  0.094835  0.094299  0.096170  0.133950  0.116066         0.492300\n",
       "  569  0.096074  0.094001  0.096085  0.124549  0.116742         0.480418\n",
       "  570  0.095626  0.096750  0.096855  0.204889  0.115867         0.513285,\n",
       "  0.0977815893879354),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  567  0.095256  0.093830  0.096063  0.123773  0.115137         0.494235\n",
       "  568  0.094835  0.094299  0.096170  0.133950  0.116066         0.492300\n",
       "  569  0.096074  0.094001  0.096085  0.124549  0.116742         0.480418\n",
       "  570  0.095626  0.096750  0.096855  0.204889  0.115867         0.513285\n",
       "  571  0.099099  0.097461  0.099265  0.109376  0.119284         0.484181,\n",
       "  0.09706021050803532),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  568  0.094835  0.094299  0.096170  0.133950  0.116066         0.492300\n",
       "  569  0.096074  0.094001  0.096085  0.124549  0.116742         0.480418\n",
       "  570  0.095626  0.096750  0.096855  0.204889  0.115867         0.513285\n",
       "  571  0.099099  0.097461  0.099265  0.109376  0.119284         0.484181\n",
       "  572  0.097542  0.096157  0.098834  0.042049  0.118900         0.481726,\n",
       "  0.0922070854765221),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  569  0.096074  0.094001  0.096085  0.124549  0.116742         0.480418\n",
       "  570  0.095626  0.096750  0.096855  0.204889  0.115867         0.513285\n",
       "  571  0.099099  0.097461  0.099265  0.109376  0.119284         0.484181\n",
       "  572  0.097542  0.096157  0.098834  0.042049  0.118900         0.481726\n",
       "  573  0.096789  0.094802  0.094171  0.178065  0.118196         0.450831,\n",
       "  0.09334546867178221),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  570  0.095626  0.096750  0.096855  0.204889  0.115867         0.513285\n",
       "  571  0.099099  0.097461  0.099265  0.109376  0.119284         0.484181\n",
       "  572  0.097542  0.096157  0.098834  0.042049  0.118900         0.481726\n",
       "  573  0.096789  0.094802  0.094171  0.178065  0.118196         0.450831\n",
       "  574  0.091918  0.091977  0.092301  0.189568  0.113456         0.495633,\n",
       "  0.0921831161720118),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  571  0.099099  0.097461  0.099265  0.109376  0.119284         0.484181\n",
       "  572  0.097542  0.096157  0.098834  0.042049  0.118900         0.481726\n",
       "  573  0.096789  0.094802  0.094171  0.178065  0.118196         0.450831\n",
       "  574  0.091918  0.091977  0.092301  0.189568  0.113456         0.495633\n",
       "  575  0.095073  0.093133  0.093566  0.153513  0.114568         0.478429,\n",
       "  0.09222146128581087),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  572  0.097542  0.096157  0.098834  0.042049  0.118900         0.481726\n",
       "  573  0.096789  0.094802  0.094171  0.178065  0.118196         0.450831\n",
       "  574  0.091918  0.091977  0.092301  0.189568  0.113456         0.495633\n",
       "  575  0.095073  0.093133  0.093566  0.153513  0.114568         0.478429\n",
       "  576  0.092654  0.092105  0.093396  0.135568  0.113433         0.487407,\n",
       "  0.09126041822789495),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  573  0.096789  0.094802  0.094171  0.178065  0.118196         0.450831\n",
       "  574  0.091918  0.091977  0.092301  0.189568  0.113456         0.495633\n",
       "  575  0.095073  0.093133  0.093566  0.153513  0.114568         0.478429\n",
       "  576  0.092654  0.092105  0.093396  0.135568  0.113433         0.487407\n",
       "  577  0.093085  0.091628  0.092658  0.136879  0.113470         0.479934,\n",
       "  0.09223104515867006),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  574  0.091918  0.091977  0.092301  0.189568  0.113456         0.495633\n",
       "  575  0.095073  0.093133  0.093566  0.153513  0.114568         0.478429\n",
       "  576  0.092654  0.092105  0.093396  0.135568  0.113433         0.487407\n",
       "  577  0.093085  0.091628  0.092658  0.136879  0.113470         0.479934\n",
       "  578  0.092368  0.091401  0.092863  0.119086  0.112532         0.494379,\n",
       "  0.09274632266028604),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  575  0.095073  0.093133  0.093566  0.153513  0.114568         0.478429\n",
       "  576  0.092654  0.092105  0.093396  0.135568  0.113433         0.487407\n",
       "  577  0.093085  0.091628  0.092658  0.136879  0.113470         0.479934\n",
       "  578  0.092368  0.091401  0.092863  0.119086  0.112532         0.494379\n",
       "  579  0.093427  0.091877  0.094205  0.099703  0.113480         0.490974,\n",
       "  0.09315613907100172),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  576  0.092654  0.092105  0.093396  0.135568  0.113433         0.487407\n",
       "  577  0.093085  0.091628  0.092658  0.136879  0.113470         0.479934\n",
       "  578  0.092368  0.091401  0.092863  0.119086  0.112532         0.494379\n",
       "  579  0.093427  0.091877  0.094205  0.099703  0.113480         0.490974\n",
       "  580  0.093321  0.092579  0.094116  0.108150  0.113983         0.490185,\n",
       "  0.09170140147012762),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  577  0.093085  0.091628  0.092658  0.136879  0.113470         0.479934\n",
       "  578  0.092368  0.091401  0.092863  0.119086  0.112532         0.494379\n",
       "  579  0.093427  0.091877  0.094205  0.099703  0.113480         0.490974\n",
       "  580  0.093321  0.092579  0.094116  0.108150  0.113983         0.490185\n",
       "  581  0.094108  0.092437  0.093607  0.113355  0.114383         0.476243,\n",
       "  0.09205369539896355),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  578  0.092368  0.091401  0.092863  0.119086  0.112532         0.494379\n",
       "  579  0.093427  0.091877  0.094205  0.099703  0.113480         0.490974\n",
       "  580  0.093321  0.092579  0.094116  0.108150  0.113983         0.490185\n",
       "  581  0.094108  0.092437  0.093607  0.113355  0.114383         0.476243\n",
       "  582  0.092113  0.091678  0.093021  0.096587  0.112962         0.489755,\n",
       "  0.0920105583487349),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  579  0.093427  0.091877  0.094205  0.099703  0.113480         0.490974\n",
       "  580  0.093321  0.092579  0.094116  0.108150  0.113983         0.490185\n",
       "  581  0.094108  0.092437  0.093607  0.113355  0.114383         0.476243\n",
       "  582  0.092113  0.091678  0.093021  0.096587  0.112962         0.489755\n",
       "  583  0.092830  0.091749  0.093941  0.079281  0.113306         0.486798,\n",
       "  0.09149529046948121),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  580  0.093321  0.092579  0.094116  0.108150  0.113983         0.490185\n",
       "  581  0.094108  0.092437  0.093607  0.113355  0.114383         0.476243\n",
       "  582  0.092113  0.091678  0.093021  0.096587  0.112962         0.489755\n",
       "  583  0.092830  0.091749  0.093941  0.079281  0.113306         0.486798\n",
       "  584  0.092572  0.091147  0.093089  0.101600  0.113264         0.483268,\n",
       "  0.090826632512669),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  581  0.094108  0.092437  0.093607  0.113355  0.114383         0.476243\n",
       "  582  0.092113  0.091678  0.093021  0.096587  0.112962         0.489755\n",
       "  583  0.092830  0.091749  0.093941  0.079281  0.113306         0.486798\n",
       "  584  0.092572  0.091147  0.093089  0.101600  0.113264         0.483268\n",
       "  585  0.092774  0.090944  0.092299  0.113302  0.112761         0.482121,\n",
       "  0.09157677263350894),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  582  0.092113  0.091678  0.093021  0.096587  0.112962         0.489755\n",
       "  583  0.092830  0.091749  0.093941  0.079281  0.113306         0.486798\n",
       "  584  0.092572  0.091147  0.093089  0.101600  0.113264         0.483268\n",
       "  585  0.092774  0.090944  0.092299  0.113302  0.112761         0.482121\n",
       "  586  0.091706  0.090529  0.092357  0.115413  0.112108         0.492730,\n",
       "  0.09114059095006818),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  583  0.092830  0.091749  0.093941  0.079281  0.113306         0.486798\n",
       "  584  0.092572  0.091147  0.093089  0.101600  0.113264         0.483268\n",
       "  585  0.092774  0.090944  0.092299  0.113302  0.112761         0.482121\n",
       "  586  0.091706  0.090529  0.092357  0.115413  0.112108         0.492730\n",
       "  587  0.092281  0.090614  0.092982  0.126162  0.112841         0.483859,\n",
       "  0.08694653038014566),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  584  0.092572  0.091147  0.093089  0.101600  0.113264         0.483268\n",
       "  585  0.092774  0.090944  0.092299  0.113302  0.112761         0.482121\n",
       "  586  0.091706  0.090529  0.092357  0.115413  0.112108         0.492730\n",
       "  587  0.092281  0.090614  0.092982  0.126162  0.112841         0.483859\n",
       "  588  0.092250  0.090290  0.088351  0.194895  0.112415         0.455759,\n",
       "  0.08834374549914004),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  585  0.092774  0.090944  0.092299  0.113302  0.112761         0.482121\n",
       "  586  0.091706  0.090529  0.092357  0.115413  0.112108         0.492730\n",
       "  587  0.092281  0.090614  0.092982  0.126162  0.112841         0.483859\n",
       "  588  0.092250  0.090290  0.088351  0.194895  0.112415         0.455759\n",
       "  589  0.087247  0.087107  0.087813  0.160122  0.108319         0.497568,\n",
       "  0.08697049006229363),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  586  0.091706  0.090529  0.092357  0.115413  0.112108         0.492730\n",
       "  587  0.092281  0.090614  0.092982  0.126162  0.112841         0.483859\n",
       "  588  0.092250  0.090290  0.088351  0.194895  0.112415         0.455759\n",
       "  589  0.087247  0.087107  0.087813  0.160122  0.108319         0.497568\n",
       "  590  0.089239  0.087626  0.088741  0.106144  0.109683         0.476852,\n",
       "  0.08536476797443808),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  587  0.092281  0.090614  0.092982  0.126162  0.112841         0.483859\n",
       "  588  0.092250  0.090290  0.088351  0.194895  0.112415         0.455759\n",
       "  589  0.087247  0.087107  0.087813  0.160122  0.108319         0.497568\n",
       "  590  0.089239  0.087626  0.088741  0.106144  0.109683         0.476852\n",
       "  591  0.087839  0.086145  0.086321  0.169032  0.108342         0.475114,\n",
       "  0.08521617945567143),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  588  0.092250  0.090290  0.088351  0.194895  0.112415         0.455759\n",
       "  589  0.087247  0.087107  0.087813  0.160122  0.108319         0.497568\n",
       "  590  0.089239  0.087626  0.088741  0.106144  0.109683         0.476852\n",
       "  591  0.087839  0.086145  0.086321  0.169032  0.108342         0.475114\n",
       "  592  0.086231  0.084578  0.086416  0.096913  0.106774         0.486009,\n",
       "  0.08568351834826685),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  589  0.087247  0.087107  0.087813  0.160122  0.108319         0.497568\n",
       "  590  0.089239  0.087626  0.088741  0.106144  0.109683         0.476852\n",
       "  591  0.087839  0.086145  0.086321  0.169032  0.108342         0.475114\n",
       "  592  0.086231  0.084578  0.086416  0.096913  0.106774         0.486009\n",
       "  593  0.085995  0.084775  0.086866  0.050358  0.106629         0.490615,\n",
       "  0.08819994891680298),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  590  0.089239  0.087626  0.088741  0.106144  0.109683         0.476852\n",
       "  591  0.087839  0.086145  0.086321  0.169032  0.108342         0.475114\n",
       "  592  0.086231  0.084578  0.086416  0.096913  0.106774         0.486009\n",
       "  593  0.085995  0.084775  0.086866  0.050358  0.106629         0.490615\n",
       "  594  0.086833  0.086815  0.087959  0.102828  0.107085         0.505938,\n",
       "  0.08688900789826591),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  591  0.087839  0.086145  0.086321  0.169032  0.108342         0.475114\n",
       "  592  0.086231  0.084578  0.086416  0.096913  0.106774         0.486009\n",
       "  593  0.085995  0.084775  0.086866  0.050358  0.106629         0.490615\n",
       "  594  0.086833  0.086815  0.087959  0.102828  0.107085         0.505938\n",
       "  595  0.088546  0.086934  0.088722  0.075700  0.109543         0.477318,\n",
       "  0.08639051179651583),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  592  0.086231  0.084578  0.086416  0.096913  0.106774         0.486009\n",
       "  593  0.085995  0.084775  0.086866  0.050358  0.106629         0.490615\n",
       "  594  0.086833  0.086815  0.087959  0.102828  0.107085         0.505938\n",
       "  595  0.088546  0.086934  0.088722  0.075700  0.109543         0.477318\n",
       "  596  0.087338  0.085960  0.088143  0.062152  0.108263         0.483393,\n",
       "  0.0880945070706273),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  593  0.085995  0.084775  0.086866  0.050358  0.106629         0.490615\n",
       "  594  0.086833  0.086815  0.087959  0.102828  0.107085         0.505938\n",
       "  595  0.088546  0.086934  0.088722  0.075700  0.109543         0.477318\n",
       "  596  0.087338  0.085960  0.088143  0.062152  0.108263         0.483393\n",
       "  597  0.088277  0.088849  0.088445  0.187352  0.107776         0.499862,\n",
       "  0.09184998998889425),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  594  0.086833  0.086815  0.087959  0.102828  0.107085         0.505938\n",
       "  595  0.088546  0.086934  0.088722  0.075700  0.109543         0.477318\n",
       "  596  0.087338  0.085960  0.088143  0.062152  0.108263         0.483393\n",
       "  597  0.088277  0.088849  0.088445  0.187352  0.107776         0.499862\n",
       "  598  0.088999  0.090576  0.090199  0.191757  0.109440         0.515203,\n",
       "  0.0927918556787295),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  595  0.088546  0.086934  0.088722  0.075700  0.109543         0.477318\n",
       "  596  0.087338  0.085960  0.088143  0.062152  0.108263         0.483393\n",
       "  597  0.088277  0.088849  0.088445  0.187352  0.107776         0.499862\n",
       "  598  0.088999  0.090576  0.090199  0.191757  0.109440         0.515203\n",
       "  599  0.092247  0.091418  0.092563  0.167057  0.113107         0.494164,\n",
       "  0.09192668021649236),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  596  0.087338  0.085960  0.088143  0.062152  0.108263         0.483393\n",
       "  597  0.088277  0.088849  0.088445  0.187352  0.107776         0.499862\n",
       "  598  0.088999  0.090576  0.090199  0.191757  0.109440         0.515203\n",
       "  599  0.092247  0.091418  0.092563  0.167057  0.113107         0.494164\n",
       "  600  0.093496  0.091979  0.093554  0.115547  0.114027         0.480651,\n",
       "  0.09238683120444341),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  597  0.088277  0.088849  0.088445  0.187352  0.107776         0.499862\n",
       "  598  0.088999  0.090576  0.090199  0.191757  0.109440         0.515203\n",
       "  599  0.092247  0.091418  0.092563  0.167057  0.113107         0.494164\n",
       "  600  0.093496  0.091979  0.093554  0.115547  0.114027         0.480651\n",
       "  601  0.092957  0.091595  0.093312  0.130798  0.113182         0.490561,\n",
       "  0.09333588479892303),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  598  0.088999  0.090576  0.090199  0.191757  0.109440         0.515203\n",
       "  599  0.092247  0.091418  0.092563  0.167057  0.113107         0.494164\n",
       "  600  0.093496  0.091979  0.093554  0.115547  0.114027         0.480651\n",
       "  601  0.092957  0.091595  0.093312  0.130798  0.113182         0.490561\n",
       "  602  0.092712  0.092851  0.093515  0.145078  0.113632         0.494217,\n",
       "  0.09579480250794173),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  599  0.092247  0.091418  0.092563  0.167057  0.113107         0.494164\n",
       "  600  0.093496  0.091979  0.093554  0.115547  0.114027         0.480651\n",
       "  601  0.092957  0.091595  0.093312  0.130798  0.113182         0.490561\n",
       "  602  0.092712  0.092851  0.093515  0.145078  0.113632         0.494217\n",
       "  603  0.095785  0.094795  0.096940  0.175232  0.114559         0.505508,\n",
       "  0.09706021050803532),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  600  0.093496  0.091979  0.093554  0.115547  0.114027         0.480651\n",
       "  601  0.092957  0.091595  0.093312  0.130798  0.113182         0.490561\n",
       "  602  0.092712  0.092851  0.093515  0.145078  0.113632         0.494217\n",
       "  603  0.095785  0.094795  0.096940  0.175232  0.114559         0.505508\n",
       "  604  0.096938  0.095565  0.097865  0.108719  0.116960         0.496583,\n",
       "  0.09688765268475841),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  601  0.092957  0.091595  0.093312  0.130798  0.113182         0.490561\n",
       "  602  0.092712  0.092851  0.093515  0.145078  0.113632         0.494217\n",
       "  603  0.095785  0.094795  0.096940  0.175232  0.114559         0.505508\n",
       "  604  0.096938  0.095565  0.097865  0.108719  0.116960         0.496583\n",
       "  605  0.098276  0.097461  0.098665  0.184005  0.118196         0.485830,\n",
       "  0.09521003230573433),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  602  0.092712  0.092851  0.093515  0.145078  0.113632         0.494217\n",
       "  603  0.095785  0.094795  0.096940  0.175232  0.114559         0.505508\n",
       "  604  0.096938  0.095565  0.097865  0.108719  0.116960         0.496583\n",
       "  605  0.098276  0.097461  0.098665  0.184005  0.118196         0.485830\n",
       "  606  0.097275  0.096221  0.096504  0.162849  0.118027         0.474576,\n",
       "  0.09294523613392572),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  603  0.095785  0.094795  0.096940  0.175232  0.114559         0.505508\n",
       "  604  0.096938  0.095565  0.097865  0.108719  0.116960         0.496583\n",
       "  605  0.098276  0.097461  0.098665  0.184005  0.118196         0.485830\n",
       "  606  0.097275  0.096221  0.096504  0.162849  0.118027         0.474576\n",
       "  607  0.095140  0.093546  0.094821  0.144190  0.116389         0.470185,\n",
       "  0.09340538712187677),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  604  0.096938  0.095565  0.097865  0.108719  0.116960         0.496583\n",
       "  605  0.098276  0.097461  0.098665  0.184005  0.118196         0.485830\n",
       "  606  0.097275  0.096221  0.096504  0.162849  0.118027         0.474576\n",
       "  607  0.095140  0.093546  0.094821  0.144190  0.116389         0.470185\n",
       "  608  0.093566  0.092190  0.094653  0.120993  0.114177         0.490561,\n",
       "  0.09127001172311644),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  605  0.098276  0.097461  0.098665  0.184005  0.118196         0.485830\n",
       "  606  0.097275  0.096221  0.096504  0.162849  0.118027         0.474576\n",
       "  607  0.095140  0.093546  0.094821  0.144190  0.116389         0.470185\n",
       "  608  0.093566  0.092190  0.094653  0.120993  0.114177         0.490561\n",
       "  609  0.094654  0.092638  0.092658  0.131355  0.114626         0.471153,\n",
       "  0.09084101794432009),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  606  0.097275  0.096221  0.096504  0.162849  0.118027         0.474576\n",
       "  607  0.095140  0.093546  0.094821  0.144190  0.116389         0.470185\n",
       "  608  0.093566  0.092190  0.094653  0.120993  0.114177         0.490561\n",
       "  609  0.094654  0.092638  0.092658  0.131355  0.114626         0.471153\n",
       "  610  0.091836  0.090766  0.092364  0.113385  0.112541         0.483913,\n",
       "  0.09563902608453068),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  607  0.095140  0.093546  0.094821  0.144190  0.116389         0.470185\n",
       "  608  0.093566  0.092190  0.094653  0.120993  0.114177         0.490561\n",
       "  609  0.094654  0.092638  0.092658  0.131355  0.114626         0.471153\n",
       "  610  0.091836  0.090766  0.092364  0.113385  0.112541         0.483913\n",
       "  611  0.092717  0.094271  0.093818  0.147287  0.112122         0.522998,\n",
       "  0.09300755055223507),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  608  0.093566  0.092190  0.094653  0.120993  0.114177         0.490561\n",
       "  609  0.094654  0.092638  0.092658  0.131355  0.114626         0.471153\n",
       "  610  0.091836  0.090766  0.092364  0.113385  0.112541         0.483913\n",
       "  611  0.092717  0.094271  0.093818  0.147287  0.112122         0.522998\n",
       "  612  0.096700  0.095446  0.094392  0.154807  0.116808         0.467443,\n",
       "  0.09486491665918047),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  609  0.094654  0.092638  0.092658  0.131355  0.114626         0.471153\n",
       "  610  0.091836  0.090766  0.092364  0.113385  0.112541         0.483913\n",
       "  611  0.092717  0.094271  0.093818  0.147287  0.112122         0.522998\n",
       "  612  0.096700  0.095446  0.094392  0.154807  0.116808         0.467443\n",
       "  613  0.094276  0.093882  0.094724  0.133597  0.114238         0.501009,\n",
       "  0.0940572636788231),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  610  0.091836  0.090766  0.092364  0.113385  0.112541         0.483913\n",
       "  611  0.092717  0.094271  0.093818  0.147287  0.112122         0.522998\n",
       "  612  0.096700  0.095446  0.094392  0.154807  0.116808         0.467443\n",
       "  613  0.094276  0.093882  0.094724  0.133597  0.114238         0.501009\n",
       "  614  0.095978  0.094084  0.095564  0.116065  0.116052         0.481081,\n",
       "  0.09450063326690825),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  611  0.092717  0.094271  0.093818  0.147287  0.112122         0.522998\n",
       "  612  0.096700  0.095446  0.094392  0.154807  0.116808         0.467443\n",
       "  613  0.094276  0.093882  0.094724  0.133597  0.114238         0.501009\n",
       "  614  0.095978  0.094084  0.095564  0.116065  0.116052         0.481081\n",
       "  615  0.095015  0.093906  0.095739  0.101588  0.115263         0.490436,\n",
       "  0.09622140031852328),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  612  0.096700  0.095446  0.094392  0.154807  0.116808         0.467443\n",
       "  613  0.094276  0.093882  0.094724  0.133597  0.114238         0.501009\n",
       "  614  0.095978  0.094084  0.095564  0.116065  0.116052         0.481081\n",
       "  615  0.095015  0.093906  0.095739  0.101588  0.115263         0.490436\n",
       "  616  0.095614  0.095565  0.096778  0.296797  0.115696         0.499988,\n",
       "  0.09148810256483682),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  613  0.094276  0.093882  0.094724  0.133597  0.114238         0.501009\n",
       "  614  0.095978  0.094084  0.095564  0.116065  0.116052         0.481081\n",
       "  615  0.095015  0.093906  0.095739  0.101588  0.115263         0.490436\n",
       "  616  0.095614  0.095565  0.096778  0.296797  0.115696         0.499988\n",
       "  617  0.097903  0.095804  0.093391  0.380776  0.117377         0.451727,\n",
       "  0.09142818411474228),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  614  0.095978  0.094084  0.095564  0.116065  0.116052         0.481081\n",
       "  615  0.095015  0.093906  0.095739  0.101588  0.115263         0.490436\n",
       "  616  0.095614  0.095565  0.096778  0.296797  0.115696         0.499988\n",
       "  617  0.097903  0.095804  0.093391  0.380776  0.117377         0.451727\n",
       "  618  0.092274  0.090825  0.092490  0.152813  0.112754         0.486673,\n",
       "  0.08799145157030411),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  615  0.095015  0.093906  0.095739  0.101588  0.115263         0.490436\n",
       "  616  0.095614  0.095565  0.096778  0.296797  0.115696         0.499988\n",
       "  617  0.097903  0.095804  0.093391  0.380776  0.117377         0.451727\n",
       "  618  0.092274  0.090825  0.092490  0.152813  0.112754         0.486673\n",
       "  619  0.091044  0.089166  0.089676  0.175200  0.112696         0.461422,\n",
       "  0.08902677926524104),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  616  0.095614  0.095565  0.096778  0.296797  0.115696         0.499988\n",
       "  617  0.097903  0.095804  0.093391  0.380776  0.117377         0.451727\n",
       "  618  0.092274  0.090825  0.092490  0.152813  0.112754         0.486673\n",
       "  619  0.091044  0.089166  0.089676  0.175200  0.112696         0.461422\n",
       "  620  0.088782  0.088052  0.089935  0.129350  0.109339         0.494862,\n",
       "  0.0886744853364051),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  617  0.097903  0.095804  0.093391  0.380776  0.117377         0.451727\n",
       "  618  0.092274  0.090825  0.092490  0.152813  0.112754         0.486673\n",
       "  619  0.091044  0.089166  0.089676  0.175200  0.112696         0.461422\n",
       "  620  0.088782  0.088052  0.089935  0.129350  0.109339         0.494862\n",
       "  621  0.090159  0.088301  0.090303  0.100111  0.110350         0.484486,\n",
       "  0.08891893182848824),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  618  0.092274  0.090825  0.092490  0.152813  0.112754         0.486673\n",
       "  619  0.091044  0.089166  0.089676  0.175200  0.112696         0.461422\n",
       "  620  0.088782  0.088052  0.089935  0.129350  0.109339         0.494862\n",
       "  621  0.090159  0.088301  0.090303  0.100111  0.110350         0.484486\n",
       "  622  0.088770  0.088159  0.089424  0.099063  0.110006         0.488948,\n",
       "  0.08672843953842528),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  619  0.091044  0.089166  0.089676  0.175200  0.112696         0.461422\n",
       "  620  0.088782  0.088052  0.089935  0.129350  0.109339         0.494862\n",
       "  621  0.090159  0.088301  0.090303  0.100111  0.110350         0.484486\n",
       "  622  0.088770  0.088159  0.089424  0.099063  0.110006         0.488948\n",
       "  623  0.089636  0.087906  0.088540  0.118081  0.110245         0.470741,\n",
       "  0.08586566004440296),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  620  0.088782  0.088052  0.089935  0.129350  0.109339         0.494862\n",
       "  621  0.090159  0.088301  0.090303  0.100111  0.110350         0.484486\n",
       "  622  0.088770  0.088159  0.089424  0.099063  0.110006         0.488948\n",
       "  623  0.089636  0.087906  0.088540  0.118081  0.110245         0.470741\n",
       "  624  0.086996  0.085467  0.086970  0.139873  0.108106         0.480669,\n",
       "  0.08605978158161307),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  621  0.090159  0.088301  0.090303  0.100111  0.110350         0.484486\n",
       "  622  0.088770  0.088159  0.089424  0.099063  0.110006         0.488948\n",
       "  623  0.089636  0.087906  0.088540  0.118081  0.110245         0.470741\n",
       "  624  0.086996  0.085467  0.086970  0.139873  0.108106         0.480669\n",
       "  625  0.086628  0.085559  0.087513  0.098720  0.107263         0.488572,\n",
       "  0.0876966705009856),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  622  0.088770  0.088159  0.089424  0.099063  0.110006         0.488948\n",
       "  623  0.089636  0.087906  0.088540  0.118081  0.110245         0.470741\n",
       "  624  0.086996  0.085467  0.086970  0.139873  0.108106         0.480669\n",
       "  625  0.086628  0.085559  0.087513  0.098720  0.107263         0.488572\n",
       "  626  0.086833  0.087064  0.087988  0.138518  0.107453         0.499361,\n",
       "  0.08662778000631689),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  623  0.089636  0.087906  0.088540  0.118081  0.110245         0.470741\n",
       "  624  0.086996  0.085467  0.086970  0.139873  0.108106         0.480669\n",
       "  625  0.086628  0.085559  0.087513  0.098720  0.107263         0.488572\n",
       "  626  0.086833  0.087064  0.087988  0.138518  0.107453         0.499361\n",
       "  627  0.088277  0.086353  0.088351  0.098229  0.109051         0.479128,\n",
       "  0.08865770393653921),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  624  0.086996  0.085467  0.086970  0.139873  0.108106         0.480669\n",
       "  625  0.086628  0.085559  0.087513  0.098720  0.107263         0.488572\n",
       "  626  0.086833  0.087064  0.087988  0.138518  0.107453         0.499361\n",
       "  627  0.088277  0.086353  0.088351  0.098229  0.109051         0.479128\n",
       "  628  0.087506  0.087306  0.088676  0.150149  0.108007         0.502300,\n",
       "  0.08917537740636998),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  625  0.086628  0.085559  0.087513  0.098720  0.107263         0.488572\n",
       "  626  0.086833  0.087064  0.087988  0.138518  0.107453         0.499361\n",
       "  627  0.088277  0.086353  0.088351  0.098229  0.109051         0.479128\n",
       "  628  0.087506  0.087306  0.088676  0.150149  0.108007         0.502300\n",
       "  629  0.088871  0.087922  0.089317  0.098842  0.109990         0.490992,\n",
       "  0.09007650201419137),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  626  0.086833  0.087064  0.087988  0.138518  0.107453         0.499361\n",
       "  627  0.088277  0.086353  0.088351  0.098229  0.109051         0.479128\n",
       "  628  0.087506  0.087306  0.088676  0.150149  0.108007         0.502300\n",
       "  629  0.088871  0.087922  0.089317  0.098842  0.109990         0.490992\n",
       "  630  0.089201  0.089327  0.090051  0.137102  0.110495         0.493859,\n",
       "  0.09007409642361427),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  627  0.088277  0.086353  0.088351  0.098229  0.109051         0.479128\n",
       "  628  0.087506  0.087306  0.088676  0.150149  0.108007         0.502300\n",
       "  629  0.088871  0.087922  0.089317  0.098842  0.109990         0.490992\n",
       "  630  0.089201  0.089327  0.090051  0.137102  0.110495         0.493859\n",
       "  631  0.091331  0.090645  0.091662  0.139615  0.111375         0.487103,\n",
       "  0.08882067788695695),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  628  0.087506  0.087306  0.088676  0.150149  0.108007         0.502300\n",
       "  629  0.088871  0.087922  0.089317  0.098842  0.109990         0.490992\n",
       "  630  0.089201  0.089327  0.090051  0.137102  0.110495         0.493859\n",
       "  631  0.091331  0.090645  0.091662  0.139615  0.111375         0.487103\n",
       "  632  0.090623  0.088917  0.090066  0.094342  0.111373         0.477748,\n",
       "  0.08745700632296974),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  629  0.088871  0.087922  0.089317  0.098842  0.109990         0.490992\n",
       "  630  0.089201  0.089327  0.090051  0.137102  0.110495         0.493859\n",
       "  631  0.091331  0.090645  0.091662  0.139615  0.111375         0.487103\n",
       "  632  0.090623  0.088917  0.090066  0.094342  0.111373         0.477748\n",
       "  633  0.089920  0.088515  0.089085  0.096465  0.110149         0.476923,\n",
       "  0.08358409209509081),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  630  0.089201  0.089327  0.090051  0.137102  0.110495         0.493859\n",
       "  631  0.091331  0.090645  0.091662  0.139615  0.111375         0.487103\n",
       "  632  0.090623  0.088917  0.090066  0.094342  0.111373         0.477748\n",
       "  633  0.089920  0.088515  0.089085  0.096465  0.110149         0.476923\n",
       "  634  0.085629  0.084853  0.085069  0.226423  0.108817         0.458160,\n",
       "  0.08374705642314623),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  631  0.091331  0.090645  0.091662  0.139615  0.111375         0.487103\n",
       "  632  0.090623  0.088917  0.090066  0.094342  0.111373         0.477748\n",
       "  633  0.089920  0.088515  0.089085  0.096465  0.110149         0.476923\n",
       "  634  0.085629  0.084853  0.085069  0.226423  0.108817         0.458160\n",
       "  635  0.084525  0.083401  0.084069  0.195276  0.105035         0.488339,\n",
       "  0.08345467132204253),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  632  0.090623  0.088917  0.090066  0.094342  0.111373         0.477748\n",
       "  633  0.089920  0.088515  0.089085  0.096465  0.110149         0.476923\n",
       "  634  0.085629  0.084853  0.085069  0.226423  0.108817         0.458160\n",
       "  635  0.084525  0.083401  0.084069  0.195276  0.105035         0.488339\n",
       "  636  0.082662  0.083104  0.083332  0.211157  0.105194         0.484934,\n",
       "  0.08116590584572365),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  633  0.089920  0.088515  0.089085  0.096465  0.110149         0.476923\n",
       "  634  0.085629  0.084853  0.085069  0.226423  0.108817         0.458160\n",
       "  635  0.084525  0.083401  0.084069  0.195276  0.105035         0.488339\n",
       "  636  0.082662  0.083104  0.083332  0.211157  0.105194         0.484934\n",
       "  637  0.083249  0.082223  0.083012  0.160039  0.104909         0.470006,\n",
       "  0.08170994458827952),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  634  0.085629  0.084853  0.085069  0.226423  0.108817         0.458160\n",
       "  635  0.084525  0.083401  0.084069  0.195276  0.105035         0.488339\n",
       "  636  0.082662  0.083104  0.083332  0.211157  0.105194         0.484934\n",
       "  637  0.083249  0.082223  0.083012  0.160039  0.104909         0.470006\n",
       "  638  0.081302  0.081464  0.082606  0.154481  0.102673         0.491189,\n",
       "  0.08568831028469644),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  635  0.084525  0.083401  0.084069  0.195276  0.105035         0.488339\n",
       "  636  0.082662  0.083104  0.083332  0.211157  0.105194         0.484934\n",
       "  637  0.083249  0.082223  0.083012  0.160039  0.104909         0.470006\n",
       "  638  0.081302  0.081464  0.082606  0.154481  0.102673         0.491189\n",
       "  639  0.083817  0.084664  0.085120  0.183149  0.103205         0.516869,\n",
       "  0.08523055526496022),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  636  0.082662  0.083104  0.083332  0.211157  0.105194         0.484934\n",
       "  637  0.083249  0.082223  0.083012  0.160039  0.104909         0.470006\n",
       "  638  0.081302  0.081464  0.082606  0.154481  0.102673         0.491189\n",
       "  639  0.083817  0.084664  0.085120  0.183149  0.103205         0.516869\n",
       "  640  0.087480  0.085645  0.086793  0.158835  0.107090         0.483698,\n",
       "  0.08501006845502504),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  637  0.083249  0.082223  0.083012  0.160039  0.104909         0.470006\n",
       "  638  0.081302  0.081464  0.082606  0.154481  0.102673         0.491189\n",
       "  639  0.083817  0.084664  0.085120  0.183149  0.103205         0.516869\n",
       "  640  0.087480  0.085645  0.086793  0.158835  0.107090         0.483698\n",
       "  641  0.086645  0.086204  0.086747  0.130345  0.106643         0.485472,\n",
       "  0.08458826258087307),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  638  0.081302  0.081464  0.082606  0.154481  0.102673         0.491189\n",
       "  639  0.083817  0.084664  0.085120  0.183149  0.103205         0.516869\n",
       "  640  0.087480  0.085645  0.086793  0.158835  0.107090         0.483698\n",
       "  641  0.086645  0.086204  0.086747  0.130345  0.106643         0.485472\n",
       "  642  0.086351  0.084521  0.085779  0.120978  0.106428         0.483966,\n",
       "  0.08501725635966942),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  639  0.083817  0.084664  0.085120  0.183149  0.103205         0.516869\n",
       "  640  0.087480  0.085645  0.086793  0.158835  0.107090         0.483698\n",
       "  641  0.086645  0.086204  0.086747  0.130345  0.106643         0.485472\n",
       "  642  0.086351  0.084521  0.085779  0.120978  0.106428         0.483966\n",
       "  643  0.085044  0.083775  0.086023  0.084194  0.106016         0.490328,\n",
       "  0.0822084310676673),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  640  0.087480  0.085645  0.086793  0.158835  0.107090         0.483698\n",
       "  641  0.086645  0.086204  0.086747  0.130345  0.106643         0.485472\n",
       "  642  0.086351  0.084521  0.085779  0.120978  0.106428         0.483966\n",
       "  643  0.085044  0.083775  0.086023  0.084194  0.106016         0.490328\n",
       "  644  0.084452  0.082988  0.084018  0.155021  0.106435         0.466117,\n",
       "  0.08339954480837758),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  641  0.086645  0.086204  0.086747  0.130345  0.106643         0.485472\n",
       "  642  0.086351  0.084521  0.085779  0.120978  0.106428         0.483966\n",
       "  643  0.085044  0.083775  0.086023  0.084194  0.106016         0.490328\n",
       "  644  0.084452  0.082988  0.084018  0.155021  0.106435         0.466117\n",
       "  645  0.082797  0.082213  0.083090  0.194902  0.103692         0.496027,\n",
       "  0.08296575909315163),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  642  0.086351  0.084521  0.085779  0.120978  0.106428         0.483966\n",
       "  643  0.085044  0.083775  0.086023  0.084194  0.106016         0.490328\n",
       "  644  0.084452  0.082988  0.084018  0.155021  0.106435         0.466117\n",
       "  645  0.082797  0.082213  0.083090  0.194902  0.103692         0.496027\n",
       "  646  0.083911  0.082488  0.084287  0.095840  0.104855         0.483877,\n",
       "  0.08168597528376922),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  643  0.085044  0.083775  0.086023  0.084194  0.106016         0.490328\n",
       "  644  0.084452  0.082988  0.084018  0.155021  0.106435         0.466117\n",
       "  645  0.082797  0.082213  0.083090  0.194902  0.103692         0.496027\n",
       "  646  0.083911  0.082488  0.084287  0.095840  0.104855         0.483877\n",
       "  647  0.083379  0.081749  0.083306  0.137552  0.104431         0.477551,\n",
       "  0.08321500714402667),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  644  0.084452  0.082988  0.084018  0.155021  0.106435         0.466117\n",
       "  645  0.082797  0.082213  0.083090  0.194902  0.103692         0.496027\n",
       "  646  0.083911  0.082488  0.084287  0.095840  0.104855         0.483877\n",
       "  647  0.083379  0.081749  0.083306  0.137552  0.104431         0.477551\n",
       "  648  0.082802  0.082175  0.083485  0.126330  0.103181         0.498554,\n",
       "  0.08274766825143125),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  645  0.082797  0.082213  0.083090  0.194902  0.103692         0.496027\n",
       "  646  0.083911  0.082488  0.084287  0.095840  0.104855         0.483877\n",
       "  647  0.083379  0.081749  0.083306  0.137552  0.104431         0.477551\n",
       "  648  0.082802  0.082175  0.083485  0.126330  0.103181         0.498554\n",
       "  649  0.083413  0.081962  0.084204  0.083126  0.104675         0.483626,\n",
       "  0.0854486461066806),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  646  0.083911  0.082488  0.084287  0.095840  0.104855         0.483877\n",
       "  647  0.083379  0.081749  0.083306  0.137552  0.104431         0.477551\n",
       "  648  0.082802  0.082175  0.083485  0.126330  0.103181         0.498554\n",
       "  649  0.083413  0.081962  0.084204  0.083126  0.104675         0.483626\n",
       "  650  0.083295  0.084088  0.084594  0.140938  0.104218         0.507318,\n",
       "  0.08676438868400956),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  647  0.083379  0.081749  0.083306  0.137552  0.104431         0.477551\n",
       "  648  0.082802  0.082175  0.083485  0.126330  0.103181         0.498554\n",
       "  649  0.083413  0.081962  0.084204  0.083126  0.104675         0.483626\n",
       "  650  0.083295  0.084088  0.084594  0.140938  0.104218         0.507318\n",
       "  651  0.085776  0.085415  0.086367  0.138078  0.106856         0.496959,\n",
       "  0.0867140541067742),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  648  0.082802  0.082175  0.083485  0.126330  0.103181         0.498554\n",
       "  649  0.083413  0.081962  0.084204  0.083126  0.104675         0.483626\n",
       "  650  0.083295  0.084088  0.084594  0.140938  0.104218         0.507318\n",
       "  651  0.085776  0.085415  0.086367  0.138078  0.106856         0.496959\n",
       "  652  0.087182  0.085704  0.087469  0.099905  0.108141         0.486744,\n",
       "  0.08747377810047331),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  649  0.083413  0.081962  0.084204  0.083126  0.104675         0.483626\n",
       "  650  0.083295  0.084088  0.084594  0.140938  0.104218         0.507318\n",
       "  651  0.085776  0.085415  0.086367  0.138078  0.106856         0.496959\n",
       "  652  0.087182  0.085704  0.087469  0.099905  0.108141         0.486744\n",
       "  653  0.086965  0.086086  0.087116  0.114474  0.108092         0.492802,\n",
       "  0.08714304788557056),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  650  0.083295  0.084088  0.084594  0.140938  0.104218         0.507318\n",
       "  651  0.085776  0.085415  0.086367  0.138078  0.106856         0.496959\n",
       "  652  0.087182  0.085704  0.087469  0.099905  0.108141         0.486744\n",
       "  653  0.086965  0.086086  0.087116  0.114474  0.108092         0.492802\n",
       "  654  0.087687  0.086140  0.088259  0.090903  0.108834         0.484647,\n",
       "  0.08672604357021049),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  651  0.085776  0.085415  0.086367  0.138078  0.106856         0.496959\n",
       "  652  0.087182  0.085704  0.087469  0.099905  0.108141         0.486744\n",
       "  653  0.086965  0.086086  0.087116  0.114474  0.108092         0.492802\n",
       "  654  0.087687  0.086140  0.088259  0.090903  0.108834         0.484647\n",
       "  655  0.087283  0.086190  0.088213  0.111587  0.108511         0.484002,\n",
       "  0.08649596326505382),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  652  0.087182  0.085704  0.087469  0.099905  0.108141         0.486744\n",
       "  653  0.086965  0.086086  0.087116  0.114474  0.108092         0.492802\n",
       "  654  0.087687  0.086140  0.088259  0.090903  0.108834         0.484647\n",
       "  655  0.087283  0.086190  0.088213  0.111587  0.108511         0.484002\n",
       "  656  0.087928  0.086322  0.086965  0.096918  0.108103         0.485400,\n",
       "  0.0858345028352483),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  653  0.086965  0.086086  0.087116  0.114474  0.108092         0.492802\n",
       "  654  0.087687  0.086140  0.088259  0.090903  0.108834         0.484647\n",
       "  655  0.087283  0.086190  0.088213  0.111587  0.108511         0.484002\n",
       "  656  0.087928  0.086322  0.086965  0.096918  0.108103         0.485400\n",
       "  657  0.087338  0.085706  0.087242  0.082122  0.107879         0.482174,\n",
       "  0.0859231729039204),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  654  0.087687  0.086140  0.088259  0.090903  0.108834         0.484647\n",
       "  655  0.087283  0.086190  0.088213  0.111587  0.108511         0.484002\n",
       "  656  0.087928  0.086322  0.086965  0.096918  0.108103         0.485400\n",
       "  657  0.087338  0.085706  0.087242  0.082122  0.107879         0.482174\n",
       "  658  0.086294  0.084552  0.086268  0.083783  0.107233         0.487784,\n",
       "  0.08929520468419676),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  655  0.087283  0.086190  0.088213  0.111587  0.108511         0.484002\n",
       "  656  0.087928  0.086322  0.086965  0.096918  0.108103         0.485400\n",
       "  657  0.087338  0.085706  0.087242  0.082122  0.107879         0.482174\n",
       "  658  0.086294  0.084552  0.086268  0.083783  0.107233         0.487784\n",
       "  659  0.087807  0.088278  0.089145  0.158050  0.107319         0.512335,\n",
       "  0.08891653586027344),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  656  0.087928  0.086322  0.086965  0.096918  0.108103         0.485400\n",
       "  657  0.087338  0.085706  0.087242  0.082122  0.107879         0.482174\n",
       "  658  0.086294  0.084552  0.086268  0.083783  0.107233         0.487784\n",
       "  659  0.087807  0.088278  0.089145  0.158050  0.107319         0.512335\n",
       "  660  0.089754  0.087981  0.090497  0.091817  0.110612         0.484289,\n",
       "  0.08903397679224773),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  657  0.087338  0.085706  0.087242  0.082122  0.107879         0.482174\n",
       "  658  0.086294  0.084552  0.086268  0.083783  0.107233         0.487784\n",
       "  659  0.087807  0.088278  0.089145  0.158050  0.107319         0.512335\n",
       "  660  0.089754  0.087981  0.090497  0.091817  0.110612         0.484289\n",
       "  661  0.089552  0.087766  0.090507  0.065961  0.110243         0.487999,\n",
       "  0.08824309558939394),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  658  0.086294  0.084552  0.086268  0.083783  0.107233         0.487784\n",
       "  659  0.087807  0.088278  0.089145  0.158050  0.107319         0.512335\n",
       "  660  0.089754  0.087981  0.090497  0.091817  0.110612         0.484289\n",
       "  661  0.089552  0.087766  0.090507  0.065961  0.110243         0.487999\n",
       "  662  0.089956  0.087981  0.089531  0.074385  0.110357         0.481207,\n",
       "  0.08783326955631596),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  659  0.087807  0.088278  0.089145  0.158050  0.107319         0.512335\n",
       "  660  0.089754  0.087981  0.090497  0.091817  0.110612         0.484289\n",
       "  661  0.089552  0.087766  0.090507  0.065961  0.110243         0.487999\n",
       "  662  0.089956  0.087981  0.089531  0.074385  0.110357         0.481207\n",
       "  663  0.088539  0.087458  0.089426  0.072371  0.109585         0.484056,\n",
       "  0.08736113872729094),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  660  0.089754  0.087981  0.090497  0.091817  0.110612         0.484289\n",
       "  661  0.089552  0.087766  0.090507  0.065961  0.110243         0.487999\n",
       "  662  0.089956  0.087981  0.089531  0.074385  0.110357         0.481207\n",
       "  663  0.088539  0.087458  0.089426  0.072371  0.109585         0.484056\n",
       "  664  0.088291  0.087128  0.088809  0.092625  0.109185         0.483590,\n",
       "  0.08804656846160673),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  661  0.089552  0.087766  0.090507  0.065961  0.110243         0.487999\n",
       "  662  0.089956  0.087981  0.089531  0.074385  0.110357         0.481207\n",
       "  663  0.088539  0.087458  0.089426  0.072371  0.109585         0.484056\n",
       "  664  0.088291  0.087128  0.088809  0.092625  0.109185         0.483590\n",
       "  665  0.087795  0.086796  0.088719  0.065769  0.108724         0.492246,\n",
       "  0.08778294460144291),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  662  0.089956  0.087981  0.089531  0.074385  0.110357         0.481207\n",
       "  663  0.088539  0.087458  0.089426  0.072371  0.109585         0.484056\n",
       "  664  0.088291  0.087128  0.088809  0.092625  0.109185         0.483590\n",
       "  665  0.087795  0.086796  0.088719  0.065769  0.108724         0.492246\n",
       "  666  0.088866  0.086979  0.088990  0.067883  0.109393         0.485149,\n",
       "  0.08969543722205325),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  663  0.088539  0.087458  0.089426  0.072371  0.109585         0.484056\n",
       "  664  0.088291  0.087128  0.088809  0.092625  0.109185         0.483590\n",
       "  665  0.087795  0.086796  0.088719  0.065769  0.108724         0.492246\n",
       "  666  0.088866  0.086979  0.088990  0.067883  0.109393         0.485149\n",
       "  667  0.088869  0.088927  0.090151  0.123416  0.109136         0.501422,\n",
       "  0.08934313367085502),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  664  0.088291  0.087128  0.088809  0.092625  0.109185         0.483590\n",
       "  665  0.087795  0.086796  0.088719  0.065769  0.108724         0.492246\n",
       "  666  0.088866  0.086979  0.088990  0.067883  0.109393         0.485149\n",
       "  667  0.088869  0.088927  0.090151  0.123416  0.109136         0.501422\n",
       "  668  0.090154  0.088785  0.090986  0.077984  0.111003         0.484486,\n",
       "  0.09011245115977563),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  665  0.087795  0.086796  0.088719  0.065769  0.108724         0.492246\n",
       "  666  0.088866  0.086979  0.088990  0.067883  0.109393         0.485149\n",
       "  667  0.088869  0.088927  0.090151  0.123416  0.109136         0.501422\n",
       "  668  0.090154  0.088785  0.090986  0.077984  0.111003         0.484486\n",
       "  669  0.089542  0.089617  0.090485  0.137830  0.110659         0.492873,\n",
       "  0.0890675299696172),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  666  0.088866  0.086979  0.088990  0.067883  0.109393         0.485149\n",
       "  667  0.088869  0.088927  0.090151  0.123416  0.109136         0.501422\n",
       "  668  0.090154  0.088785  0.090986  0.077984  0.111003         0.484486\n",
       "  669  0.089542  0.089617  0.090485  0.137830  0.110659         0.492873\n",
       "  670  0.090322  0.090102  0.090499  0.267694  0.111411         0.479307,\n",
       "  0.09166304673396623),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  667  0.088869  0.088927  0.090151  0.123416  0.109136         0.501422\n",
       "  668  0.090154  0.088785  0.090986  0.077984  0.111003         0.484486\n",
       "  669  0.089542  0.089617  0.090485  0.137830  0.110659         0.492873\n",
       "  670  0.090322  0.090102  0.090499  0.267694  0.111411         0.479307\n",
       "  671  0.094178  0.092602  0.093510  0.295712  0.110390         0.506529,\n",
       "  0.09084820584896448),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  668  0.090154  0.088785  0.090986  0.077984  0.111003         0.484486\n",
       "  669  0.089542  0.089617  0.090485  0.137830  0.110659         0.492873\n",
       "  670  0.090322  0.090102  0.090499  0.267694  0.111411         0.479307\n",
       "  671  0.094178  0.092602  0.093510  0.295712  0.110390         0.506529\n",
       "  672  0.091670  0.090825  0.092599  0.137929  0.112925         0.481027,\n",
       "  0.09047673455204785),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  669  0.089542  0.089617  0.090485  0.137830  0.110659         0.492873\n",
       "  670  0.090322  0.090102  0.090499  0.267694  0.111411         0.479307\n",
       "  671  0.094178  0.092602  0.093510  0.295712  0.110390         0.506529\n",
       "  672  0.091670  0.090825  0.092599  0.137929  0.112925         0.481027\n",
       "  673  0.091311  0.089635  0.091943  0.089772  0.112129         0.484343,\n",
       "  0.09058697795701545),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  670  0.090322  0.090102  0.090499  0.267694  0.111411         0.479307\n",
       "  671  0.094178  0.092602  0.093510  0.295712  0.110390         0.506529\n",
       "  672  0.091670  0.090825  0.092599  0.137929  0.112925         0.481027\n",
       "  673  0.091311  0.089635  0.091943  0.089772  0.112129         0.484343\n",
       "  674  0.091646  0.089965  0.092079  0.096390  0.111766         0.487945,\n",
       "  0.09135149388714416),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  671  0.094178  0.092602  0.093510  0.295712  0.110390         0.506529\n",
       "  672  0.091670  0.090825  0.092599  0.137929  0.112925         0.481027\n",
       "  673  0.091311  0.089635  0.091943  0.089772  0.112129         0.484343\n",
       "  674  0.091646  0.089965  0.092079  0.096390  0.111766         0.487945\n",
       "  675  0.091189  0.090695  0.092321  0.100235  0.111874         0.492837,\n",
       "  0.09083142444909859),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  672  0.091670  0.090825  0.092599  0.137929  0.112925         0.481027\n",
       "  673  0.091311  0.089635  0.091943  0.089772  0.112129         0.484343\n",
       "  674  0.091646  0.089965  0.092079  0.096390  0.111766         0.487945\n",
       "  675  0.091189  0.090695  0.092321  0.100235  0.111874         0.492837\n",
       "  676  0.091663  0.090209  0.092616  0.071067  0.112621         0.483232,\n",
       "  0.08900281958309306),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  673  0.091311  0.089635  0.091943  0.089772  0.112129         0.484343\n",
       "  674  0.091646  0.089965  0.092079  0.096390  0.111766         0.487945\n",
       "  675  0.091189  0.090695  0.092321  0.100235  0.111874         0.492837\n",
       "  676  0.091663  0.090209  0.092616  0.071067  0.112621         0.483232\n",
       "  677  0.091441  0.089960  0.090933  0.088470  0.112113         0.473447,\n",
       "  0.08843242519017444),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  674  0.091646  0.089965  0.092079  0.096390  0.111766         0.487945\n",
       "  675  0.091189  0.090695  0.092321  0.100235  0.111874         0.492837\n",
       "  676  0.091663  0.090209  0.092616  0.071067  0.112621         0.483232\n",
       "  677  0.091441  0.089960  0.090933  0.088470  0.112113         0.473447\n",
       "  678  0.089766  0.087936  0.089187  0.088884  0.110327         0.482855,\n",
       "  0.08766072135540134),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  675  0.091189  0.090695  0.092321  0.100235  0.111874         0.492837\n",
       "  676  0.091663  0.090209  0.092616  0.071067  0.112621         0.483232\n",
       "  677  0.091441  0.089960  0.090933  0.088470  0.112113         0.473447\n",
       "  678  0.089766  0.087936  0.089187  0.088884  0.110327         0.482855\n",
       "  679  0.088914  0.087526  0.089564  0.074399  0.109770         0.481350,\n",
       "  0.0894461891711782),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  676  0.091663  0.090209  0.092616  0.071067  0.112621         0.483232\n",
       "  677  0.091441  0.089960  0.090933  0.088470  0.112113         0.473447\n",
       "  678  0.089766  0.087936  0.089187  0.088884  0.110327         0.482855\n",
       "  679  0.088914  0.087526  0.089564  0.074399  0.109770         0.481350\n",
       "  680  0.088329  0.088235  0.089458  0.087327  0.109016         0.500472,\n",
       "  0.08894050516478373),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  677  0.091441  0.089960  0.090933  0.088470  0.112113         0.473447\n",
       "  678  0.089766  0.087936  0.089187  0.088884  0.110327         0.482855\n",
       "  679  0.088914  0.087526  0.089564  0.074399  0.109770         0.481350\n",
       "  680  0.088329  0.088235  0.089458  0.087327  0.109016         0.500472\n",
       "  681  0.089268  0.088417  0.089601  0.095981  0.110760         0.483339,\n",
       "  0.08801780722066688),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  678  0.089766  0.087936  0.089187  0.088884  0.110327         0.482855\n",
       "  679  0.088914  0.087526  0.089564  0.074399  0.109770         0.481350\n",
       "  680  0.088329  0.088235  0.089458  0.087327  0.109016         0.500472\n",
       "  681  0.089268  0.088417  0.089601  0.095981  0.110760         0.483339\n",
       "  682  0.089754  0.087939  0.089744  0.073363  0.110266         0.480221,\n",
       "  0.08790756381569928),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  679  0.088914  0.087526  0.089564  0.074399  0.109770         0.481350\n",
       "  680  0.088329  0.088235  0.089458  0.087327  0.109016         0.500472\n",
       "  681  0.089268  0.088417  0.089601  0.095981  0.110760         0.483339\n",
       "  682  0.089754  0.087939  0.089744  0.073363  0.110266         0.480221\n",
       "  683  0.088308  0.086837  0.089322  0.070566  0.109365         0.486296,\n",
       "  0.08849234364026899),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  680  0.088329  0.088235  0.089458  0.087327  0.109016         0.500472\n",
       "  681  0.089268  0.088417  0.089601  0.095981  0.110760         0.483339\n",
       "  682  0.089754  0.087939  0.089744  0.073363  0.110266         0.480221\n",
       "  683  0.088308  0.086837  0.089322  0.070566  0.109365         0.486296\n",
       "  684  0.088313  0.087680  0.089116  0.094519  0.109257         0.491493,\n",
       "  0.08662778000631689),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  681  0.089268  0.088417  0.089601  0.095981  0.110760         0.483339\n",
       "  682  0.089754  0.087939  0.089744  0.073363  0.110266         0.480221\n",
       "  683  0.088308  0.086837  0.089322  0.070566  0.109365         0.486296\n",
       "  684  0.088313  0.087680  0.089116  0.094519  0.109257         0.491493\n",
       "  685  0.088527  0.087150  0.088424  0.089567  0.109828         0.473178,\n",
       "  0.08789079203819569),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  682  0.089754  0.087939  0.089744  0.073363  0.110266         0.480221\n",
       "  683  0.088308  0.086837  0.089322  0.070566  0.109365         0.486296\n",
       "  684  0.088313  0.087680  0.089116  0.094519  0.109257         0.491493\n",
       "  685  0.088527  0.087150  0.088424  0.089567  0.109828         0.473178\n",
       "  686  0.087273  0.086559  0.088419  0.071514  0.108007         0.496565,\n",
       "  0.08670207426570022),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  683  0.088308  0.086837  0.089322  0.070566  0.109365         0.486296\n",
       "  684  0.088313  0.087680  0.089116  0.094519  0.109257         0.491493\n",
       "  685  0.088527  0.087150  0.088424  0.089567  0.109828         0.473178\n",
       "  686  0.087273  0.086559  0.088419  0.071514  0.108007         0.496565\n",
       "  687  0.088151  0.086680  0.088177  0.094101  0.109241         0.478232,\n",
       "  0.08579615772144923),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  684  0.088313  0.087680  0.089116  0.094519  0.109257         0.491493\n",
       "  685  0.088527  0.087150  0.088424  0.089567  0.109828         0.473178\n",
       "  686  0.087273  0.086559  0.088419  0.071514  0.108007         0.496565\n",
       "  687  0.088151  0.086680  0.088177  0.094101  0.109241         0.478232\n",
       "  688  0.087304  0.085503  0.087549  0.100092  0.108080         0.480346,\n",
       "  0.08929760065241156),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  685  0.088527  0.087150  0.088424  0.089567  0.109828         0.473178\n",
       "  686  0.087273  0.086559  0.088419  0.071514  0.108007         0.496565\n",
       "  687  0.088151  0.086680  0.088177  0.094101  0.109241         0.478232\n",
       "  688  0.087304  0.085503  0.087549  0.100092  0.108080         0.480346\n",
       "  689  0.087314  0.088015  0.087939  0.159324  0.107195         0.513303,\n",
       "  0.08890216005098467),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  686  0.087273  0.086559  0.088419  0.071514  0.108007         0.496565\n",
       "  687  0.088151  0.086680  0.088177  0.094101  0.109241         0.478232\n",
       "  688  0.087304  0.085503  0.087549  0.100092  0.108080         0.480346\n",
       "  689  0.087314  0.088015  0.087939  0.159324  0.107195         0.513303\n",
       "  690  0.089831  0.088507  0.090795  0.113265  0.110615         0.484164,\n",
       "  0.08874877959578842),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  687  0.088151  0.086680  0.088177  0.094101  0.109241         0.478232\n",
       "  688  0.087304  0.085503  0.087549  0.100092  0.108080         0.480346\n",
       "  689  0.087314  0.088015  0.087939  0.159324  0.107195         0.513303\n",
       "  690  0.089831  0.088507  0.090795  0.113265  0.110615         0.484164\n",
       "  691  0.089728  0.087910  0.090538  0.089801  0.110229         0.485974,\n",
       "  0.08881588595052735),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  688  0.087304  0.085503  0.087549  0.100092  0.108080         0.480346\n",
       "  689  0.087314  0.088015  0.087939  0.159324  0.107195         0.513303\n",
       "  690  0.089831  0.088507  0.090795  0.113265  0.110615         0.484164\n",
       "  691  0.089728  0.087910  0.090538  0.089801  0.110229         0.485974\n",
       "  692  0.089126  0.089451  0.089804  0.149719  0.110079         0.487622,\n",
       "  0.09007650201419137),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  689  0.087314  0.088015  0.087939  0.159324  0.107195         0.513303\n",
       "  690  0.089831  0.088507  0.090795  0.113265  0.110615         0.484164\n",
       "  691  0.089728  0.087910  0.090538  0.089801  0.110229         0.485974\n",
       "  692  0.089126  0.089451  0.089804  0.149719  0.110079         0.487622\n",
       "  693  0.089961  0.089406  0.091325  0.093281  0.110144         0.496547,\n",
       "  0.08962353893088473),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  690  0.089831  0.088507  0.090795  0.113265  0.110615         0.484164\n",
       "  691  0.089728  0.087910  0.090538  0.089801  0.110229         0.485974\n",
       "  692  0.089126  0.089451  0.089804  0.149719  0.110079         0.487622\n",
       "  693  0.089961  0.089406  0.091325  0.093281  0.110144         0.496547\n",
       "  694  0.091843  0.090448  0.091507  0.122985  0.111375         0.483733,\n",
       "  0.08970981303134203),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  691  0.089728  0.087910  0.090538  0.089801  0.110229         0.485974\n",
       "  692  0.089126  0.089451  0.089804  0.149719  0.110079         0.487622\n",
       "  693  0.089961  0.089406  0.091325  0.093281  0.110144         0.496547\n",
       "  694  0.091843  0.090448  0.091507  0.122985  0.111375         0.483733\n",
       "  695  0.090479  0.089451  0.090962  0.101420  0.110933         0.487766,\n",
       "  0.09191229478484128),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  692  0.089126  0.089451  0.089804  0.149719  0.110079         0.487622\n",
       "  693  0.089961  0.089406  0.091325  0.093281  0.110144         0.496547\n",
       "  694  0.091843  0.090448  0.091507  0.122985  0.111375         0.483733\n",
       "  695  0.090479  0.089451  0.090962  0.101420  0.110933         0.487766\n",
       "  696  0.091574  0.090813  0.092360  0.129997  0.111017         0.503590,\n",
       "  0.09277267831064881),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  693  0.089961  0.089406  0.091325  0.093281  0.110144         0.496547\n",
       "  694  0.091843  0.090448  0.091507  0.122985  0.111375         0.483733\n",
       "  695  0.090479  0.089451  0.090962  0.101420  0.110933         0.487766\n",
       "  696  0.091574  0.090813  0.092360  0.129997  0.111017         0.503590\n",
       "  697  0.092849  0.092437  0.093990  0.126823  0.113168         0.493554,\n",
       "  0.09552638671134829),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  694  0.091843  0.090448  0.091507  0.122985  0.111375         0.483733\n",
       "  695  0.090479  0.089451  0.090962  0.101420  0.110933         0.487766\n",
       "  696  0.091574  0.090813  0.092360  0.129997  0.111017         0.503590\n",
       "  697  0.092849  0.092437  0.093990  0.126823  0.113168         0.493554\n",
       "  698  0.092729  0.094105  0.093748  0.176146  0.114009         0.507712,\n",
       "  0.09536101679271577),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  695  0.090479  0.089451  0.090962  0.101420  0.110933         0.487766\n",
       "  696  0.091574  0.090813  0.092360  0.129997  0.111017         0.503590\n",
       "  697  0.092849  0.092437  0.093990  0.126823  0.113168         0.493554\n",
       "  698  0.092729  0.094105  0.093748  0.176146  0.114009         0.507712\n",
       "  699  0.096594  0.096461  0.097153  0.216973  0.116698         0.485884,\n",
       "  0.09595777645835947),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  696  0.091574  0.090813  0.092360  0.129997  0.111017         0.503590\n",
       "  697  0.092849  0.092437  0.093990  0.126823  0.113168         0.493554\n",
       "  698  0.092729  0.094105  0.093748  0.176146  0.114009         0.507712\n",
       "  699  0.096594  0.096461  0.097153  0.216973  0.116698         0.485884\n",
       "  700  0.096700  0.095569  0.097364  0.116632  0.116536         0.491583,\n",
       "  0.09755631064157062),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  697  0.092849  0.092437  0.093990  0.126823  0.113168         0.493554\n",
       "  698  0.092729  0.094105  0.093748  0.176146  0.114009         0.507712\n",
       "  699  0.096594  0.096461  0.097153  0.216973  0.116698         0.485884\n",
       "  700  0.096700  0.095569  0.097364  0.116632  0.116536         0.491583\n",
       "  701  0.095956  0.096870  0.097281  0.172624  0.117119         0.499074,\n",
       "  0.10037711577464674),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  698  0.092729  0.094105  0.093748  0.176146  0.114009         0.507712\n",
       "  699  0.096594  0.096461  0.097153  0.216973  0.116698         0.485884\n",
       "  700  0.096700  0.095569  0.097364  0.116632  0.116536         0.491583\n",
       "  701  0.095956  0.096870  0.097281  0.172624  0.117119         0.499074\n",
       "  702  0.098805  0.098882  0.099466  0.254013  0.118680         0.508214,\n",
       "  0.10023571516052449),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  699  0.096594  0.096461  0.097153  0.216973  0.116698         0.485884\n",
       "  700  0.096700  0.095569  0.097364  0.116632  0.116536         0.491583\n",
       "  701  0.095956  0.096870  0.097281  0.172624  0.117119         0.499074\n",
       "  702  0.098805  0.098882  0.099466  0.254013  0.118680         0.508214\n",
       "  703  0.100490  0.099304  0.101561  0.191735  0.121435         0.486063,\n",
       "  0.09947119923039577),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  700  0.096700  0.095569  0.097364  0.116632  0.116536         0.491583\n",
       "  701  0.095956  0.096870  0.097281  0.172624  0.117119         0.499074\n",
       "  702  0.098805  0.098882  0.099466  0.254013  0.118680         0.508214\n",
       "  703  0.100490  0.099304  0.101561  0.191735  0.121435         0.486063\n",
       "  704  0.101212  0.100660  0.100895  0.258473  0.121297         0.481404,\n",
       "  0.09957425473071896),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  701  0.095956  0.096870  0.097281  0.172624  0.117119         0.499074\n",
       "  702  0.098805  0.098882  0.099466  0.254013  0.118680         0.508214\n",
       "  703  0.100490  0.099304  0.101561  0.191735  0.121435         0.486063\n",
       "  704  0.101212  0.100660  0.100895  0.258473  0.121297         0.481404\n",
       "  705  0.100358  0.099034  0.100157  0.154557  0.120550         0.487891,\n",
       "  0.09857965849543357),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  702  0.098805  0.098882  0.099466  0.254013  0.118680         0.508214\n",
       "  703  0.100490  0.099304  0.101561  0.191735  0.121435         0.486063\n",
       "  704  0.101212  0.100660  0.100895  0.258473  0.121297         0.481404\n",
       "  705  0.100358  0.099034  0.100157  0.154557  0.120550         0.487891\n",
       "  706  0.099833  0.098705  0.100287  0.112952  0.120651         0.479683,\n",
       "  0.0970050936167327),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  703  0.100490  0.099304  0.101561  0.191735  0.121435         0.486063\n",
       "  704  0.101212  0.100660  0.100895  0.258473  0.121297         0.481404\n",
       "  705  0.100358  0.099034  0.100157  0.154557  0.120550         0.487891\n",
       "  706  0.099833  0.098705  0.100287  0.112952  0.120651         0.479683\n",
       "  707  0.098555  0.097145  0.098633  0.156048  0.119680         0.475346,\n",
       "  0.09711773298991508),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  704  0.101212  0.100660  0.100895  0.258473  0.121297         0.481404\n",
       "  705  0.100358  0.099034  0.100157  0.154557  0.120550         0.487891\n",
       "  706  0.099833  0.098705  0.100287  0.112952  0.120651         0.479683\n",
       "  707  0.098555  0.097145  0.098633  0.156048  0.119680         0.475346\n",
       "  708  0.098165  0.096404  0.097548  0.170994  0.118142         0.487963,\n",
       "  0.09654254666056687),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  705  0.100358  0.099034  0.100157  0.154557  0.120550         0.487891\n",
       "  706  0.099833  0.098705  0.100287  0.112952  0.120651         0.479683\n",
       "  707  0.098555  0.097145  0.098633  0.156048  0.119680         0.475346\n",
       "  708  0.098165  0.096404  0.097548  0.170994  0.118142         0.487963\n",
       "  709  0.097754  0.095773  0.098277  0.112307  0.118252         0.482820,\n",
       "  0.0972735094133261),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  706  0.099833  0.098705  0.100287  0.112952  0.120651         0.479683\n",
       "  707  0.098555  0.097145  0.098633  0.156048  0.119680         0.475346\n",
       "  708  0.098165  0.096404  0.097548  0.170994  0.118142         0.487963\n",
       "  709  0.097754  0.095773  0.098277  0.112307  0.118252         0.482820\n",
       "  710  0.098430  0.096513  0.098163  0.150079  0.117690         0.492586,\n",
       "  0.09950475240776524),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  707  0.098555  0.097145  0.098633  0.156048  0.119680         0.475346\n",
       "  708  0.098165  0.096404  0.097548  0.170994  0.118142         0.487963\n",
       "  709  0.097754  0.095773  0.098277  0.112307  0.118252         0.482820\n",
       "  710  0.098430  0.096513  0.098163  0.150079  0.117690         0.492586\n",
       "  711  0.097946  0.098172  0.098892  0.117529  0.118404         0.503805,\n",
       "  0.09929145350247444),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  708  0.098165  0.096404  0.097548  0.170994  0.118142         0.487963\n",
       "  709  0.097754  0.095773  0.098277  0.112307  0.118252         0.482820\n",
       "  710  0.098430  0.096513  0.098163  0.150079  0.117690         0.492586\n",
       "  711  0.097946  0.098172  0.098892  0.117529  0.118404         0.503805\n",
       "  712  0.099831  0.098468  0.100660  0.105840  0.120583         0.485526,\n",
       "  0.09825132424874561),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  709  0.097754  0.095773  0.098277  0.112307  0.118252         0.482820\n",
       "  710  0.098430  0.096513  0.098163  0.150079  0.117690         0.492586\n",
       "  711  0.097946  0.098172  0.098892  0.117529  0.118404         0.503805\n",
       "  712  0.099831  0.098468  0.100660  0.105840  0.120583         0.485526\n",
       "  713  0.100541  0.098823  0.100060  0.105424  0.120375         0.479343,\n",
       "  0.0992435245158162),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  710  0.098430  0.096513  0.098163  0.150079  0.117690         0.492586\n",
       "  711  0.097946  0.098172  0.098892  0.117529  0.118404         0.503805\n",
       "  712  0.099831  0.098468  0.100660  0.105840  0.120583         0.485526\n",
       "  713  0.100541  0.098823  0.100060  0.105424  0.120375         0.479343\n",
       "  714  0.099101  0.098003  0.099386  0.107163  0.119359         0.494540,\n",
       "  0.10184863477538673),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  711  0.097946  0.098172  0.098892  0.117529  0.118404         0.503805\n",
       "  712  0.099831  0.098468  0.100660  0.105840  0.120583         0.485526\n",
       "  713  0.100541  0.098823  0.100060  0.105424  0.120375         0.479343\n",
       "  714  0.099101  0.098003  0.099386  0.107163  0.119359         0.494540\n",
       "  715  0.100411  0.100302  0.101765  0.175122  0.120328         0.506601,\n",
       "  0.10243340497759414),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  712  0.099831  0.098468  0.100660  0.105840  0.120583         0.485526\n",
       "  713  0.100541  0.098823  0.100060  0.105424  0.120375         0.479343\n",
       "  714  0.099101  0.098003  0.099386  0.107163  0.119359         0.494540\n",
       "  715  0.100411  0.100302  0.101765  0.175122  0.120328         0.506601\n",
       "  716  0.103433  0.102672  0.103528  0.192667  0.122872         0.491493,\n",
       "  0.10311404277548035),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  713  0.100541  0.098823  0.100060  0.105424  0.120375         0.479343\n",
       "  714  0.099101  0.098003  0.099386  0.107163  0.119359         0.494540\n",
       "  715  0.100411  0.100302  0.101765  0.175122  0.120328         0.506601\n",
       "  716  0.103433  0.102672  0.103528  0.192667  0.122872         0.491493\n",
       "  717  0.104336  0.102248  0.104221  0.138292  0.123443         0.492210,\n",
       "  0.10216259321278591),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  714  0.099101  0.098003  0.099386  0.107163  0.119359         0.494540\n",
       "  715  0.100411  0.100302  0.101765  0.175122  0.120328         0.506601\n",
       "  716  0.103433  0.102672  0.103528  0.192667  0.122872         0.491493\n",
       "  717  0.104336  0.102248  0.104221  0.138292  0.123443         0.492210\n",
       "  718  0.102475  0.101013  0.102603  0.148838  0.124108         0.480006,\n",
       "  0.10185582268003114),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  715  0.100411  0.100302  0.101765  0.175122  0.120328         0.506601\n",
       "  716  0.103433  0.102672  0.103528  0.192667  0.122872         0.491493\n",
       "  717  0.104336  0.102248  0.104221  0.138292  0.123443         0.492210\n",
       "  718  0.102475  0.101013  0.102603  0.148838  0.124108         0.480006\n",
       "  719  0.102447  0.101371  0.103630  0.101293  0.123179         0.484827,\n",
       "  0.10130220968697841),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  716  0.103433  0.102672  0.103528  0.192667  0.122872         0.491493\n",
       "  717  0.104336  0.102248  0.104221  0.138292  0.123443         0.492210\n",
       "  718  0.102475  0.101013  0.102603  0.148838  0.124108         0.480006\n",
       "  719  0.102447  0.101371  0.103630  0.101293  0.123179         0.484827\n",
       "  720  0.102721  0.100873  0.102579  0.094293  0.122879         0.482981,\n",
       "  0.10314280401642022),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  717  0.104336  0.102248  0.104221  0.138292  0.123443         0.492210\n",
       "  718  0.102475  0.101013  0.102603  0.148838  0.124108         0.480006\n",
       "  719  0.102447  0.101371  0.103630  0.101293  0.123179         0.484827\n",
       "  720  0.102721  0.100873  0.102579  0.094293  0.122879         0.482981\n",
       "  721  0.102593  0.101928  0.103727  0.084734  0.122338         0.500884,\n",
       "  0.10409185761089985),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  718  0.102475  0.101013  0.102603  0.148838  0.124108         0.480006\n",
       "  719  0.102447  0.101371  0.103630  0.101293  0.123179         0.484827\n",
       "  720  0.102721  0.100873  0.102579  0.094293  0.122879         0.482981\n",
       "  721  0.102593  0.101928  0.103727  0.084734  0.122338         0.500884\n",
       "  722  0.103934  0.102532  0.104526  0.045432  0.124136         0.494217,\n",
       "  0.10583898031287765),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  719  0.102447  0.101371  0.103630  0.101293  0.123179         0.484827\n",
       "  720  0.102721  0.100873  0.102579  0.094293  0.122879         0.482981\n",
       "  721  0.102593  0.101928  0.103727  0.084734  0.122338         0.500884\n",
       "  722  0.103934  0.102532  0.104526  0.045432  0.124136         0.494217\n",
       "  723  0.105017  0.104902  0.105652  0.120132  0.125063         0.500185,\n",
       "  0.10530454468790561),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  720  0.102721  0.100873  0.102579  0.094293  0.122879         0.482981\n",
       "  721  0.102593  0.101928  0.103727  0.084734  0.122338         0.500884\n",
       "  722  0.103934  0.102532  0.104526  0.045432  0.124136         0.494217\n",
       "  723  0.105017  0.104902  0.105652  0.120132  0.125063         0.500185\n",
       "  724  0.106386  0.104776  0.107246  0.066720  0.126769         0.483124,\n",
       "  0.10606187271338992),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  721  0.102593  0.101928  0.103727  0.084734  0.122338         0.500884\n",
       "  722  0.103934  0.102532  0.104526  0.045432  0.124136         0.494217\n",
       "  723  0.105017  0.104902  0.105652  0.120132  0.125063         0.500185\n",
       "  724  0.106386  0.104776  0.107246  0.066720  0.126769         0.483124\n",
       "  725  0.106807  0.105930  0.107617  0.090625  0.126247         0.492784,\n",
       "  0.10624880634595564),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  722  0.103934  0.102532  0.104526  0.045432  0.124136         0.494217\n",
       "  723  0.105017  0.104902  0.105652  0.120132  0.125063         0.500185\n",
       "  724  0.106386  0.104776  0.107246  0.066720  0.126769         0.483124\n",
       "  725  0.106807  0.105930  0.107617  0.090625  0.126247         0.492784\n",
       "  726  0.106998  0.105518  0.107954  0.093695  0.126987         0.488518,\n",
       "  0.10651961811076387),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  723  0.105017  0.104902  0.105652  0.120132  0.125063         0.500185\n",
       "  724  0.106386  0.104776  0.107246  0.066720  0.126769         0.483124\n",
       "  725  0.106807  0.105930  0.107617  0.090625  0.126247         0.492784\n",
       "  726  0.106998  0.105518  0.107954  0.093695  0.126987         0.488518\n",
       "  727  0.106954  0.105399  0.107556  0.080397  0.127169         0.489146,\n",
       "  0.10672572911141025),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  724  0.106386  0.104776  0.107246  0.066720  0.126769         0.483124\n",
       "  725  0.106807  0.105930  0.107617  0.090625  0.126247         0.492784\n",
       "  726  0.106998  0.105518  0.107954  0.093695  0.126987         0.488518\n",
       "  727  0.106954  0.105399  0.107556  0.080397  0.127169         0.489146\n",
       "  728  0.107496  0.105594  0.107607  0.083615  0.127434         0.488662,\n",
       "  0.10836260840842048),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  725  0.106807  0.105930  0.107617  0.090625  0.126247         0.492784\n",
       "  726  0.106998  0.105518  0.107954  0.093695  0.126987         0.488518\n",
       "  727  0.106954  0.105399  0.107556  0.080397  0.127169         0.489146\n",
       "  728  0.107496  0.105594  0.107607  0.083615  0.127434         0.488662\n",
       "  729  0.107989  0.106862  0.109310  0.127288  0.127635         0.499361,\n",
       "  0.10856153150442248),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  726  0.106998  0.105518  0.107954  0.093695  0.126987         0.488518\n",
       "  727  0.106954  0.105399  0.107556  0.080397  0.127169         0.489146\n",
       "  728  0.107496  0.105594  0.107607  0.083615  0.127434         0.488662\n",
       "  729  0.107989  0.106862  0.109310  0.127288  0.127635         0.499361\n",
       "  730  0.108564  0.108262  0.109809  0.160426  0.129234         0.488608,\n",
       "  0.10904324620630669),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  727  0.106954  0.105399  0.107556  0.080397  0.127169         0.489146\n",
       "  728  0.107496  0.105594  0.107607  0.083615  0.127434         0.488662\n",
       "  729  0.107989  0.106862  0.109310  0.127288  0.127635         0.499361\n",
       "  730  0.108564  0.108262  0.109809  0.160426  0.129234         0.488608\n",
       "  731  0.109705  0.108061  0.110553  0.105190  0.129428         0.490723,\n",
       "  0.10772511728312524),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  728  0.107496  0.105594  0.107607  0.083615  0.127434         0.488662\n",
       "  729  0.107989  0.106862  0.109310  0.127288  0.127635         0.499361\n",
       "  730  0.108564  0.108262  0.109809  0.160426  0.129234         0.488608\n",
       "  731  0.109705  0.108061  0.110553  0.105190  0.129428         0.490723\n",
       "  732  0.109428  0.107295  0.108477  0.147790  0.129898         0.477264,\n",
       "  0.10750702644140485),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  729  0.107989  0.106862  0.109310  0.127288  0.127635         0.499361\n",
       "  730  0.108564  0.108262  0.109809  0.160426  0.129234         0.488608\n",
       "  731  0.109705  0.108061  0.110553  0.105190  0.129428         0.490723\n",
       "  732  0.109428  0.107295  0.108477  0.147790  0.129898         0.477264\n",
       "  733  0.109325  0.107063  0.108096  0.270555  0.128611         0.485490,\n",
       "  0.10068388630740153),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  730  0.108564  0.108262  0.109809  0.160426  0.129234         0.488608\n",
       "  731  0.109705  0.108061  0.110553  0.105190  0.129428         0.490723\n",
       "  732  0.109428  0.107295  0.108477  0.147790  0.129898         0.477264\n",
       "  733  0.109325  0.107063  0.108096  0.270555  0.128611         0.485490\n",
       "  734  0.099323  0.099873  0.100166  0.432186  0.128398         0.436099,\n",
       "  0.09886006375546329),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  731  0.109705  0.108061  0.110553  0.105190  0.129428         0.490723\n",
       "  732  0.109428  0.107295  0.108477  0.147790  0.129898         0.477264\n",
       "  733  0.109325  0.107063  0.108096  0.270555  0.128611         0.485490\n",
       "  734  0.099323  0.099873  0.100166  0.432186  0.128398         0.436099\n",
       "  735  0.101034  0.099119  0.100808  0.154520  0.121735         0.473483,\n",
       "  0.09921715924309113),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  732  0.109428  0.107295  0.108477  0.147790  0.129898         0.477264\n",
       "  733  0.109325  0.107063  0.108096  0.270555  0.128611         0.485490\n",
       "  734  0.099323  0.099873  0.100166  0.432186  0.128398         0.436099\n",
       "  735  0.101034  0.099119  0.100808  0.154520  0.121735         0.473483\n",
       "  736  0.098697  0.098809  0.099587  0.135436  0.119953         0.489791,\n",
       "  0.09820099929387256),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  733  0.109325  0.107063  0.108096  0.270555  0.128611         0.485490\n",
       "  734  0.099323  0.099873  0.100166  0.432186  0.128398         0.436099\n",
       "  735  0.101034  0.099119  0.100808  0.154520  0.121735         0.473483\n",
       "  736  0.098697  0.098809  0.099587  0.135436  0.119953         0.489791\n",
       "  737  0.100545  0.098413  0.099212  0.134777  0.120302         0.479522,\n",
       "  0.09777919341972059),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  734  0.099323  0.099873  0.100166  0.432186  0.128398         0.436099\n",
       "  735  0.101034  0.099119  0.100808  0.154520  0.121735         0.473483\n",
       "  736  0.098697  0.098809  0.099587  0.135436  0.119953         0.489791\n",
       "  737  0.100545  0.098413  0.099212  0.134777  0.120302         0.479522\n",
       "  738  0.098562  0.097363  0.097594  0.167331  0.119310         0.483966,\n",
       "  0.09871147523669664),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  735  0.101034  0.099119  0.100808  0.154520  0.121735         0.473483\n",
       "  736  0.098697  0.098809  0.099587  0.135436  0.119953         0.489791\n",
       "  737  0.100545  0.098413  0.099212  0.134777  0.120302         0.479522\n",
       "  738  0.098562  0.097363  0.097594  0.167331  0.119310         0.483966\n",
       "  739  0.098512  0.098318  0.099197  0.133904  0.118898         0.494092,\n",
       "  0.09972284324948562),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  736  0.098697  0.098809  0.099587  0.135436  0.119953         0.489791\n",
       "  737  0.100545  0.098413  0.099212  0.134777  0.120302         0.479522\n",
       "  738  0.098562  0.097363  0.097594  0.167331  0.119310         0.483966\n",
       "  739  0.098512  0.098318  0.099197  0.133904  0.118898         0.494092\n",
       "  740  0.099568  0.098963  0.100408  0.096305  0.119808         0.494683,\n",
       "  0.09825851215338999),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  737  0.100545  0.098413  0.099212  0.134777  0.120302         0.479522\n",
       "  738  0.098562  0.097363  0.097594  0.167331  0.119310         0.483966\n",
       "  739  0.098512  0.098318  0.099197  0.133904  0.118898         0.494092\n",
       "  740  0.099568  0.098963  0.100408  0.096305  0.119808         0.494683\n",
       "  741  0.101328  0.099224  0.100287  0.103777  0.120796         0.476171,\n",
       "  0.09896311925578648),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  738  0.098562  0.097363  0.097594  0.167331  0.119310         0.483966\n",
       "  739  0.098512  0.098318  0.099197  0.133904  0.118898         0.494092\n",
       "  740  0.099568  0.098963  0.100408  0.096305  0.119808         0.494683\n",
       "  741  0.101328  0.099224  0.100287  0.103777  0.120796         0.476171\n",
       "  742  0.098986  0.098292  0.099837  0.107613  0.119366         0.492389,\n",
       "  0.09850057229962066),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  739  0.098512  0.098318  0.099197  0.133904  0.118898         0.494092\n",
       "  740  0.099568  0.098963  0.100408  0.096305  0.119808         0.494683\n",
       "  741  0.101328  0.099224  0.100287  0.103777  0.120796         0.476171\n",
       "  742  0.098986  0.098292  0.099837  0.107613  0.119366         0.492389\n",
       "  743  0.099761  0.097932  0.100045  0.076642  0.120054         0.483662,\n",
       "  0.0965808917743659),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  740  0.099568  0.098963  0.100408  0.096305  0.119808         0.494683\n",
       "  741  0.101328  0.099224  0.100287  0.103777  0.120796         0.476171\n",
       "  742  0.098986  0.098292  0.099837  0.107613  0.119366         0.492389\n",
       "  743  0.099761  0.097932  0.100045  0.076642  0.120054         0.483662\n",
       "  744  0.098878  0.097508  0.098592  0.077160  0.119602         0.472766,\n",
       "  0.09825851215338999),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  741  0.101328  0.099224  0.100287  0.103777  0.120796         0.476171\n",
       "  742  0.098986  0.098292  0.099837  0.107613  0.119366         0.492389\n",
       "  743  0.099761  0.097932  0.100045  0.076642  0.120054         0.483662\n",
       "  744  0.098878  0.097508  0.098592  0.077160  0.119602         0.472766\n",
       "  745  0.097181  0.096785  0.098471  0.088721  0.117728         0.499665,\n",
       "  0.09970126991319013),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  742  0.098986  0.098292  0.099837  0.107613  0.119366         0.492389\n",
       "  743  0.099761  0.097932  0.100045  0.076642  0.120054         0.483662\n",
       "  744  0.098878  0.097508  0.098592  0.077160  0.119602         0.472766\n",
       "  745  0.097181  0.096785  0.098471  0.088721  0.117728         0.499665\n",
       "  746  0.098805  0.099091  0.100055  0.103626  0.119366         0.497909,\n",
       "  0.10204036034438203),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  743  0.099761  0.097932  0.100045  0.076642  0.120054         0.483662\n",
       "  744  0.098878  0.097508  0.098592  0.077160  0.119602         0.472766\n",
       "  745  0.097181  0.096785  0.098471  0.088721  0.117728         0.499665\n",
       "  746  0.098805  0.099091  0.100055  0.103626  0.119366         0.497909\n",
       "  747  0.101113  0.100489  0.102005  0.098871  0.120775         0.504611,\n",
       "  0.09939211303458285),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  744  0.098878  0.097508  0.098592  0.077160  0.119602         0.472766\n",
       "  745  0.097181  0.096785  0.098471  0.088721  0.117728         0.499665\n",
       "  746  0.098805  0.099091  0.100055  0.103626  0.119366         0.497909\n",
       "  747  0.101113  0.100489  0.102005  0.098871  0.120775         0.504611\n",
       "  748  0.101465  0.100736  0.101409  0.117787  0.123059         0.467318,\n",
       "  0.0996365691490283),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  745  0.097181  0.096785  0.098471  0.088721  0.117728         0.499665\n",
       "  746  0.098805  0.099091  0.100055  0.103626  0.119366         0.497909\n",
       "  747  0.101113  0.100489  0.102005  0.098871  0.120775         0.504611\n",
       "  748  0.101465  0.100736  0.101409  0.117787  0.123059         0.467318\n",
       "  749  0.098909  0.098816  0.099229  0.142809  0.120473         0.488949,\n",
       "  0.09957665069893376),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  746  0.098805  0.099091  0.100055  0.103626  0.119366         0.497909\n",
       "  747  0.101113  0.100489  0.102005  0.098871  0.120775         0.504611\n",
       "  748  0.101465  0.100736  0.101409  0.117787  0.123059         0.467318\n",
       "  749  0.098909  0.098816  0.099229  0.142809  0.120473         0.488949\n",
       "  750  0.101161  0.099060  0.101021  0.077235  0.120712         0.486673,\n",
       "  0.09792299000205765),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  747  0.101113  0.100489  0.102005  0.098871  0.120775         0.504611\n",
       "  748  0.101465  0.100736  0.101409  0.117787  0.123059         0.467318\n",
       "  749  0.098909  0.098816  0.099229  0.142809  0.120473         0.488949\n",
       "  750  0.101161  0.099060  0.101021  0.077235  0.120712         0.486673\n",
       "  751  0.100242  0.098503  0.099803  0.088288  0.120653         0.474755,\n",
       "  0.09527474269225845),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  748  0.101465  0.100736  0.101409  0.117787  0.123059         0.467318\n",
       "  749  0.098909  0.098816  0.099229  0.142809  0.120473         0.488949\n",
       "  750  0.101161  0.099060  0.101021  0.077235  0.120712         0.486673\n",
       "  751  0.100242  0.098503  0.099803  0.088288  0.120653         0.474755\n",
       "  752  0.098625  0.097150  0.097068  0.131481  0.119038         0.467318,\n",
       "  0.09382958896424354),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  749  0.098909  0.098816  0.099229  0.142809  0.120473         0.488949\n",
       "  750  0.101161  0.099060  0.101021  0.077235  0.120712         0.486673\n",
       "  751  0.100242  0.098503  0.099803  0.088288  0.120653         0.474755\n",
       "  752  0.098625  0.097150  0.097068  0.131481  0.119038         0.467318\n",
       "  753  0.094539  0.093534  0.093132  0.210194  0.116452         0.476314,\n",
       "  0.09587149273553985),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  750  0.101161  0.099060  0.101021  0.077235  0.120712         0.486673\n",
       "  751  0.100242  0.098503  0.099803  0.088288  0.120653         0.474755\n",
       "  752  0.098625  0.097150  0.097068  0.131481  0.119038         0.467318\n",
       "  753  0.094539  0.093534  0.093132  0.210194  0.116452         0.476314\n",
       "  754  0.095843  0.094617  0.095843  0.133181  0.115041         0.502389,\n",
       "  0.09536341276093056),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  751  0.100242  0.098503  0.099803  0.088288  0.120653         0.474755\n",
       "  752  0.098625  0.097150  0.097068  0.131481  0.119038         0.467318\n",
       "  753  0.094539  0.093534  0.093132  0.210194  0.116452         0.476314\n",
       "  754  0.095843  0.094617  0.095843  0.133181  0.115041         0.502389\n",
       "  755  0.097051  0.094986  0.096896  0.065511  0.117035         0.483321,\n",
       "  0.09744606723660304),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  752  0.098625  0.097150  0.097068  0.131481  0.119038         0.467318\n",
       "  753  0.094539  0.093534  0.093132  0.210194  0.116452         0.476314\n",
       "  754  0.095843  0.094617  0.095843  0.133181  0.115041         0.502389\n",
       "  755  0.097051  0.094986  0.096896  0.065511  0.117035         0.483321\n",
       "  756  0.096204  0.096314  0.097325  0.087726  0.116539         0.502694,\n",
       "  0.09891758623734304),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  753  0.094539  0.093534  0.093132  0.210194  0.116452         0.476314\n",
       "  754  0.095843  0.094617  0.095843  0.133181  0.115041         0.502389\n",
       "  755  0.097051  0.094986  0.096896  0.065511  0.117035         0.483321\n",
       "  756  0.096204  0.096314  0.097325  0.087726  0.116539         0.502694\n",
       "  757  0.098856  0.098231  0.100106  0.079021  0.118573         0.498124,\n",
       "  0.09878337352786516),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  754  0.095843  0.094617  0.095843  0.133181  0.115041         0.502389\n",
       "  755  0.097051  0.094986  0.096896  0.065511  0.117035         0.483321\n",
       "  756  0.096204  0.096314  0.097325  0.087726  0.116539         0.502694\n",
       "  757  0.098856  0.098231  0.100106  0.079021  0.118573         0.498124\n",
       "  758  0.100310  0.098202  0.099561  0.074740  0.120010         0.486117,\n",
       "  0.09945682342110698),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  755  0.097051  0.094986  0.096896  0.065511  0.117035         0.483321\n",
       "  756  0.096204  0.096314  0.097325  0.087726  0.116539         0.502694\n",
       "  757  0.098856  0.098231  0.100106  0.079021  0.118573         0.498124\n",
       "  758  0.100310  0.098202  0.099561  0.074740  0.120010         0.486117\n",
       "  759  0.099494  0.098065  0.099924  0.060048  0.119879         0.492156,\n",
       "  0.0990398094833846),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  756  0.096204  0.096314  0.097325  0.087726  0.116539         0.502694\n",
       "  757  0.098856  0.098231  0.100106  0.079021  0.118573         0.498124\n",
       "  758  0.100310  0.098202  0.099561  0.074740  0.120010         0.486117\n",
       "  759  0.099494  0.098065  0.099924  0.060048  0.119879         0.492156\n",
       "  760  0.099932  0.098515  0.100626  0.056458  0.120536         0.484002,\n",
       "  0.09739573265936768),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  757  0.098856  0.098231  0.100106  0.079021  0.118573         0.498124\n",
       "  758  0.100310  0.098202  0.099561  0.074740  0.120010         0.486117\n",
       "  759  0.099494  0.098065  0.099924  0.060048  0.119879         0.492156\n",
       "  760  0.099932  0.098515  0.100626  0.056458  0.120536         0.484002\n",
       "  761  0.099234  0.097930  0.099268  0.079536  0.120129         0.474827,\n",
       "  0.0989487338241354),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  758  0.100310  0.098202  0.099561  0.074740  0.120010         0.486117\n",
       "  759  0.099494  0.098065  0.099924  0.060048  0.119879         0.492156\n",
       "  760  0.099932  0.098515  0.100626  0.056458  0.120536         0.484002\n",
       "  761  0.099234  0.097930  0.099268  0.079536  0.120129         0.474827\n",
       "  762  0.098346  0.097532  0.099616  0.061912  0.118523         0.498733,\n",
       "  0.09859404392708467),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  759  0.099494  0.098065  0.099924  0.060048  0.119879         0.492156\n",
       "  760  0.099932  0.098515  0.100626  0.056458  0.120536         0.484002\n",
       "  761  0.099234  0.097930  0.099268  0.079536  0.120129         0.474827\n",
       "  762  0.098346  0.097532  0.099616  0.061912  0.118523         0.498733\n",
       "  763  0.099434  0.098029  0.100428  0.064402  0.120040         0.484468,\n",
       "  0.09951673224883921),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  760  0.099932  0.098515  0.100626  0.056458  0.120536         0.484002\n",
       "  761  0.099234  0.097930  0.099268  0.079536  0.120129         0.474827\n",
       "  762  0.098346  0.097532  0.099616  0.061912  0.118523         0.498733\n",
       "  763  0.099434  0.098029  0.100428  0.064402  0.120040         0.484468\n",
       "  764  0.099612  0.098290  0.100643  0.072329  0.119694         0.494020,\n",
       "  0.10188937585740059),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  761  0.099234  0.097930  0.099268  0.079536  0.120129         0.474827\n",
       "  762  0.098346  0.097532  0.099616  0.061912  0.118523         0.498733\n",
       "  763  0.099434  0.098029  0.100428  0.064402  0.120040         0.484468\n",
       "  764  0.099612  0.098290  0.100643  0.072329  0.119694         0.494020\n",
       "  765  0.100074  0.101015  0.101406  0.089747  0.120595         0.504862,\n",
       "  0.10252448063684334),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  762  0.098346  0.097532  0.099616  0.061912  0.118523         0.498733\n",
       "  763  0.099434  0.098029  0.100428  0.064402  0.120040         0.484468\n",
       "  764  0.099612  0.098290  0.100643  0.072329  0.119694         0.494020\n",
       "  765  0.100074  0.101015  0.101406  0.089747  0.120595         0.504862\n",
       "  766  0.102090  0.101366  0.103255  0.080485  0.122912         0.491870,\n",
       "  0.10149872719240328),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  763  0.099434  0.098029  0.100428  0.064402  0.120040         0.484468\n",
       "  764  0.099612  0.098290  0.100643  0.072329  0.119694         0.494020\n",
       "  765  0.100074  0.101015  0.101406  0.089747  0.120595         0.504862\n",
       "  766  0.102090  0.101366  0.103255  0.080485  0.122912         0.491870\n",
       "  767  0.103525  0.101449  0.102283  0.088098  0.123532         0.479450,\n",
       "  0.1004993390206883),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  764  0.099612  0.098290  0.100643  0.072329  0.119694         0.494020\n",
       "  765  0.100074  0.101015  0.101406  0.089747  0.120595         0.504862\n",
       "  766  0.102090  0.101366  0.103255  0.080485  0.122912         0.491870\n",
       "  767  0.103525  0.101449  0.102283  0.088098  0.123532         0.479450\n",
       "  768  0.100757  0.099413  0.101934  0.089015  0.122530         0.479647,\n",
       "  0.0993321945844883),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  765  0.100074  0.101015  0.101406  0.089747  0.120595         0.504862\n",
       "  766  0.102090  0.101366  0.103255  0.080485  0.122912         0.491870\n",
       "  767  0.103525  0.101449  0.102283  0.088098  0.123532         0.479450\n",
       "  768  0.100757  0.099413  0.101934  0.089015  0.122530         0.479647\n",
       "  769  0.101580  0.099610  0.100501  0.078366  0.121554         0.478393,\n",
       "  0.10097387544029043),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  766  0.102090  0.101366  0.103255  0.080485  0.122912         0.491870\n",
       "  767  0.103525  0.101449  0.102283  0.088098  0.123532         0.479450\n",
       "  768  0.100757  0.099413  0.101934  0.089015  0.122530         0.479647\n",
       "  769  0.101580  0.099610  0.100501  0.078366  0.121554         0.478393\n",
       "  770  0.100548  0.099510  0.101675  0.065665  0.120415         0.499396,\n",
       "  0.1012902202235421),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  767  0.103525  0.101449  0.102283  0.088098  0.123532         0.479450\n",
       "  768  0.100757  0.099413  0.101934  0.089015  0.122530         0.479647\n",
       "  769  0.101580  0.099610  0.100501  0.078366  0.121554         0.478393\n",
       "  770  0.100548  0.099510  0.101675  0.065665  0.120415         0.499396\n",
       "  771  0.101400  0.101011  0.102467  0.072534  0.122018         0.489486,\n",
       "  0.10180070578872848),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  768  0.100757  0.099413  0.101934  0.089015  0.122530         0.479647\n",
       "  769  0.101580  0.099610  0.100501  0.078366  0.121554         0.478393\n",
       "  770  0.100548  0.099510  0.101675  0.065665  0.120415         0.499396\n",
       "  771  0.101400  0.101011  0.102467  0.072534  0.122018         0.489486\n",
       "  772  0.102249  0.100828  0.103489  0.045902  0.122327         0.490938,\n",
       "  0.10275215535142292),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  769  0.101580  0.099610  0.100501  0.078366  0.121554         0.478393\n",
       "  770  0.100548  0.099510  0.101675  0.065665  0.120415         0.499396\n",
       "  771  0.101400  0.101011  0.102467  0.072534  0.122018         0.489486\n",
       "  772  0.102249  0.100828  0.103489  0.045902  0.122327         0.490938\n",
       "  773  0.102042  0.101553  0.103247  0.067156  0.122825         0.494235,\n",
       "  0.10192532500298486),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  770  0.100548  0.099510  0.101675  0.065665  0.120415         0.499396\n",
       "  771  0.101400  0.101011  0.102467  0.072534  0.122018         0.489486\n",
       "  772  0.102249  0.100828  0.103489  0.045902  0.122327         0.490938\n",
       "  773  0.102042  0.101553  0.103247  0.067156  0.122825         0.494235\n",
       "  774  0.102844  0.101319  0.103695  0.053355  0.123754         0.480938,\n",
       "  0.1043147403890498),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  771  0.101400  0.101011  0.102467  0.072534  0.122018         0.489486\n",
       "  772  0.102249  0.100828  0.103489  0.045902  0.122327         0.490938\n",
       "  773  0.102042  0.101553  0.103247  0.067156  0.122825         0.494235\n",
       "  774  0.102844  0.101319  0.103695  0.053355  0.123754         0.480938\n",
       "  775  0.102841  0.103207  0.103744  0.102444  0.122947         0.504988,\n",
       "  0.10709001250368248),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  772  0.102249  0.100828  0.103489  0.045902  0.122327         0.490938\n",
       "  773  0.102042  0.101553  0.103247  0.067156  0.122825         0.494235\n",
       "  774  0.102844  0.101319  0.103695  0.053355  0.123754         0.480938\n",
       "  775  0.102841  0.103207  0.103744  0.102444  0.122947         0.504988\n",
       "  776  0.105910  0.106099  0.107278  0.134337  0.125280         0.507873,\n",
       "  0.10852318639062343),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  773  0.102042  0.101553  0.103247  0.067156  0.122825         0.494235\n",
       "  774  0.102844  0.101319  0.103695  0.053355  0.123754         0.480938\n",
       "  775  0.102841  0.103207  0.103744  0.102444  0.122947         0.504988\n",
       "  776  0.105910  0.106099  0.107278  0.134337  0.125280         0.507873\n",
       "  777  0.107770  0.107840  0.109014  0.134291  0.127991         0.497837,\n",
       "  0.11026551715617162),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  774  0.102844  0.101319  0.103695  0.053355  0.123754         0.480938\n",
       "  775  0.102841  0.103207  0.103744  0.102444  0.122947         0.504988\n",
       "  776  0.105910  0.106099  0.107278  0.134337  0.125280         0.507873\n",
       "  777  0.107770  0.107840  0.109014  0.134291  0.127991         0.497837\n",
       "  778  0.110018  0.108786  0.110662  0.194775  0.129390         0.500149,\n",
       "  0.1121636339674932),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  775  0.102841  0.103207  0.103744  0.102444  0.122947         0.504988\n",
       "  776  0.105910  0.106099  0.107278  0.134337  0.125280         0.507873\n",
       "  777  0.107770  0.107840  0.109014  0.134291  0.127991         0.497837\n",
       "  778  0.110018  0.108786  0.110662  0.194775  0.129390         0.500149\n",
       "  779  0.111139  0.111314  0.112398  0.128745  0.131092         0.501314,\n",
       "  0.11239849658671715),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  776  0.105910  0.106099  0.107278  0.134337  0.125280         0.507873\n",
       "  777  0.107770  0.107840  0.109014  0.134291  0.127991         0.497837\n",
       "  778  0.110018  0.108786  0.110662  0.194775  0.129390         0.500149\n",
       "  779  0.111139  0.111314  0.112398  0.128745  0.131092         0.501314\n",
       "  780  0.111861  0.110864  0.113091  0.066321  0.132946         0.488877,\n",
       "  0.11219718714486267),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  777  0.107770  0.107840  0.109014  0.134291  0.127991         0.497837\n",
       "  778  0.110018  0.108786  0.110662  0.194775  0.129390         0.500149\n",
       "  779  0.111139  0.111314  0.112398  0.128745  0.131092         0.501314\n",
       "  780  0.111861  0.110864  0.113091  0.066321  0.132946         0.488877\n",
       "  781  0.113401  0.111393  0.113321  0.081292  0.133175         0.485615,\n",
       "  0.11203900513087452),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  778  0.110018  0.108786  0.110662  0.194775  0.129390         0.500149\n",
       "  779  0.111139  0.111314  0.112398  0.128745  0.131092         0.501314\n",
       "  780  0.111861  0.110864  0.113091  0.066321  0.132946         0.488877\n",
       "  781  0.113401  0.111393  0.113321  0.081292  0.133175         0.485615\n",
       "  782  0.113722  0.111381  0.113798  0.049920  0.132978         0.485938,\n",
       "  0.11198388823957188),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  779  0.111139  0.111314  0.112398  0.128745  0.131092         0.501314\n",
       "  780  0.111861  0.110864  0.113091  0.066321  0.132946         0.488877\n",
       "  781  0.113401  0.111393  0.113321  0.081292  0.133175         0.485615\n",
       "  782  0.113722  0.111381  0.113798  0.049920  0.132978         0.485938\n",
       "  783  0.112583  0.110862  0.113396  0.064106  0.132824         0.486708,\n",
       "  0.11564589953037484),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  780  0.111861  0.110864  0.113091  0.066321  0.132946         0.488877\n",
       "  781  0.113401  0.111393  0.113321  0.081292  0.133175         0.485615\n",
       "  782  0.113722  0.111381  0.113798  0.049920  0.132978         0.485938\n",
       "  783  0.112583  0.110862  0.113396  0.064106  0.132824         0.486708\n",
       "  784  0.113298  0.114369  0.114726  0.114499  0.132770         0.514504,\n",
       "  0.1160868827726075),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  781  0.113401  0.111393  0.113321  0.081292  0.133175         0.485615\n",
       "  782  0.113722  0.111381  0.113798  0.049920  0.132978         0.485938\n",
       "  783  0.112583  0.110862  0.113396  0.064106  0.132824         0.486708\n",
       "  784  0.113298  0.114369  0.114726  0.114499  0.132770         0.514504\n",
       "  785  0.116525  0.117322  0.117245  0.171809  0.136346         0.490418,\n",
       "  0.11599820308157306),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  782  0.113722  0.111381  0.113798  0.049920  0.132978         0.485938\n",
       "  783  0.112583  0.110862  0.113396  0.064106  0.132824         0.486708\n",
       "  784  0.113298  0.114369  0.114726  0.114499  0.132770         0.514504\n",
       "  785  0.116525  0.117322  0.117245  0.171809  0.136346         0.490418\n",
       "  786  0.117216  0.115469  0.117330  0.094266  0.136777         0.486457,\n",
       "  0.11480229740443319),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  783  0.112583  0.110862  0.113396  0.064106  0.132824         0.486708\n",
       "  784  0.113298  0.114369  0.114726  0.114499  0.132770         0.514504\n",
       "  785  0.116525  0.117322  0.117245  0.171809  0.136346         0.490418\n",
       "  786  0.117216  0.115469  0.117330  0.094266  0.136777         0.486457\n",
       "  787  0.116937  0.114544  0.116530  0.072551  0.136690         0.478178,\n",
       "  0.11840200389928913),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  784  0.113298  0.114369  0.114726  0.114499  0.132770         0.514504\n",
       "  785  0.116525  0.117322  0.117245  0.171809  0.136346         0.490418\n",
       "  786  0.117216  0.115469  0.117330  0.094266  0.136777         0.486457\n",
       "  787  0.116937  0.114544  0.116530  0.072551  0.136690         0.478178\n",
       "  788  0.117423  0.117130  0.118940  0.123192  0.135522         0.514038,\n",
       "  0.12213351751304581),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  785  0.116525  0.117322  0.117245  0.171809  0.136346         0.490418\n",
       "  786  0.117216  0.115469  0.117330  0.094266  0.136777         0.486457\n",
       "  787  0.116937  0.114544  0.116530  0.072551  0.136690         0.478178\n",
       "  788  0.117423  0.117130  0.118940  0.123192  0.135522         0.514038\n",
       "  789  0.119322  0.120509  0.120621  0.122202  0.139038         0.515024,\n",
       "  0.12346603186787834),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  786  0.117216  0.115469  0.117330  0.094266  0.136777         0.486457\n",
       "  787  0.116937  0.114544  0.116530  0.072551  0.136690         0.478178\n",
       "  788  0.117423  0.117130  0.118940  0.123192  0.135522         0.514038\n",
       "  789  0.119322  0.120509  0.120621  0.122202  0.139038         0.515024\n",
       "  790  0.124161  0.123713  0.124119  0.213166  0.142682         0.497085,\n",
       "  0.12591296973582305),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  787  0.116937  0.114544  0.116530  0.072551  0.136690         0.478178\n",
       "  788  0.117423  0.117130  0.118940  0.123192  0.135522         0.514038\n",
       "  789  0.119322  0.120509  0.120621  0.122202  0.139038         0.515024\n",
       "  790  0.124161  0.123713  0.124119  0.213166  0.142682         0.497085\n",
       "  791  0.125666  0.124164  0.126156  0.130900  0.143983         0.505418,\n",
       "  0.12510051519667378),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  788  0.117423  0.117130  0.118940  0.123192  0.135522         0.514038\n",
       "  789  0.119322  0.120509  0.120621  0.122202  0.139038         0.515024\n",
       "  790  0.124161  0.123713  0.124119  0.213166  0.142682         0.497085\n",
       "  791  0.125666  0.124164  0.126156  0.130900  0.143983         0.505418\n",
       "  792  0.128621  0.127891  0.124267  0.286895  0.146373         0.481045,\n",
       "  0.12878890138256413),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  789  0.119322  0.120509  0.120621  0.122202  0.139038         0.515024\n",
       "  790  0.124161  0.123713  0.124119  0.213166  0.142682         0.497085\n",
       "  791  0.125666  0.124164  0.126156  0.130900  0.143983         0.505418\n",
       "  792  0.128621  0.127891  0.124267  0.286895  0.146373         0.481045\n",
       "  793  0.126296  0.127178  0.126505  0.165879  0.145580         0.514701,\n",
       "  0.12464756173572943),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  790  0.124161  0.123713  0.124119  0.213166  0.142682         0.497085\n",
       "  791  0.125666  0.124164  0.126156  0.130900  0.143983         0.505418\n",
       "  792  0.128621  0.127891  0.124267  0.286895  0.146373         0.481045\n",
       "  793  0.126296  0.127178  0.126505  0.165879  0.145580         0.514701\n",
       "  794  0.129783  0.127524  0.125853  0.168801  0.149182         0.456153,\n",
       "  0.12366255899566554),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  791  0.125666  0.124164  0.126156  0.130900  0.143983         0.505418\n",
       "  792  0.128621  0.127891  0.124267  0.286895  0.146373         0.481045\n",
       "  793  0.126296  0.127178  0.126505  0.165879  0.145580         0.514701\n",
       "  794  0.129783  0.127524  0.125853  0.168801  0.149182         0.456153\n",
       "  795  0.124975  0.124220  0.124991  0.146460  0.145137         0.479755,\n",
       "  0.12785182762915848),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  792  0.128621  0.127891  0.124267  0.286895  0.146373         0.481045\n",
       "  793  0.126296  0.127178  0.126505  0.165879  0.145580         0.514701\n",
       "  794  0.129783  0.127524  0.125853  0.168801  0.149182         0.456153\n",
       "  795  0.124975  0.124220  0.124991  0.146460  0.145137         0.479755\n",
       "  796  0.127853  0.126135  0.127316  0.146586  0.144175         0.518447,\n",
       "  0.12932333700753618),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  793  0.126296  0.127178  0.126505  0.165879  0.145580         0.514701\n",
       "  794  0.129783  0.127524  0.125853  0.168801  0.149182         0.456153\n",
       "  795  0.124975  0.124220  0.124991  0.146460  0.145137         0.479755\n",
       "  796  0.127853  0.126135  0.127316  0.146586  0.144175         0.518447\n",
       "  797  0.129047  0.127882  0.128990  0.298819  0.148267         0.498124,\n",
       "  0.13054321198918634),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  794  0.129783  0.127524  0.125853  0.168801  0.149182         0.456153\n",
       "  795  0.124975  0.124220  0.124991  0.146460  0.145137         0.479755\n",
       "  796  0.127853  0.126135  0.127316  0.146586  0.144175         0.518447\n",
       "  797  0.129047  0.127882  0.128990  0.298819  0.148267         0.498124\n",
       "  798  0.133655  0.131939  0.132558  0.383945  0.149704         0.496242,\n",
       "  0.13199076168541604),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  795  0.124975  0.124220  0.124991  0.146460  0.145137         0.479755\n",
       "  796  0.127853  0.126135  0.127316  0.146586  0.144175         0.518447\n",
       "  797  0.129047  0.127882  0.128990  0.298819  0.148267         0.498124\n",
       "  798  0.133655  0.131939  0.132558  0.383945  0.149704         0.496242\n",
       "  799  0.129831  0.131112  0.130874  0.162005  0.150895         0.497945,\n",
       "  0.13798708109334362),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  796  0.127853  0.126135  0.127316  0.146586  0.144175         0.518447\n",
       "  797  0.129047  0.127882  0.128990  0.298819  0.148267         0.498124\n",
       "  798  0.133655  0.131939  0.132558  0.383945  0.149704         0.496242\n",
       "  799  0.129831  0.131112  0.130874  0.162005  0.150895         0.497945\n",
       "  800  0.135265  0.136468  0.136620  0.165150  0.152309         0.531959,\n",
       "  0.1379990705567799),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  797  0.129047  0.127882  0.128990  0.298819  0.148267         0.498124\n",
       "  798  0.133655  0.131939  0.132558  0.383945  0.149704         0.496242\n",
       "  799  0.129831  0.131112  0.130874  0.162005  0.150895         0.497945\n",
       "  800  0.135265  0.136468  0.136620  0.165150  0.152309         0.531959\n",
       "  801  0.138023  0.136437  0.136513  0.179999  0.158165         0.487210,\n",
       "  0.13624712705128558),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  798  0.133655  0.131939  0.132558  0.383945  0.149704         0.496242\n",
       "  799  0.129831  0.131112  0.130874  0.162005  0.150895         0.497945\n",
       "  800  0.135265  0.136468  0.136620  0.165150  0.152309         0.531959\n",
       "  801  0.138023  0.136437  0.136513  0.179999  0.158165         0.487210\n",
       "  802  0.139460  0.136792  0.137480  0.140870  0.158176         0.474020,\n",
       "  0.13770668545567621),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  799  0.129831  0.131112  0.130874  0.162005  0.150895         0.497945\n",
       "  800  0.135265  0.136468  0.136620  0.165150  0.152309         0.531959\n",
       "  801  0.138023  0.136437  0.136513  0.179999  0.158165         0.487210\n",
       "  802  0.139460  0.136792  0.137480  0.140870  0.158176         0.474020\n",
       "  803  0.138341  0.136216  0.138572  0.081482  0.156465         0.498035,\n",
       "  0.13881629778763416),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  800  0.135265  0.136468  0.136620  0.165150  0.152309         0.531959\n",
       "  801  0.138023  0.136437  0.136513  0.179999  0.158165         0.487210\n",
       "  802  0.139460  0.136792  0.137480  0.140870  0.158176         0.474020\n",
       "  803  0.138341  0.136216  0.138572  0.081482  0.156465         0.498035\n",
       "  804  0.139241  0.137036  0.139548  0.074487  0.157891         0.495418,\n",
       "  0.1425406427414711),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  801  0.138023  0.136437  0.136513  0.179999  0.158165         0.487210\n",
       "  802  0.139460  0.136792  0.137480  0.140870  0.158176         0.474020\n",
       "  803  0.138341  0.136216  0.138572  0.081482  0.156465         0.498035\n",
       "  804  0.139241  0.137036  0.139548  0.074487  0.157891         0.495418\n",
       "  805  0.139179  0.141755  0.140861  0.167750  0.158974         0.514970,\n",
       "  0.14547168165715246),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  802  0.139460  0.136792  0.137480  0.140870  0.158176         0.474020\n",
       "  803  0.138341  0.136216  0.138572  0.081482  0.156465         0.498035\n",
       "  804  0.139241  0.137036  0.139548  0.074487  0.157891         0.495418\n",
       "  805  0.139179  0.141755  0.140861  0.167750  0.158974         0.514970\n",
       "  806  0.144757  0.143435  0.145349  0.167171  0.162612         0.509038,\n",
       "  0.1445633691446867),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  803  0.138341  0.136216  0.138572  0.081482  0.156465         0.498035\n",
       "  804  0.139241  0.137036  0.139548  0.074487  0.157891         0.495418\n",
       "  805  0.139179  0.141755  0.140861  0.167750  0.158974         0.514970\n",
       "  806  0.144757  0.143435  0.145349  0.167171  0.162612         0.509038\n",
       "  807  0.145262  0.145027  0.146740  0.158667  0.165474         0.480328,\n",
       "  0.14649022795222352),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  804  0.139241  0.137036  0.139548  0.074487  0.157891         0.495418\n",
       "  805  0.139179  0.141755  0.140861  0.167750  0.158974         0.514970\n",
       "  806  0.144757  0.143435  0.145349  0.167171  0.162612         0.509038\n",
       "  807  0.145262  0.145027  0.146740  0.158667  0.165474         0.480328\n",
       "  808  0.147120  0.144994  0.145664  0.141989  0.164587         0.501529,\n",
       "  0.14994134592831282),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  805  0.139179  0.141755  0.140861  0.167750  0.158974         0.514970\n",
       "  806  0.144757  0.143435  0.145349  0.167171  0.162612         0.509038\n",
       "  807  0.145262  0.145027  0.146740  0.158667  0.165474         0.480328\n",
       "  808  0.147120  0.144994  0.145664  0.141989  0.164587         0.501529\n",
       "  809  0.146273  0.148940  0.147779  0.215985  0.166469         0.512927,\n",
       "  0.15380946821976216),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  806  0.144757  0.143435  0.145349  0.167171  0.162612         0.509038\n",
       "  807  0.145262  0.145027  0.146740  0.158667  0.165474         0.480328\n",
       "  808  0.147120  0.144994  0.145664  0.141989  0.164587         0.501529\n",
       "  809  0.146273  0.148940  0.147779  0.215985  0.166469         0.512927\n",
       "  810  0.153630  0.151679  0.152364  0.205103  0.169839         0.516045,\n",
       "  0.15168847825265294),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  807  0.145262  0.145027  0.146740  0.158667  0.165474         0.480328\n",
       "  808  0.147120  0.144994  0.145664  0.141989  0.164587         0.501529\n",
       "  809  0.146273  0.148940  0.147779  0.215985  0.166469         0.512927\n",
       "  810  0.153630  0.151679  0.152364  0.205103  0.169839         0.516045\n",
       "  811  0.154489  0.152971  0.151913  0.201143  0.173617         0.471261,\n",
       "  0.14231774071859649),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  808  0.147120  0.144994  0.145664  0.141989  0.164587         0.501529\n",
       "  809  0.146273  0.148940  0.147779  0.215985  0.166469         0.512927\n",
       "  810  0.153630  0.151679  0.152364  0.205103  0.169839         0.516045\n",
       "  811  0.154489  0.152971  0.151913  0.201143  0.173617         0.471261\n",
       "  812  0.152918  0.150044  0.140781  0.401536  0.171545         0.417049,\n",
       "  0.1351590784332608),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  809  0.146273  0.148940  0.147779  0.215985  0.166469         0.512927\n",
       "  810  0.153630  0.151679  0.152364  0.205103  0.169839         0.516045\n",
       "  811  0.154489  0.152971  0.151913  0.201143  0.173617         0.471261\n",
       "  812  0.152918  0.150044  0.140781  0.401536  0.171545         0.417049\n",
       "  813  0.138762  0.137482  0.136913  0.276901  0.162394         0.433590,\n",
       "  0.127513899887249),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  810  0.153630  0.151679  0.152364  0.205103  0.169839         0.516045\n",
       "  811  0.154489  0.152971  0.151913  0.201143  0.173617         0.471261\n",
       "  812  0.152918  0.150044  0.140781  0.401536  0.171545         0.417049\n",
       "  813  0.138762  0.137482  0.136913  0.276901  0.162394         0.433590\n",
       "  814  0.134421  0.134650  0.128435  0.248666  0.155403         0.429952,\n",
       "  0.13433944561182945),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  811  0.154489  0.152971  0.151913  0.201143  0.173617         0.471261\n",
       "  812  0.152918  0.150044  0.140781  0.401536  0.171545         0.417049\n",
       "  813  0.138762  0.137482  0.136913  0.276901  0.162394         0.433590\n",
       "  814  0.134421  0.134650  0.128435  0.248666  0.155403         0.429952\n",
       "  815  0.131352  0.132515  0.129855  0.205324  0.147937         0.538160,\n",
       "  0.12981704117285667),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  812  0.152918  0.150044  0.140781  0.401536  0.171545         0.417049\n",
       "  813  0.138762  0.137482  0.136913  0.276901  0.162394         0.433590\n",
       "  814  0.134421  0.134650  0.128435  0.248666  0.155403         0.429952\n",
       "  815  0.131352  0.132515  0.129855  0.205324  0.147937         0.538160\n",
       "  816  0.138161  0.135968  0.130872  0.196794  0.154602         0.453304,\n",
       "  0.1269339216214712),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  813  0.138762  0.137482  0.136913  0.276901  0.162394         0.433590\n",
       "  814  0.134421  0.134650  0.128435  0.248666  0.155403         0.429952\n",
       "  815  0.131352  0.132515  0.129855  0.205324  0.147937         0.538160\n",
       "  816  0.138161  0.135968  0.130872  0.196794  0.154602         0.453304\n",
       "  817  0.129807  0.129334  0.127900  0.169382  0.150186         0.465562,\n",
       "  0.12788777677474275),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  814  0.134421  0.134650  0.128435  0.248666  0.155403         0.429952\n",
       "  815  0.131352  0.132515  0.129855  0.205324  0.147937         0.538160\n",
       "  816  0.138161  0.135968  0.130872  0.196794  0.154602         0.453304\n",
       "  817  0.129807  0.129334  0.127900  0.169382  0.150186         0.465562\n",
       "  818  0.128693  0.126488  0.125967  0.219855  0.147370         0.494253,\n",
       "  0.12602321314079065),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  815  0.131352  0.132515  0.129855  0.205324  0.147937         0.538160\n",
       "  816  0.138161  0.135968  0.130872  0.196794  0.154602         0.453304\n",
       "  817  0.129807  0.129334  0.127900  0.169382  0.150186         0.465562\n",
       "  818  0.128693  0.126488  0.125967  0.219855  0.147370         0.494253\n",
       "  819  0.127646  0.126792  0.126568  0.134349  0.148302         0.473178,\n",
       "  0.1314611083745113),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  816  0.138161  0.135968  0.130872  0.196794  0.154602         0.453304\n",
       "  817  0.129807  0.129334  0.127900  0.169382  0.150186         0.465562\n",
       "  818  0.128693  0.126488  0.125967  0.219855  0.147370         0.494253\n",
       "  819  0.127646  0.126792  0.126568  0.134349  0.148302         0.473178\n",
       "  820  0.129304  0.132083  0.130049  0.239260  0.146481         0.527783,\n",
       "  0.13433224808482278),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  817  0.129807  0.129334  0.127900  0.169382  0.150186         0.465562\n",
       "  818  0.128693  0.126488  0.125967  0.219855  0.147370         0.494253\n",
       "  819  0.127646  0.126792  0.126568  0.134349  0.148302         0.473178\n",
       "  820  0.129304  0.132083  0.130049  0.239260  0.146481         0.527783\n",
       "  821  0.131059  0.134659  0.132279  0.170496  0.151791         0.508590,\n",
       "  0.1382099638714936),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  818  0.128693  0.126488  0.125967  0.219855  0.147370         0.494253\n",
       "  819  0.127646  0.126792  0.126568  0.134349  0.148302         0.473178\n",
       "  820  0.129304  0.132083  0.130049  0.239260  0.146481         0.527783\n",
       "  821  0.131059  0.134659  0.132279  0.170496  0.151791         0.508590\n",
       "  822  0.137371  0.136629  0.138584  0.066518  0.154595         0.516117,\n",
       "  0.1356456043162992),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  819  0.127646  0.126792  0.126568  0.134349  0.148302         0.473178\n",
       "  820  0.129304  0.132083  0.130049  0.239260  0.146481         0.527783\n",
       "  821  0.131059  0.134659  0.132279  0.170496  0.151791         0.508590\n",
       "  822  0.137371  0.136629  0.138584  0.066518  0.154595         0.516117\n",
       "  823  0.139826  0.140212  0.137831  0.165062  0.158382         0.467945,\n",
       "  0.13745981412829136),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  820  0.129304  0.132083  0.130049  0.239260  0.146481         0.527783\n",
       "  821  0.131059  0.134659  0.132279  0.170496  0.151791         0.508590\n",
       "  822  0.137371  0.136629  0.138584  0.066518  0.154595         0.516117\n",
       "  823  0.139826  0.140212  0.137831  0.165062  0.158382         0.467945\n",
       "  824  0.138526  0.136190  0.134261  0.216491  0.155878         0.500687,\n",
       "  0.14193908151703546),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  821  0.131059  0.134659  0.132279  0.170496  0.151791         0.508590\n",
       "  822  0.137371  0.136629  0.138584  0.066518  0.154595         0.516117\n",
       "  823  0.139826  0.140212  0.137831  0.165062  0.158382         0.467945\n",
       "  824  0.138526  0.136190  0.134261  0.216491  0.155878         0.500687\n",
       "  825  0.140286  0.140425  0.139560  0.192460  0.157650         0.520615,\n",
       "  0.14307508798880544),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  822  0.137371  0.136629  0.138584  0.066518  0.154595         0.516117\n",
       "  823  0.139826  0.140212  0.137831  0.165062  0.158382         0.467945\n",
       "  824  0.138526  0.136190  0.134261  0.216491  0.155878         0.500687\n",
       "  825  0.140286  0.140425  0.139560  0.192460  0.157650         0.520615\n",
       "  826  0.142365  0.142437  0.143346  0.150903  0.162024         0.495615,\n",
       "  0.1421164216543797),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  823  0.139826  0.140212  0.137831  0.165062  0.158382         0.467945\n",
       "  824  0.138526  0.136190  0.134261  0.216491  0.155878         0.500687\n",
       "  825  0.140286  0.140425  0.139560  0.192460  0.157650         0.520615\n",
       "  826  0.142365  0.142437  0.143346  0.150903  0.162024         0.495615\n",
       "  827  0.147238  0.144397  0.141975  0.191949  0.163133         0.479952,\n",
       "  0.1393675244348344),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  824  0.138526  0.136190  0.134261  0.216491  0.155878         0.500687\n",
       "  825  0.140286  0.140425  0.139560  0.192460  0.157650         0.520615\n",
       "  826  0.142365  0.142437  0.143346  0.150903  0.162024         0.495615\n",
       "  827  0.147238  0.144397  0.141975  0.191949  0.163133         0.479952\n",
       "  828  0.142427  0.140591  0.141740  0.105105  0.162197         0.466565,\n",
       "  0.13999782765548527),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  825  0.140286  0.140425  0.139560  0.192460  0.157650         0.520615\n",
       "  826  0.142365  0.142437  0.143346  0.150903  0.162024         0.495615\n",
       "  827  0.147238  0.144397  0.141975  0.191949  0.163133         0.479952\n",
       "  828  0.142427  0.140591  0.141740  0.105105  0.162197         0.466565\n",
       "  829  0.139371  0.139880  0.140767  0.102836  0.159513         0.491834,\n",
       "  0.14343695616813823),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  826  0.142365  0.142437  0.143346  0.150903  0.162024         0.495615\n",
       "  827  0.147238  0.144397  0.141975  0.191949  0.163133         0.479952\n",
       "  828  0.142427  0.140591  0.141740  0.105105  0.162197         0.466565\n",
       "  829  0.139371  0.139880  0.140767  0.102836  0.159513         0.491834\n",
       "  830  0.142841  0.141522  0.143281  0.102296  0.160128         0.512837,\n",
       "  0.14745128063250176),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  827  0.147238  0.144397  0.141975  0.191949  0.163133         0.479952\n",
       "  828  0.142427  0.140591  0.141740  0.105105  0.162197         0.466565\n",
       "  829  0.139371  0.139880  0.140767  0.102836  0.159513         0.491834\n",
       "  830  0.142841  0.141522  0.143281  0.102296  0.160128         0.512837\n",
       "  831  0.144062  0.145700  0.145584  0.119307  0.163487         0.517138,\n",
       "  0.14735780900503775),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  828  0.142427  0.140591  0.141740  0.105105  0.162197         0.466565\n",
       "  829  0.139371  0.139880  0.140767  0.102836  0.159513         0.491834\n",
       "  830  0.142841  0.141522  0.143281  0.102296  0.160128         0.512837\n",
       "  831  0.144062  0.145700  0.145584  0.119307  0.163487         0.517138\n",
       "  832  0.148199  0.146042  0.148852  0.093600  0.167407         0.486422,\n",
       "  0.14820860865798607),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  829  0.139371  0.139880  0.140767  0.102836  0.159513         0.491834\n",
       "  830  0.142841  0.141522  0.143281  0.102296  0.160128         0.512837\n",
       "  831  0.144062  0.145700  0.145584  0.119307  0.163487         0.517138\n",
       "  832  0.148199  0.146042  0.148852  0.093600  0.167407         0.486422\n",
       "  833  0.148439  0.147653  0.149700  0.093700  0.167316         0.493483,\n",
       "  0.14360231646440846),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  830  0.142841  0.141522  0.143281  0.102296  0.160128         0.512837\n",
       "  831  0.144062  0.145700  0.145584  0.119307  0.163487         0.517138\n",
       "  832  0.148199  0.146042  0.148852  0.093600  0.167407         0.486422\n",
       "  833  0.148439  0.147653  0.149700  0.093700  0.167316         0.493483\n",
       "  834  0.149390  0.146750  0.146013  0.149228  0.168147         0.452676,\n",
       "  0.14363827523235506),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  831  0.144062  0.145700  0.145584  0.119307  0.163487         0.517138\n",
       "  832  0.148199  0.146042  0.148852  0.093600  0.167407         0.486422\n",
       "  833  0.148439  0.147653  0.149700  0.093700  0.167316         0.493483\n",
       "  834  0.149390  0.146750  0.146013  0.149228  0.168147         0.452676\n",
       "  835  0.147960  0.145169  0.143523  0.149714  0.163648         0.487389,\n",
       "  0.14236806567346955),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  832  0.148199  0.146042  0.148852  0.093600  0.167407         0.486422\n",
       "  833  0.148439  0.147653  0.149700  0.093700  0.167316         0.493483\n",
       "  834  0.149390  0.146750  0.146013  0.149228  0.168147         0.452676\n",
       "  835  0.147960  0.145169  0.143523  0.149714  0.163648         0.487389\n",
       "  836  0.143703  0.141212  0.141757  0.122513  0.163683         0.477622,\n",
       "  0.14138785486983524),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  833  0.148439  0.147653  0.149700  0.093700  0.167316         0.493483\n",
       "  834  0.149390  0.146750  0.146013  0.149228  0.168147         0.452676\n",
       "  835  0.147960  0.145169  0.143523  0.149714  0.163648         0.487389\n",
       "  836  0.143703  0.141212  0.141757  0.122513  0.163683         0.477622\n",
       "  837  0.141585  0.141705  0.143223  0.089256  0.162443         0.479791,\n",
       "  0.13641970411928714),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  834  0.149390  0.146750  0.146013  0.149228  0.168147         0.452676\n",
       "  835  0.147960  0.145169  0.143523  0.149714  0.163648         0.487389\n",
       "  836  0.143703  0.141212  0.141757  0.122513  0.163683         0.477622\n",
       "  837  0.141585  0.141705  0.143223  0.089256  0.162443         0.479791\n",
       "  838  0.141703  0.140691  0.137509  0.133342  0.161486         0.449970,\n",
       "  0.13740710282756582),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  835  0.147960  0.145169  0.143523  0.149714  0.163648         0.487389\n",
       "  836  0.143703  0.141212  0.141757  0.122513  0.163683         0.477622\n",
       "  837  0.141585  0.141705  0.143223  0.089256  0.162443         0.479791\n",
       "  838  0.141703  0.140691  0.137509  0.133342  0.161486         0.449970\n",
       "  839  0.138372  0.136257  0.134803  0.174222  0.156634         0.494504,\n",
       "  0.13837054185369652),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  836  0.143703  0.141212  0.141757  0.122513  0.163683         0.477622\n",
       "  837  0.141585  0.141705  0.143223  0.089256  0.162443         0.479791\n",
       "  838  0.141703  0.140691  0.137509  0.133342  0.161486         0.449970\n",
       "  839  0.138372  0.136257  0.134803  0.174222  0.156634         0.494504\n",
       "  840  0.138384  0.136918  0.138800  0.107440  0.157598         0.494325,\n",
       "  0.14132314448331112),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  837  0.141585  0.141705  0.143223  0.089256  0.162443         0.479791\n",
       "  838  0.141703  0.140691  0.137509  0.133342  0.161486         0.449970\n",
       "  839  0.138372  0.136257  0.134803  0.174222  0.156634         0.494504\n",
       "  840  0.138384  0.136918  0.138800  0.107440  0.157598         0.494325\n",
       "  841  0.141181  0.139643  0.141612  0.107467  0.158539         0.509199,\n",
       "  0.14300077448469747),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  838  0.141703  0.140691  0.137509  0.133342  0.161486         0.449970\n",
       "  839  0.138372  0.136257  0.134803  0.174222  0.156634         0.494504\n",
       "  840  0.138384  0.136918  0.138800  0.107440  0.157598         0.494325\n",
       "  841  0.141181  0.139643  0.141612  0.107467  0.158539         0.509199\n",
       "  842  0.144081  0.141601  0.144671  0.130812  0.161423         0.499665,\n",
       "  0.1439689958248955),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  839  0.138372  0.136257  0.134803  0.174222  0.156634         0.494504\n",
       "  840  0.138384  0.136918  0.138800  0.107440  0.157598         0.494325\n",
       "  841  0.141181  0.139643  0.141612  0.107467  0.158539         0.509199\n",
       "  842  0.144081  0.141601  0.144671  0.130812  0.161423         0.499665\n",
       "  843  0.143385  0.141949  0.144628  0.039512  0.163061         0.494361,\n",
       "  0.14639198363305453),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  840  0.138384  0.136918  0.138800  0.107440  0.157598         0.494325\n",
       "  841  0.141181  0.139643  0.141612  0.107467  0.158539         0.509199\n",
       "  842  0.144081  0.141601  0.144671  0.130812  0.161423         0.499665\n",
       "  843  0.143385  0.141949  0.144628  0.039512  0.163061         0.494361\n",
       "  844  0.144348  0.144909  0.145875  0.061442  0.164006         0.505239,\n",
       "  0.1439714014154726),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  841  0.141181  0.139643  0.141612  0.107467  0.158539         0.509199\n",
       "  842  0.144081  0.141601  0.144671  0.130812  0.161423         0.499665\n",
       "  843  0.143385  0.141949  0.144628  0.039512  0.163061         0.494361\n",
       "  844  0.144348  0.144909  0.145875  0.061442  0.164006         0.505239\n",
       "  845  0.146292  0.145568  0.146488  0.071475  0.166373         0.469020,\n",
       "  0.14440038557190665),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  842  0.144081  0.141601  0.144671  0.130812  0.161423         0.499665\n",
       "  843  0.143385  0.141949  0.144628  0.039512  0.163061         0.494361\n",
       "  844  0.144348  0.144909  0.145875  0.061442  0.164006         0.505239\n",
       "  845  0.146292  0.145568  0.146488  0.071475  0.166373         0.469020\n",
       "  846  0.145777  0.143660  0.145470  0.062237  0.164009         0.490328,\n",
       "  0.1417521478844698),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  843  0.143385  0.141949  0.144628  0.039512  0.163061         0.494361\n",
       "  844  0.144348  0.144909  0.145875  0.061442  0.164006         0.505239\n",
       "  845  0.146292  0.145568  0.146488  0.071475  0.166373         0.469020\n",
       "  846  0.145777  0.143660  0.145470  0.062237  0.164009         0.490328\n",
       "  847  0.144247  0.142366  0.144027  0.057701  0.164428         0.467318,\n",
       "  0.1402446797381455),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  844  0.144348  0.144909  0.145875  0.061442  0.164006         0.505239\n",
       "  845  0.146292  0.145568  0.146488  0.071475  0.166373         0.469020\n",
       "  846  0.145777  0.143660  0.145470  0.062237  0.164009         0.490328\n",
       "  847  0.144247  0.142366  0.144027  0.057701  0.164428         0.467318\n",
       "  848  0.142875  0.141153  0.140914  0.104662  0.161842         0.475848,\n",
       "  0.14027822329315265),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  845  0.146292  0.145568  0.146488  0.071475  0.166373         0.469020\n",
       "  846  0.145777  0.143660  0.145470  0.062237  0.164009         0.490328\n",
       "  847  0.144247  0.142366  0.144027  0.057701  0.164428         0.467318\n",
       "  848  0.142875  0.141153  0.140914  0.104662  0.161842         0.475848\n",
       "  849  0.141044  0.138660  0.140621  0.079016  0.160369         0.487371,\n",
       "  0.13348864595888113),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  846  0.145777  0.143660  0.145470  0.062237  0.164009         0.490328\n",
       "  847  0.144247  0.142366  0.144027  0.057701  0.164428         0.467318\n",
       "  848  0.142875  0.141153  0.140914  0.104662  0.161842         0.475848\n",
       "  849  0.141044  0.138660  0.140621  0.079016  0.160369         0.487371\n",
       "  850  0.139703  0.137264  0.135409  0.130272  0.160402         0.436350,\n",
       "  0.1316312606072111),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  847  0.144247  0.142366  0.144027  0.057701  0.164428         0.467318\n",
       "  848  0.142875  0.141153  0.140914  0.104662  0.161842         0.475848\n",
       "  849  0.141044  0.138660  0.140621  0.079016  0.160369         0.487371\n",
       "  850  0.139703  0.137264  0.135409  0.130272  0.160402         0.436350\n",
       "  851  0.133506  0.132837  0.131134  0.155656  0.153771         0.473232,\n",
       "  0.12742043788214732),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  848  0.142875  0.141153  0.140914  0.104662  0.161842         0.475848\n",
       "  849  0.141044  0.138660  0.140621  0.079016  0.160369         0.487371\n",
       "  850  0.139703  0.137264  0.135409  0.130272  0.160402         0.436350\n",
       "  851  0.133506  0.132837  0.131134  0.155656  0.153771         0.473232\n",
       "  852  0.133280  0.132287  0.129596  0.129769  0.151958         0.455633,\n",
       "  0.13257793747820057),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  849  0.141044  0.138660  0.140621  0.079016  0.160369         0.487371\n",
       "  850  0.139703  0.137264  0.135409  0.130272  0.160402         0.436350\n",
       "  851  0.133506  0.132837  0.131134  0.155656  0.153771         0.473232\n",
       "  852  0.133280  0.132287  0.129596  0.129769  0.151958         0.455633\n",
       "  853  0.127754  0.130718  0.127539  0.163836  0.147845         0.525687,\n",
       "  0.13102733228164762),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  850  0.139703  0.137264  0.135409  0.130272  0.160402         0.436350\n",
       "  851  0.133506  0.132837  0.131134  0.155656  0.153771         0.473232\n",
       "  852  0.133280  0.132287  0.129596  0.129769  0.151958         0.455633\n",
       "  853  0.127754  0.130718  0.127539  0.163836  0.147845         0.525687\n",
       "  854  0.131357  0.131633  0.131802  0.153973  0.152882         0.475526,\n",
       "  0.12899500276084822),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  851  0.133506  0.132837  0.131134  0.155656  0.153771         0.473232\n",
       "  852  0.133280  0.132287  0.129596  0.129769  0.151958         0.455633\n",
       "  853  0.127754  0.130718  0.127539  0.163836  0.147845         0.525687\n",
       "  854  0.131357  0.131633  0.131802  0.153973  0.152882         0.475526\n",
       "  855  0.130801  0.129801  0.129380  0.120961  0.151368         0.471923,\n",
       "  0.13272652599696722),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  852  0.133280  0.132287  0.129596  0.129769  0.151958         0.455633\n",
       "  853  0.127754  0.130718  0.127539  0.163836  0.147845         0.525687\n",
       "  854  0.131357  0.131633  0.131802  0.153973  0.152882         0.475526\n",
       "  855  0.130801  0.129801  0.129380  0.120961  0.151368         0.471923\n",
       "  856  0.132832  0.131681  0.133047  0.108082  0.149383         0.515024,\n",
       "  0.12885120617851117),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  853  0.127754  0.130718  0.127539  0.163836  0.147845         0.525687\n",
       "  854  0.131357  0.131633  0.131802  0.153973  0.152882         0.475526\n",
       "  855  0.130801  0.129801  0.129380  0.120961  0.151368         0.471923\n",
       "  856  0.132832  0.131681  0.133047  0.108082  0.149383         0.515024\n",
       "  857  0.131571  0.129702  0.130656  0.135341  0.153027         0.458142,\n",
       "  0.12365056953222923),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  854  0.131357  0.131633  0.131802  0.153973  0.152882         0.475526\n",
       "  855  0.130801  0.129801  0.129380  0.120961  0.151368         0.471923\n",
       "  856  0.132832  0.131681  0.133047  0.108082  0.149383         0.515024\n",
       "  857  0.131571  0.129702  0.130656  0.135341  0.153027         0.458142\n",
       "  858  0.127497  0.127555  0.122554  0.256759  0.149243         0.448232,\n",
       "  0.12001731948236617),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  855  0.130801  0.129801  0.129380  0.120961  0.151368         0.471923\n",
       "  856  0.132832  0.131681  0.133047  0.108082  0.149383         0.515024\n",
       "  857  0.131571  0.129702  0.130656  0.135341  0.153027         0.458142\n",
       "  858  0.127497  0.127555  0.122554  0.256759  0.149243         0.448232\n",
       "  859  0.125521  0.124178  0.121605  0.199764  0.144164         0.459952,\n",
       "  0.11988789870931789),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  856  0.132832  0.131681  0.133047  0.108082  0.149383         0.515024\n",
       "  857  0.131571  0.129702  0.130656  0.135341  0.153027         0.458142\n",
       "  858  0.127497  0.127555  0.122554  0.256759  0.149243         0.448232\n",
       "  859  0.125521  0.124178  0.121605  0.199764  0.144164         0.459952\n",
       "  860  0.122537  0.120445  0.121711  0.207616  0.140615         0.486153,\n",
       "  0.11607728927738599),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  857  0.131571  0.129702  0.130656  0.135341  0.153027         0.458142\n",
       "  858  0.127497  0.127555  0.122554  0.256759  0.149243         0.448232\n",
       "  859  0.125521  0.124178  0.121605  0.199764  0.144164         0.459952\n",
       "  860  0.122537  0.120445  0.121711  0.207616  0.140615         0.486153\n",
       "  861  0.111387  0.117485  0.112689  0.231012  0.140489         0.458626,\n",
       "  0.10751421434604923),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  858  0.127497  0.127555  0.122554  0.256759  0.149243         0.448232\n",
       "  859  0.125521  0.124178  0.121605  0.199764  0.144164         0.459952\n",
       "  860  0.122537  0.120445  0.121711  0.207616  0.140615         0.486153\n",
       "  861  0.111387  0.117485  0.112689  0.231012  0.140489         0.458626\n",
       "  862  0.111069  0.110494  0.102467  0.412567  0.136768         0.423089,\n",
       "  0.11371423916404609),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  859  0.125521  0.124178  0.121605  0.199764  0.144164         0.459952\n",
       "  860  0.122537  0.120445  0.121711  0.207616  0.140615         0.486153\n",
       "  861  0.111387  0.117485  0.112689  0.231012  0.140489         0.458626\n",
       "  862  0.111069  0.110494  0.102467  0.412567  0.136768         0.423089\n",
       "  863  0.110610  0.113265  0.110979  0.228565  0.128405         0.533482,\n",
       "  0.1117753812707107),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  860  0.122537  0.120445  0.121711  0.207616  0.140615         0.486153\n",
       "  861  0.111387  0.117485  0.112689  0.231012  0.140489         0.458626\n",
       "  862  0.111069  0.110494  0.102467  0.412567  0.136768         0.423089\n",
       "  863  0.110610  0.113265  0.110979  0.228565  0.128405         0.533482\n",
       "  864  0.118554  0.116893  0.113895  0.169340  0.134460         0.472622,\n",
       "  0.10927811844789294),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  861  0.111387  0.117485  0.112689  0.231012  0.140489         0.458626\n",
       "  862  0.111069  0.110494  0.102467  0.412567  0.136768         0.423089\n",
       "  863  0.110610  0.113265  0.110979  0.228565  0.128405         0.533482\n",
       "  864  0.118554  0.116893  0.113895  0.169340  0.134460         0.472622\n",
       "  865  0.113539  0.111499  0.109637  0.141386  0.132566         0.468447,\n",
       "  0.10796957339757067),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  862  0.111069  0.110494  0.102467  0.412567  0.136768         0.423089\n",
       "  863  0.110610  0.113265  0.110979  0.228565  0.128405         0.533482\n",
       "  864  0.118554  0.116893  0.113895  0.169340  0.134460         0.472622\n",
       "  865  0.113539  0.111499  0.109637  0.141386  0.132566         0.468447\n",
       "  866  0.111012  0.108914  0.107716  0.152728  0.130128         0.477336,\n",
       "  0.10743033621380675),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  863  0.110610  0.113265  0.110979  0.228565  0.128405         0.533482\n",
       "  864  0.118554  0.116893  0.113895  0.169340  0.134460         0.472622\n",
       "  865  0.113539  0.111499  0.109637  0.141386  0.132566         0.468447\n",
       "  866  0.111012  0.108914  0.107716  0.152728  0.130128         0.477336\n",
       "  867  0.108297  0.108701  0.108404  0.193022  0.128850         0.483088,\n",
       "  0.111272093232531),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  864  0.118554  0.116893  0.113895  0.169340  0.134460         0.472622\n",
       "  865  0.113539  0.111499  0.109637  0.141386  0.132566         0.468447\n",
       "  866  0.111012  0.108914  0.107716  0.152728  0.130128         0.477336\n",
       "  867  0.108297  0.108701  0.108404  0.193022  0.128850         0.483088\n",
       "  868  0.105847  0.111679  0.106171  0.361624  0.128323         0.515848,\n",
       "  0.09967251829461256),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  865  0.113539  0.111499  0.109637  0.141386  0.132566         0.468447\n",
       "  866  0.111012  0.108914  0.107716  0.152728  0.130128         0.477336\n",
       "  867  0.108297  0.108701  0.108404  0.193022  0.128850         0.483088\n",
       "  868  0.105847  0.111679  0.106171  0.361624  0.128323         0.515848\n",
       "  869  0.103359  0.103070  0.100287  0.428007  0.132075         0.400383,\n",
       "  0.09476666271764918),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  866  0.111012  0.108914  0.107716  0.152728  0.130128         0.477336\n",
       "  867  0.108297  0.108701  0.108404  0.193022  0.128850         0.483088\n",
       "  868  0.105847  0.111679  0.106171  0.361624  0.128323         0.515848\n",
       "  869  0.103359  0.103070  0.100287  0.428007  0.132075         0.400383\n",
       "  870  0.098642  0.097408  0.096061  0.319929  0.120747         0.450436,\n",
       "  0.0974916002550465),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  867  0.108297  0.108701  0.108404  0.193022  0.128850         0.483088\n",
       "  868  0.105847  0.111679  0.106171  0.361624  0.128323         0.515848\n",
       "  869  0.103359  0.103070  0.100287  0.428007  0.132075         0.400383\n",
       "  870  0.098642  0.097408  0.096061  0.319929  0.120747         0.450436\n",
       "  871  0.093915  0.096513  0.095085  0.272406  0.115956         0.507497,\n",
       "  0.09627172527339634),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  868  0.105847  0.111679  0.106171  0.361624  0.128323         0.515848\n",
       "  869  0.103359  0.103070  0.100287  0.428007  0.132075         0.400383\n",
       "  870  0.098642  0.097408  0.096061  0.319929  0.120747         0.450436\n",
       "  871  0.093915  0.096513  0.095085  0.272406  0.115956         0.507497\n",
       "  872  0.099140  0.097027  0.097364  0.185647  0.118617         0.477999,\n",
       "  0.09704823066696135),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  869  0.103359  0.103070  0.100287  0.428007  0.132075         0.400383\n",
       "  870  0.098642  0.097408  0.096061  0.319929  0.120747         0.450436\n",
       "  871  0.093915  0.096513  0.095085  0.272406  0.115956         0.507497\n",
       "  872  0.099140  0.097027  0.097364  0.185647  0.118617         0.477999\n",
       "  873  0.095703  0.097743  0.096596  0.192761  0.117426         0.492927,\n",
       "  0.09986184789539307),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  870  0.098642  0.097408  0.096061  0.319929  0.120747         0.450436\n",
       "  871  0.093915  0.096513  0.095085  0.272406  0.115956         0.507497\n",
       "  872  0.099140  0.097027  0.097364  0.185647  0.118617         0.477999\n",
       "  873  0.095703  0.097743  0.096596  0.192761  0.117426         0.492927\n",
       "  874  0.098724  0.098581  0.099972  0.166006  0.118184         0.508160,\n",
       "  0.10093312473591427),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  871  0.093915  0.096513  0.095085  0.272406  0.115956         0.507497\n",
       "  872  0.099140  0.097027  0.097364  0.185647  0.118617         0.477999\n",
       "  873  0.095703  0.097743  0.096596  0.192761  0.117426         0.492927\n",
       "  874  0.098724  0.098581  0.099972  0.166006  0.118184         0.508160\n",
       "  875  0.101397  0.099999  0.101111  0.141612  0.120932         0.495131,\n",
       "  0.10019737004672542),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  872  0.099140  0.097027  0.097364  0.185647  0.118617         0.477999\n",
       "  873  0.095703  0.097743  0.096596  0.192761  0.117426         0.492927\n",
       "  874  0.098724  0.098581  0.099972  0.166006  0.118184         0.508160\n",
       "  875  0.101397  0.099999  0.101111  0.141612  0.120932         0.495131\n",
       "  876  0.102088  0.101631  0.101021  0.161953  0.121978         0.481619,\n",
       "  0.10415896396563877),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  873  0.095703  0.097743  0.096596  0.192761  0.117426         0.492927\n",
       "  874  0.098724  0.098581  0.099972  0.166006  0.118184         0.508160\n",
       "  875  0.101397  0.099999  0.101111  0.141612  0.120932         0.495131\n",
       "  876  0.102088  0.101631  0.101021  0.161953  0.121978         0.481619\n",
       "  877  0.101874  0.102672  0.102392  0.161039  0.121259         0.516744,\n",
       "  0.10359096554093496),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  874  0.098724  0.098581  0.099972  0.166006  0.118184         0.508160\n",
       "  875  0.101397  0.099999  0.101111  0.141612  0.120932         0.495131\n",
       "  876  0.102088  0.101631  0.101021  0.161953  0.121978         0.481619\n",
       "  877  0.101874  0.102672  0.102392  0.161039  0.121259         0.516744\n",
       "  878  0.105688  0.104106  0.105374  0.157439  0.125128         0.482873,\n",
       "  0.10296545425671368),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  875  0.101397  0.099999  0.101111  0.141612  0.120932         0.495131\n",
       "  876  0.102088  0.101631  0.101021  0.161953  0.121978         0.481619\n",
       "  877  0.101874  0.102672  0.102392  0.161039  0.121259         0.516744\n",
       "  878  0.105688  0.104106  0.105374  0.157439  0.125128         0.482873\n",
       "  879  0.103272  0.102120  0.103758  0.127361  0.124574         0.482443,\n",
       "  0.09800686813430015),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  876  0.102088  0.101631  0.101021  0.161953  0.121978         0.481619\n",
       "  877  0.101874  0.102672  0.102392  0.161039  0.121259         0.516744\n",
       "  878  0.105688  0.104106  0.105374  0.157439  0.125128         0.482873\n",
       "  879  0.103272  0.102120  0.103758  0.127361  0.124574         0.482443\n",
       "  880  0.104868  0.102688  0.099440  0.154367  0.123963         0.450042,\n",
       "  0.09801885759773644),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  877  0.101874  0.102672  0.102392  0.161039  0.121259         0.516744\n",
       "  878  0.105688  0.104106  0.105374  0.157439  0.125128         0.482873\n",
       "  879  0.103272  0.102120  0.103758  0.127361  0.124574         0.482443\n",
       "  880  0.104868  0.102688  0.099440  0.154367  0.123963         0.450042\n",
       "  881  0.097304  0.096989  0.097579  0.161951  0.119120         0.487210,\n",
       "  0.09654733859699645),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  878  0.105688  0.104106  0.105374  0.157439  0.125128         0.482873\n",
       "  879  0.103272  0.102120  0.103758  0.127361  0.124574         0.482443\n",
       "  880  0.104868  0.102688  0.099440  0.154367  0.123963         0.450042\n",
       "  881  0.097304  0.096989  0.097579  0.161951  0.119120         0.487210\n",
       "  882  0.099551  0.097510  0.097744  0.138007  0.119132         0.476117,\n",
       "  0.09773126443306235),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  879  0.103272  0.102120  0.103758  0.127361  0.124574         0.482443\n",
       "  880  0.104868  0.102688  0.099440  0.154367  0.123963         0.450042\n",
       "  881  0.097304  0.096989  0.097579  0.161951  0.119120         0.487210\n",
       "  882  0.099551  0.097510  0.097744  0.138007  0.119132         0.476117\n",
       "  883  0.096955  0.096513  0.097272  0.134069  0.117695         0.495974,\n",
       "  0.09261210995080818),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  880  0.104868  0.102688  0.099440  0.154367  0.123963         0.450042\n",
       "  881  0.097304  0.096989  0.097579  0.161951  0.119120         0.487210\n",
       "  882  0.099551  0.097510  0.097744  0.138007  0.119132         0.476117\n",
       "  883  0.096955  0.096513  0.097272  0.134069  0.117695         0.495974\n",
       "  884  0.097891  0.095920  0.094411  0.203559  0.118851         0.448841,\n",
       "  0.08727965656326322),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  881  0.097304  0.096989  0.097579  0.161951  0.119120         0.487210\n",
       "  882  0.099551  0.097510  0.097744  0.138007  0.119132         0.476117\n",
       "  883  0.096955  0.096513  0.097272  0.134069  0.117695         0.495974\n",
       "  884  0.097891  0.095920  0.094411  0.203559  0.118851         0.448841\n",
       "  885  0.087121  0.086434  0.084991  0.566329  0.113852         0.447246,\n",
       "  0.0893575191025061),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  882  0.099551  0.097510  0.097744  0.138007  0.119132         0.476117\n",
       "  883  0.096955  0.096513  0.097272  0.134069  0.117695         0.495974\n",
       "  884  0.097891  0.095920  0.094411  0.203559  0.118851         0.448841\n",
       "  885  0.087121  0.086434  0.084991  0.566329  0.113852         0.447246\n",
       "  886  0.086864  0.088571  0.088089  0.246103  0.108644         0.502658,\n",
       "  0.08996385301864668),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  883  0.096955  0.096513  0.097272  0.134069  0.117695         0.495974\n",
       "  884  0.097891  0.095920  0.094411  0.203559  0.118851         0.448841\n",
       "  885  0.087121  0.086434  0.084991  0.566329  0.113852         0.447246\n",
       "  886  0.086864  0.088571  0.088089  0.246103  0.108644         0.502658\n",
       "  887  0.089360  0.089425  0.089959  0.160120  0.110673         0.491655,\n",
       "  0.0889548809740725),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  884  0.097891  0.095920  0.094411  0.203559  0.118851         0.448841\n",
       "  885  0.087121  0.086434  0.084991  0.566329  0.113852         0.447246\n",
       "  886  0.086864  0.088571  0.088089  0.246103  0.108644         0.502658\n",
       "  887  0.089360  0.089425  0.089959  0.160120  0.110673         0.491655\n",
       "  888  0.089689  0.089579  0.089303  0.229161  0.111265         0.479576,\n",
       "  0.08556128547986298),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  885  0.087121  0.086434  0.084991  0.566329  0.113852         0.447246\n",
       "  886  0.086864  0.088571  0.088089  0.246103  0.108644         0.502658\n",
       "  887  0.089360  0.089425  0.089959  0.160120  0.110673         0.491655\n",
       "  888  0.089689  0.089579  0.089303  0.229161  0.111265         0.479576\n",
       "  889  0.089603  0.087915  0.085781  0.183655  0.110280         0.461744,\n",
       "  0.08258470392337582),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  886  0.086864  0.088571  0.088089  0.246103  0.108644         0.502658\n",
       "  887  0.089360  0.089425  0.089959  0.160120  0.110673         0.491655\n",
       "  888  0.089689  0.089579  0.089303  0.229161  0.111265         0.479576\n",
       "  889  0.089603  0.087915  0.085781  0.183655  0.110280         0.461744\n",
       "  890  0.084655  0.083327  0.082310  0.331229  0.106966         0.464863,\n",
       "  0.08332764651720904),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  887  0.089360  0.089425  0.089959  0.160120  0.110673         0.491655\n",
       "  888  0.089689  0.089579  0.089303  0.229161  0.111265         0.479576\n",
       "  889  0.089603  0.087915  0.085781  0.183655  0.110280         0.461744\n",
       "  890  0.084655  0.083327  0.082310  0.331229  0.106966         0.464863\n",
       "  891  0.083283  0.083519  0.084301  0.180787  0.104059         0.492676,\n",
       "  0.07973274158114504),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  888  0.089689  0.089579  0.089303  0.229161  0.111265         0.479576\n",
       "  889  0.089603  0.087915  0.085781  0.183655  0.110280         0.461744\n",
       "  890  0.084655  0.083327  0.082310  0.331229  0.106966         0.464863\n",
       "  891  0.083283  0.083519  0.084301  0.180787  0.104059         0.492676\n",
       "  892  0.083870  0.083313  0.081196  0.181602  0.104785         0.460239,\n",
       "  0.07988851800455607),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  889  0.089603  0.087915  0.085781  0.183655  0.110280         0.461744\n",
       "  890  0.084655  0.083327  0.082310  0.331229  0.106966         0.464863\n",
       "  891  0.083283  0.083519  0.084301  0.180787  0.104059         0.492676\n",
       "  892  0.083870  0.083313  0.081196  0.181602  0.104785         0.460239\n",
       "  893  0.079343  0.080161  0.079999  0.196237  0.101274         0.488285,\n",
       "  0.0751600121872992),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  890  0.084655  0.083327  0.082310  0.331229  0.106966         0.464863\n",
       "  891  0.083283  0.083519  0.084301  0.180787  0.104059         0.492676\n",
       "  892  0.083870  0.083313  0.081196  0.181602  0.104785         0.460239\n",
       "  893  0.079343  0.080161  0.079999  0.196237  0.101274         0.488285\n",
       "  894  0.079331  0.078028  0.076802  0.194189  0.101426         0.451762,\n",
       "  0.08144391513753858),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  891  0.083283  0.083519  0.084301  0.180787  0.104059         0.492676\n",
       "  892  0.083870  0.083313  0.081196  0.181602  0.104785         0.460239\n",
       "  893  0.079343  0.080161  0.079999  0.196237  0.101274         0.488285\n",
       "  894  0.079331  0.078028  0.076802  0.194189  0.101426         0.451762\n",
       "  895  0.078472  0.080197  0.079614  0.214604  0.096808         0.534109,\n",
       "  0.0815253973015663),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  892  0.083870  0.083313  0.081196  0.181602  0.104785         0.460239\n",
       "  893  0.079343  0.080161  0.079999  0.196237  0.101274         0.488285\n",
       "  894  0.079331  0.078028  0.076802  0.194189  0.101426         0.451762\n",
       "  895  0.078472  0.080197  0.079614  0.214604  0.096808         0.534109\n",
       "  896  0.082022  0.082028  0.082865  0.161701  0.102945         0.487730,\n",
       "  0.08220363913123771),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  893  0.079343  0.080161  0.079999  0.196237  0.101274         0.488285\n",
       "  894  0.079331  0.078028  0.076802  0.194189  0.101426         0.451762\n",
       "  895  0.078472  0.080197  0.079614  0.214604  0.096808         0.534109\n",
       "  896  0.082022  0.082028  0.082865  0.161701  0.102945         0.487730\n",
       "  897  0.080255  0.081815  0.080615  0.187841  0.103025         0.492192,\n",
       "  0.08098376414958755),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  894  0.079331  0.078028  0.076802  0.194189  0.101426         0.451762\n",
       "  895  0.078472  0.080197  0.079614  0.214604  0.096808         0.534109\n",
       "  896  0.082022  0.082028  0.082865  0.161701  0.102945         0.487730\n",
       "  897  0.080255  0.081815  0.080615  0.187841  0.103025         0.492192\n",
       "  898  0.082737  0.082374  0.081060  0.159815  0.103687         0.477999,\n",
       "  0.07665789646076429),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  895  0.078472  0.080197  0.079614  0.214604  0.096808         0.534109\n",
       "  896  0.082022  0.082028  0.082865  0.161701  0.102945         0.487730\n",
       "  897  0.080255  0.081815  0.080615  0.187841  0.103025         0.492192\n",
       "  898  0.082737  0.082374  0.081060  0.159815  0.103687         0.477999\n",
       "  899  0.079129  0.078670  0.076577  0.191771  0.102496         0.454773,\n",
       "  0.08128095080948314),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  896  0.082022  0.082028  0.082865  0.161701  0.102945         0.487730\n",
       "  897  0.080255  0.081815  0.080615  0.187841  0.103025         0.492192\n",
       "  898  0.082737  0.082374  0.081060  0.159815  0.103687         0.477999\n",
       "  899  0.079129  0.078670  0.076577  0.191771  0.102496         0.454773\n",
       "  900  0.079367  0.080360  0.079827  0.175937  0.098271         0.521690,\n",
       "  0.0795649756942977),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  897  0.080255  0.081815  0.080615  0.187841  0.103025         0.492192\n",
       "  898  0.082737  0.082374  0.081060  0.159815  0.103687         0.477999\n",
       "  899  0.079129  0.078670  0.076577  0.191771  0.102496         0.454773\n",
       "  900  0.079367  0.080360  0.079827  0.175937  0.098271         0.521690\n",
       "  901  0.082287  0.081938  0.081315  0.150198  0.102786         0.474289,\n",
       "  0.07993644699121433),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  898  0.082737  0.082374  0.081060  0.159815  0.103687         0.477999\n",
       "  899  0.079129  0.078670  0.076577  0.191771  0.102496         0.454773\n",
       "  900  0.079367  0.080360  0.079827  0.175937  0.098271         0.521690\n",
       "  901  0.082287  0.081938  0.081315  0.150198  0.102786         0.474289\n",
       "  902  0.078968  0.079142  0.077882  0.241031  0.101110         0.489898,\n",
       "  0.08640968916459651),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  899  0.079129  0.078670  0.076577  0.191771  0.102496         0.454773\n",
       "  900  0.079367  0.080360  0.079827  0.175937  0.098271         0.521690\n",
       "  901  0.082287  0.081938  0.081315  0.150198  0.102786         0.474289\n",
       "  902  0.078968  0.079142  0.077882  0.241031  0.101110         0.489898\n",
       "  903  0.081642  0.086270  0.082780  0.164415  0.101473         0.535525,\n",
       "  0.08406580679697502),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  900  0.079367  0.080360  0.079827  0.175937  0.098271         0.521690\n",
       "  901  0.082287  0.081938  0.081315  0.150198  0.102786         0.474289\n",
       "  902  0.078968  0.079142  0.077882  0.241031  0.101110         0.489898\n",
       "  903  0.081642  0.086270  0.082780  0.164415  0.101473         0.535525\n",
       "  904  0.086221  0.084301  0.084786  0.141748  0.107794         0.469594,\n",
       "  0.08584169073989269),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  901  0.082287  0.081938  0.081315  0.150198  0.102786         0.474289\n",
       "  902  0.078968  0.079142  0.077882  0.241031  0.101110         0.489898\n",
       "  903  0.081642  0.086270  0.082780  0.164415  0.101473         0.535525\n",
       "  904  0.086221  0.084301  0.084786  0.141748  0.107794         0.469594\n",
       "  905  0.085049  0.085581  0.085582  0.127011  0.105505         0.500400,\n",
       "  0.08246007508675714),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  902  0.078968  0.079142  0.077882  0.241031  0.101110         0.489898\n",
       "  903  0.081642  0.086270  0.082780  0.164415  0.101473         0.535525\n",
       "  904  0.086221  0.084301  0.084786  0.141748  0.107794         0.469594\n",
       "  905  0.085049  0.085581  0.085582  0.127011  0.105505         0.500400\n",
       "  906  0.083463  0.082201  0.083451  0.141763  0.107240         0.461834,\n",
       "  0.08102211888574892),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  903  0.081642  0.086270  0.082780  0.164415  0.101473         0.535525\n",
       "  904  0.086221  0.084301  0.084786  0.141748  0.107794         0.469594\n",
       "  905  0.085049  0.085581  0.085582  0.127011  0.105505         0.500400\n",
       "  906  0.083463  0.082201  0.083451  0.141763  0.107240         0.461834\n",
       "  907  0.083815  0.083377  0.081954  0.106348  0.103937         0.476368,\n",
       "  0.08159489962452003),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  904  0.086221  0.084301  0.084786  0.141748  0.107794         0.469594\n",
       "  905  0.085049  0.085581  0.085582  0.127011  0.105505         0.500400\n",
       "  906  0.083463  0.082201  0.083451  0.141763  0.107240         0.461834\n",
       "  907  0.083815  0.083377  0.081954  0.106348  0.103937         0.476368\n",
       "  908  0.080970  0.080798  0.081397  0.108055  0.102533         0.491404,\n",
       "  0.08764393995553545),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  905  0.085049  0.085581  0.085582  0.127011  0.105505         0.500400\n",
       "  906  0.083463  0.082201  0.083451  0.141763  0.107240         0.461834\n",
       "  907  0.083815  0.083377  0.081954  0.106348  0.103937         0.476368\n",
       "  908  0.080970  0.080798  0.081397  0.108055  0.102533         0.491404\n",
       "  909  0.083882  0.086441  0.084996  0.148109  0.103092         0.532353,\n",
       "  0.08764154398732064),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  906  0.083463  0.082201  0.083451  0.141763  0.107240         0.461834\n",
       "  907  0.083815  0.083377  0.081954  0.106348  0.103937         0.476368\n",
       "  908  0.080970  0.080798  0.081397  0.108055  0.102533         0.491404\n",
       "  909  0.083882  0.086441  0.084996  0.148109  0.103092         0.532353\n",
       "  910  0.089215  0.088631  0.088271  0.145820  0.109000         0.487103,\n",
       "  0.08510593605070384),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  907  0.083815  0.083377  0.081954  0.106348  0.103937         0.476368\n",
       "  908  0.080970  0.080798  0.081397  0.108055  0.102533         0.491404\n",
       "  909  0.083882  0.086441  0.084996  0.148109  0.103092         0.532353\n",
       "  910  0.089215  0.088631  0.088271  0.145820  0.109000         0.487103\n",
       "  911  0.087249  0.085680  0.085301  0.164777  0.108998         0.468160,\n",
       "  0.08893331726013934),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  908  0.080970  0.080798  0.081397  0.108055  0.102533         0.491404\n",
       "  909  0.083882  0.086441  0.084996  0.148109  0.103092         0.532353\n",
       "  910  0.089215  0.088631  0.088271  0.145820  0.109000         0.487103\n",
       "  911  0.087249  0.085680  0.085301  0.164777  0.108998         0.468160\n",
       "  912  0.086113  0.089126  0.087256  0.143344  0.106521         0.515740,\n",
       "  0.09030657269698572),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  909  0.083882  0.086441  0.084996  0.148109  0.103092         0.532353\n",
       "  910  0.089215  0.088631  0.088271  0.145820  0.109000         0.487103\n",
       "  911  0.087249  0.085680  0.085301  0.164777  0.108998         0.468160\n",
       "  912  0.086113  0.089126  0.087256  0.143344  0.106521         0.515740\n",
       "  913  0.090931  0.090929  0.091454  0.144470  0.110259         0.497389,\n",
       "  0.08814722799371513),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  910  0.089215  0.088631  0.088271  0.145820  0.109000         0.487103\n",
       "  911  0.087249  0.085680  0.085301  0.164777  0.108998         0.468160\n",
       "  912  0.086113  0.089126  0.087256  0.143344  0.106521         0.515740\n",
       "  913  0.090931  0.090929  0.091454  0.144470  0.110259         0.497389\n",
       "  914  0.089971  0.088252  0.088663  0.110502  0.111600         0.470974,\n",
       "  0.08727965656326322),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  911  0.087249  0.085680  0.085301  0.164777  0.108998         0.468160\n",
       "  912  0.086113  0.089126  0.087256  0.143344  0.106521         0.515740\n",
       "  913  0.090931  0.090929  0.091454  0.144470  0.110259         0.497389\n",
       "  914  0.089971  0.088252  0.088663  0.110502  0.111600         0.470974\n",
       "  915  0.089030  0.087744  0.087581  0.147014  0.109491         0.480633,\n",
       "  0.08845159293589283),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  912  0.086113  0.089126  0.087256  0.143344  0.106521         0.515740\n",
       "  913  0.090931  0.090929  0.091454  0.144470  0.110259         0.497389\n",
       "  914  0.089971  0.088252  0.088663  0.110502  0.111600         0.470974\n",
       "  915  0.089030  0.087744  0.087581  0.147014  0.109491         0.480633\n",
       "  916  0.088026  0.088185  0.088625  0.123282  0.108644         0.495884,\n",
       "  0.08566434098018617),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  913  0.090931  0.090929  0.091454  0.144470  0.110259         0.497389\n",
       "  914  0.089971  0.088252  0.088663  0.110502  0.111600         0.470974\n",
       "  915  0.089030  0.087744  0.087581  0.147014  0.109491         0.480633\n",
       "  916  0.088026  0.088185  0.088625  0.123282  0.108644         0.495884\n",
       "  917  0.087812  0.086621  0.086968  0.101325  0.109789         0.466278,\n",
       "  0.08427670973405099),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  914  0.089971  0.088252  0.088663  0.110502  0.111600         0.470974\n",
       "  915  0.089030  0.087744  0.087581  0.147014  0.109491         0.480633\n",
       "  916  0.088026  0.088185  0.088625  0.123282  0.108644         0.495884\n",
       "  917  0.087812  0.086621  0.086968  0.101325  0.109789         0.466278\n",
       "  918  0.086149  0.084296  0.085791  0.093369  0.107067         0.476744,\n",
       "  0.08312154513892496),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  915  0.089030  0.087744  0.087581  0.147014  0.109491         0.480633\n",
       "  916  0.088026  0.088185  0.088625  0.123282  0.108644         0.495884\n",
       "  917  0.087812  0.086621  0.086968  0.101325  0.109789         0.466278\n",
       "  918  0.086149  0.084296  0.085791  0.093369  0.107067         0.476744\n",
       "  919  0.086383  0.084834  0.084233  0.111247  0.105711         0.478483,\n",
       "  0.08508436271440836),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  916  0.088026  0.088185  0.088625  0.123282  0.108644         0.495884\n",
       "  917  0.087812  0.086621  0.086968  0.101325  0.109789         0.466278\n",
       "  918  0.086149  0.084296  0.085791  0.093369  0.107067         0.476744\n",
       "  919  0.086383  0.084834  0.084233  0.111247  0.105711         0.478483\n",
       "  920  0.083078  0.084493  0.083574  0.185508  0.104583         0.501798,\n",
       "  0.08376862975944172),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  917  0.087812  0.086621  0.086968  0.101325  0.109789         0.466278\n",
       "  918  0.086149  0.084296  0.085791  0.093369  0.107067         0.476744\n",
       "  919  0.086383  0.084834  0.084233  0.111247  0.105711         0.478483\n",
       "  920  0.083078  0.084493  0.083574  0.185508  0.104583         0.501798\n",
       "  921  0.085781  0.084751  0.084911  0.324694  0.106500         0.477282,\n",
       "  0.10530694065612041),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  918  0.086149  0.084296  0.085791  0.093369  0.107067         0.476744\n",
       "  919  0.086383  0.084834  0.084233  0.111247  0.105711         0.478483\n",
       "  920  0.083078  0.084493  0.083574  0.185508  0.104583         0.501798\n",
       "  921  0.085781  0.084751  0.084911  0.324694  0.106500         0.477282\n",
       "  922  0.104933  0.105684  0.103865  0.443455  0.105215         0.648177,\n",
       "  0.10491868795933788),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  919  0.086383  0.084834  0.084233  0.111247  0.105711         0.478483\n",
       "  920  0.083078  0.084493  0.083574  0.185508  0.104583         0.501798\n",
       "  921  0.085781  0.084751  0.084911  0.324694  0.106500         0.477282\n",
       "  922  0.104933  0.105684  0.103865  0.443455  0.105215         0.648177\n",
       "  923  0.105939  0.104473  0.105202  0.180862  0.126249         0.484217,\n",
       "  0.10904324620630669),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  920  0.083078  0.084493  0.083574  0.185508  0.104583         0.501798\n",
       "  921  0.085781  0.084751  0.084911  0.324694  0.106500         0.477282\n",
       "  922  0.104933  0.105684  0.103865  0.443455  0.105215         0.648177\n",
       "  923  0.105939  0.104473  0.105202  0.180862  0.126249         0.484217\n",
       "  924  0.105501  0.108795  0.106963  0.192995  0.125870         0.517963,\n",
       "  0.10700373840322518),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  921  0.085781  0.084751  0.084911  0.324694  0.106500         0.477282\n",
       "  922  0.104933  0.105684  0.103865  0.443455  0.105215         0.648177\n",
       "  923  0.105939  0.104473  0.105202  0.180862  0.126249         0.484217\n",
       "  924  0.105501  0.108795  0.106963  0.192995  0.125870         0.517963\n",
       "  925  0.110403  0.108435  0.107784  0.119611  0.129898         0.471870,\n",
       "  0.10617690805478712),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  922  0.104933  0.105684  0.103865  0.443455  0.105215         0.648177\n",
       "  923  0.105939  0.104473  0.105202  0.180862  0.126249         0.484217\n",
       "  924  0.105501  0.108795  0.106963  0.192995  0.125870         0.517963\n",
       "  925  0.110403  0.108435  0.107784  0.119611  0.129898         0.471870\n",
       "  926  0.108802  0.107293  0.107559  0.100484  0.127906         0.480938,\n",
       "  0.10642136416923255),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  923  0.105939  0.104473  0.105202  0.180862  0.126249         0.484217\n",
       "  924  0.105501  0.108795  0.106963  0.192995  0.125870         0.517963\n",
       "  925  0.110403  0.108435  0.107784  0.119611  0.129898         0.471870\n",
       "  926  0.108802  0.107293  0.107559  0.100484  0.127906         0.480938\n",
       "  927  0.108256  0.106940  0.108215  0.101196  0.127099         0.488949,\n",
       "  0.10835302453556127),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  924  0.105501  0.108795  0.106963  0.192995  0.125870         0.517963\n",
       "  925  0.110403  0.108435  0.107784  0.119611  0.129898         0.471870\n",
       "  926  0.108802  0.107293  0.107559  0.100484  0.127906         0.480938\n",
       "  927  0.108256  0.106940  0.108215  0.101196  0.127099         0.488949\n",
       "  928  0.107500  0.107843  0.107312  0.097407  0.127338         0.501565,\n",
       "  0.10987486849117432),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  925  0.110403  0.108435  0.107784  0.119611  0.129898         0.471870\n",
       "  926  0.108802  0.107293  0.107559  0.100484  0.127906         0.480938\n",
       "  927  0.108256  0.106940  0.108215  0.101196  0.127099         0.488949\n",
       "  928  0.107500  0.107843  0.107312  0.097407  0.127338         0.501565\n",
       "  929  0.108692  0.109404  0.109978  0.105613  0.129224         0.498500,\n",
       "  0.11366630055502554),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  926  0.108802  0.107293  0.107559  0.100484  0.127906         0.480938\n",
       "  927  0.108256  0.106940  0.108215  0.101196  0.127099         0.488949\n",
       "  928  0.107500  0.107843  0.107312  0.097407  0.127338         0.501565\n",
       "  929  0.108692  0.109404  0.109978  0.105613  0.129224         0.498500\n",
       "  930  0.111430  0.114490  0.112028  0.192131  0.130710         0.515472,\n",
       "  0.11816952762591766),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  927  0.108256  0.106940  0.108215  0.101196  0.127099         0.488949\n",
       "  928  0.107500  0.107843  0.107312  0.097407  0.127338         0.501565\n",
       "  929  0.108692  0.109404  0.109978  0.105613  0.129224         0.498500\n",
       "  930  0.111430  0.114490  0.112028  0.192131  0.130710         0.515472\n",
       "  931  0.115305  0.116876  0.116509  0.160506  0.134413         0.520794,\n",
       "  0.11534393055641194),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  928  0.107500  0.107843  0.107312  0.097407  0.127338         0.501565\n",
       "  929  0.108692  0.109404  0.109978  0.105613  0.129224         0.498500\n",
       "  930  0.111430  0.114490  0.112028  0.192131  0.130710         0.515472\n",
       "  931  0.115305  0.116876  0.116509  0.160506  0.134413         0.520794\n",
       "  932  0.120162  0.118658  0.117073  0.170138  0.138811         0.465992,\n",
       "  0.11860570930935842),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  929  0.108692  0.109404  0.109978  0.105613  0.129224         0.498500\n",
       "  930  0.111430  0.114490  0.112028  0.192131  0.130710         0.515472\n",
       "  931  0.115305  0.116876  0.116509  0.160506  0.134413         0.520794\n",
       "  932  0.120162  0.118658  0.117073  0.170138  0.138811         0.465992\n",
       "  933  0.120251  0.117841  0.118970  0.152679  0.136051         0.511511,\n",
       "  0.1165590136016325),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  930  0.111430  0.114490  0.112028  0.192131  0.130710         0.515472\n",
       "  931  0.115305  0.116876  0.116509  0.160506  0.134413         0.520794\n",
       "  932  0.120162  0.118658  0.117073  0.170138  0.138811         0.465992\n",
       "  933  0.120251  0.117841  0.118970  0.152679  0.136051         0.511511\n",
       "  934  0.118359  0.116182  0.117969  0.112504  0.139237         0.471816,\n",
       "  0.11479510949978881),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  931  0.115305  0.116876  0.116509  0.160506  0.134413         0.520794\n",
       "  932  0.120162  0.118658  0.117073  0.170138  0.138811         0.465992\n",
       "  933  0.120251  0.117841  0.118970  0.152679  0.136051         0.511511\n",
       "  934  0.118359  0.116182  0.117969  0.112504  0.139237         0.471816\n",
       "  935  0.118183  0.117957  0.116378  0.160754  0.137238         0.473931,\n",
       "  0.11575614293534243),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  932  0.120162  0.118658  0.117073  0.170138  0.138811         0.465992\n",
       "  933  0.120251  0.117841  0.118970  0.152679  0.136051         0.511511\n",
       "  934  0.118359  0.116182  0.117969  0.112504  0.139237         0.471816\n",
       "  935  0.118183  0.117957  0.116378  0.160754  0.137238         0.473931\n",
       "  936  0.117204  0.115542  0.116979  0.124510  0.135515         0.494307,\n",
       "  0.11340507266307652),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  933  0.120251  0.117841  0.118970  0.152679  0.136051         0.511511\n",
       "  934  0.118359  0.116182  0.117969  0.112504  0.139237         0.471816\n",
       "  935  0.118183  0.117957  0.116378  0.160754  0.137238         0.473931\n",
       "  936  0.117204  0.115542  0.116979  0.124510  0.135515         0.494307\n",
       "  937  0.115471  0.114523  0.115135  0.108992  0.136454         0.469540,\n",
       "  0.11621868989150826),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  934  0.118359  0.116182  0.117969  0.112504  0.139237         0.471816\n",
       "  935  0.118183  0.117957  0.116378  0.160754  0.137238         0.473931\n",
       "  936  0.117204  0.115542  0.116979  0.124510  0.135515         0.494307\n",
       "  937  0.115471  0.114523  0.115135  0.108992  0.136454         0.469540\n",
       "  938  0.114448  0.114938  0.114556  0.118210  0.134158         0.508160,\n",
       "  0.11575374696712763),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  935  0.118183  0.117957  0.116378  0.160754  0.137238         0.473931\n",
       "  936  0.117204  0.115542  0.116979  0.124510  0.135515         0.494307\n",
       "  937  0.115471  0.114523  0.115135  0.108992  0.136454         0.469540\n",
       "  938  0.114448  0.114938  0.114556  0.118210  0.134158         0.508160\n",
       "  939  0.117211  0.115222  0.116891  0.125498  0.136906         0.483644,\n",
       "  0.11414802487927207),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  936  0.117204  0.115542  0.116979  0.124510  0.135515         0.494307\n",
       "  937  0.115471  0.114523  0.115135  0.108992  0.136454         0.469540\n",
       "  938  0.114448  0.114938  0.114556  0.118210  0.134158         0.508160\n",
       "  939  0.117211  0.115222  0.116891  0.125498  0.136906         0.483644\n",
       "  940  0.117274  0.115990  0.116092  0.106338  0.136452         0.475114,\n",
       "  0.11527442823345822),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  937  0.115471  0.114523  0.115135  0.108992  0.136454         0.469540\n",
       "  938  0.114448  0.114938  0.114556  0.118210  0.134158         0.508160\n",
       "  939  0.117211  0.115222  0.116891  0.125498  0.136906         0.483644\n",
       "  940  0.117274  0.115990  0.116092  0.106338  0.136452         0.475114\n",
       "  941  0.115471  0.114037  0.116179  0.105533  0.134884         0.495543,\n",
       "  0.11505154545530825),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  938  0.114448  0.114938  0.114556  0.118210  0.134158         0.508160\n",
       "  939  0.117211  0.115222  0.116891  0.125498  0.136906         0.483644\n",
       "  940  0.117274  0.115990  0.116092  0.106338  0.136452         0.475114\n",
       "  941  0.115471  0.114037  0.116179  0.105533  0.134884         0.495543\n",
       "  942  0.116056  0.114447  0.116836  0.103864  0.135984         0.485454,\n",
       "  0.11444040998037579),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  939  0.117211  0.115222  0.116891  0.125498  0.136906         0.483644\n",
       "  940  0.117274  0.115990  0.116092  0.106338  0.136452         0.475114\n",
       "  941  0.115471  0.114037  0.116179  0.105533  0.134884         0.495543\n",
       "  942  0.116056  0.114447  0.116836  0.103864  0.135984         0.485454\n",
       "  943  0.115363  0.115443  0.115673  0.136224  0.135766         0.482551,\n",
       "  0.11469924190411),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  940  0.117274  0.115990  0.116092  0.106338  0.136452         0.475114\n",
       "  941  0.115471  0.114037  0.116179  0.105533  0.134884         0.495543\n",
       "  942  0.116056  0.114447  0.116836  0.103864  0.135984         0.485454\n",
       "  943  0.115363  0.115443  0.115673  0.136224  0.135766         0.482551\n",
       "  944  0.114419  0.113926  0.115525  0.080502  0.135169         0.489056,\n",
       "  0.10784254859273722),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  941  0.115471  0.114037  0.116179  0.105533  0.134884         0.495543\n",
       "  942  0.116056  0.114447  0.116836  0.103864  0.135984         0.485454\n",
       "  943  0.115363  0.115443  0.115673  0.136224  0.135766         0.482551\n",
       "  944  0.114419  0.113926  0.115525  0.080502  0.135169         0.489056\n",
       "  945  0.115356  0.113672  0.109465  0.157230  0.135422         0.435848,\n",
       "  0.10771552378790374),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  942  0.116056  0.114447  0.116836  0.103864  0.135984         0.485454\n",
       "  943  0.115363  0.115443  0.115673  0.136224  0.135766         0.482551\n",
       "  944  0.114419  0.113926  0.115525  0.080502  0.135169         0.489056\n",
       "  945  0.115356  0.113672  0.109465  0.157230  0.135422         0.435848\n",
       "  946  0.108961  0.107227  0.107614  0.123377  0.128726         0.486171,\n",
       "  0.10655556725634811),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  943  0.115363  0.115443  0.115673  0.136224  0.135766         0.482551\n",
       "  944  0.114419  0.113926  0.115525  0.080502  0.135169         0.489056\n",
       "  945  0.115356  0.113672  0.109465  0.157230  0.135422         0.435848\n",
       "  946  0.108961  0.107227  0.107614  0.123377  0.128726         0.486171\n",
       "  947  0.107760  0.106940  0.107023  0.107693  0.128602         0.478447,\n",
       "  0.11045724272516692),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  944  0.114419  0.113926  0.115525  0.080502  0.135169         0.489056\n",
       "  945  0.115356  0.113672  0.109465  0.157230  0.135422         0.435848\n",
       "  946  0.108961  0.107227  0.107614  0.123377  0.128726         0.486171\n",
       "  947  0.107760  0.106940  0.107023  0.107693  0.128602         0.478447\n",
       "  948  0.107279  0.109215  0.108486  0.093926  0.127469         0.516296,\n",
       "  0.11221635489058103),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  945  0.115356  0.113672  0.109465  0.157230  0.135422         0.435848\n",
       "  946  0.108961  0.107227  0.107614  0.123377  0.128726         0.486171\n",
       "  947  0.107760  0.106940  0.107023  0.107693  0.128602         0.478447\n",
       "  948  0.107279  0.109215  0.108486  0.093926  0.127469         0.516296\n",
       "  949  0.112809  0.111322  0.112665  0.098423  0.131279         0.500275,\n",
       "  0.11575374696712763),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  946  0.108961  0.107227  0.107614  0.123377  0.128726         0.486171\n",
       "  947  0.107760  0.106940  0.107023  0.107693  0.128602         0.478447\n",
       "  948  0.107279  0.109215  0.108486  0.093926  0.127469         0.516296\n",
       "  949  0.112809  0.111322  0.112665  0.098423  0.131279         0.500275\n",
       "  950  0.114458  0.114732  0.115595  0.117751  0.132997         0.513572,\n",
       "  0.11642480089215464),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  947  0.107760  0.106940  0.107023  0.107693  0.128602         0.478447\n",
       "  948  0.107279  0.109215  0.108486  0.093926  0.127469         0.516296\n",
       "  949  0.112809  0.111322  0.112665  0.098423  0.131279         0.500275\n",
       "  950  0.114458  0.114732  0.115595  0.117751  0.132997         0.513572\n",
       "  951  0.116547  0.115689  0.117557  0.078352  0.136452         0.492138,\n",
       "  0.11383646241008767),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  948  0.107279  0.109215  0.108486  0.093926  0.127469         0.516296\n",
       "  949  0.112809  0.111322  0.112665  0.098423  0.131279         0.500275\n",
       "  950  0.114458  0.114732  0.115595  0.117751  0.132997         0.513572\n",
       "  951  0.116547  0.115689  0.117557  0.078352  0.136452         0.492138\n",
       "  952  0.116313  0.114260  0.115128  0.089273  0.137107         0.467766,\n",
       "  0.11199107614421626),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  949  0.112809  0.111322  0.112665  0.098423  0.131279         0.500275\n",
       "  950  0.114458  0.114732  0.115595  0.117751  0.132997         0.513572\n",
       "  951  0.116547  0.115689  0.117557  0.078352  0.136452         0.492138\n",
       "  952  0.116313  0.114260  0.115128  0.089273  0.137107         0.467766\n",
       "  953  0.114869  0.113457  0.112546  0.104623  0.134579         0.473321,\n",
       "  0.11317021004385257),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  950  0.114458  0.114732  0.115595  0.117751  0.132997         0.513572\n",
       "  951  0.116547  0.115689  0.117557  0.078352  0.136452         0.492138\n",
       "  952  0.116313  0.114260  0.115128  0.089273  0.137107         0.467766\n",
       "  953  0.114869  0.113457  0.112546  0.104623  0.134579         0.473321\n",
       "  954  0.112181  0.112864  0.113500  0.081708  0.132777         0.495938,\n",
       "  0.11654462816998142),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  951  0.116547  0.115689  0.117557  0.078352  0.136452         0.492138\n",
       "  952  0.116313  0.114260  0.115128  0.089273  0.137107         0.467766\n",
       "  953  0.114869  0.113457  0.112546  0.104623  0.134579         0.473321\n",
       "  954  0.112181  0.112864  0.113500  0.081708  0.132777         0.495938\n",
       "  955  0.115009  0.115244  0.116325  0.095171  0.133929         0.512353,\n",
       "  0.11191917785304774),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  952  0.116313  0.114260  0.115128  0.089273  0.137107         0.467766\n",
       "  953  0.114869  0.113457  0.112546  0.104623  0.134579         0.473321\n",
       "  954  0.112181  0.112864  0.113500  0.081708  0.132777         0.495938\n",
       "  955  0.115009  0.115244  0.116325  0.095171  0.133929         0.512353\n",
       "  956  0.115652  0.113509  0.114094  0.115063  0.137224         0.452533,\n",
       "  0.10973107190883728),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  953  0.114869  0.113457  0.112546  0.104623  0.134579         0.473321\n",
       "  954  0.112181  0.112864  0.113500  0.081708  0.132777         0.495938\n",
       "  955  0.115009  0.115244  0.116325  0.095171  0.133929         0.512353\n",
       "  956  0.115652  0.113509  0.114094  0.115063  0.137224         0.452533\n",
       "  957  0.112838  0.110968  0.108862  0.128533  0.132707         0.470759,\n",
       "  0.10884433273266698),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  954  0.112181  0.112864  0.113500  0.081708  0.132777         0.495938\n",
       "  955  0.115009  0.115244  0.116325  0.095171  0.133929         0.512353\n",
       "  956  0.115652  0.113509  0.114094  0.115063  0.137224         0.452533\n",
       "  957  0.112838  0.110968  0.108862  0.128533  0.132707         0.470759\n",
       "  958  0.108386  0.108319  0.109196  0.088864  0.130570         0.480490,\n",
       "  0.10669457190225558),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  955  0.115009  0.115244  0.116325  0.095171  0.133929         0.512353\n",
       "  956  0.115652  0.113509  0.114094  0.115063  0.137224         0.452533\n",
       "  957  0.112838  0.110968  0.108862  0.128533  0.132707         0.470759\n",
       "  958  0.108386  0.108319  0.109196  0.088864  0.130570         0.480490\n",
       "  959  0.109994  0.107968  0.108634  0.092647  0.129704         0.471045,\n",
       "  0.1085519476315633),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  956  0.115652  0.113509  0.114094  0.115063  0.137224         0.452533\n",
       "  957  0.112838  0.110968  0.108862  0.128533  0.132707         0.470759\n",
       "  958  0.108386  0.108319  0.109196  0.088864  0.130570         0.480490\n",
       "  959  0.109994  0.107968  0.108634  0.092647  0.129704         0.471045\n",
       "  960  0.108194  0.108125  0.109220  0.133478  0.127605         0.501009,\n",
       "  0.11300004818879043),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  957  0.112838  0.110968  0.108862  0.128533  0.132707         0.470759\n",
       "  958  0.108386  0.108319  0.109196  0.088864  0.130570         0.480490\n",
       "  959  0.109994  0.107968  0.108634  0.092647  0.129704         0.471045\n",
       "  960  0.108194  0.108125  0.109220  0.133478  0.127605         0.501009\n",
       "  961  0.111257  0.112319  0.112723  0.150329  0.129418         0.520382,\n",
       "  0.11331161065797483),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  958  0.108386  0.108319  0.109196  0.088864  0.130570         0.480490\n",
       "  959  0.109994  0.107968  0.108634  0.092647  0.129704         0.471045\n",
       "  960  0.108194  0.108125  0.109220  0.133478  0.127605         0.501009\n",
       "  961  0.111257  0.112319  0.112723  0.150329  0.129418         0.520382\n",
       "  962  0.112463  0.113125  0.113973  0.086077  0.133762         0.489450,\n",
       "  0.11250873999168474),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  959  0.109994  0.107968  0.108634  0.092647  0.129704         0.471045\n",
       "  960  0.108194  0.108125  0.109220  0.133478  0.127605         0.501009\n",
       "  961  0.111257  0.112319  0.112723  0.150329  0.129418         0.520382\n",
       "  962  0.112463  0.113125  0.113973  0.086077  0.133762         0.489450\n",
       "  963  0.114833  0.112881  0.114428  0.084133  0.134067         0.481117,\n",
       "  0.11081194224457998),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  960  0.108194  0.108125  0.109220  0.133478  0.127605         0.501009\n",
       "  961  0.111257  0.112319  0.112723  0.150329  0.129418         0.520382\n",
       "  962  0.112463  0.113125  0.113973  0.086077  0.133762         0.489450\n",
       "  963  0.114833  0.112881  0.114428  0.084133  0.134067         0.481117\n",
       "  964  0.111984  0.110729  0.112195  0.082146  0.133283         0.474432,\n",
       "  0.11028948646068192),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  961  0.111257  0.112319  0.112723  0.150329  0.129418         0.520382\n",
       "  962  0.112463  0.113125  0.113973  0.086077  0.133762         0.489450\n",
       "  963  0.114833  0.112881  0.114428  0.084133  0.134067         0.481117\n",
       "  964  0.111984  0.110729  0.112195  0.082146  0.133283         0.474432\n",
       "  965  0.109780  0.109494  0.110172  0.138136  0.131626         0.483214,\n",
       "  0.1069893529715741),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  962  0.112463  0.113125  0.113973  0.086077  0.133762         0.489450\n",
       "  963  0.114833  0.112881  0.114428  0.084133  0.134067         0.481117\n",
       "  964  0.111984  0.110729  0.112195  0.082146  0.133283         0.474432\n",
       "  965  0.109780  0.109494  0.110172  0.138136  0.131626         0.483214\n",
       "  966  0.110172  0.107883  0.108646  0.145433  0.131115         0.462443,\n",
       "  0.10669696787047038),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  963  0.114833  0.112881  0.114428  0.084133  0.134067         0.481117\n",
       "  964  0.111984  0.110729  0.112195  0.082146  0.133283         0.474432\n",
       "  965  0.109780  0.109494  0.110172  0.138136  0.131626         0.483214\n",
       "  966  0.110172  0.107883  0.108646  0.145433  0.131115         0.462443\n",
       "  967  0.107375  0.106975  0.108043  0.088341  0.127892         0.484934,\n",
       "  0.1059995582950806),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  964  0.111984  0.110729  0.112195  0.082146  0.133283         0.474432\n",
       "  965  0.109780  0.109494  0.110172  0.138136  0.131626         0.483214\n",
       "  966  0.110172  0.107883  0.108646  0.145433  0.131115         0.462443\n",
       "  967  0.107375  0.106975  0.108043  0.088341  0.127892         0.484934\n",
       "  968  0.107322  0.106511  0.106367  0.113557  0.127607         0.481906,\n",
       "  0.10808460873896787),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  965  0.109780  0.109494  0.110172  0.138136  0.131626         0.483214\n",
       "  966  0.110172  0.107883  0.108646  0.145433  0.131115         0.462443\n",
       "  967  0.107375  0.106975  0.108043  0.088341  0.127892         0.484934\n",
       "  968  0.107322  0.106511  0.106367  0.113557  0.127607         0.481906\n",
       "  969  0.107281  0.108077  0.108443  0.100170  0.126926         0.502712,\n",
       "  0.1027689271289265),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  966  0.110172  0.107883  0.108646  0.145433  0.131115         0.462443\n",
       "  967  0.107375  0.106975  0.108043  0.088341  0.127892         0.484934\n",
       "  968  0.107322  0.106511  0.106367  0.113557  0.127607         0.481906\n",
       "  969  0.107281  0.108077  0.108443  0.100170  0.126926         0.502712\n",
       "  970  0.107072  0.105027  0.104710  0.137562  0.128962         0.447372,\n",
       "  0.10258918140100516),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  967  0.107375  0.106975  0.108043  0.088341  0.127892         0.484934\n",
       "  968  0.107322  0.106511  0.106367  0.113557  0.127607         0.481906\n",
       "  969  0.107281  0.108077  0.108443  0.100170  0.126926         0.502712\n",
       "  970  0.107072  0.105027  0.104710  0.137562  0.128962         0.447372\n",
       "  971  0.103120  0.101489  0.101520  0.132405  0.123771         0.485776,\n",
       "  0.10219374079957828),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  968  0.107322  0.106511  0.106367  0.113557  0.127607         0.481906\n",
       "  969  0.107281  0.108077  0.108443  0.100170  0.126926         0.502712\n",
       "  970  0.107072  0.105027  0.104710  0.137562  0.128962         0.447372\n",
       "  971  0.103120  0.101489  0.101520  0.132405  0.123771         0.485776\n",
       "  972  0.104273  0.103385  0.103451  0.091489  0.123595         0.484164,\n",
       "  0.10418532923836384),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  969  0.107281  0.108077  0.108443  0.100170  0.126926         0.502712\n",
       "  970  0.107072  0.105027  0.104710  0.137562  0.128962         0.447372\n",
       "  971  0.103120  0.101489  0.101520  0.132405  0.123771         0.485776\n",
       "  972  0.104273  0.103385  0.103451  0.091489  0.123595         0.484164\n",
       "  973  0.101171  0.103082  0.101983  0.120545  0.123209         0.502013,\n",
       "  0.10234233894070725),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  970  0.107072  0.105027  0.104710  0.137562  0.128962         0.447372\n",
       "  971  0.103120  0.101489  0.101520  0.132405  0.123771         0.485776\n",
       "  972  0.104273  0.103385  0.103451  0.091489  0.123595         0.484164\n",
       "  973  0.101171  0.103082  0.101983  0.120545  0.123209         0.502013\n",
       "  974  0.105245  0.103949  0.104177  0.102622  0.125154         0.473339,\n",
       "  0.10472935835855739),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  971  0.103120  0.101489  0.101520  0.132405  0.123771         0.485776\n",
       "  972  0.104273  0.103385  0.103451  0.091489  0.123595         0.484164\n",
       "  973  0.101171  0.103082  0.101983  0.120545  0.123209         0.502013\n",
       "  974  0.105245  0.103949  0.104177  0.102622  0.125154         0.473339\n",
       "  975  0.103891  0.103677  0.104526  0.058292  0.123354         0.504970,\n",
       "  0.1063854150236483),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  972  0.104273  0.103385  0.103451  0.091489  0.123595         0.484164\n",
       "  973  0.101171  0.103082  0.101983  0.120545  0.123209         0.502013\n",
       "  974  0.105245  0.103949  0.104177  0.102622  0.125154         0.473339\n",
       "  975  0.103891  0.103677  0.104526  0.058292  0.123354         0.504970\n",
       "  976  0.106639  0.105992  0.106488  0.103405  0.125685         0.499504,\n",
       "  0.10893060683312429),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  973  0.101171  0.103082  0.101983  0.120545  0.123209         0.502013\n",
       "  974  0.105245  0.103949  0.104177  0.102622  0.125154         0.473339\n",
       "  975  0.103891  0.103677  0.104526  0.058292  0.123354         0.504970\n",
       "  976  0.106639  0.105992  0.106488  0.103405  0.125685         0.499504\n",
       "  977  0.107527  0.107459  0.107554  0.119877  0.127303         0.506153,\n",
       "  0.10581981256715929),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  974  0.105245  0.103949  0.104177  0.102622  0.125154         0.473339\n",
       "  975  0.103891  0.103677  0.104526  0.058292  0.123354         0.504970\n",
       "  976  0.106639  0.105992  0.106488  0.103405  0.125685         0.499504\n",
       "  977  0.107527  0.107459  0.107554  0.119877  0.127303         0.506153\n",
       "  978  0.108675  0.107575  0.107731  0.100943  0.129788         0.463859,\n",
       "  0.10558494032557302),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  975  0.103891  0.103677  0.104526  0.058292  0.123354         0.504970\n",
       "  976  0.106639  0.105992  0.106488  0.103405  0.125685         0.499504\n",
       "  977  0.107527  0.107459  0.107554  0.119877  0.127303         0.506153\n",
       "  978  0.108675  0.107575  0.107731  0.100943  0.129788         0.463859\n",
       "  979  0.107289  0.106110  0.105306  0.105265  0.126750         0.485364,\n",
       "  0.10396243683785157),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  976  0.106639  0.105992  0.106488  0.103405  0.125685         0.499504\n",
       "  977  0.107527  0.107459  0.107554  0.119877  0.127303         0.506153\n",
       "  978  0.108675  0.107575  0.107731  0.100943  0.129788         0.463859\n",
       "  979  0.107289  0.106110  0.105306  0.105265  0.126750         0.485364\n",
       "  980  0.105243  0.103741  0.102571  0.121068  0.126521         0.474988,\n",
       "  0.10104337776324417),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  977  0.107527  0.107459  0.107554  0.119877  0.127303         0.506153\n",
       "  978  0.108675  0.107575  0.107731  0.100943  0.129788         0.463859\n",
       "  979  0.107289  0.106110  0.105306  0.105265  0.126750         0.485364\n",
       "  980  0.105243  0.103741  0.102571  0.121068  0.126521         0.474988\n",
       "  981  0.105845  0.103873  0.101607  0.107530  0.124936         0.465293,\n",
       "  0.09971805131305601),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  978  0.108675  0.107575  0.107731  0.100943  0.129788         0.463859\n",
       "  979  0.107289  0.106110  0.105306  0.105265  0.126750         0.485364\n",
       "  980  0.105243  0.103741  0.102571  0.121068  0.126521         0.474988\n",
       "  981  0.105845  0.103873  0.101607  0.107530  0.124936         0.465293\n",
       "  982  0.100377  0.100897  0.098132  0.147571  0.122086         0.477210,\n",
       "  0.10439382658486272),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  979  0.107289  0.106110  0.105306  0.105265  0.126750         0.485364\n",
       "  980  0.105243  0.103741  0.102571  0.121068  0.126521         0.474988\n",
       "  981  0.105845  0.103873  0.101607  0.107530  0.124936         0.465293\n",
       "  982  0.100377  0.100897  0.098132  0.147571  0.122086         0.477210\n",
       "  983  0.099838  0.103030  0.100433  0.115250  0.120791         0.522084,\n",
       "  0.10387616273739426),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  980  0.105243  0.103741  0.102571  0.121068  0.126521         0.474988\n",
       "  981  0.105845  0.103873  0.101607  0.107530  0.124936         0.465293\n",
       "  982  0.100377  0.100897  0.098132  0.147571  0.122086         0.477210\n",
       "  983  0.099838  0.103030  0.100433  0.115250  0.120791         0.522084\n",
       "  984  0.104680  0.103160  0.103800  0.213643  0.125358         0.483250,\n",
       "  0.09138504706451363),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  981  0.105845  0.103873  0.101607  0.107530  0.124936         0.465293\n",
       "  982  0.100377  0.100897  0.098132  0.147571  0.122086         0.477210\n",
       "  983  0.099838  0.103030  0.100433  0.115250  0.120791         0.522084\n",
       "  984  0.104680  0.103160  0.103800  0.213643  0.125358         0.483250\n",
       "  985  0.096062  0.094138  0.092582  0.274573  0.124852         0.393716,\n",
       "  0.0883844962035162),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  982  0.100377  0.100897  0.098132  0.147571  0.122086         0.477210\n",
       "  983  0.099838  0.103030  0.100433  0.115250  0.120791         0.522084\n",
       "  984  0.104680  0.103160  0.103800  0.213643  0.125358         0.483250\n",
       "  985  0.096062  0.094138  0.092582  0.274573  0.124852         0.393716\n",
       "  986  0.091858  0.090609  0.089557  0.143448  0.112653         0.464683,\n",
       "  0.09037607501993947),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  983  0.099838  0.103030  0.100433  0.115250  0.120791         0.522084\n",
       "  984  0.104680  0.103160  0.103800  0.213643  0.125358         0.483250\n",
       "  985  0.096062  0.094138  0.092582  0.274573  0.124852         0.393716\n",
       "  986  0.091858  0.090609  0.089557  0.143448  0.112653         0.464683\n",
       "  987  0.088450  0.089699  0.089533  0.114017  0.109723         0.502013,\n",
       "  0.09327836231704328),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  984  0.104680  0.103160  0.103800  0.213643  0.125358         0.483250\n",
       "  985  0.096062  0.094138  0.092582  0.274573  0.124852         0.393716\n",
       "  986  0.091858  0.090609  0.089557  0.143448  0.112653         0.464683\n",
       "  987  0.088450  0.089699  0.089533  0.114017  0.109723         0.502013\n",
       "  988  0.092033  0.093723  0.092561  0.118945  0.111668         0.508823,\n",
       "  0.09001897953231162),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  985  0.096062  0.094138  0.092582  0.274573  0.124852         0.393716\n",
       "  986  0.091858  0.090609  0.089557  0.143448  0.112653         0.464683\n",
       "  987  0.088450  0.089699  0.089533  0.114017  0.109723         0.502013\n",
       "  988  0.092033  0.093723  0.092561  0.118945  0.111668         0.508823\n",
       "  989  0.095665  0.093638  0.091960  0.086031  0.114502         0.462748,\n",
       "  0.09393983236921112),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  986  0.091858  0.090609  0.089557  0.143448  0.112653         0.464683\n",
       "  987  0.088450  0.089699  0.089533  0.114017  0.109723         0.502013\n",
       "  988  0.092033  0.093723  0.092561  0.118945  0.111668         0.508823\n",
       "  989  0.095665  0.093638  0.091960  0.086031  0.114502         0.462748\n",
       "  990  0.093208  0.092752  0.093384  0.077337  0.111319         0.516439,\n",
       "  0.09037847098815424),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  987  0.088450  0.089699  0.089533  0.114017  0.109723         0.502013\n",
       "  988  0.092033  0.093723  0.092561  0.118945  0.111668         0.508823\n",
       "  989  0.095665  0.093638  0.091960  0.086031  0.114502         0.462748\n",
       "  990  0.093208  0.092752  0.093384  0.077337  0.111319         0.516439\n",
       "  991  0.094555  0.092505  0.091841  0.076766  0.115148         0.460490,\n",
       "  0.09181404084330998),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  988  0.092033  0.093723  0.092561  0.118945  0.111668         0.508823\n",
       "  989  0.095665  0.093638  0.091960  0.086031  0.114502         0.462748\n",
       "  990  0.093208  0.092752  0.093384  0.077337  0.111319         0.516439\n",
       "  991  0.094555  0.092505  0.091841  0.076766  0.115148         0.460490\n",
       "  992  0.091477  0.091361  0.092536  0.068078  0.111670         0.497855,\n",
       "  0.09171577727941639),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  989  0.095665  0.093638  0.091960  0.086031  0.114502         0.462748\n",
       "  990  0.093208  0.092752  0.093384  0.077337  0.111319         0.516439\n",
       "  991  0.094555  0.092505  0.091841  0.076766  0.115148         0.460490\n",
       "  992  0.091477  0.091361  0.092536  0.068078  0.111670         0.497855\n",
       "  993  0.092969  0.091067  0.091279  0.084810  0.113072         0.486386,\n",
       "  0.08957080838543457),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  990  0.093208  0.092752  0.093384  0.077337  0.111319         0.516439\n",
       "  991  0.094555  0.092505  0.091841  0.076766  0.115148         0.460490\n",
       "  992  0.091477  0.091361  0.092536  0.068078  0.111670         0.497855\n",
       "  993  0.092969  0.091067  0.091279  0.084810  0.113072         0.486386\n",
       "  994  0.090337  0.089851  0.090948  0.069593  0.112976         0.471081,\n",
       "  0.08815920783478913),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  991  0.094555  0.092505  0.091841  0.076766  0.115148         0.460490\n",
       "  992  0.091477  0.091361  0.092536  0.068078  0.111670         0.497855\n",
       "  993  0.092969  0.091067  0.091279  0.084810  0.113072         0.486386\n",
       "  994  0.090337  0.089851  0.090948  0.069593  0.112976         0.471081\n",
       "  995  0.089843  0.088033  0.088782  0.073066  0.110882         0.476565,\n",
       "  0.08699445936680392),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  992  0.091477  0.091361  0.092536  0.068078  0.111670         0.497855\n",
       "  993  0.092969  0.091067  0.091279  0.084810  0.113072         0.486386\n",
       "  994  0.090337  0.089851  0.090948  0.069593  0.112976         0.471081\n",
       "  995  0.089843  0.088033  0.088782  0.073066  0.110882         0.476565\n",
       "  996  0.088787  0.087984  0.088637  0.060396  0.109503         0.478411,\n",
       "  0.09103274351331539),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  993  0.092969  0.091067  0.091279  0.084810  0.113072         0.486386\n",
       "  994  0.090337  0.089851  0.090948  0.069593  0.112976         0.471081\n",
       "  995  0.089843  0.088033  0.088782  0.073066  0.110882         0.476565\n",
       "  996  0.088787  0.087984  0.088637  0.060396  0.109503         0.478411\n",
       "  997  0.088659  0.089659  0.089710  0.087091  0.108366         0.517317,\n",
       "  0.09258814064629789),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  994  0.090337  0.089851  0.090948  0.069593  0.112976         0.471081\n",
       "  995  0.089843  0.088033  0.088782  0.073066  0.110882         0.476565\n",
       "  996  0.088787  0.087984  0.088637  0.060396  0.109503         0.478411\n",
       "  997  0.088659  0.089659  0.089710  0.087091  0.108366         0.517317\n",
       "  998  0.091254  0.091956  0.091207  0.082015  0.112309         0.498751,\n",
       "  0.09085778972182366),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  995  0.089843  0.088033  0.088782  0.073066  0.110882         0.476565\n",
       "  996  0.088787  0.087984  0.088637  0.060396  0.109503         0.478411\n",
       "  997  0.088659  0.089659  0.089710  0.087091  0.108366         0.517317\n",
       "  998  0.091254  0.091956  0.091207  0.082015  0.112309         0.498751\n",
       "  999  0.092127  0.090588  0.092151  0.067372  0.113828         0.474182,\n",
       "  0.09466600318554078),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  996   0.088787  0.087984  0.088637  0.060396  0.109503         0.478411\n",
       "  997   0.088659  0.089659  0.089710  0.087091  0.108366         0.517317\n",
       "  998   0.091254  0.091956  0.091207  0.082015  0.112309         0.498751\n",
       "  999   0.092127  0.090588  0.092151  0.067372  0.113828         0.474182\n",
       "  1000  0.091682  0.093373  0.091977  0.090856  0.112138         0.515597,\n",
       "  0.09606321830453514),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  997   0.088659  0.089659  0.089710  0.087091  0.108366         0.517317\n",
       "  998   0.091254  0.091956  0.091207  0.082015  0.112309         0.498751\n",
       "  999   0.092127  0.090588  0.092151  0.067372  0.113828         0.474182\n",
       "  1000  0.091682  0.093373  0.091977  0.090856  0.112138         0.515597\n",
       "  1001  0.094647  0.096484  0.095874  0.103018  0.115858         0.497568,\n",
       "  0.09648742014690191),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  998   0.091254  0.091956  0.091207  0.082015  0.112309         0.498751\n",
       "  999   0.092127  0.090588  0.092151  0.067372  0.113828         0.474182\n",
       "  1000  0.091682  0.093373  0.091977  0.090856  0.112138         0.515597\n",
       "  1001  0.094647  0.096484  0.095874  0.103018  0.115858         0.497568\n",
       "  1002  0.096940  0.095833  0.097381  0.066932  0.117222         0.490293,\n",
       "  0.09586909676732505),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  999   0.092127  0.090588  0.092151  0.067372  0.113828         0.474182\n",
       "  1000  0.091682  0.093373  0.091977  0.090856  0.112138         0.515597\n",
       "  1001  0.094647  0.096484  0.095874  0.103018  0.115858         0.497568\n",
       "  1002  0.096940  0.095833  0.097381  0.066932  0.117222         0.490293\n",
       "  1003  0.096844  0.095219  0.096383  0.088088  0.117636         0.482497,\n",
       "  0.0971776418176473),\n",
       " ...]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2ed8afa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(          Open      High       Low    Volume  Previous  last_difference\n",
       "  4171  0.277898  0.299436  0.301787  0.261932  0.295073         0.563030\n",
       "  4172  0.275667  0.288924  0.289746  0.214483  0.314435         0.495263\n",
       "  4173  0.264342  0.262776  0.274356  0.229856  0.313364         0.363479\n",
       "  4174  0.250062  0.245690  0.262412  0.464760  0.272557         0.516035\n",
       "  4175  0.249904  0.249420  0.268726  0.376045  0.277749         0.487443,\n",
       "  0.28500876880022696),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4172  0.275667  0.288924  0.289746  0.214483  0.314435         0.495263\n",
       "  4173  0.264342  0.262776  0.274356  0.229856  0.313364         0.363479\n",
       "  4174  0.250062  0.245690  0.262412  0.464760  0.272557         0.516035\n",
       "  4175  0.249904  0.249420  0.268726  0.376045  0.277749         0.487443\n",
       "  4176  0.257334  0.260337  0.278623  0.231235  0.274320         0.534266,\n",
       "  0.2791323798588976),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4173  0.264342  0.262776  0.274356  0.229856  0.313364         0.363479\n",
       "  4174  0.250062  0.245690  0.262412  0.464760  0.272557         0.516035\n",
       "  4175  0.249904  0.249420  0.268726  0.376045  0.277749         0.487443\n",
       "  4176  0.257334  0.260337  0.278623  0.231235  0.274320         0.534266\n",
       "  4177  0.267635  0.264844  0.280380  0.178761  0.285009         0.479327,\n",
       "  0.27861175326583987),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4174  0.250062  0.245690  0.262412  0.464760  0.272557         0.516035\n",
       "  4175  0.249904  0.249420  0.268726  0.376045  0.277749         0.487443\n",
       "  4176  0.257334  0.260337  0.278623  0.231235  0.274320         0.534266\n",
       "  4177  0.267635  0.264844  0.280380  0.178761  0.285009         0.479327\n",
       "  4178  0.252142  0.250091  0.257182  0.244021  0.279132         0.497089,\n",
       "  0.27205845751771185),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4175  0.249904  0.249420  0.268726  0.376045  0.277749         0.487443\n",
       "  4176  0.257334  0.260337  0.278623  0.231235  0.274320         0.534266\n",
       "  4177  0.267635  0.264844  0.280380  0.178761  0.285009         0.479327\n",
       "  4178  0.252142  0.250091  0.257182  0.244021  0.279132         0.497089\n",
       "  4179  0.247658  0.244081  0.259658  0.262400  0.278612         0.477082,\n",
       "  0.2872328125311945),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4176  0.257334  0.260337  0.278623  0.231235  0.274320         0.534266\n",
       "  4177  0.267635  0.264844  0.280380  0.178761  0.285009         0.479327\n",
       "  4178  0.252142  0.250091  0.257182  0.244021  0.279132         0.497089\n",
       "  4179  0.247658  0.244081  0.259658  0.262400  0.278612         0.477082\n",
       "  4180  0.239384  0.249756  0.260862  0.197435  0.272058         0.549142,\n",
       "  0.28692782436728215),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4177  0.267635  0.264844  0.280380  0.178761  0.285009         0.479327\n",
       "  4178  0.252142  0.250091  0.257182  0.244021  0.279132         0.497089\n",
       "  4179  0.247658  0.244081  0.259658  0.262400  0.278612         0.477082\n",
       "  4180  0.239384  0.249756  0.260862  0.197435  0.272058         0.549142\n",
       "  4181  0.261802  0.259176  0.282243  0.149838  0.287233         0.497804,\n",
       "  0.2966646990682784),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4178  0.252142  0.250091  0.257182  0.244021  0.279132         0.497089\n",
       "  4179  0.247658  0.244081  0.259658  0.262400  0.278612         0.477082\n",
       "  4180  0.239384  0.249756  0.260862  0.197435  0.272058         0.549142\n",
       "  4181  0.261802  0.259176  0.282243  0.149838  0.287233         0.497804\n",
       "  4182  0.262925  0.276374  0.291812  0.254122  0.286928         0.531108,\n",
       "  0.3480786430003824),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4179  0.247658  0.244081  0.259658  0.262400  0.278612         0.477082\n",
       "  4180  0.239384  0.249756  0.260862  0.197435  0.272058         0.549142\n",
       "  4181  0.261802  0.259176  0.282243  0.149838  0.287233         0.497804\n",
       "  4182  0.262925  0.276374  0.291812  0.254122  0.286928         0.531108\n",
       "  4183  0.292185  0.314218  0.322024  0.313027  0.296665         0.669331,\n",
       "  0.41338754181444104),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4180  0.239384  0.249756  0.260862  0.197435  0.272058         0.549142\n",
       "  4181  0.261802  0.259176  0.282243  0.149838  0.287233         0.497804\n",
       "  4182  0.262925  0.276374  0.291812  0.254122  0.286928         0.531108\n",
       "  4183  0.292185  0.314218  0.322024  0.313027  0.296665         0.669331\n",
       "  4184  0.333622  0.386201  0.363214  0.427806  0.348079         0.715413,\n",
       "  0.4127925761230433),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4181  0.261802  0.259176  0.282243  0.149838  0.287233         0.497804\n",
       "  4182  0.262925  0.276374  0.291812  0.254122  0.286928         0.531108\n",
       "  4183  0.292185  0.314218  0.322024  0.313027  0.296665         0.669331\n",
       "  4184  0.333622  0.386201  0.363214  0.427806  0.348079         0.715413\n",
       "  4185  0.386038  0.386468  0.410762  0.227786  0.413388         0.496843,\n",
       "  0.43133640990385436),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4182  0.262925  0.276374  0.291812  0.254122  0.286928         0.531108\n",
       "  4183  0.292185  0.314218  0.322024  0.313027  0.296665         0.669331\n",
       "  4184  0.333622  0.386201  0.363214  0.427806  0.348079         0.715413\n",
       "  4185  0.386038  0.386468  0.410762  0.227786  0.413388         0.496843\n",
       "  4186  0.388774  0.399695  0.418687  0.212241  0.412793         0.560317,\n",
       "  0.4430593152811164),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4183  0.292185  0.314218  0.322024  0.313027  0.296665         0.669331\n",
       "  4184  0.333622  0.386201  0.363214  0.427806  0.348079         0.715413\n",
       "  4185  0.386038  0.386468  0.410762  0.227786  0.413388         0.496843\n",
       "  4186  0.388774  0.399695  0.418687  0.212241  0.412793         0.560317\n",
       "  4187  0.427830  0.421819  0.443033  0.229092  0.431336         0.537695,\n",
       "  0.4582632898953185),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4184  0.333622  0.386201  0.363214  0.427806  0.348079         0.715413\n",
       "  4185  0.386038  0.386468  0.410762  0.227786  0.413388         0.496843\n",
       "  4186  0.388774  0.399695  0.418687  0.212241  0.412793         0.560317\n",
       "  4187  0.427830  0.421819  0.443033  0.229092  0.431336         0.537695\n",
       "  4188  0.411809  0.425660  0.440048  0.151760  0.443059         0.549240,\n",
       "  0.43513743331089616),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4185  0.386038  0.386468  0.410762  0.227786  0.413388         0.496843\n",
       "  4186  0.388774  0.399695  0.418687  0.212241  0.412793         0.560317\n",
       "  4187  0.427830  0.421819  0.443033  0.229092  0.431336         0.537695\n",
       "  4188  0.411809  0.425660  0.440048  0.151760  0.443059         0.549240\n",
       "  4189  0.419006  0.415194  0.428572  0.280236  0.458263         0.422119,\n",
       "  0.4444205375140795),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4186  0.388774  0.399695  0.418687  0.212241  0.412793         0.560317\n",
       "  4187  0.427830  0.421819  0.443033  0.229092  0.431336         0.537695\n",
       "  4188  0.411809  0.425660  0.440048  0.151760  0.443059         0.549240\n",
       "  4189  0.419006  0.415194  0.428572  0.280236  0.458263         0.422119\n",
       "  4190  0.415404  0.416636  0.441836  0.181594  0.435137         0.529603,\n",
       "  0.43517455079145617),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4187  0.427830  0.421819  0.443033  0.229092  0.431336         0.537695\n",
       "  4188  0.411809  0.425660  0.440048  0.151760  0.443059         0.549240\n",
       "  4189  0.419006  0.415194  0.428572  0.280236  0.458263         0.422119\n",
       "  4190  0.415404  0.416636  0.441836  0.181594  0.435137         0.529603\n",
       "  4191  0.425720  0.417231  0.436644  0.143531  0.444421         0.468151,\n",
       "  0.46628187071268923),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4188  0.411809  0.425660  0.440048  0.151760  0.443059         0.549240\n",
       "  4189  0.419006  0.415194  0.428572  0.280236  0.458263         0.422119\n",
       "  4190  0.415404  0.416636  0.441836  0.181594  0.435137         0.529603\n",
       "  4191  0.425720  0.417231  0.436644  0.143531  0.444421         0.468151\n",
       "  4192  0.427091  0.439052  0.449295  0.232861  0.435175         0.601983,\n",
       "  0.4671000917270387),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4189  0.419006  0.415194  0.428572  0.280236  0.458263         0.422119\n",
       "  4190  0.415404  0.416636  0.441836  0.181594  0.435137         0.529603\n",
       "  4191  0.425720  0.417231  0.436644  0.143531  0.444421         0.468151\n",
       "  4192  0.427091  0.439052  0.449295  0.232861  0.435175         0.601983\n",
       "  4193  0.446698  0.438932  0.463180  0.172799  0.466282         0.501529,\n",
       "  0.47055154136325905),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4190  0.415404  0.416636  0.441836  0.181594  0.435137         0.529603\n",
       "  4191  0.425720  0.417231  0.436644  0.143531  0.444421         0.468151\n",
       "  4192  0.427091  0.439052  0.449295  0.232861  0.435175         0.601983\n",
       "  4193  0.446698  0.438932  0.463180  0.172799  0.466282         0.501529\n",
       "  4194  0.438394  0.448093  0.465735  0.198568  0.467100         0.510263,\n",
       "  0.46402808836703646),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4191  0.425720  0.417231  0.436644  0.143531  0.444421         0.468151\n",
       "  4192  0.427091  0.439052  0.449295  0.232861  0.435175         0.601983\n",
       "  4193  0.446698  0.438932  0.463180  0.172799  0.466282         0.501529\n",
       "  4194  0.438394  0.448093  0.465735  0.198568  0.467100         0.510263\n",
       "  4195  0.450383  0.441342  0.453592  0.162674  0.470552         0.477181,\n",
       "  0.46377509442939946),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4192  0.427091  0.439052  0.449295  0.232861  0.435175         0.601983\n",
       "  4193  0.446698  0.438932  0.463180  0.172799  0.466282         0.501529\n",
       "  4194  0.438394  0.448093  0.465735  0.198568  0.467100         0.510263\n",
       "  4195  0.450383  0.441342  0.453592  0.162674  0.470552         0.477181\n",
       "  4196  0.433323  0.430450  0.443703  0.189231  0.464028         0.497977,\n",
       "  0.44490404662657745),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4193  0.446698  0.438932  0.463180  0.172799  0.466282         0.501529\n",
       "  4194  0.438394  0.448093  0.465735  0.198568  0.467100         0.510263\n",
       "  4195  0.450383  0.441342  0.453592  0.162674  0.470552         0.477181\n",
       "  4196  0.433323  0.430450  0.443703  0.189231  0.464028         0.497977\n",
       "  4197  0.439336  0.436889  0.442107  0.154322  0.463775         0.436230,\n",
       "  0.48014698284273827),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4194  0.438394  0.448093  0.465735  0.198568  0.467100         0.510263\n",
       "  4195  0.450383  0.441342  0.453592  0.162674  0.470552         0.477181\n",
       "  4196  0.433323  0.430450  0.443703  0.189231  0.464028         0.497977\n",
       "  4197  0.439336  0.436889  0.442107  0.154322  0.463775         0.436230\n",
       "  4198  0.431982  0.453715  0.458484  0.247618  0.444904         0.615699,\n",
       "  0.4886564665152271),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4195  0.450383  0.441342  0.453592  0.162674  0.470552         0.477181\n",
       "  4196  0.433323  0.430450  0.443703  0.189231  0.464028         0.497977\n",
       "  4197  0.439336  0.436889  0.442107  0.154322  0.463775         0.436230\n",
       "  4198  0.431982  0.453715  0.458484  0.247618  0.444904         0.615699\n",
       "  4199  0.459456  0.465492  0.485298  0.151095  0.480147         0.527038,\n",
       "  0.47406245335210406),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4196  0.433323  0.430450  0.443703  0.189231  0.464028         0.497977\n",
       "  4197  0.439336  0.436889  0.442107  0.154322  0.463775         0.436230\n",
       "  4198  0.431982  0.453715  0.458484  0.247618  0.444904         0.615699\n",
       "  4199  0.459456  0.465492  0.485298  0.151095  0.480147         0.527038\n",
       "  4200  0.471565  0.462586  0.478096  0.288292  0.488656         0.450415,\n",
       "  0.5282063098764838),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4197  0.439336  0.436889  0.442107  0.154322  0.463775         0.436230\n",
       "  4198  0.431982  0.453715  0.458484  0.247618  0.444904         0.615699\n",
       "  4199  0.459456  0.465492  0.485298  0.151095  0.480147         0.527038\n",
       "  4200  0.471565  0.462586  0.478096  0.288292  0.488656         0.450415\n",
       "  4201  0.525176  0.550250  0.531214  0.630190  0.474062         0.678384,\n",
       "  0.5653832974193815),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4198  0.431982  0.453715  0.458484  0.247618  0.444904         0.615699\n",
       "  4199  0.459456  0.465492  0.485298  0.151095  0.480147         0.527038\n",
       "  4200  0.471565  0.462586  0.478096  0.288292  0.488656         0.450415\n",
       "  4201  0.525176  0.550250  0.531214  0.630190  0.474062         0.678384\n",
       "  4202  0.527576  0.538154  0.551857  0.381687  0.528206         0.622114,\n",
       "  0.5506776045261723),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4199  0.459456  0.465492  0.485298  0.151095  0.480147         0.527038\n",
       "  4200  0.471565  0.462586  0.478096  0.288292  0.488656         0.450415\n",
       "  4201  0.525176  0.550250  0.531214  0.630190  0.474062         0.678384\n",
       "  4202  0.527576  0.538154  0.551857  0.381687  0.528206         0.622114\n",
       "  4203  0.523175  0.530999  0.551767  0.376612  0.565383         0.450044,\n",
       "  0.5395572668573687),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4200  0.471565  0.462586  0.478096  0.288292  0.488656         0.450415\n",
       "  4201  0.525176  0.550250  0.531214  0.630190  0.474062         0.678384\n",
       "  4202  0.527576  0.538154  0.551857  0.381687  0.528206         0.622114\n",
       "  4203  0.523175  0.530999  0.551767  0.376612  0.565383         0.450044\n",
       "  4204  0.521841  0.525324  0.538476  0.310662  0.550678         0.461935,\n",
       "  0.5091194897538049),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4201  0.525176  0.550250  0.531214  0.630190  0.474062         0.678384\n",
       "  4202  0.527576  0.538154  0.551857  0.381687  0.528206         0.622114\n",
       "  4203  0.523175  0.530999  0.551767  0.376612  0.565383         0.450044\n",
       "  4204  0.521841  0.525324  0.538476  0.310662  0.550678         0.461935\n",
       "  4205  0.496990  0.494477  0.483688  0.327069  0.539557         0.397869,\n",
       "  0.5109715850883949),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4202  0.527576  0.538154  0.551857  0.381687  0.528206         0.622114\n",
       "  4203  0.523175  0.530999  0.551767  0.376612  0.565383         0.450044\n",
       "  4204  0.521841  0.525324  0.538476  0.310662  0.550678         0.461935\n",
       "  4205  0.496990  0.494477  0.483688  0.327069  0.539557         0.397869\n",
       "  4206  0.495995  0.496720  0.513573  0.163068  0.509119         0.504958,\n",
       "  0.5292774058145047),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4203  0.523175  0.530999  0.551767  0.376612  0.565383         0.450044\n",
       "  4204  0.521841  0.525324  0.538476  0.310662  0.550678         0.461935\n",
       "  4205  0.496990  0.494477  0.483688  0.327069  0.539557         0.397869\n",
       "  4206  0.495995  0.496720  0.513573  0.163068  0.509119         0.504958\n",
       "  4207  0.482597  0.497109  0.506745  0.148434  0.510972         0.559527,\n",
       "  0.542175618733524),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4204  0.521841  0.525324  0.538476  0.310662  0.550678         0.461935\n",
       "  4205  0.496990  0.494477  0.483688  0.327069  0.539557         0.397869\n",
       "  4206  0.495995  0.496720  0.513573  0.163068  0.509119         0.504958\n",
       "  4207  0.482597  0.497109  0.506745  0.148434  0.510972         0.559527\n",
       "  4208  0.519792  0.522815  0.542615  0.181126  0.529277         0.541593,\n",
       "  0.49975454857244084),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4205  0.496990  0.494477  0.483688  0.327069  0.539557         0.397869\n",
       "  4206  0.495995  0.496720  0.513573  0.163068  0.509119         0.504958\n",
       "  4207  0.482597  0.497109  0.506745  0.148434  0.510972         0.559527\n",
       "  4208  0.519792  0.522815  0.542615  0.181126  0.529277         0.541593\n",
       "  4209  0.500870  0.493516  0.501020  0.214803  0.542176         0.358126,\n",
       "  0.47528236137751634),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4206  0.495995  0.496720  0.513573  0.163068  0.509119         0.504958\n",
       "  4207  0.482597  0.497109  0.506745  0.148434  0.510972         0.559527\n",
       "  4208  0.519792  0.522815  0.542615  0.181126  0.529277         0.541593\n",
       "  4209  0.500870  0.493516  0.501020  0.214803  0.542176         0.358126\n",
       "  4210  0.438304  0.450801  0.462089  0.289992  0.499755         0.417654,\n",
       "  0.42345174955141407),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4207  0.482597  0.497109  0.506745  0.148434  0.510972         0.559527\n",
       "  4208  0.519792  0.522815  0.542615  0.181126  0.529277         0.541593\n",
       "  4209  0.500870  0.493516  0.501020  0.214803  0.542176         0.358126\n",
       "  4210  0.438304  0.450801  0.462089  0.289992  0.499755         0.417654\n",
       "  4211  0.415585  0.422860  0.422239  0.324778  0.475282         0.326919,\n",
       "  0.44047819989948667),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4208  0.519792  0.522815  0.542615  0.181126  0.529277         0.541593\n",
       "  4209  0.500870  0.493516  0.501020  0.214803  0.542176         0.358126\n",
       "  4210  0.438304  0.450801  0.462089  0.289992  0.499755         0.417654\n",
       "  4211  0.415585  0.422860  0.422239  0.324778  0.475282         0.326919\n",
       "  4212  0.415117  0.416172  0.431774  0.222982  0.423452         0.555284,\n",
       "  0.48078668290850235),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4209  0.500870  0.493516  0.501020  0.214803  0.542176         0.358126\n",
       "  4210  0.438304  0.450801  0.462089  0.289992  0.499755         0.417654\n",
       "  4211  0.415585  0.422860  0.422239  0.324778  0.475282         0.326919\n",
       "  4212  0.415117  0.416172  0.431774  0.222982  0.423452         0.555284\n",
       "  4213  0.438281  0.450145  0.462329  0.222416  0.440478         0.632499,\n",
       "  0.4846918881657898),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4210  0.438304  0.450801  0.462089  0.289992  0.499755         0.417654\n",
       "  4211  0.415585  0.422860  0.422239  0.324778  0.475282         0.326919\n",
       "  4212  0.415117  0.416172  0.431774  0.222982  0.423452         0.555284\n",
       "  4213  0.438281  0.450145  0.462329  0.222416  0.440478         0.632499\n",
       "  4214  0.451461  0.451823  0.471120  0.138875  0.480787         0.511767,\n",
       "  0.47131766401411324),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4211  0.415585  0.422860  0.422239  0.324778  0.475282         0.326919\n",
       "  4212  0.415117  0.416172  0.431774  0.222982  0.423452         0.555284\n",
       "  4213  0.438281  0.450145  0.462329  0.222416  0.440478         0.632499\n",
       "  4214  0.451461  0.451823  0.471120  0.138875  0.480787         0.511767\n",
       "  4215  0.472248  0.466934  0.477261  0.107588  0.484692         0.454460,\n",
       "  0.47524512488299053),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4212  0.415117  0.416172  0.431774  0.222982  0.423452         0.555284\n",
       "  4213  0.438281  0.450145  0.462329  0.222416  0.440478         0.632499\n",
       "  4214  0.451461  0.451823  0.471120  0.138875  0.480787         0.511767\n",
       "  4215  0.472248  0.466934  0.477261  0.107588  0.484692         0.454460\n",
       "  4216  0.417612  0.446804  0.447428  0.132815  0.471318         0.511841,\n",
       "  0.5104880759758972),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4213  0.438281  0.450145  0.462329  0.222416  0.440478         0.632499\n",
       "  4214  0.451461  0.451823  0.471120  0.138875  0.480787         0.511767\n",
       "  4215  0.472248  0.466934  0.477261  0.107588  0.484692         0.454460\n",
       "  4216  0.417612  0.446804  0.447428  0.132815  0.471318         0.511841\n",
       "  4217  0.465567  0.480046  0.490754  0.188024  0.475245         0.615699,\n",
       "  0.5023133042052603),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4214  0.451461  0.451823  0.471120  0.138875  0.480787         0.511767\n",
       "  4215  0.472248  0.466934  0.477261  0.107588  0.484692         0.454460\n",
       "  4216  0.417612  0.446804  0.447428  0.132815  0.471318         0.511841\n",
       "  4217  0.465567  0.480046  0.490754  0.188024  0.475245         0.615699\n",
       "  4218  0.494164  0.486651  0.506770  0.175706  0.510488         0.471704,\n",
       "  0.5481114700274783),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4215  0.472248  0.466934  0.477261  0.107588  0.484692         0.454460\n",
       "  4216  0.417612  0.446804  0.447428  0.132815  0.471318         0.511841\n",
       "  4217  0.465567  0.480046  0.490754  0.188024  0.475245         0.615699\n",
       "  4218  0.494164  0.486651  0.506770  0.175706  0.510488         0.471704\n",
       "  4219  0.494925  0.524394  0.520120  0.156145  0.502313         0.650706,\n",
       "  0.5498966051296028),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4216  0.417612  0.446804  0.447428  0.132815  0.471318         0.511841\n",
       "  4217  0.465567  0.480046  0.490754  0.188024  0.475245         0.615699\n",
       "  4218  0.494164  0.486651  0.506770  0.175706  0.510488         0.471704\n",
       "  4219  0.494925  0.524394  0.520120  0.156145  0.502313         0.650706\n",
       "  4220  0.534863  0.535385  0.552391  0.126508  0.548111         0.504736,\n",
       "  0.5680908502628466),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4217  0.465567  0.480046  0.490754  0.188024  0.475245         0.615699\n",
       "  4218  0.494164  0.486651  0.506770  0.175706  0.510488         0.471704\n",
       "  4219  0.494925  0.524394  0.520120  0.156145  0.502313         0.650706\n",
       "  4220  0.534863  0.535385  0.552391  0.126508  0.548111         0.504736\n",
       "  4221  0.529463  0.543054  0.559560  0.163684  0.549897         0.559157,\n",
       "  0.5450171069186656),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4218  0.494164  0.486651  0.506770  0.175706  0.510488         0.471704\n",
       "  4219  0.494925  0.524394  0.520120  0.156145  0.502313         0.650706\n",
       "  4220  0.534863  0.535385  0.552391  0.126508  0.548111         0.504736\n",
       "  4221  0.529463  0.543054  0.559560  0.163684  0.549897         0.559157\n",
       "  4222  0.547628  0.540808  0.551646  0.374395  0.568091         0.422291,\n",
       "  0.5517488194781588),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4219  0.494925  0.524394  0.520120  0.156145  0.502313         0.650706\n",
       "  4220  0.534863  0.535385  0.552391  0.126508  0.548111         0.504736\n",
       "  4221  0.529463  0.543054  0.559560  0.163684  0.549897         0.559157\n",
       "  4222  0.547628  0.540808  0.551646  0.374395  0.568091         0.422291\n",
       "  4223  0.536339  0.531762  0.555929  0.191424  0.545017         0.521142,\n",
       "  0.565323820190011),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4220  0.534863  0.535385  0.552391  0.126508  0.548111         0.504736\n",
       "  4221  0.529463  0.543054  0.559560  0.163684  0.549897         0.559157\n",
       "  4222  0.547628  0.540808  0.551646  0.374395  0.568091         0.422291\n",
       "  4223  0.536339  0.531762  0.555929  0.191424  0.545017         0.521142\n",
       "  4224  0.536370  0.539261  0.553814  0.130548  0.551749         0.543837,\n",
       "  0.5589789178958651),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4221  0.529463  0.543054  0.559560  0.163684  0.549897         0.559157\n",
       "  4222  0.547628  0.540808  0.551646  0.374395  0.568091         0.422291\n",
       "  4223  0.536339  0.531762  0.555929  0.191424  0.545017         0.521142\n",
       "  4224  0.536370  0.539261  0.553814  0.130548  0.551749         0.543837\n",
       "  4225  0.546399  0.542334  0.553302  0.105814  0.565324         0.477773,\n",
       "  0.5465790908350587),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4222  0.547628  0.540808  0.551646  0.374395  0.568091         0.422291\n",
       "  4223  0.536339  0.531762  0.555929  0.191424  0.545017         0.521142\n",
       "  4224  0.536370  0.539261  0.553814  0.130548  0.551749         0.543837\n",
       "  4225  0.546399  0.542334  0.553302  0.105814  0.565324         0.477773\n",
       "  4226  0.516039  0.517731  0.537219  0.120448  0.558979         0.457692,\n",
       "  0.5816509891057293),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4223  0.536339  0.531762  0.555929  0.191424  0.545017         0.521142\n",
       "  4224  0.536370  0.539261  0.553814  0.130548  0.551749         0.543837\n",
       "  4225  0.546399  0.542334  0.553302  0.105814  0.565324         0.477773\n",
       "  4226  0.516039  0.517731  0.537219  0.120448  0.558979         0.457692\n",
       "  4227  0.536761  0.551380  0.563515  0.214089  0.546579         0.615132,\n",
       "  0.5923101774108457),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4224  0.536370  0.539261  0.553814  0.130548  0.551749         0.543837\n",
       "  4225  0.546399  0.542334  0.553302  0.105814  0.565324         0.477773\n",
       "  4226  0.516039  0.517731  0.537219  0.120448  0.558979         0.457692\n",
       "  4227  0.536761  0.551380  0.563515  0.214089  0.546579         0.615132\n",
       "  4228  0.558463  0.562166  0.581052  0.188246  0.581651         0.534167,\n",
       "  0.6047769647041172),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4225  0.546399  0.542334  0.553302  0.105814  0.565324         0.477773\n",
       "  4226  0.516039  0.517731  0.537219  0.120448  0.558979         0.457692\n",
       "  4227  0.536761  0.551380  0.563515  0.214089  0.546579         0.615132\n",
       "  4228  0.558463  0.562166  0.581052  0.188246  0.581651         0.534167\n",
       "  4229  0.580000  0.582376  0.601498  0.203150  0.592310         0.540162,\n",
       "  0.6111367437439785),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4226  0.516039  0.517731  0.537219  0.120448  0.558979         0.457692\n",
       "  4227  0.536761  0.551380  0.563515  0.214089  0.546579         0.615132\n",
       "  4228  0.558463  0.562166  0.581052  0.188246  0.581651         0.534167\n",
       "  4229  0.580000  0.582376  0.601498  0.203150  0.592310         0.540162\n",
       "  4230  0.594769  0.590389  0.615865  0.142374  0.604777         0.519908,\n",
       "  0.6336006595277923),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4227  0.536761  0.551380  0.563515  0.214089  0.546579         0.615132\n",
       "  4228  0.558463  0.562166  0.581052  0.188246  0.581651         0.534167\n",
       "  4229  0.580000  0.582376  0.601498  0.203150  0.592310         0.540162\n",
       "  4230  0.594769  0.590389  0.615865  0.142374  0.604777         0.519908\n",
       "  4231  0.590632  0.604317  0.620884  0.279202  0.611137         0.573318,\n",
       "  0.627895442422665),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4228  0.558463  0.562166  0.581052  0.188246  0.581651         0.534167\n",
       "  4229  0.580000  0.582376  0.601498  0.203150  0.592310         0.540162\n",
       "  4230  0.594769  0.590389  0.615865  0.142374  0.604777         0.519908\n",
       "  4231  0.590632  0.604317  0.620884  0.279202  0.611137         0.573318\n",
       "  4232  0.613924  0.607162  0.624113  0.205368  0.633601         0.479894,\n",
       "  0.6376470599786648),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4229  0.580000  0.582376  0.601498  0.203150  0.592310         0.540162\n",
       "  4230  0.594769  0.590389  0.615865  0.142374  0.604777         0.519908\n",
       "  4231  0.590632  0.604317  0.620884  0.279202  0.611137         0.573318\n",
       "  4232  0.613924  0.607162  0.624113  0.205368  0.633601         0.479894\n",
       "  4233  0.605236  0.608108  0.626107  0.172232  0.627895         0.531157,\n",
       "  0.6329013780954378),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4230  0.594769  0.590389  0.615865  0.142374  0.604777         0.519908\n",
       "  4231  0.590632  0.604317  0.620884  0.279202  0.611137         0.573318\n",
       "  4232  0.613924  0.607162  0.624113  0.205368  0.633601         0.479894\n",
       "  4233  0.605236  0.608108  0.626107  0.172232  0.627895         0.531157\n",
       "  4234  0.618084  0.614981  0.635161  0.167699  0.637647         0.483077,\n",
       "  0.627895442422665),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4231  0.590632  0.604317  0.620884  0.279202  0.611137         0.573318\n",
       "  4232  0.613924  0.607162  0.624113  0.205368  0.633601         0.479894\n",
       "  4233  0.605236  0.608108  0.626107  0.172232  0.627895         0.531157\n",
       "  4234  0.618084  0.614981  0.635161  0.167699  0.637647         0.483077\n",
       "  4235  0.613947  0.609817  0.612605  0.218622  0.632901         0.482214,\n",
       "  0.637929792530987),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4232  0.613924  0.607162  0.624113  0.205368  0.633601         0.479894\n",
       "  4233  0.605236  0.608108  0.626107  0.172232  0.627895         0.531157\n",
       "  4234  0.618084  0.614981  0.635161  0.167699  0.637647         0.483077\n",
       "  4235  0.613947  0.609817  0.612605  0.218622  0.632901         0.482214\n",
       "  4236  0.603028  0.620084  0.633151  0.211724  0.627895         0.532095,\n",
       "  0.6259762529648982),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4233  0.605236  0.608108  0.626107  0.172232  0.627895         0.531157\n",
       "  4234  0.618084  0.614981  0.635161  0.167699  0.637647         0.483077\n",
       "  4235  0.613947  0.609817  0.612605  0.218622  0.632901         0.482214\n",
       "  4236  0.603028  0.620084  0.633151  0.211724  0.627895         0.532095\n",
       "  4237  0.605778  0.607765  0.619424  0.551084  0.637930         0.459172,\n",
       "  0.6391794391710841),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4234  0.618084  0.614981  0.635161  0.167699  0.637647         0.483077\n",
       "  4235  0.613947  0.609817  0.612605  0.218622  0.632901         0.482214\n",
       "  4236  0.603028  0.620084  0.633151  0.211724  0.627895         0.532095\n",
       "  4237  0.605778  0.607765  0.619424  0.551084  0.637930         0.459172\n",
       "  4238  0.606298  0.617647  0.627421  0.217883  0.625976         0.542604,\n",
       "  0.6472797379526698),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4235  0.613947  0.609817  0.612605  0.218622  0.632901         0.482214\n",
       "  4236  0.603028  0.620084  0.633151  0.211724  0.627895         0.532095\n",
       "  4237  0.605778  0.607765  0.619424  0.551084  0.637930         0.459172\n",
       "  4238  0.606298  0.617647  0.627421  0.217883  0.625976         0.542604\n",
       "  4239  0.616998  0.621205  0.640850  0.152942  0.639179         0.525680,\n",
       "  0.6392761380182346),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4236  0.603028  0.620084  0.633151  0.211724  0.627895         0.532095\n",
       "  4237  0.605778  0.607765  0.619424  0.551084  0.637930         0.459172\n",
       "  4238  0.606298  0.617647  0.627421  0.217883  0.625976         0.542604\n",
       "  4239  0.616998  0.621205  0.640850  0.152942  0.639179         0.525680\n",
       "  4240  0.618506  0.629229  0.644245  0.137052  0.647280         0.472272,\n",
       "  0.6514824483707746),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4237  0.605778  0.607765  0.619424  0.551084  0.637930         0.459172\n",
       "  4238  0.606298  0.617647  0.627421  0.217883  0.625976         0.542604\n",
       "  4239  0.616998  0.621205  0.640850  0.152942  0.639179         0.525680\n",
       "  4240  0.618506  0.629229  0.644245  0.137052  0.647280         0.472272\n",
       "  4241  0.626094  0.625255  0.654901  0.127691  0.639276         0.539298,\n",
       "  0.6472128967341704),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4238  0.606298  0.617647  0.627421  0.217883  0.625976         0.542604\n",
       "  4239  0.616998  0.621205  0.640850  0.152942  0.639179         0.525680\n",
       "  4240  0.618506  0.629229  0.644245  0.137052  0.647280         0.472272\n",
       "  4241  0.626094  0.625255  0.654901  0.127691  0.639276         0.539298\n",
       "  4242  0.624639  0.624790  0.647135  0.306917  0.651482         0.484656,\n",
       "  0.6446020427378554),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4239  0.616998  0.621205  0.640850  0.152942  0.639179         0.525680\n",
       "  4240  0.618506  0.629229  0.644245  0.137052  0.647280         0.472272\n",
       "  4241  0.626094  0.625255  0.654901  0.127691  0.639276         0.539298\n",
       "  4242  0.624639  0.624790  0.647135  0.306917  0.651482         0.484656\n",
       "  4243  0.625287  0.622227  0.640057  0.240720  0.647213         0.490157,\n",
       "  0.6326857247895468),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4240  0.618506  0.629229  0.644245  0.137052  0.647280         0.472272\n",
       "  4241  0.626094  0.625255  0.654901  0.127691  0.639276         0.539298\n",
       "  4242  0.624639  0.624790  0.647135  0.306917  0.651482         0.484656\n",
       "  4243  0.625287  0.622227  0.640057  0.240720  0.647213         0.490157\n",
       "  4244  0.621859  0.617086  0.631977  0.152524  0.644602         0.459295,\n",
       "  0.6222348258011923),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4241  0.626094  0.625255  0.654901  0.127691  0.639276         0.539298\n",
       "  4242  0.624639  0.624790  0.647135  0.306917  0.651482         0.484656\n",
       "  4243  0.625287  0.622227  0.640057  0.240720  0.647213         0.490157\n",
       "  4244  0.621859  0.617086  0.631977  0.152524  0.644602         0.459295\n",
       "  4245  0.604995  0.598780  0.621750  0.190167  0.632686         0.464155,\n",
       "  0.6378925709132068),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4242  0.624639  0.624790  0.647135  0.306917  0.651482         0.484656\n",
       "  4243  0.625287  0.622227  0.640057  0.240720  0.647213         0.490157\n",
       "  4244  0.621859  0.617086  0.631977  0.152524  0.644602         0.459295\n",
       "  4245  0.604995  0.598780  0.621750  0.190167  0.632686         0.464155\n",
       "  4246  0.592881  0.608886  0.623138  0.105346  0.622235         0.550745,\n",
       "  0.6728602724568866),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4243  0.625287  0.622227  0.640057  0.240720  0.647213         0.490157\n",
       "  4244  0.621859  0.617086  0.631977  0.152524  0.644602         0.459295\n",
       "  4245  0.604995  0.598780  0.621750  0.190167  0.632686         0.464155\n",
       "  4246  0.592881  0.608886  0.623138  0.105346  0.622235         0.550745\n",
       "  4247  0.622868  0.645278  0.652026  0.155110  0.637893         0.614786,\n",
       "  0.6885106387030262),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4244  0.621859  0.617086  0.631977  0.152524  0.644602         0.459295\n",
       "  4245  0.604995  0.598780  0.621750  0.190167  0.632686         0.464155\n",
       "  4246  0.592881  0.608886  0.623138  0.105346  0.622235         0.550745\n",
       "  4247  0.622868  0.645278  0.652026  0.155110  0.637893         0.614786\n",
       "  4248  0.662203  0.661091  0.676711  0.157451  0.672860         0.550720,\n",
       "  0.693070405895047),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4245  0.604995  0.598780  0.621750  0.190167  0.632686         0.464155\n",
       "  4246  0.592881  0.608886  0.623138  0.105346  0.622235         0.550745\n",
       "  4247  0.622868  0.645278  0.652026  0.155110  0.637893         0.614786\n",
       "  4248  0.662203  0.661091  0.676711  0.157451  0.672860         0.550720\n",
       "  4249  0.675639  0.672615  0.697046  0.099926  0.688511         0.513938,\n",
       "  0.6796739410085257),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4246  0.592881  0.608886  0.623138  0.105346  0.622235         0.550745\n",
       "  4247  0.622868  0.645278  0.652026  0.155110  0.637893         0.614786\n",
       "  4248  0.662203  0.661091  0.676711  0.157451  0.672860         0.550720\n",
       "  4249  0.675639  0.672615  0.697046  0.099926  0.688511         0.513938\n",
       "  4250  0.644126  0.663349  0.671157  0.138112  0.693070         0.454386,\n",
       "  0.6855873135397039),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4247  0.622868  0.645278  0.652026  0.155110  0.637893         0.614786\n",
       "  4248  0.662203  0.661091  0.676711  0.157451  0.672860         0.550720\n",
       "  4249  0.675639  0.672615  0.697046  0.099926  0.688511         0.513938\n",
       "  4250  0.644126  0.663349  0.671157  0.138112  0.693070         0.454386\n",
       "  4251  0.654593  0.660557  0.684756  0.080537  0.679674         0.518427,\n",
       "  0.7003078831786285),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4248  0.662203  0.661091  0.676711  0.157451  0.672860         0.550720\n",
       "  4249  0.675639  0.672615  0.697046  0.099926  0.688511         0.513938\n",
       "  4250  0.644126  0.663349  0.671157  0.138112  0.693070         0.454386\n",
       "  4251  0.654593  0.660557  0.684756  0.080537  0.679674         0.518427\n",
       "  4252  0.667990  0.674600  0.694638  0.103178  0.685587         0.547637,\n",
       "  0.7067125007300761),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4249  0.675639  0.672615  0.697046  0.099926  0.688511         0.513938\n",
       "  4250  0.644126  0.663349  0.671157  0.138112  0.693070         0.454386\n",
       "  4251  0.654593  0.660557  0.684756  0.080537  0.679674         0.518427\n",
       "  4252  0.667990  0.674600  0.694638  0.103178  0.685587         0.547637\n",
       "  4253  0.683785  0.694005  0.710246  0.099162  0.700308         0.520057,\n",
       "  0.7228983554382433),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4250  0.644126  0.663349  0.671157  0.138112  0.693070         0.454386\n",
       "  4251  0.654593  0.660557  0.684756  0.080537  0.679674         0.518427\n",
       "  4252  0.667990  0.674600  0.694638  0.103178  0.685587         0.547637\n",
       "  4253  0.683785  0.694005  0.710246  0.099162  0.700308         0.520057\n",
       "  4254  0.699157  0.708558  0.729227  0.115102  0.706713         0.552496,\n",
       "  0.7107587970437286),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4251  0.654593  0.660557  0.684756  0.080537  0.679674         0.518427\n",
       "  4252  0.667990  0.674600  0.694638  0.103178  0.685587         0.547637\n",
       "  4253  0.683785  0.694005  0.710246  0.099162  0.700308         0.520057\n",
       "  4254  0.699157  0.708558  0.729227  0.115102  0.706713         0.552496\n",
       "  4255  0.708177  0.702441  0.709660  0.098768  0.722898         0.458555,\n",
       "  0.7193725815804286),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4252  0.667990  0.674600  0.694638  0.103178  0.685587         0.547637\n",
       "  4253  0.683785  0.694005  0.710246  0.099162  0.700308         0.520057\n",
       "  4254  0.699157  0.708558  0.729227  0.115102  0.706713         0.552496\n",
       "  4255  0.708177  0.702441  0.709660  0.098768  0.722898         0.458555\n",
       "  4256  0.695231  0.696156  0.713024  0.077458  0.710759         0.527383,\n",
       "  0.6808193909216325),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4253  0.683785  0.694005  0.710246  0.099162  0.700308         0.520057\n",
       "  4254  0.699157  0.708558  0.729227  0.115102  0.706713         0.552496\n",
       "  4255  0.708177  0.702441  0.709660  0.098768  0.722898         0.458555\n",
       "  4256  0.695231  0.696156  0.713024  0.077458  0.710759         0.527383\n",
       "  4257  0.687914  0.681876  0.678638  0.211157  0.719373         0.370954,\n",
       "  0.7083042232611547),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4254  0.699157  0.708558  0.729227  0.115102  0.706713         0.552496\n",
       "  4255  0.708177  0.702441  0.709660  0.098768  0.722898         0.458555\n",
       "  4256  0.695231  0.696156  0.713024  0.077458  0.710759         0.527383\n",
       "  4257  0.687914  0.681876  0.678638  0.211157  0.719373         0.370954\n",
       "  4258  0.670560  0.693384  0.688443  0.129538  0.680819         0.589969,\n",
       "  0.7306043609513868),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4255  0.708177  0.702441  0.709660  0.098768  0.722898         0.458555\n",
       "  4256  0.695231  0.696156  0.713024  0.077458  0.710759         0.527383\n",
       "  4257  0.687914  0.681876  0.678638  0.211157  0.719373         0.370954\n",
       "  4258  0.670560  0.693384  0.688443  0.129538  0.680819         0.589969\n",
       "  4259  0.682360  0.702780  0.709712  0.076054  0.708304         0.572774,\n",
       "  0.7414348103531796),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4256  0.695231  0.696156  0.713024  0.077458  0.710759         0.527383\n",
       "  4257  0.687914  0.681876  0.678638  0.211157  0.719373         0.370954\n",
       "  4258  0.670560  0.693384  0.688443  0.129538  0.680819         0.589969\n",
       "  4259  0.682360  0.702780  0.709712  0.076054  0.708304         0.572774\n",
       "  4260  0.710437  0.716316  0.736783  0.062085  0.730604         0.534735,\n",
       "  0.8081940282302911),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4257  0.687914  0.681876  0.678638  0.211157  0.719373         0.370954\n",
       "  4258  0.670560  0.693384  0.688443  0.129538  0.680819         0.589969\n",
       "  4259  0.682360  0.702780  0.709712  0.076054  0.708304         0.572774\n",
       "  4260  0.710437  0.716316  0.736783  0.062085  0.730604         0.534735\n",
       "  4261  0.749772  0.797231  0.771409  0.219386  0.741435         0.720223,\n",
       "  0.8353962619082023),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4258  0.670560  0.693384  0.688443  0.129538  0.680819         0.589969\n",
       "  4259  0.682360  0.702780  0.709712  0.076054  0.708304         0.572774\n",
       "  4260  0.710437  0.716316  0.736783  0.062085  0.730604         0.534735\n",
       "  4261  0.749772  0.797231  0.771409  0.219386  0.741435         0.720223\n",
       "  4262  0.794834  0.811030  0.815819  0.178416  0.808194         0.589032,\n",
       "  0.7930271859733946),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4259  0.682360  0.702780  0.709712  0.076054  0.708304         0.572774\n",
       "  4260  0.710437  0.716316  0.736783  0.062085  0.730604         0.534735\n",
       "  4261  0.749772  0.797231  0.771409  0.219386  0.741435         0.720223\n",
       "  4262  0.794834  0.811030  0.815819  0.178416  0.808194         0.589032\n",
       "  4263  0.821373  0.815576  0.777423  0.413837  0.835396         0.358298,\n",
       "  0.786853425761959),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4260  0.710437  0.716316  0.736783  0.062085  0.730604         0.534735\n",
       "  4261  0.749772  0.797231  0.771409  0.219386  0.741435         0.720223\n",
       "  4262  0.794834  0.811030  0.815819  0.178416  0.808194         0.589032\n",
       "  4263  0.821373  0.815576  0.777423  0.413837  0.835396         0.358298\n",
       "  4264  0.799536  0.810465  0.796237  0.568107  0.793027         0.478340,\n",
       "  0.7892187837004776),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4261  0.749772  0.797231  0.771409  0.219386  0.741435         0.720223\n",
       "  4262  0.794834  0.811030  0.815819  0.178416  0.808194         0.589032\n",
       "  4263  0.821373  0.815576  0.777423  0.413837  0.835396         0.358298\n",
       "  4264  0.799536  0.810465  0.796237  0.568107  0.793027         0.478340\n",
       "  4265  0.766660  0.771953  0.793046  0.132002  0.786853         0.506660,\n",
       "  0.769588858221965),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4262  0.794834  0.811030  0.815819  0.178416  0.808194         0.589032\n",
       "  4263  0.821373  0.815576  0.777423  0.413837  0.835396         0.358298\n",
       "  4264  0.799536  0.810465  0.796237  0.568107  0.793027         0.478340\n",
       "  4265  0.766660  0.771953  0.793046  0.132002  0.786853         0.506660\n",
       "  4266  0.753555  0.750898  0.773121  0.189527  0.789219         0.433713,\n",
       "  0.781021771194996),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4263  0.821373  0.815576  0.777423  0.413837  0.835396         0.358298\n",
       "  4264  0.799536  0.810465  0.796237  0.568107  0.793027         0.478340\n",
       "  4265  0.766660  0.771953  0.793046  0.132002  0.786853         0.506660\n",
       "  4266  0.753555  0.750898  0.773121  0.189527  0.789219         0.433713\n",
       "  4267  0.753155  0.754699  0.770943  0.142546  0.769589         0.536733,\n",
       "  0.785343421195096),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4264  0.799536  0.810465  0.796237  0.568107  0.793027         0.478340\n",
       "  4265  0.766660  0.771953  0.793046  0.132002  0.786853         0.506660\n",
       "  4266  0.753555  0.750898  0.773121  0.189527  0.789219         0.433713\n",
       "  4267  0.753155  0.754699  0.770943  0.142546  0.769589         0.536733\n",
       "  4268  0.760924  0.759504  0.763628  0.129292  0.781022         0.513149,\n",
       "  0.7816019791546445),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4265  0.766660  0.771953  0.793046  0.132002  0.786853         0.506660\n",
       "  4266  0.753555  0.750898  0.773121  0.189527  0.789219         0.433713\n",
       "  4267  0.753155  0.754699  0.770943  0.142546  0.769589         0.536733\n",
       "  4268  0.760924  0.759504  0.763628  0.129292  0.781022         0.513149\n",
       "  4269  0.764685  0.762593  0.782164  0.098054  0.785343         0.486407,\n",
       "  0.7951621179975266),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4266  0.753555  0.750898  0.773121  0.189527  0.789219         0.433713\n",
       "  4267  0.753155  0.754699  0.770943  0.142546  0.769589         0.536733\n",
       "  4268  0.760924  0.759504  0.763628  0.129292  0.781022         0.513149\n",
       "  4269  0.764685  0.762593  0.782164  0.098054  0.785343         0.486407\n",
       "  4270  0.761354  0.768879  0.784948  0.040627  0.781602         0.543788,\n",
       "  0.79659018144899),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4267  0.753155  0.754699  0.770943  0.142546  0.769589         0.536733\n",
       "  4268  0.760924  0.759504  0.763628  0.129292  0.781022         0.513149\n",
       "  4269  0.764685  0.762593  0.782164  0.098054  0.785343         0.486407\n",
       "  4270  0.761354  0.768879  0.784948  0.040627  0.781602         0.543788\n",
       "  4271  0.765370  0.770863  0.791669  0.061494  0.795162         0.503552,\n",
       "  0.8109611773170926),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4268  0.760924  0.759504  0.763628  0.129292  0.781022         0.513149\n",
       "  4269  0.764685  0.762593  0.782164  0.098054  0.785343         0.486407\n",
       "  4270  0.761354  0.768879  0.784948  0.040627  0.781602         0.543788\n",
       "  4271  0.765370  0.770863  0.791669  0.061494  0.795162         0.503552\n",
       "  4272  0.775227  0.789802  0.797423  0.046885  0.796590         0.546477,\n",
       "  0.8123670000337107),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4269  0.764685  0.762593  0.782164  0.098054  0.785343         0.486407\n",
       "  4270  0.761354  0.768879  0.784948  0.040627  0.781602         0.543788\n",
       "  4271  0.765370  0.770863  0.791669  0.061494  0.795162         0.503552\n",
       "  4272  0.775227  0.789802  0.797423  0.046885  0.796590         0.546477\n",
       "  4273  0.792264  0.793310  0.809430  0.091968  0.810961         0.503478,\n",
       "  0.8063121942810161),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4270  0.761354  0.768879  0.784948  0.040627  0.781602         0.543788\n",
       "  4271  0.765370  0.770863  0.791669  0.061494  0.795162         0.503552\n",
       "  4272  0.775227  0.789802  0.797423  0.046885  0.796590         0.546477\n",
       "  4273  0.792264  0.793310  0.809430  0.091968  0.810961         0.503478\n",
       "  4274  0.795331  0.797830  0.811289  0.081745  0.812367         0.478735,\n",
       "  0.8167258716515906),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4271  0.765370  0.770863  0.791669  0.061494  0.795162         0.503552\n",
       "  4272  0.775227  0.789802  0.797423  0.046885  0.796590         0.546477\n",
       "  4273  0.792264  0.793310  0.809430  0.091968  0.810961         0.503478\n",
       "  4274  0.795331  0.797830  0.811289  0.081745  0.812367         0.478735\n",
       "  4275  0.786741  0.791274  0.802694  0.074871  0.806312         0.533353,\n",
       "  0.8169714867233522),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4272  0.775227  0.789802  0.797423  0.046885  0.796590         0.546477\n",
       "  4273  0.792264  0.793310  0.809430  0.091968  0.810961         0.503478\n",
       "  4274  0.795331  0.797830  0.811289  0.081745  0.812367         0.478735\n",
       "  4275  0.786741  0.791274  0.802694  0.074871  0.806312         0.533353\n",
       "  4276  0.796454  0.795179  0.821148  0.049422  0.816726         0.499630,\n",
       "  0.8245585377777551),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4273  0.792264  0.793310  0.809430  0.091968  0.810961         0.503478\n",
       "  4274  0.795331  0.797830  0.811289  0.081745  0.812367         0.478735\n",
       "  4275  0.786741  0.791274  0.802694  0.074871  0.806312         0.533353\n",
       "  4276  0.796454  0.795179  0.821148  0.049422  0.816726         0.499630\n",
       "  4277  0.791066  0.800007  0.793464  0.116678  0.816971         0.523978,\n",
       "  0.8005252592116736),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4274  0.795331  0.797830  0.811289  0.081745  0.812367         0.478735\n",
       "  4275  0.786741  0.791274  0.802694  0.074871  0.806312         0.533353\n",
       "  4276  0.796454  0.795179  0.821148  0.049422  0.816726         0.499630\n",
       "  4277  0.791066  0.800007  0.793464  0.116678  0.816971         0.523978\n",
       "  4278  0.793945  0.795858  0.802822  0.156490  0.824559         0.419109,\n",
       "  0.7896577964665406),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4275  0.786741  0.791274  0.802694  0.074871  0.806312         0.533353\n",
       "  4276  0.796454  0.795179  0.821148  0.049422  0.816726         0.499630\n",
       "  4277  0.791066  0.800007  0.793464  0.116678  0.816971         0.523978\n",
       "  4278  0.793945  0.795858  0.802822  0.156490  0.824559         0.419109\n",
       "  4279  0.777736  0.789382  0.797306  0.078419  0.800525         0.462774,\n",
       "  0.7947678247290846),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4276  0.796454  0.795179  0.821148  0.049422  0.816726         0.499630\n",
       "  4277  0.791066  0.800007  0.793464  0.116678  0.816971         0.523978\n",
       "  4278  0.793945  0.795858  0.802822  0.156490  0.824559         0.419109\n",
       "  4279  0.777736  0.789382  0.797306  0.078419  0.800525         0.462774\n",
       "  4280  0.752899  0.776537  0.781276  0.119832  0.789658         0.515763,\n",
       "  0.8174325170730741),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4277  0.791066  0.800007  0.793464  0.116678  0.816971         0.523978\n",
       "  4278  0.793945  0.795858  0.802822  0.156490  0.824559         0.419109\n",
       "  4279  0.777736  0.789382  0.797306  0.078419  0.800525         0.462774\n",
       "  4280  0.752899  0.776537  0.781276  0.119832  0.789658         0.515763\n",
       "  4281  0.777246  0.794271  0.797998  0.086179  0.794768         0.573983,\n",
       "  0.8570419566776668),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4278  0.793945  0.795858  0.802822  0.156490  0.824559         0.419109\n",
       "  4279  0.777736  0.789382  0.797306  0.078419  0.800525         0.462774\n",
       "  4280  0.752899  0.776537  0.781276  0.119832  0.789658         0.515763\n",
       "  4281  0.777246  0.794271  0.797998  0.086179  0.794768         0.573983\n",
       "  4282  0.806114  0.848619  0.832331  0.154248  0.817433         0.630181,\n",
       "  0.8763667898550469),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4279  0.777736  0.789382  0.797306  0.078419  0.800525         0.462774\n",
       "  4280  0.752899  0.776537  0.781276  0.119832  0.789658         0.515763\n",
       "  4281  0.777246  0.794271  0.797998  0.086179  0.794768         0.573983\n",
       "  4282  0.806114  0.848619  0.832331  0.154248  0.817433         0.630181\n",
       "  4283  0.844469  0.861289  0.871548  0.080784  0.857042         0.562907,\n",
       "  0.8845712853636238),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4280  0.752899  0.776537  0.781276  0.119832  0.789658         0.515763\n",
       "  4281  0.777246  0.794271  0.797998  0.086179  0.794768         0.573983\n",
       "  4282  0.806114  0.848619  0.832331  0.154248  0.817433         0.630181\n",
       "  4283  0.844469  0.861289  0.871548  0.080784  0.857042         0.562907\n",
       "  4284  0.864657  0.865950  0.887895  0.052600  0.876367         0.526026,\n",
       "  0.8722682761639333),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4281  0.777246  0.794271  0.797998  0.086179  0.794768         0.573983\n",
       "  4282  0.806114  0.848619  0.832331  0.154248  0.817433         0.630181\n",
       "  4283  0.844469  0.861289  0.871548  0.080784  0.857042         0.562907\n",
       "  4284  0.864657  0.865950  0.887895  0.052600  0.876367         0.526026\n",
       "  4285  0.860671  0.863231  0.882657  0.078271  0.884571         0.458013,\n",
       "  0.908381665648651),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4282  0.806114  0.848619  0.832331  0.154248  0.817433         0.630181\n",
       "  4283  0.844469  0.861289  0.871548  0.080784  0.857042         0.562907\n",
       "  4284  0.864657  0.865950  0.887895  0.052600  0.876367         0.526026\n",
       "  4285  0.860671  0.863231  0.882657  0.078271  0.884571         0.458013\n",
       "  4286  0.853045  0.891853  0.881581  0.197016  0.872268         0.618586,\n",
       "  0.9220532610704335),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4283  0.844469  0.861289  0.871548  0.080784  0.857042         0.562907\n",
       "  4284  0.864657  0.865950  0.887895  0.052600  0.876367         0.526026\n",
       "  4285  0.860671  0.863231  0.882657  0.078271  0.884571         0.458013\n",
       "  4286  0.853045  0.891853  0.881581  0.197016  0.872268         0.618586\n",
       "  4287  0.892108  0.914409  0.920414  0.102833  0.908382         0.544158,\n",
       "  0.9219418044915335),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4284  0.864657  0.865950  0.887895  0.052600  0.876367         0.526026\n",
       "  4285  0.860671  0.863231  0.882657  0.078271  0.884571         0.458013\n",
       "  4286  0.853045  0.891853  0.881581  0.197016  0.872268         0.618586\n",
       "  4287  0.892108  0.914409  0.920414  0.102833  0.908382         0.544158\n",
       "  4288  0.909892  0.908650  0.926435  0.224042  0.922053         0.498446,\n",
       "  0.9275948042191995),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4285  0.860671  0.863231  0.882657  0.078271  0.884571         0.458013\n",
       "  4286  0.853045  0.891853  0.881581  0.197016  0.872268         0.618586\n",
       "  4287  0.892108  0.914409  0.920414  0.102833  0.908382         0.544158\n",
       "  4288  0.909892  0.908650  0.926435  0.224042  0.922053         0.498446\n",
       "  4289  0.906358  0.919458  0.935684  0.089382  0.921942         0.517564,\n",
       "  0.9034498310462864),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4286  0.853045  0.891853  0.881581  0.197016  0.872268         0.618586\n",
       "  4287  0.892108  0.914409  0.920414  0.102833  0.908382         0.544158\n",
       "  4288  0.909892  0.908650  0.926435  0.224042  0.922053         0.498446\n",
       "  4289  0.906358  0.919458  0.935684  0.089382  0.921942         0.517564\n",
       "  4290  0.910871  0.911899  0.912986  0.163536  0.927595         0.418739,\n",
       "  0.911721301664074),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4287  0.892108  0.914409  0.920414  0.102833  0.908382         0.544158\n",
       "  4288  0.909892  0.908650  0.926435  0.224042  0.922053         0.498446\n",
       "  4289  0.906358  0.919458  0.935684  0.089382  0.921942         0.517564\n",
       "  4290  0.910871  0.911899  0.912986  0.163536  0.927595         0.418739\n",
       "  4291  0.883691  0.897437  0.903932  0.129785  0.903450         0.526248,\n",
       "  0.9227896451066024),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4288  0.909892  0.908650  0.926435  0.224042  0.922053         0.498446\n",
       "  4289  0.906358  0.919458  0.935684  0.089382  0.921942         0.517564\n",
       "  4290  0.910871  0.911899  0.912986  0.163536  0.927595         0.418739\n",
       "  4291  0.883691  0.897437  0.903932  0.129785  0.903450         0.526248\n",
       "  4292  0.892786  0.904256  0.919526  0.081326  0.911721         0.535524,\n",
       "  0.9133355029579284),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4289  0.906358  0.919458  0.935684  0.089382  0.921942         0.517564\n",
       "  4290  0.910871  0.911899  0.912986  0.163536  0.927595         0.418739\n",
       "  4291  0.883691  0.897437  0.903932  0.129785  0.903450         0.526248\n",
       "  4292  0.892786  0.904256  0.919526  0.081326  0.911721         0.535524\n",
       "  4293  0.902492  0.900091  0.914393  0.085218  0.922790         0.467461,\n",
       "  0.913781790452646),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4290  0.910871  0.911899  0.912986  0.163536  0.927595         0.418739\n",
       "  4291  0.883691  0.897437  0.903932  0.129785  0.903450         0.526248\n",
       "  4292  0.892786  0.904256  0.919526  0.081326  0.911721         0.535524\n",
       "  4293  0.902492  0.900091  0.914393  0.085218  0.922790         0.467461\n",
       "  4294  0.894806  0.901899  0.917915  0.076743  0.913336         0.500296,\n",
       "  0.8692632330363963),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4291  0.883691  0.897437  0.903932  0.129785  0.903450         0.526248\n",
       "  4292  0.892786  0.904256  0.919526  0.081326  0.911721         0.535524\n",
       "  4293  0.902492  0.900091  0.914393  0.085218  0.922790         0.467461\n",
       "  4294  0.894806  0.901899  0.917915  0.076743  0.913336         0.500296\n",
       "  4295  0.903246  0.907231  0.877389  0.299674  0.913782         0.351170,\n",
       "  0.8922327945303323),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4292  0.892786  0.904256  0.919526  0.081326  0.911721         0.535524\n",
       "  4293  0.902492  0.900091  0.914393  0.085218  0.922790         0.467461\n",
       "  4294  0.894806  0.901899  0.917915  0.076743  0.913336         0.500296\n",
       "  4295  0.903246  0.907231  0.877389  0.299674  0.913782         0.351170\n",
       "  4296  0.869449  0.879344  0.885532  0.142989  0.869263         0.574994,\n",
       "  0.8913552153005779),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4293  0.902492  0.900091  0.914393  0.085218  0.922790         0.467461\n",
       "  4294  0.894806  0.901899  0.917915  0.076743  0.913336         0.500296\n",
       "  4295  0.903246  0.907231  0.877389  0.299674  0.913782         0.351170\n",
       "  4296  0.869449  0.879344  0.885532  0.142989  0.869263         0.574994\n",
       "  4297  0.883917  0.887528  0.894909  0.127469  0.892233         0.495905,\n",
       "  0.918133179067431),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4294  0.894806  0.901899  0.917915  0.076743  0.913336         0.500296\n",
       "  4295  0.903246  0.907231  0.877389  0.299674  0.913782         0.351170\n",
       "  4296  0.869449  0.879344  0.885532  0.142989  0.869263         0.574994\n",
       "  4297  0.883917  0.887528  0.894909  0.127469  0.892233         0.495905\n",
       "  4298  0.877859  0.900557  0.885133  0.148804  0.891355         0.587625,\n",
       "  0.9057484519035255),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4295  0.903246  0.907231  0.877389  0.299674  0.913782         0.351170\n",
       "  4296  0.869449  0.879344  0.885532  0.142989  0.869263         0.574994\n",
       "  4297  0.883917  0.887528  0.894909  0.127469  0.892233         0.495905\n",
       "  4298  0.877859  0.900557  0.885133  0.148804  0.891355         0.587625\n",
       "  4299  0.898385  0.894737  0.902598  0.144418  0.918133         0.457742,\n",
       "  0.8624570623645973),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4296  0.869449  0.879344  0.885532  0.142989  0.869263         0.574994\n",
       "  4297  0.883917  0.887528  0.894909  0.127469  0.892233         0.495905\n",
       "  4298  0.877859  0.900557  0.885133  0.148804  0.891355         0.587625\n",
       "  4299  0.898385  0.894737  0.902598  0.144418  0.918133         0.457742\n",
       "  4300  0.878454  0.880236  0.867153  0.634033  0.905748         0.355240,\n",
       "  0.8260610593415236),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4297  0.883917  0.887528  0.894909  0.127469  0.892233         0.495905\n",
       "  4298  0.877859  0.900557  0.885133  0.148804  0.891355         0.587625\n",
       "  4299  0.898385  0.894737  0.902598  0.144418  0.918133         0.457742\n",
       "  4300  0.878454  0.880236  0.867153  0.634033  0.905748         0.355240\n",
       "  4301  0.806140  0.805683  0.806818  0.324581  0.862457         0.378108,\n",
       "  0.8354260005228877),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4298  0.877859  0.900557  0.885133  0.148804  0.891355         0.587625\n",
       "  4299  0.898385  0.894737  0.902598  0.144418  0.918133         0.457742\n",
       "  4300  0.878454  0.880236  0.867153  0.634033  0.905748         0.355240\n",
       "  4301  0.806140  0.805683  0.806818  0.324581  0.862457         0.378108\n",
       "  4302  0.822971  0.827789  0.834702  0.117787  0.826061         0.529875,\n",
       "  0.8546468601244628),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4299  0.898385  0.894737  0.902598  0.144418  0.918133         0.457742\n",
       "  4300  0.878454  0.880236  0.867153  0.634033  0.905748         0.355240\n",
       "  4301  0.806140  0.805683  0.806818  0.324581  0.862457         0.378108\n",
       "  4302  0.822971  0.827789  0.834702  0.117787  0.826061         0.529875\n",
       "  4303  0.821969  0.839565  0.843225  0.166295  0.835426         0.562562,\n",
       "  0.8678574251965239),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4300  0.878454  0.880236  0.867153  0.634033  0.905748         0.355240\n",
       "  4301  0.806140  0.805683  0.806818  0.324581  0.862457         0.378108\n",
       "  4302  0.822971  0.827789  0.834702  0.117787  0.826061         0.529875\n",
       "  4303  0.821969  0.839565  0.843225  0.166295  0.835426         0.562562\n",
       "  4304  0.845464  0.849770  0.867680  0.107218  0.854647         0.542629,\n",
       "  0.8798553420950819),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4301  0.806140  0.805683  0.806818  0.324581  0.862457         0.378108\n",
       "  4302  0.822971  0.827789  0.834702  0.117787  0.826061         0.529875\n",
       "  4303  0.821969  0.839565  0.843225  0.166295  0.835426         0.562562\n",
       "  4304  0.845464  0.849770  0.867680  0.107218  0.854647         0.542629\n",
       "  4305  0.835465  0.859702  0.863977  0.078616  0.867857         0.538607,\n",
       "  0.8630150295894008),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4302  0.822971  0.827789  0.834702  0.117787  0.826061         0.529875\n",
       "  4303  0.821969  0.839565  0.843225  0.166295  0.835426         0.562562\n",
       "  4304  0.845464  0.849770  0.867680  0.107218  0.854647         0.542629\n",
       "  4305  0.835465  0.859702  0.863977  0.078616  0.867857         0.538607\n",
       "  4306  0.845102  0.853547  0.858702  0.126582  0.879855         0.442965,\n",
       "  0.7839153577436335),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4303  0.821969  0.839565  0.843225  0.166295  0.835426         0.562562\n",
       "  4304  0.845464  0.849770  0.867680  0.107218  0.854647         0.542629\n",
       "  4305  0.835465  0.859702  0.863977  0.078616  0.867857         0.538607\n",
       "  4306  0.845102  0.853547  0.858702  0.126582  0.879855         0.442965\n",
       "  4307  0.807471  0.809405  0.786453  0.414157  0.863015         0.236481,\n",
       "  0.7591751808513905),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4304  0.845464  0.849770  0.867680  0.107218  0.854647         0.542629\n",
       "  4305  0.835465  0.859702  0.863977  0.078616  0.867857         0.538607\n",
       "  4306  0.845102  0.853547  0.858702  0.126582  0.879855         0.442965\n",
       "  4307  0.807471  0.809405  0.786453  0.414157  0.863015         0.236481\n",
       "  4308  0.777648  0.775721  0.764628  0.218893  0.783915         0.416765,\n",
       "  0.7404975307428692),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4305  0.835465  0.859702  0.863977  0.078616  0.867857         0.538607\n",
       "  4306  0.845102  0.853547  0.858702  0.126582  0.879855         0.442965\n",
       "  4307  0.807471  0.809405  0.786453  0.414157  0.863015         0.236481\n",
       "  4308  0.777648  0.775721  0.764628  0.218893  0.783915         0.416765\n",
       "  4309  0.735681  0.748131  0.745814  0.329213  0.759175         0.436871,\n",
       "  0.788058352904436),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4306  0.845102  0.853547  0.858702  0.126582  0.879855         0.442965\n",
       "  4307  0.807471  0.809405  0.786453  0.414157  0.863015         0.236481\n",
       "  4308  0.777648  0.775721  0.764628  0.218893  0.783915         0.416765\n",
       "  4309  0.735681  0.748131  0.745814  0.329213  0.759175         0.436871\n",
       "  4310  0.724069  0.770725  0.751496  0.244145  0.740498         0.656552,\n",
       "  0.7479284057207511),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4307  0.807471  0.809405  0.786453  0.414157  0.863015         0.236481\n",
       "  4308  0.777648  0.775721  0.764628  0.218893  0.783915         0.416765\n",
       "  4309  0.735681  0.748131  0.745814  0.329213  0.759175         0.436871\n",
       "  4310  0.724069  0.770725  0.751496  0.244145  0.740498         0.656552\n",
       "  4311  0.756396  0.749809  0.718216  0.282848  0.788058         0.365724,\n",
       "  0.783811161016642),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4308  0.777648  0.775721  0.764628  0.218893  0.783915         0.416765\n",
       "  4309  0.735681  0.748131  0.745814  0.329213  0.759175         0.436871\n",
       "  4310  0.724069  0.770725  0.751496  0.244145  0.740498         0.656552\n",
       "  4311  0.756396  0.749809  0.718216  0.282848  0.788058         0.365724\n",
       "  4312  0.730783  0.775164  0.760866  0.191645  0.747928         0.617821,\n",
       "  0.8013211204772126),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4309  0.735681  0.748131  0.745814  0.329213  0.759175         0.436871\n",
       "  4310  0.724069  0.770725  0.751496  0.244145  0.740498         0.656552\n",
       "  4311  0.756396  0.749809  0.718216  0.282848  0.788058         0.365724\n",
       "  4312  0.730783  0.775164  0.760866  0.191645  0.747928         0.617821\n",
       "  4313  0.740210  0.782639  0.768188  0.137914  0.783811         0.556888,\n",
       "  0.8285678356248132),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4310  0.724069  0.770725  0.751496  0.244145  0.740498         0.656552\n",
       "  4311  0.756396  0.749809  0.718216  0.282848  0.788058         0.365724\n",
       "  4312  0.730783  0.775164  0.760866  0.191645  0.747928         0.617821\n",
       "  4313  0.740210  0.782639  0.768188  0.137914  0.783811         0.556888\n",
       "  4314  0.804072  0.817764  0.829547  0.119265  0.801321         0.589180,\n",
       "  0.8415180278933627),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4311  0.756396  0.749809  0.718216  0.282848  0.788058         0.365724\n",
       "  4312  0.730783  0.775164  0.760866  0.191645  0.747928         0.617821\n",
       "  4313  0.740210  0.782639  0.768188  0.137914  0.783811         0.556888\n",
       "  4314  0.804072  0.817764  0.829547  0.119265  0.801321         0.589180\n",
       "  4315  0.819791  0.820244  0.842587  0.127567  0.828568         0.541765,\n",
       "  0.8235395402032297),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4312  0.730783  0.775164  0.760866  0.191645  0.747928         0.617821\n",
       "  4313  0.740210  0.782639  0.768188  0.137914  0.783811         0.556888\n",
       "  4314  0.804072  0.817764  0.829547  0.119265  0.801321         0.589180\n",
       "  4315  0.819791  0.820244  0.842587  0.127567  0.828568         0.541765\n",
       "  4316  0.818194  0.827058  0.833212  0.098743  0.841518         0.439190,\n",
       "  0.791785022336392),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4313  0.740210  0.782639  0.768188  0.137914  0.783811         0.556888\n",
       "  4314  0.804072  0.817764  0.829547  0.119265  0.801321         0.589180\n",
       "  4315  0.819791  0.820244  0.842587  0.127567  0.828568         0.541765\n",
       "  4316  0.818194  0.827058  0.833212  0.098743  0.841518         0.439190\n",
       "  4317  0.815744  0.810755  0.794732  0.172060  0.823540         0.393502,\n",
       "  0.8094436748703882),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4314  0.804072  0.817764  0.829547  0.119265  0.801321         0.589180\n",
       "  4315  0.819791  0.820244  0.842587  0.127567  0.828568         0.541765\n",
       "  4316  0.818194  0.827058  0.833212  0.098743  0.841518         0.439190\n",
       "  4317  0.815744  0.810755  0.794732  0.172060  0.823540         0.393502\n",
       "  4318  0.787298  0.793288  0.805667  0.096230  0.791785         0.557381,\n",
       "  0.8616909397137433),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4315  0.819791  0.820244  0.842587  0.127567  0.828568         0.541765\n",
       "  4316  0.818194  0.827058  0.833212  0.098743  0.841518         0.439190\n",
       "  4317  0.815744  0.810755  0.794732  0.172060  0.823540         0.393502\n",
       "  4318  0.787298  0.793288  0.805667  0.096230  0.791785         0.557381\n",
       "  4319  0.820484  0.840603  0.841227  0.158387  0.809444         0.672094,\n",
       "  0.8656035238369053),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4316  0.818194  0.827058  0.833212  0.098743  0.841518         0.439190\n",
       "  4317  0.815744  0.810755  0.794732  0.172060  0.823540         0.393502\n",
       "  4318  0.787298  0.793288  0.805667  0.096230  0.791785         0.557381\n",
       "  4319  0.820484  0.840603  0.841227  0.158387  0.809444         0.672094\n",
       "  4320  0.854363  0.848970  0.867199  0.156219  0.861691         0.511792,\n",
       "  0.8847274614401448),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4317  0.815744  0.810755  0.794732  0.172060  0.823540         0.393502\n",
       "  4318  0.787298  0.793288  0.805667  0.096230  0.791785         0.557381\n",
       "  4319  0.820484  0.840603  0.841227  0.158387  0.809444         0.672094\n",
       "  4320  0.854363  0.848970  0.867199  0.156219  0.861691         0.511792\n",
       "  4321  0.839496  0.861156  0.869441  0.098497  0.865604         0.562240,\n",
       "  0.8975439415182029),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4318  0.787298  0.793288  0.805667  0.096230  0.791785         0.557381\n",
       "  4319  0.820484  0.840603  0.841227  0.158387  0.809444         0.672094\n",
       "  4320  0.854363  0.848970  0.867199  0.156219  0.861691         0.511792\n",
       "  4321  0.839496  0.861156  0.869441  0.098497  0.865604         0.562240\n",
       "  4322  0.870813  0.878062  0.897776  0.083124  0.884727         0.541322,\n",
       "  0.8766121668988778),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4319  0.820484  0.840603  0.841227  0.158387  0.809444         0.672094\n",
       "  4320  0.854363  0.848970  0.867199  0.156219  0.861691         0.511792\n",
       "  4321  0.839496  0.861156  0.869441  0.098497  0.865604         0.562240\n",
       "  4322  0.870813  0.878062  0.897776  0.083124  0.884727         0.541322\n",
       "  4323  0.884844  0.880210  0.879955  0.115446  0.897544         0.429395,\n",
       "  0.8820497513485843),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4320  0.854363  0.848970  0.867199  0.156219  0.861691         0.511792\n",
       "  4321  0.839496  0.861156  0.869441  0.098497  0.865604         0.562240\n",
       "  4322  0.870813  0.878062  0.897776  0.083124  0.884727         0.541322\n",
       "  4323  0.884844  0.880210  0.879955  0.115446  0.897544         0.429395\n",
       "  4324  0.854243  0.858879  0.875816  0.077384  0.876612         0.516850,\n",
       "  0.8202294047745606),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4321  0.839496  0.861156  0.869441  0.098497  0.865604         0.562240\n",
       "  4322  0.870813  0.878062  0.897776  0.083124  0.884727         0.541322\n",
       "  4323  0.884844  0.880210  0.879955  0.115446  0.897544         0.429395\n",
       "  4324  0.854243  0.858879  0.875816  0.077384  0.876612         0.516850\n",
       "  4325  0.826497  0.839184  0.808587  0.266243  0.882050         0.293788,\n",
       "  0.8224310887567181),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4322  0.870813  0.878062  0.897776  0.083124  0.884727         0.541322\n",
       "  4323  0.884844  0.880210  0.879955  0.115446  0.897544         0.429395\n",
       "  4324  0.854243  0.858879  0.875816  0.077384  0.876612         0.516850\n",
       "  4325  0.826497  0.839184  0.808587  0.266243  0.882050         0.293788\n",
       "  4326  0.803281  0.803292  0.802235  0.154150  0.820229         0.506118,\n",
       "  0.8358053129083947),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4323  0.884844  0.880210  0.879955  0.115446  0.897544         0.429395\n",
       "  4324  0.854243  0.858879  0.875816  0.077384  0.876612         0.516850\n",
       "  4325  0.826497  0.839184  0.808587  0.266243  0.882050         0.293788\n",
       "  4326  0.803281  0.803292  0.802235  0.154150  0.820229         0.506118\n",
       "  4327  0.830341  0.828215  0.836207  0.242543  0.822431         0.543171,\n",
       "  0.93630528760305),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4324  0.854243  0.858879  0.875816  0.077384  0.876612         0.516850\n",
       "  4325  0.826497  0.839184  0.808587  0.266243  0.882050         0.293788\n",
       "  4326  0.803281  0.803292  0.802235  0.154150  0.820229         0.506118\n",
       "  4327  0.830341  0.828215  0.836207  0.242543  0.822431         0.543171\n",
       "  4328  0.819738  0.954508  0.849708  0.533149  0.835805         0.832125,\n",
       "  0.9318644748697695),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4325  0.826497  0.839184  0.808587  0.266243  0.882050         0.293788\n",
       "  4326  0.803281  0.803292  0.802235  0.154150  0.820229         0.506118\n",
       "  4327  0.830341  0.828215  0.836207  0.242543  0.822431         0.543171\n",
       "  4328  0.819738  0.954508  0.849708  0.533149  0.835805         0.832125\n",
       "  4329  0.931209  0.928673  0.922875  0.293786  0.936305         0.484088,\n",
       "  0.9637229514356667),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4326  0.803281  0.803292  0.802235  0.154150  0.820229         0.506118\n",
       "  4327  0.830341  0.828215  0.836207  0.242543  0.822431         0.543171\n",
       "  4328  0.819738  0.954508  0.849708  0.533149  0.835805         0.832125\n",
       "  4329  0.931209  0.928673  0.922875  0.293786  0.936305         0.484088\n",
       "  4330  0.904399  0.946804  0.928941  0.251117  0.931864         0.604475,\n",
       "  0.8968297982168789),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4327  0.830341  0.828215  0.836207  0.242543  0.822431         0.543171\n",
       "  4328  0.819738  0.954508  0.849708  0.533149  0.835805         0.832125\n",
       "  4329  0.931209  0.928673  0.922875  0.293786  0.936305         0.484088\n",
       "  4330  0.904399  0.946804  0.928941  0.251117  0.931864         0.604475\n",
       "  4331  0.944261  0.943547  0.905054  0.291988  0.963723         0.276964,\n",
       "  0.927907394400173),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4328  0.819738  0.954508  0.849708  0.533149  0.835805         0.832125\n",
       "  4329  0.931209  0.928673  0.922875  0.293786  0.936305         0.484088\n",
       "  4330  0.904399  0.946804  0.928941  0.251117  0.931864         0.604475\n",
       "  4331  0.944261  0.943547  0.905054  0.291988  0.963723         0.276964\n",
       "  4332  0.893689  0.920984  0.921031  0.154987  0.896830         0.601885,\n",
       "  0.9416979294039505),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4329  0.931209  0.928673  0.922875  0.293786  0.936305         0.484088\n",
       "  4330  0.904399  0.946804  0.928941  0.251117  0.931864         0.604475\n",
       "  4331  0.944261  0.943547  0.905054  0.291988  0.963723         0.276964\n",
       "  4332  0.893689  0.920984  0.921031  0.154987  0.896830         0.601885\n",
       "  4333  0.915777  0.920831  0.927240  0.114781  0.927907         0.544552,\n",
       "  0.9698596983037622),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4330  0.904399  0.946804  0.928941  0.251117  0.931864         0.604475\n",
       "  4331  0.944261  0.943547  0.905054  0.291988  0.963723         0.276964\n",
       "  4332  0.893689  0.920984  0.921031  0.154987  0.896830         0.601885\n",
       "  4333  0.915777  0.920831  0.927240  0.114781  0.927907         0.544552\n",
       "  4334  0.929717  0.967200  0.951044  0.198716  0.941698         0.592214,\n",
       "  0.9781608926594891),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4331  0.944261  0.943547  0.905054  0.291988  0.963723         0.276964\n",
       "  4332  0.893689  0.920984  0.921031  0.154987  0.896830         0.601885\n",
       "  4333  0.915777  0.920831  0.927240  0.114781  0.927907         0.544552\n",
       "  4334  0.929717  0.967200  0.951044  0.198716  0.941698         0.592214\n",
       "  4335  0.962172  0.976670  0.981404  0.145847  0.969860         0.526347,\n",
       "  0.9798048325680291),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4332  0.893689  0.920984  0.921031  0.154987  0.896830         0.601885\n",
       "  4333  0.915777  0.920831  0.927240  0.114781  0.927907         0.544552\n",
       "  4334  0.929717  0.967200  0.951044  0.198716  0.941698         0.592214\n",
       "  4335  0.962172  0.976670  0.981404  0.145847  0.969860         0.526347\n",
       "  4336  0.971916  0.983745  0.988448  0.120965  0.978161         0.504268,\n",
       "  0.9782725872663207),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4333  0.915777  0.920831  0.927240  0.114781  0.927907         0.544552\n",
       "  4334  0.929717  0.967200  0.951044  0.198716  0.941698         0.592214\n",
       "  4335  0.962172  0.976670  0.981404  0.145847  0.969860         0.526347\n",
       "  4336  0.971916  0.983745  0.988448  0.120965  0.978161         0.504268\n",
       "  4337  0.968088  0.973737  0.964169  0.102340  0.979805         0.493734,\n",
       "  0.9392583513810571),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4334  0.929717  0.967200  0.951044  0.198716  0.941698         0.592214\n",
       "  4335  0.962172  0.976670  0.981404  0.145847  0.969860         0.526347\n",
       "  4336  0.971916  0.983745  0.988448  0.120965  0.978161         0.504268\n",
       "  4337  0.968088  0.973737  0.964169  0.102340  0.979805         0.493734\n",
       "  4338  0.941921  0.948131  0.931326  0.174179  0.978273         0.369425,\n",
       "  0.9410732250978671),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4335  0.962172  0.976670  0.981404  0.145847  0.969860         0.526347\n",
       "  4336  0.971916  0.983745  0.988448  0.120965  0.978161         0.504268\n",
       "  4337  0.968088  0.973737  0.964169  0.102340  0.979805         0.493734\n",
       "  4338  0.941921  0.948131  0.931326  0.174179  0.978273         0.369425\n",
       "  4339  0.928316  0.945114  0.951940  0.047993  0.939258         0.504835,\n",
       "  0.9841784767944037),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4336  0.971916  0.983745  0.988448  0.120965  0.978161         0.504268\n",
       "  4337  0.968088  0.973737  0.964169  0.102340  0.979805         0.493734\n",
       "  4338  0.941921  0.948131  0.931326  0.174179  0.978273         0.369425\n",
       "  4339  0.928316  0.945114  0.951940  0.047993  0.939258         0.504835\n",
       "  4340  0.939235  0.965820  0.948320  0.104459  0.941073         0.641775,\n",
       "  0.9803478189098971),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4337  0.968088  0.973737  0.964169  0.102340  0.979805         0.493734\n",
       "  4338  0.941921  0.948131  0.931326  0.174179  0.978273         0.369425\n",
       "  4339  0.928316  0.945114  0.951940  0.047993  0.939258         0.504835\n",
       "  4340  0.939235  0.965820  0.948320  0.104459  0.941073         0.641775\n",
       "  4341  0.971916  0.975240  0.981411  0.094604  0.984178         0.486111,\n",
       "  0.9757063337536611),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4338  0.941921  0.948131  0.931326  0.174179  0.978273         0.369425\n",
       "  4339  0.928316  0.945114  0.951940  0.047993  0.939258         0.504835\n",
       "  4340  0.939235  0.965820  0.948320  0.104459  0.941073         0.641775\n",
       "  4341  0.971916  0.975240  0.981411  0.094604  0.984178         0.486111\n",
       "  4342  0.959414  0.965408  0.976858  0.106996  0.980348         0.483422,\n",
       "  0.9754979402996786),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4339  0.928316  0.945114  0.951940  0.047993  0.939258         0.504835\n",
       "  4340  0.939235  0.965820  0.948320  0.104459  0.941073         0.641775\n",
       "  4341  0.971916  0.975240  0.981411  0.094604  0.984178         0.486111\n",
       "  4342  0.959414  0.965408  0.976858  0.106996  0.980348         0.483422\n",
       "  4343  0.960296  0.962258  0.980064  0.082804  0.975706         0.498125,\n",
       "  0.9999999999999998),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4340  0.939235  0.965820  0.948320  0.104459  0.941073         0.641775\n",
       "  4341  0.971916  0.975240  0.981411  0.094604  0.984178         0.486111\n",
       "  4342  0.959414  0.965408  0.976858  0.106996  0.980348         0.483422\n",
       "  4343  0.960296  0.962258  0.980064  0.082804  0.975706         0.498125\n",
       "  4344  0.959045  0.992525  0.986619  0.222835  0.975498         0.580077,\n",
       "  0.9887457269895197),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4341  0.971916  0.975240  0.981411  0.094604  0.984178         0.486111\n",
       "  4342  0.959414  0.965408  0.976858  0.106996  0.980348         0.483422\n",
       "  4343  0.960296  0.962258  0.980064  0.082804  0.975706         0.498125\n",
       "  4344  0.959045  0.992525  0.986619  0.222835  0.975498         0.580077\n",
       "  4345  0.986986  0.996186  1.000000  0.137939  1.000000         0.461491,\n",
       "  0.9459900639405505),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4342  0.959414  0.965408  0.976858  0.106996  0.980348         0.483422\n",
       "  4343  0.960296  0.962258  0.980064  0.082804  0.975706         0.498125\n",
       "  4344  0.959045  0.992525  0.986619  0.222835  0.975498         0.580077\n",
       "  4345  0.986986  0.996186  1.000000  0.137939  1.000000         0.461491\n",
       "  4346  0.974052  0.979321  0.956621  0.197583  0.988746         0.357016,\n",
       "  0.9412071604395436),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4343  0.960296  0.962258  0.980064  0.082804  0.975706         0.498125\n",
       "  4344  0.959045  0.992525  0.986619  0.222835  0.975498         0.580077\n",
       "  4345  0.986986  0.996186  1.000000  0.137939  1.000000         0.461491\n",
       "  4346  0.974052  0.979321  0.956621  0.197583  0.988746         0.357016\n",
       "  4347  0.928406  0.932784  0.924771  0.117738  0.945990         0.482953,\n",
       "  0.940619454600055),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4344  0.959045  0.992525  0.986619  0.222835  0.975498         0.580077\n",
       "  4345  0.986986  0.996186  1.000000  0.137939  1.000000         0.461491\n",
       "  4346  0.974052  0.979321  0.956621  0.197583  0.988746         0.357016\n",
       "  4347  0.928406  0.932784  0.924771  0.117738  0.945990         0.482953\n",
       "  4348  0.916907  0.922197  0.929430  0.097265  0.941207         0.496867,\n",
       "  0.8824290786108369),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4345  0.986986  0.996186  1.000000  0.137939  1.000000         0.461491\n",
       "  4346  0.974052  0.979321  0.956621  0.197583  0.988746         0.357016\n",
       "  4347  0.928406  0.932784  0.924771  0.117738  0.945990         0.482953\n",
       "  4348  0.916907  0.922197  0.929430  0.097265  0.941207         0.496867\n",
       "  4349  0.896795  0.896216  0.888587  0.103769  0.940619         0.305827,\n",
       "  0.931641323684038),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4346  0.974052  0.979321  0.956621  0.197583  0.988746         0.357016\n",
       "  4347  0.928406  0.932784  0.924771  0.117738  0.945990         0.482953\n",
       "  4348  0.916907  0.922197  0.929430  0.097265  0.941207         0.496867\n",
       "  4349  0.896795  0.896216  0.888587  0.103769  0.940619         0.305827\n",
       "  4350  0.885989  0.920091  0.915876  0.218129  0.882429         0.662029,\n",
       "  0.8771626511205863),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4347  0.928406  0.932784  0.924771  0.117738  0.945990         0.482953\n",
       "  4348  0.916907  0.922197  0.929430  0.097265  0.941207         0.496867\n",
       "  4349  0.896795  0.896216  0.888587  0.103769  0.940619         0.305827\n",
       "  4350  0.885989  0.920091  0.915876  0.218129  0.882429         0.662029\n",
       "  4351  0.903347  0.916529  0.882273  0.406767  0.931641         0.318137,\n",
       "  0.8647554451939052),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4348  0.916907  0.922197  0.929430  0.097265  0.941207         0.496867\n",
       "  4349  0.896795  0.896216  0.888587  0.103769  0.940619         0.305827\n",
       "  4350  0.885989  0.920091  0.915876  0.218129  0.882429         0.662029\n",
       "  4351  0.903347  0.916529  0.882273  0.406767  0.931641         0.318137\n",
       "  4352  0.884693  0.914556  0.873754  0.245229  0.877163         0.457667,\n",
       "  0.8968670347114045),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4349  0.896795  0.896216  0.888587  0.103769  0.940619         0.305827\n",
       "  4350  0.885989  0.920091  0.915876  0.218129  0.882429         0.662029\n",
       "  4351  0.903347  0.916529  0.882273  0.406767  0.931641         0.318137\n",
       "  4352  0.884693  0.914556  0.873754  0.245229  0.877163         0.457667\n",
       "  4353  0.848697  0.886728  0.865957  0.156219  0.864755         0.605314,\n",
       "  0.8781816635718569),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4350  0.885989  0.920091  0.915876  0.218129  0.882429         0.662029\n",
       "  4351  0.903347  0.916529  0.882273  0.406767  0.931641         0.318137\n",
       "  4352  0.884693  0.914556  0.873754  0.245229  0.877163         0.457667\n",
       "  4353  0.848697  0.886728  0.865957  0.156219  0.864755         0.605314\n",
       "  4354  0.888958  0.894935  0.868485  0.223204  0.896867         0.436846,\n",
       "  0.8971644059815109),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4351  0.903347  0.916529  0.882273  0.406767  0.931641         0.318137\n",
       "  4352  0.884693  0.914556  0.873754  0.245229  0.877163         0.457667\n",
       "  4353  0.848697  0.886728  0.865957  0.156219  0.864755         0.605314\n",
       "  4354  0.888958  0.894935  0.868485  0.223204  0.896867         0.436846\n",
       "  4355  0.875071  0.881792  0.860914  0.167896  0.878182         0.561772,\n",
       "  0.9602418821987266),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4352  0.884693  0.914556  0.873754  0.245229  0.877163         0.457667\n",
       "  4353  0.848697  0.886728  0.865957  0.156219  0.864755         0.605314\n",
       "  4354  0.888958  0.894935  0.868485  0.223204  0.896867         0.436846\n",
       "  4355  0.875071  0.881792  0.860914  0.167896  0.878182         0.561772\n",
       "  4356  0.910879  0.942029  0.937008  0.180953  0.897164         0.708013,\n",
       "  0.9704176655285657),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4353  0.848697  0.886728  0.865957  0.156219  0.864755         0.605314\n",
       "  4354  0.888958  0.894935  0.868485  0.223204  0.896867         0.436846\n",
       "  4355  0.875071  0.881792  0.860914  0.167896  0.878182         0.561772\n",
       "  4356  0.910879  0.942029  0.937008  0.180953  0.897164         0.708013\n",
       "  4357  0.946770  0.955095  0.959548  0.128060  0.960242         0.532564,\n",
       "  0.9612758755329331),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4354  0.888958  0.894935  0.868485  0.223204  0.896867         0.436846\n",
       "  4355  0.875071  0.881792  0.860914  0.167896  0.878182         0.561772\n",
       "  4356  0.910879  0.942029  0.937008  0.180953  0.897164         0.708013\n",
       "  4357  0.946770  0.955095  0.959548  0.128060  0.960242         0.532564\n",
       "  4358  0.944426  0.961937  0.964501  0.123330  0.970418         0.468497,\n",
       "  0.9697407438450216),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4355  0.875071  0.881792  0.860914  0.167896  0.878182         0.561772\n",
       "  4356  0.910879  0.942029  0.937008  0.180953  0.897164         0.708013\n",
       "  4357  0.946770  0.955095  0.959548  0.128060  0.960242         0.532564\n",
       "  4358  0.944426  0.961937  0.964501  0.123330  0.970418         0.468497\n",
       "  4359  0.958352  0.958810  0.961919  0.160949  0.961276         0.526890,\n",
       "  0.9404260420290085),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4356  0.910879  0.942029  0.937008  0.180953  0.897164         0.708013\n",
       "  4357  0.946770  0.955095  0.959548  0.128060  0.960242         0.532564\n",
       "  4358  0.944426  0.961937  0.964501  0.123330  0.970418         0.468497\n",
       "  4359  0.958352  0.958810  0.961919  0.160949  0.961276         0.526890\n",
       "  4360  0.948465  0.946033  0.946905  0.191374  0.969741         0.401593,\n",
       "  0.9146298690956465),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4357  0.946770  0.955095  0.959548  0.128060  0.960242         0.532564\n",
       "  4358  0.944426  0.961937  0.964501  0.123330  0.970418         0.468497\n",
       "  4359  0.958352  0.958810  0.961919  0.160949  0.961276         0.526890\n",
       "  4360  0.948465  0.946033  0.946905  0.191374  0.969741         0.401593\n",
       "  4361  0.893095  0.898429  0.884929  0.199677  0.940426         0.413263,\n",
       "  0.9503042309375556),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4358  0.944426  0.961937  0.964501  0.123330  0.970418         0.468497\n",
       "  4359  0.958352  0.958810  0.961919  0.160949  0.961276         0.526890\n",
       "  4360  0.948465  0.946033  0.946905  0.191374  0.969741         0.401593\n",
       "  4361  0.893095  0.898429  0.884929  0.199677  0.940426         0.413263\n",
       "  4362  0.887007  0.930088  0.891899  0.230496  0.914630         0.617130,\n",
       "  0.9126660791542249),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4359  0.958352  0.958810  0.961919  0.160949  0.961276         0.526890\n",
       "  4360  0.948465  0.946033  0.946905  0.191374  0.969741         0.401593\n",
       "  4361  0.893095  0.898429  0.884929  0.199677  0.940426         0.413263\n",
       "  4362  0.887007  0.930088  0.891899  0.230496  0.914630         0.617130\n",
       "  4363  0.942934  0.945866  0.912775  0.231974  0.950304         0.373989,\n",
       "  0.8823843591132163),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4360  0.948465  0.946033  0.946905  0.191374  0.969741         0.401593\n",
       "  4361  0.893095  0.898429  0.884929  0.199677  0.940426         0.413263\n",
       "  4362  0.887007  0.930088  0.891899  0.230496  0.914630         0.617130\n",
       "  4363  0.942934  0.945866  0.912775  0.231974  0.950304         0.373989\n",
       "  4364  0.862117  0.883449  0.878088  0.429111  0.912666         0.398386,\n",
       "  0.8764115093526674),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4361  0.893095  0.898429  0.884929  0.199677  0.940426         0.413263\n",
       "  4362  0.887007  0.930088  0.891899  0.230496  0.914630         0.617130\n",
       "  4363  0.942934  0.945866  0.912775  0.231974  0.950304         0.373989\n",
       "  4364  0.862117  0.883449  0.878088  0.429111  0.912666         0.398386\n",
       "  4365  0.831450  0.855233  0.854939  0.144073  0.882384         0.479007,\n",
       "  0.9034723098090625),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4362  0.887007  0.930088  0.891899  0.230496  0.914630         0.617130\n",
       "  4363  0.942934  0.945866  0.912775  0.231974  0.950304         0.373989\n",
       "  4364  0.862117  0.883449  0.878088  0.429111  0.912666         0.398386\n",
       "  4365  0.831450  0.855233  0.854939  0.144073  0.882384         0.479007\n",
       "  4366  0.868681  0.886988  0.877291  0.135254  0.876412         0.588563,\n",
       "  0.9440635104936546),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4363  0.942934  0.945866  0.912775  0.231974  0.950304         0.373989\n",
       "  4364  0.862117  0.883449  0.878088  0.429111  0.912666         0.398386\n",
       "  4365  0.831450  0.855233  0.854939  0.144073  0.882384         0.479007\n",
       "  4366  0.868681  0.886988  0.877291  0.135254  0.876412         0.588563\n",
       "  4367  0.882998  0.926819  0.910826  0.121581  0.903472         0.633437,\n",
       "  0.9469421161593561),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4364  0.862117  0.883449  0.878088  0.429111  0.912666         0.398386\n",
       "  4365  0.831450  0.855233  0.854939  0.144073  0.882384         0.479007\n",
       "  4366  0.868681  0.886988  0.877291  0.135254  0.876412         0.588563\n",
       "  4367  0.882998  0.926819  0.910826  0.121581  0.903472         0.633437\n",
       "  4368  0.928052  0.946188  0.955798  0.064672  0.944064         0.508363,\n",
       "  0.960650933198919),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4365  0.831450  0.855233  0.854939  0.144073  0.882384         0.479007\n",
       "  4366  0.868681  0.886988  0.877291  0.135254  0.876412         0.588563\n",
       "  4367  0.882998  0.926819  0.910826  0.121581  0.903472         0.633437\n",
       "  4368  0.928052  0.946188  0.955798  0.064672  0.944064         0.508363\n",
       "  4369  0.933689  0.943959  0.960301  0.057749  0.946942         0.544281,\n",
       "  0.936610156752997),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4366  0.868681  0.886988  0.877291  0.135254  0.876412         0.588563\n",
       "  4367  0.882998  0.926819  0.910826  0.121581  0.903472         0.633437\n",
       "  4368  0.928052  0.946188  0.955798  0.064672  0.944064         0.508363\n",
       "  4369  0.933689  0.943959  0.960301  0.057749  0.946942         0.544281\n",
       "  4370  0.947418  0.943165  0.940516  0.123872  0.960651         0.419084,\n",
       "  0.9374507375161565),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4367  0.882998  0.926819  0.910826  0.121581  0.903472         0.633437\n",
       "  4368  0.928052  0.946188  0.955798  0.064672  0.944064         0.508363\n",
       "  4369  0.933689  0.943959  0.960301  0.057749  0.946942         0.544281\n",
       "  4370  0.947418  0.943165  0.940516  0.123872  0.960651         0.419084\n",
       "  4371  0.918105  0.925000  0.934028  0.104139  0.936610         0.501604,\n",
       "  0.9299826260437485),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4368  0.928052  0.946188  0.955798  0.064672  0.944064         0.508363\n",
       "  4369  0.933689  0.943959  0.960301  0.057749  0.946942         0.544281\n",
       "  4370  0.947418  0.943165  0.940516  0.123872  0.960651         0.419084\n",
       "  4371  0.918105  0.925000  0.934028  0.104139  0.936610         0.501604\n",
       "  4372  0.918414  0.923150  0.937851  0.054325  0.937451         0.474048,\n",
       "  0.9103007212157064),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4369  0.933689  0.943959  0.960301  0.057749  0.946942         0.544281\n",
       "  4370  0.947418  0.943165  0.940516  0.123872  0.960651         0.419084\n",
       "  4371  0.918105  0.925000  0.934028  0.104139  0.936610         0.501604\n",
       "  4372  0.918414  0.923150  0.937851  0.054325  0.937451         0.474048\n",
       "  4373  0.904760  0.912510  0.921581  0.107538  0.929983         0.433541,\n",
       "  0.9161768721291039),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4370  0.947418  0.943165  0.940516  0.123872  0.960651         0.419084\n",
       "  4371  0.918105  0.925000  0.934028  0.104139  0.936610         0.501604\n",
       "  4372  0.918414  0.923150  0.937851  0.054325  0.937451         0.474048\n",
       "  4373  0.904760  0.912510  0.921581  0.107538  0.929983         0.433541\n",
       "  4374  0.888657  0.900076  0.903895  0.205047  0.910301         0.518304,\n",
       "  0.906388137092544),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4371  0.918105  0.925000  0.934028  0.104139  0.936610         0.501604\n",
       "  4372  0.918414  0.923150  0.937851  0.054325  0.937451         0.474048\n",
       "  4373  0.904760  0.912510  0.921581  0.107538  0.929983         0.433541\n",
       "  4374  0.888657  0.900076  0.903895  0.205047  0.910301         0.518304\n",
       "  4375  0.904858  0.916247  0.908616  0.176889  0.916177         0.466351,\n",
       "  0.8057766909422426),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4372  0.918414  0.923150  0.937851  0.054325  0.937451         0.474048\n",
       "  4373  0.904760  0.912510  0.921581  0.107538  0.929983         0.433541\n",
       "  4374  0.888657  0.900076  0.903895  0.205047  0.910301         0.518304\n",
       "  4375  0.904858  0.916247  0.908616  0.176889  0.916177         0.466351\n",
       "  4376  0.884219  0.880976  0.813900  0.505951  0.906388         0.165137,\n",
       "  0.8042517054924436),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4373  0.904760  0.912510  0.921581  0.107538  0.929983         0.433541\n",
       "  4374  0.888657  0.900076  0.903895  0.205047  0.910301         0.518304\n",
       "  4375  0.904858  0.916247  0.908616  0.176889  0.916177         0.466351\n",
       "  4376  0.884219  0.880976  0.813900  0.505951  0.906388         0.165137\n",
       "  4377  0.783493  0.810618  0.802461  0.252299  0.805777         0.493758,\n",
       "  0.7961216532194275),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4374  0.888657  0.900076  0.903895  0.205047  0.910301         0.518304\n",
       "  4375  0.904858  0.916247  0.908616  0.176889  0.916177         0.466351\n",
       "  4376  0.884219  0.880976  0.813900  0.505951  0.906388         0.165137\n",
       "  4377  0.783493  0.810618  0.802461  0.252299  0.805777         0.493758\n",
       "  4378  0.789634  0.788783  0.787793  0.133529  0.804252         0.471852,\n",
       "  0.8194707651268012),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4375  0.904858  0.916247  0.908616  0.176889  0.916177         0.466351\n",
       "  4376  0.884219  0.880976  0.813900  0.505951  0.906388         0.165137\n",
       "  4377  0.783493  0.810618  0.802461  0.252299  0.805777         0.493758\n",
       "  4378  0.789634  0.788783  0.787793  0.133529  0.804252         0.471852\n",
       "  4379  0.747346  0.794660  0.747929  0.314456  0.796122         0.576253,\n",
       "  0.8409453029368092),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4376  0.884219  0.880976  0.813900  0.505951  0.906388         0.165137\n",
       "  4377  0.783493  0.810618  0.802461  0.252299  0.805777         0.493758\n",
       "  4378  0.789634  0.788783  0.787793  0.133529  0.804252         0.471852\n",
       "  4379  0.747346  0.794660  0.747929  0.314456  0.796122         0.576253\n",
       "  4380  0.793786  0.820435  0.803214  0.183959  0.819471         0.570036,\n",
       "  0.8652017326886228),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4377  0.783493  0.810618  0.802461  0.252299  0.805777         0.493758\n",
       "  4378  0.789634  0.788783  0.787793  0.133529  0.804252         0.471852\n",
       "  4379  0.747346  0.794660  0.747929  0.314456  0.796122         0.576253\n",
       "  4380  0.793786  0.820435  0.803214  0.183959  0.819471         0.570036\n",
       "  4381  0.844635  0.858341  0.867913  0.185684  0.840945         0.579262,\n",
       "  0.8277569934763387),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4378  0.789634  0.788783  0.787793  0.133529  0.804252         0.471852\n",
       "  4379  0.747346  0.794660  0.747929  0.314456  0.796122         0.576253\n",
       "  4380  0.793786  0.820435  0.803214  0.183959  0.819471         0.570036\n",
       "  4381  0.844635  0.858341  0.867913  0.185684  0.840945         0.579262\n",
       "  4382  0.848945  0.864005  0.834830  0.221701  0.865202         0.374630,\n",
       "  0.8375087300463042),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4379  0.747346  0.794660  0.747929  0.314456  0.796122         0.576253\n",
       "  4380  0.793786  0.820435  0.803214  0.183959  0.819471         0.570036\n",
       "  4381  0.844635  0.858341  0.867913  0.185684  0.840945         0.579262\n",
       "  4382  0.848945  0.864005  0.834830  0.221701  0.865202         0.374630\n",
       "  4383  0.783531  0.833722  0.813547  0.187950  0.827757         0.531157,\n",
       "  0.7854995972716166),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4380  0.793786  0.820435  0.803214  0.183959  0.819471         0.570036\n",
       "  4381  0.844635  0.858341  0.867913  0.185684  0.840945         0.579262\n",
       "  4382  0.848945  0.864005  0.834830  0.221701  0.865202         0.374630\n",
       "  4383  0.783531  0.833722  0.813547  0.187950  0.827757         0.531157\n",
       "  4384  0.769967  0.775614  0.785204  0.231753  0.837509         0.326327,\n",
       "  0.776000735625322),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4381  0.844635  0.858341  0.867913  0.185684  0.840945         0.579262\n",
       "  4382  0.848945  0.864005  0.834830  0.221701  0.865202         0.374630\n",
       "  4383  0.783531  0.833722  0.813547  0.187950  0.827757         0.531157\n",
       "  4384  0.769967  0.775614  0.785204  0.231753  0.837509         0.326327\n",
       "  4385  0.775069  0.790992  0.783443  0.150627  0.785500         0.467313,\n",
       "  0.7440827818300542),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4382  0.848945  0.864005  0.834830  0.221701  0.865202         0.374630\n",
       "  4383  0.783531  0.833722  0.813547  0.187950  0.827757         0.531157\n",
       "  4384  0.769967  0.775614  0.785204  0.231753  0.837509         0.326327\n",
       "  4385  0.775069  0.790992  0.783443  0.150627  0.785500         0.467313\n",
       "  4386  0.768671  0.783555  0.747989  0.164595  0.776001         0.392960,\n",
       "  0.6932860592009382),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4383  0.783531  0.833722  0.813547  0.187950  0.827757         0.531157\n",
       "  4384  0.769967  0.775614  0.785204  0.231753  0.837509         0.326327\n",
       "  4385  0.775069  0.790992  0.783443  0.150627  0.785500         0.467313\n",
       "  4386  0.768671  0.783555  0.747989  0.164595  0.776001         0.392960\n",
       "  4387  0.715893  0.735477  0.700674  0.410831  0.744083         0.330348,\n",
       "  0.6974515331245172),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4384  0.769967  0.775614  0.785204  0.231753  0.837509         0.326327\n",
       "  4385  0.775069  0.790992  0.783443  0.150627  0.785500         0.467313\n",
       "  4386  0.768671  0.783555  0.747989  0.164595  0.776001         0.392960\n",
       "  4387  0.715893  0.735477  0.700674  0.410831  0.744083         0.330348\n",
       "  4388  0.610631  0.674729  0.620011  0.573774  0.693286         0.512631,\n",
       "  0.6433522919605381),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4385  0.775069  0.790992  0.783443  0.150627  0.785500         0.467313\n",
       "  4386  0.768671  0.783555  0.747989  0.164595  0.776001         0.392960\n",
       "  4387  0.715893  0.735477  0.700674  0.410831  0.744083         0.330348\n",
       "  4388  0.610631  0.674729  0.620011  0.573774  0.693286         0.512631\n",
       "  4389  0.646921  0.652761  0.646141  0.338008  0.697452         0.319395,\n",
       "  0.6806112206188362),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4386  0.768671  0.783555  0.747989  0.164595  0.776001         0.392960\n",
       "  4387  0.715893  0.735477  0.700674  0.410831  0.744083         0.330348\n",
       "  4388  0.610631  0.674729  0.620011  0.573774  0.693286         0.512631\n",
       "  4389  0.646921  0.652761  0.646141  0.338008  0.697452         0.319395\n",
       "  4390  0.679429  0.705683  0.657814  0.382623  0.643352         0.622385,\n",
       "  0.6788408432484612),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4387  0.715893  0.735477  0.700674  0.410831  0.744083         0.330348\n",
       "  4388  0.610631  0.674729  0.620011  0.573774  0.693286         0.512631\n",
       "  4389  0.646921  0.652761  0.646141  0.338008  0.697452         0.319395\n",
       "  4390  0.679429  0.705683  0.657814  0.382623  0.643352         0.622385\n",
       "  4391  0.691011  0.702669  0.684779  0.267056  0.680611         0.492944,\n",
       "  0.7408546023935312),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4388  0.610631  0.674729  0.620011  0.573774  0.693286         0.512631\n",
       "  4389  0.646921  0.652761  0.646141  0.338008  0.697452         0.319395\n",
       "  4390  0.679429  0.705683  0.657814  0.382623  0.643352         0.622385\n",
       "  4391  0.691011  0.702669  0.684779  0.267056  0.680611         0.492944\n",
       "  4392  0.670500  0.714271  0.681009  0.270382  0.678841         0.704485,\n",
       "  0.7766926381918016),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4389  0.646921  0.652761  0.646141  0.338008  0.697452         0.319395\n",
       "  4390  0.679429  0.705683  0.657814  0.382623  0.643352         0.622385\n",
       "  4391  0.691011  0.702669  0.684779  0.267056  0.680611         0.492944\n",
       "  4392  0.670500  0.714271  0.681009  0.270382  0.678841         0.704485\n",
       "  4393  0.733767  0.751998  0.734886  0.313963  0.740855         0.617673,\n",
       "  0.8091238248375061),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4390  0.679429  0.705683  0.657814  0.382623  0.643352         0.622385\n",
       "  4391  0.691011  0.702669  0.684779  0.267056  0.680611         0.492944\n",
       "  4392  0.670500  0.714271  0.681009  0.270382  0.678841         0.704485\n",
       "  4393  0.733767  0.751998  0.734886  0.313963  0.740855         0.617673\n",
       "  4394  0.788580  0.787948  0.769419  0.525191  0.776693         0.606374,\n",
       "  0.9602418821987266),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4391  0.691011  0.702669  0.684779  0.267056  0.680611         0.492944\n",
       "  4392  0.670500  0.714271  0.681009  0.270382  0.678841         0.704485\n",
       "  4393  0.733767  0.751998  0.734886  0.313963  0.740855         0.617673\n",
       "  4394  0.788580  0.787948  0.769419  0.525191  0.776693         0.606374\n",
       "  4395  1.000000  1.000000  0.934826  1.000000  0.809124         1.000000,\n",
       "  0.8801157000218478),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4392  0.670500  0.714271  0.681009  0.270382  0.678841         0.704485\n",
       "  4393  0.733767  0.751998  0.734886  0.313963  0.740855         0.617673\n",
       "  4394  0.788580  0.787948  0.769419  0.525191  0.776693         0.606374\n",
       "  4395  1.000000  1.000000  0.934826  1.000000  0.809124         1.000000\n",
       "  4396  0.901008  0.954503  0.884365  0.595724  0.960242         0.233077,\n",
       "  0.8855532844715548),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4393  0.733767  0.751998  0.734886  0.313963  0.740855         0.617673\n",
       "  4394  0.788580  0.787948  0.769419  0.525191  0.776693         0.606374\n",
       "  4395  1.000000  1.000000  0.934826  1.000000  0.809124         1.000000\n",
       "  4396  0.901008  0.954503  0.884365  0.595724  0.960242         0.233077\n",
       "  4397  0.866676  0.884596  0.848429  0.500802  0.880116         0.516850,\n",
       "  0.8248858856904777),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4394  0.788580  0.787948  0.769419  0.525191  0.776693         0.606374\n",
       "  4395  1.000000  1.000000  0.934826  1.000000  0.809124         1.000000\n",
       "  4396  0.901008  0.954503  0.884365  0.595724  0.960242         0.233077\n",
       "  4397  0.866676  0.884596  0.848429  0.500802  0.880116         0.516850\n",
       "  4398  0.877105  0.874088  0.831594  0.443967  0.885553         0.297612,\n",
       "  0.8289768866250051),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4395  1.000000  1.000000  0.934826  1.000000  0.809124         1.000000\n",
       "  4396  0.901008  0.954503  0.884365  0.595724  0.960242         0.233077\n",
       "  4397  0.866676  0.884596  0.848429  0.500802  0.880116         0.516850\n",
       "  4398  0.877105  0.874088  0.831594  0.443967  0.885553         0.297612\n",
       "  4399  0.806005  0.812923  0.804109  0.316427  0.824886         0.512384,\n",
       "  0.8623008862880763),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4396  0.901008  0.954503  0.884365  0.595724  0.960242         0.233077\n",
       "  4397  0.866676  0.884596  0.848429  0.500802  0.880116         0.516850\n",
       "  4398  0.877105  0.874088  0.831594  0.443967  0.885553         0.297612\n",
       "  4399  0.806005  0.812923  0.804109  0.316427  0.824886         0.512384\n",
       "  4400  0.834014  0.848597  0.856850  0.247101  0.828977         0.609335,\n",
       "  0.8198945738587431),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4397  0.866676  0.884596  0.848429  0.500802  0.880116         0.516850\n",
       "  4398  0.877105  0.874088  0.831594  0.443967  0.885553         0.297612\n",
       "  4399  0.806005  0.812923  0.804109  0.316427  0.824886         0.512384\n",
       "  4400  0.834014  0.848597  0.856850  0.247101  0.828977         0.609335\n",
       "  4401  0.813672  0.837239  0.821825  0.301177  0.862301         0.358175,\n",
       "  0.7533585071673632),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4398  0.877105  0.874088  0.831594  0.443967  0.885553         0.297612\n",
       "  4399  0.806005  0.812923  0.804109  0.316427  0.824886         0.512384\n",
       "  4400  0.834014  0.848597  0.856850  0.247101  0.828977         0.609335\n",
       "  4401  0.813672  0.837239  0.821825  0.301177  0.862301         0.358175\n",
       "  4402  0.802369  0.804474  0.750156  0.372498  0.819895         0.278148,\n",
       "  0.7707642699009425),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4399  0.806005  0.812923  0.804109  0.316427  0.824886         0.512384\n",
       "  4400  0.834014  0.848597  0.856850  0.247101  0.828977         0.609335\n",
       "  4401  0.813672  0.837239  0.821825  0.301177  0.862301         0.358175\n",
       "  4402  0.802369  0.804474  0.750156  0.372498  0.819895         0.278148\n",
       "  4403  0.721220  0.756926  0.750359  0.224485  0.753359         0.556542,\n",
       "  0.7875081067106591),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4400  0.834014  0.848597  0.856850  0.247101  0.828977         0.609335\n",
       "  4401  0.813672  0.837239  0.821825  0.301177  0.862301         0.358175\n",
       "  4402  0.802369  0.804474  0.750156  0.372498  0.819895         0.278148\n",
       "  4403  0.721220  0.756926  0.750359  0.224485  0.753359         0.556542\n",
       "  4404  0.783101  0.783371  0.784038  0.221849  0.770764         0.554347,\n",
       "  0.8033071511534791),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4401  0.813672  0.837239  0.821825  0.301177  0.862301         0.358175\n",
       "  4402  0.802369  0.804474  0.750156  0.372498  0.819895         0.278148\n",
       "  4403  0.721220  0.756926  0.750359  0.224485  0.753359         0.556542\n",
       "  4404  0.783101  0.783371  0.784038  0.221849  0.770764         0.554347\n",
       "  4405  0.767412  0.784073  0.773263  0.209925  0.787508         0.551213,\n",
       "  0.7262604702164426),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4402  0.802369  0.804474  0.750156  0.372498  0.819895         0.278148\n",
       "  4403  0.721220  0.756926  0.750359  0.224485  0.753359         0.556542\n",
       "  4404  0.783101  0.783371  0.784038  0.221849  0.770764         0.554347\n",
       "  4405  0.767412  0.784073  0.773263  0.209925  0.787508         0.551213\n",
       "  4406  0.763185  0.767193  0.733802  0.275925  0.803307         0.243290,\n",
       "  0.698872336724071),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4403  0.721220  0.756926  0.750359  0.224485  0.753359         0.556542\n",
       "  4404  0.783101  0.783371  0.784038  0.221849  0.770764         0.554347\n",
       "  4405  0.767412  0.784073  0.773263  0.209925  0.787508         0.551213\n",
       "  4406  0.763185  0.767193  0.733802  0.275925  0.803307         0.243290\n",
       "  4407  0.716277  0.721190  0.705280  0.286888  0.726260         0.407983,\n",
       "  0.6830285579068849),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4404  0.783101  0.783371  0.784038  0.221849  0.770764         0.554347\n",
       "  4405  0.767412  0.784073  0.773263  0.209925  0.787508         0.551213\n",
       "  4406  0.763185  0.767193  0.733802  0.275925  0.803307         0.243290\n",
       "  4407  0.716277  0.721190  0.705280  0.286888  0.726260         0.407983\n",
       "  4408  0.670274  0.691850  0.666657  0.373705  0.698872         0.446270,\n",
       "  0.6559901171993001),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4405  0.767412  0.784073  0.773263  0.209925  0.787508         0.551213\n",
       "  4406  0.763185  0.767193  0.733802  0.275925  0.803307         0.243290\n",
       "  4407  0.716277  0.721190  0.705280  0.286888  0.726260         0.407983\n",
       "  4408  0.670274  0.691850  0.666657  0.373705  0.698872         0.446270\n",
       "  4409  0.686754  0.689535  0.663082  0.220051  0.683029         0.409143,\n",
       "  0.731690333635123),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4406  0.763185  0.767193  0.733802  0.275925  0.803307         0.243290\n",
       "  4407  0.716277  0.721190  0.705280  0.286888  0.726260         0.407983\n",
       "  4408  0.670274  0.691850  0.666657  0.373705  0.698872         0.446270\n",
       "  4409  0.686754  0.689535  0.663082  0.220051  0.683029         0.409143\n",
       "  4410  0.595146  0.709184  0.621855  0.426180  0.655990         0.749876,\n",
       "  0.759152940116546),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4407  0.716277  0.721190  0.705280  0.286888  0.726260         0.407983\n",
       "  4408  0.670274  0.691850  0.666657  0.373705  0.698872         0.446270\n",
       "  4409  0.686754  0.689535  0.663082  0.220051  0.683029         0.409143\n",
       "  4410  0.595146  0.709184  0.621855  0.426180  0.655990         0.749876\n",
       "  4411  0.723632  0.745065  0.727225  0.217636  0.731690         0.589896,\n",
       "  0.7646797255335624),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4408  0.670274  0.691850  0.666657  0.373705  0.698872         0.446270\n",
       "  4409  0.686754  0.689535  0.663082  0.220051  0.683029         0.409143\n",
       "  4410  0.595146  0.709184  0.621855  0.426180  0.655990         0.749876\n",
       "  4411  0.723632  0.745065  0.727225  0.217636  0.731690         0.589896\n",
       "  4412  0.720000  0.748902  0.743184  0.260010  0.759153         0.517145,\n",
       "  0.7539237342440759),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4409  0.686754  0.689535  0.663082  0.220051  0.683029         0.409143\n",
       "  4410  0.595146  0.709184  0.621855  0.426180  0.655990         0.749876\n",
       "  4411  0.723632  0.745065  0.727225  0.217636  0.731690         0.589896\n",
       "  4412  0.720000  0.748902  0.743184  0.260010  0.759153         0.517145\n",
       "  4413  0.738017  0.756079  0.751511  0.197977  0.764680         0.463143,\n",
       "  0.7626042558620554),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4410  0.595146  0.709184  0.621855  0.426180  0.655990         0.749876\n",
       "  4411  0.723632  0.745065  0.727225  0.217636  0.731690         0.589896\n",
       "  4412  0.720000  0.748902  0.743184  0.260010  0.759153         0.517145\n",
       "  4413  0.738017  0.756079  0.751511  0.197977  0.764680         0.463143\n",
       "  4414  0.739950  0.748520  0.752324  0.189675  0.753924         0.527605,\n",
       "  0.756006478644238),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4411  0.723632  0.745065  0.727225  0.217636  0.731690         0.589896\n",
       "  4412  0.720000  0.748902  0.743184  0.260010  0.759153         0.517145\n",
       "  4413  0.738017  0.756079  0.751511  0.197977  0.764680         0.463143\n",
       "  4414  0.739950  0.748520  0.752324  0.189675  0.753924         0.527605\n",
       "  4415  0.760600  0.765275  0.752301  0.138112  0.762604         0.476934,\n",
       "  0.7234858381265463),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4412  0.720000  0.748902  0.743184  0.260010  0.759153         0.517145\n",
       "  4413  0.738017  0.756079  0.751511  0.197977  0.764680         0.463143\n",
       "  4414  0.739950  0.748520  0.752324  0.189675  0.753924         0.527605\n",
       "  4415  0.760600  0.765275  0.752301  0.138112  0.762604         0.476934\n",
       "  4416  0.721477  0.726911  0.706807  0.195587  0.756006         0.390961,\n",
       "  0.6393207385018895),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4413  0.738017  0.756079  0.751511  0.197977  0.764680         0.463143\n",
       "  4414  0.739950  0.748520  0.752324  0.189675  0.753924         0.527605\n",
       "  4415  0.760600  0.765275  0.752301  0.138112  0.762604         0.476934\n",
       "  4416  0.721477  0.726911  0.706807  0.195587  0.756006         0.390961\n",
       "  4417  0.699195  0.691899  0.646623  0.377056  0.723486         0.219682,\n",
       "  0.6514304541444991),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4414  0.739950  0.748520  0.752324  0.189675  0.753924         0.527605\n",
       "  4415  0.760600  0.765275  0.752301  0.138112  0.762604         0.476934\n",
       "  4416  0.721477  0.726911  0.706807  0.195587  0.756006         0.390961\n",
       "  4417  0.699195  0.691899  0.646623  0.377056  0.723486         0.219682\n",
       "  4418  0.613992  0.681869  0.638356  0.328671  0.639321         0.538978,\n",
       "  0.7494309272845197),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4415  0.760600  0.765275  0.752301  0.138112  0.762604         0.476934\n",
       "  4416  0.721477  0.726911  0.706807  0.195587  0.756006         0.390961\n",
       "  4417  0.699195  0.691899  0.646623  0.377056  0.723486         0.219682\n",
       "  4418  0.613992  0.681869  0.638356  0.328671  0.639321         0.538978\n",
       "  4419  0.691599  0.726903  0.701983  0.291815  0.651430         0.823835,\n",
       "  0.7318170091248899),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4416  0.721477  0.726911  0.706807  0.195587  0.756006         0.390961\n",
       "  4417  0.699195  0.691899  0.646623  0.377056  0.723486         0.219682\n",
       "  4418  0.613992  0.681869  0.638356  0.328671  0.639321         0.538978\n",
       "  4419  0.691599  0.726903  0.701983  0.291815  0.651430         0.823835\n",
       "  4420  0.692541  0.716834  0.721746  0.193370  0.749431         0.440399,\n",
       "  0.6989912763060657),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4417  0.699195  0.691899  0.646623  0.377056  0.723486         0.219682\n",
       "  4418  0.613992  0.681869  0.638356  0.328671  0.639321         0.538978\n",
       "  4419  0.691599  0.726903  0.701983  0.291815  0.651430         0.823835\n",
       "  4420  0.692541  0.716834  0.721746  0.193370  0.749431         0.440399\n",
       "  4421  0.730775  0.726926  0.705121  0.222120  0.731817         0.389949,\n",
       "  0.6434341140619728),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4418  0.613992  0.681869  0.638356  0.328671  0.639321         0.538978\n",
       "  4419  0.691599  0.726903  0.701983  0.291815  0.651430         0.823835\n",
       "  4420  0.692541  0.716834  0.721746  0.193370  0.749431         0.440399\n",
       "  4421  0.730775  0.726926  0.705121  0.222120  0.731817         0.389949\n",
       "  4422  0.679135  0.678505  0.646668  0.267130  0.698991         0.314560,\n",
       "  0.6868669219456722),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4419  0.691599  0.726903  0.701983  0.291815  0.651430         0.823835\n",
       "  4420  0.692541  0.716834  0.721746  0.193370  0.749431         0.440399\n",
       "  4421  0.730775  0.726926  0.705121  0.222120  0.731817         0.389949\n",
       "  4422  0.679135  0.678505  0.646668  0.267130  0.698991         0.314560\n",
       "  4423  0.636462  0.670744  0.649001  0.267598  0.643434         0.642861,\n",
       "  0.7468201923021707),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4420  0.692541  0.716834  0.721746  0.193370  0.749431         0.440399\n",
       "  4421  0.730775  0.726926  0.705121  0.222120  0.731817         0.389949\n",
       "  4422  0.679135  0.678505  0.646668  0.267130  0.698991         0.314560\n",
       "  4423  0.636462  0.670744  0.649001  0.267598  0.643434         0.642861\n",
       "  4424  0.685571  0.720381  0.688632  0.289352  0.686867         0.697651,\n",
       "  0.7603578523822769),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4421  0.730775  0.726926  0.705121  0.222120  0.731817         0.389949\n",
       "  4422  0.679135  0.678505  0.646668  0.267130  0.698991         0.314560\n",
       "  4423  0.636462  0.670744  0.649001  0.267598  0.643434         0.642861\n",
       "  4424  0.685571  0.720381  0.688632  0.289352  0.686867         0.697651\n",
       "  4425  0.720550  0.735156  0.741584  0.190019  0.746820         0.543714,\n",
       "  0.793101629208955),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4422  0.679135  0.678505  0.646668  0.267130  0.698991         0.314560\n",
       "  4423  0.636462  0.670744  0.649001  0.267598  0.643434         0.642861\n",
       "  4424  0.685571  0.720381  0.688632  0.289352  0.686867         0.697651\n",
       "  4425  0.720550  0.735156  0.741584  0.190019  0.746820         0.543714\n",
       "  4426  0.729035  0.768322  0.744790  0.459857  0.760358         0.607411,\n",
       "  0.7882964700963575),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4423  0.636462  0.670744  0.649001  0.267598  0.643434         0.642861\n",
       "  4424  0.685571  0.720381  0.688632  0.289352  0.686867         0.697651\n",
       "  4425  0.720550  0.735156  0.741584  0.190019  0.746820         0.543714\n",
       "  4426  0.729035  0.768322  0.744790  0.459857  0.760358         0.607411\n",
       "  4427  0.773697  0.778528  0.770070  0.222514  0.793102         0.482879,\n",
       "  0.8448131675623511),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4424  0.685571  0.720381  0.688632  0.289352  0.686867         0.697651\n",
       "  4425  0.720550  0.735156  0.741584  0.190019  0.746820         0.543714\n",
       "  4426  0.729035  0.768322  0.744790  0.459857  0.760358         0.607411\n",
       "  4427  0.773697  0.778528  0.770070  0.222514  0.793102         0.482879\n",
       "  4428  0.768460  0.838291  0.798495  0.261242  0.788296         0.686254,\n",
       "  0.8184217909096596),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4425  0.720550  0.735156  0.741584  0.190019  0.746820         0.543714\n",
       "  4426  0.729035  0.768322  0.744790  0.459857  0.760358         0.607411\n",
       "  4427  0.773697  0.778528  0.770070  0.222514  0.793102         0.482879\n",
       "  4428  0.768460  0.838291  0.798495  0.261242  0.788296         0.686254\n",
       "  4429  0.808224  0.815789  0.823579  0.206131  0.844813         0.411289,\n",
       "  0.8602031610049787),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4426  0.729035  0.768322  0.744790  0.459857  0.760358         0.607411\n",
       "  4427  0.773697  0.778528  0.770070  0.222514  0.793102         0.482879\n",
       "  4428  0.768460  0.838291  0.798495  0.261242  0.788296         0.686254\n",
       "  4429  0.808224  0.815789  0.823579  0.206131  0.844813         0.411289\n",
       "  4430  0.810244  0.836712  0.821665  0.147522  0.818422         0.637384,\n",
       "  0.8633198987393473),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4427  0.773697  0.778528  0.770070  0.222514  0.793102         0.482879\n",
       "  4428  0.768460  0.838291  0.798495  0.261242  0.788296         0.686254\n",
       "  4429  0.808224  0.815789  0.823579  0.206131  0.844813         0.411289\n",
       "  4430  0.810244  0.836712  0.821665  0.147522  0.818422         0.637384\n",
       "  4431  0.847642  0.845301  0.846653  0.131829  0.860203         0.509152,\n",
       "  0.8696945396481783),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4428  0.768460  0.838291  0.798495  0.261242  0.788296         0.686254\n",
       "  4429  0.808224  0.815789  0.823579  0.206131  0.844813         0.411289\n",
       "  4430  0.810244  0.836712  0.821665  0.147522  0.818422         0.637384\n",
       "  4431  0.847642  0.845301  0.846653  0.131829  0.860203         0.509152\n",
       "  4432  0.831524  0.845561  0.848589  0.187310  0.863320         0.519957,\n",
       "  0.8890343537084944),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4429  0.808224  0.815789  0.823579  0.206131  0.844813         0.411289\n",
       "  4430  0.810244  0.836712  0.821665  0.147522  0.818422         0.637384\n",
       "  4431  0.847642  0.845301  0.846653  0.131829  0.860203         0.509152\n",
       "  4432  0.831524  0.845561  0.848589  0.187310  0.863320         0.519957\n",
       "  4433  0.868839  0.878909  0.888564  0.247717  0.869695         0.562956,\n",
       "  0.8800264990545379),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4430  0.810244  0.836712  0.821665  0.147522  0.818422         0.637384\n",
       "  4431  0.847642  0.845301  0.846653  0.131829  0.860203         0.509152\n",
       "  4432  0.831524  0.845561  0.848589  0.187310  0.863320         0.519957\n",
       "  4433  0.868839  0.878909  0.888564  0.247717  0.869695         0.562956\n",
       "  4434  0.864461  0.868505  0.883808  0.153706  0.889034         0.468941,\n",
       "  0.8354707200205083),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4431  0.847642  0.845301  0.846653  0.131829  0.860203         0.509152\n",
       "  4432  0.831524  0.845561  0.848589  0.187310  0.863320         0.519957\n",
       "  4433  0.868839  0.878909  0.888564  0.247717  0.869695         0.562956\n",
       "  4434  0.864461  0.868505  0.883808  0.153706  0.889034         0.468941\n",
       "  4435  0.858109  0.855751  0.845441  0.258039  0.880026         0.351046,\n",
       "  0.8510986075038722),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4432  0.831524  0.845561  0.848589  0.187310  0.863320         0.519957\n",
       "  4433  0.868839  0.878909  0.888564  0.247717  0.869695         0.562956\n",
       "  4434  0.864461  0.868505  0.883808  0.153706  0.889034         0.468941\n",
       "  4435  0.858109  0.855751  0.845441  0.258039  0.880026         0.351046\n",
       "  4436  0.821358  0.829901  0.833069  0.183688  0.835471         0.550646,\n",
       "  0.8948735061552984),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4433  0.868839  0.878909  0.888564  0.247717  0.869695         0.562956\n",
       "  4434  0.864461  0.868505  0.883808  0.153706  0.889034         0.468941\n",
       "  4435  0.858109  0.855751  0.845441  0.258039  0.880026         0.351046\n",
       "  4436  0.821358  0.829901  0.833069  0.183688  0.835471         0.550646\n",
       "  4437  0.833633  0.877098  0.863586  0.129440  0.851099         0.643996,\n",
       "  0.8564989703357988),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4434  0.864461  0.868505  0.883808  0.153706  0.889034         0.468941\n",
       "  4435  0.858109  0.855751  0.845441  0.258039  0.880026         0.351046\n",
       "  4436  0.821358  0.829901  0.833069  0.183688  0.835471         0.550646\n",
       "  4437  0.833633  0.877098  0.863586  0.129440  0.851099         0.643996\n",
       "  4438  0.872441  0.870175  0.865377  0.131657  0.894874         0.371546,\n",
       "  0.7986729258491521),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4435  0.858109  0.855751  0.845441  0.258039  0.880026         0.351046\n",
       "  4436  0.821358  0.829901  0.833069  0.183688  0.835471         0.550646\n",
       "  4437  0.833633  0.877098  0.863586  0.129440  0.851099         0.643996\n",
       "  4438  0.872441  0.870175  0.865377  0.131657  0.894874         0.371546\n",
       "  4439  0.808571  0.813097  0.797262  0.184846  0.856499         0.307035,\n",
       "  0.7880955745222165),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4436  0.821358  0.829901  0.833069  0.183688  0.835471         0.550646\n",
       "  4437  0.833633  0.877098  0.863586  0.129440  0.851099         0.643996\n",
       "  4438  0.872441  0.870175  0.865377  0.131657  0.894874         0.371546\n",
       "  4439  0.808571  0.813097  0.797262  0.184846  0.856499         0.307035\n",
       "  4440  0.770238  0.780343  0.773769  0.134022  0.798673         0.463736,\n",
       "  0.7515806319171476),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4437  0.833633  0.877098  0.863586  0.129440  0.851099         0.643996\n",
       "  4438  0.872441  0.870175  0.865377  0.131657  0.894874         0.371546\n",
       "  4439  0.808571  0.813097  0.797262  0.184846  0.856499         0.307035\n",
       "  4440  0.770238  0.780343  0.773769  0.134022  0.798673         0.463736\n",
       "  4441  0.764692  0.758200  0.757140  0.096896  0.788096         0.377714,\n",
       "  0.6888899659652785),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4438  0.872441  0.870175  0.865377  0.131657  0.894874         0.371546\n",
       "  4439  0.808571  0.813097  0.797262  0.184846  0.856499         0.307035\n",
       "  4440  0.770238  0.780343  0.773769  0.134022  0.798673         0.463736\n",
       "  4441  0.764692  0.758200  0.757140  0.096896  0.788096         0.377714\n",
       "  4442  0.714205  0.707692  0.694901  0.192409  0.751581         0.290902,\n",
       "  0.6677352633114069),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4439  0.808571  0.813097  0.797262  0.184846  0.856499         0.307035\n",
       "  4440  0.770238  0.780343  0.773769  0.134022  0.798673         0.463736\n",
       "  4441  0.764692  0.758200  0.757140  0.096896  0.788096         0.377714\n",
       "  4442  0.714205  0.707692  0.694901  0.192409  0.751581         0.290902\n",
       "  4443  0.707024  0.699824  0.664173  0.177825  0.688890         0.428656,\n",
       "  0.6961721478697349),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4440  0.770238  0.780343  0.773769  0.134022  0.798673         0.463736\n",
       "  4441  0.764692  0.758200  0.757140  0.096896  0.788096         0.377714\n",
       "  4442  0.714205  0.707692  0.694901  0.192409  0.751581         0.290902\n",
       "  4443  0.707024  0.699824  0.664173  0.177825  0.688890         0.428656\n",
       "  4444  0.649800  0.672856  0.677157  0.135180  0.667735         0.593127,\n",
       "  0.6510510227450268),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4441  0.764692  0.758200  0.757140  0.096896  0.788096         0.377714\n",
       "  4442  0.714205  0.707692  0.694901  0.192409  0.751581         0.290902\n",
       "  4443  0.707024  0.699824  0.664173  0.177825  0.688890         0.428656\n",
       "  4444  0.649800  0.672856  0.677157  0.135180  0.667735         0.593127\n",
       "  4445  0.680288  0.673688  0.657182  0.183737  0.696172         0.349171,\n",
       "  0.6615837735883077),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4442  0.714205  0.707692  0.694901  0.192409  0.751581         0.290902\n",
       "  4443  0.707024  0.699824  0.664173  0.177825  0.688890         0.428656\n",
       "  4444  0.649800  0.672856  0.677157  0.135180  0.667735         0.593127\n",
       "  4445  0.680288  0.673688  0.657182  0.183737  0.696172         0.349171\n",
       "  4446  0.631466  0.643204  0.649158  0.078222  0.651051         0.533748,\n",
       "  0.6998171142142215),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4443  0.707024  0.699824  0.664173  0.177825  0.688890         0.428656\n",
       "  4444  0.649800  0.672856  0.677157  0.135180  0.667735         0.593127\n",
       "  4445  0.680288  0.673688  0.657182  0.183737  0.696172         0.349171\n",
       "  4446  0.631466  0.643204  0.649158  0.078222  0.651051         0.533748\n",
       "  4447  0.641519  0.676640  0.662299  0.174326  0.661584         0.625617,\n",
       "  0.6658162077443517),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4444  0.649800  0.672856  0.677157  0.135180  0.667735         0.593127\n",
       "  4445  0.680288  0.673688  0.657182  0.183737  0.696172         0.349171\n",
       "  4446  0.631466  0.643204  0.649158  0.078222  0.651051         0.533748\n",
       "  4447  0.641519  0.676640  0.662299  0.174326  0.661584         0.625617\n",
       "  4448  0.689851  0.692197  0.668960  0.172971  0.699817         0.386051,\n",
       "  0.6166039477944047),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4445  0.680288  0.673688  0.657182  0.183737  0.696172         0.349171\n",
       "  4446  0.631466  0.643204  0.649158  0.078222  0.651051         0.533748\n",
       "  4447  0.641519  0.676640  0.662299  0.174326  0.661584         0.625617\n",
       "  4448  0.689851  0.692197  0.668960  0.172971  0.699817         0.386051\n",
       "  4449  0.660704  0.667544  0.620132  0.265947  0.665816         0.335603,\n",
       "  0.537407562224741),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4446  0.631466  0.643204  0.649158  0.078222  0.651051         0.533748\n",
       "  4447  0.641519  0.676640  0.662299  0.174326  0.661584         0.625617\n",
       "  4448  0.689851  0.692197  0.668960  0.172971  0.699817         0.386051\n",
       "  4449  0.660704  0.667544  0.620132  0.265947  0.665816         0.335603\n",
       "  4450  0.595146  0.593471  0.537204  0.466139  0.616604         0.236160,\n",
       "  0.5914994393995912),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4447  0.641519  0.676640  0.662299  0.174326  0.661584         0.625617\n",
       "  4448  0.689851  0.692197  0.668960  0.172971  0.699817         0.386051\n",
       "  4449  0.660704  0.667544  0.620132  0.265947  0.665816         0.335603\n",
       "  4450  0.595146  0.593471  0.537204  0.466139  0.616604         0.236160\n",
       "  4451  0.511194  0.560305  0.531616  0.319703  0.537408         0.678212,\n",
       "  0.5358008588107273),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4448  0.689851  0.692197  0.668960  0.172971  0.699817         0.386051\n",
       "  4449  0.660704  0.667544  0.620132  0.265947  0.665816         0.335603\n",
       "  4450  0.595146  0.593471  0.537204  0.466139  0.616604         0.236160\n",
       "  4451  0.511194  0.560305  0.531616  0.319703  0.537408         0.678212\n",
       "  4452  0.561236  0.552250  0.537526  0.502896  0.591499         0.314091,\n",
       "  0.469071260534335),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4449  0.660704  0.667544  0.620132  0.265947  0.665816         0.335603\n",
       "  4450  0.595146  0.593471  0.537204  0.466139  0.616604         0.236160\n",
       "  4451  0.511194  0.560305  0.531616  0.319703  0.537408         0.678212\n",
       "  4452  0.561236  0.552250  0.537526  0.502896  0.591499         0.314091\n",
       "  4453  0.434988  0.472159  0.446649  0.661108  0.535801         0.277506,\n",
       "  0.5343950360941092),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4450  0.595146  0.593471  0.537204  0.466139  0.616604         0.236160\n",
       "  4451  0.511194  0.560305  0.531616  0.319703  0.537408         0.678212\n",
       "  4452  0.561236  0.552250  0.537526  0.502896  0.591499         0.314091\n",
       "  4453  0.434988  0.472159  0.446649  0.661108  0.535801         0.277506\n",
       "  4454  0.476312  0.516987  0.477048  0.347640  0.469071         0.715463,\n",
       "  0.46826790138895524),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4451  0.511194  0.560305  0.531616  0.319703  0.537408         0.678212\n",
       "  4452  0.561236  0.552250  0.537526  0.502896  0.591499         0.314091\n",
       "  4453  0.434988  0.472159  0.446649  0.661108  0.535801         0.277506\n",
       "  4454  0.476312  0.516987  0.477048  0.347640  0.469071         0.715463\n",
       "  4455  0.483290  0.494432  0.470277  0.309504  0.534395         0.279505,\n",
       "  0.5008553980018924),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4452  0.561236  0.552250  0.537526  0.502896  0.591499         0.314091\n",
       "  4453  0.434988  0.472159  0.446649  0.661108  0.535801         0.277506\n",
       "  4454  0.476312  0.516987  0.477048  0.347640  0.469071         0.715463\n",
       "  4455  0.483290  0.494432  0.470277  0.309504  0.534395         0.279505\n",
       "  4456  0.427958  0.469710  0.450792  0.267450  0.468268         0.606892,\n",
       "  0.5153230778403999),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4453  0.434988  0.472159  0.446649  0.661108  0.535801         0.277506\n",
       "  4454  0.476312  0.516987  0.477048  0.347640  0.469071         0.715463\n",
       "  4455  0.483290  0.494432  0.470277  0.309504  0.534395         0.279505\n",
       "  4456  0.427958  0.469710  0.450792  0.267450  0.468268         0.606892\n",
       "  4457  0.471038  0.499619  0.499372  0.155800  0.500855         0.546798,\n",
       "  0.5814575914114284),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4454  0.476312  0.516987  0.477048  0.347640  0.469071         0.715463\n",
       "  4455  0.483290  0.494432  0.470277  0.309504  0.534395         0.279505\n",
       "  4456  0.427958  0.469710  0.450792  0.267450  0.468268         0.606892\n",
       "  4457  0.471038  0.499619  0.499372  0.155800  0.500855         0.546798\n",
       "  4458  0.489703  0.558246  0.485998  0.303813  0.515323         0.718151,\n",
       "  0.4947484938857021),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4455  0.483290  0.494432  0.470277  0.309504  0.534395         0.279505\n",
       "  4456  0.427958  0.469710  0.450792  0.267450  0.468268         0.606892\n",
       "  4457  0.471038  0.499619  0.499372  0.155800  0.500855         0.546798\n",
       "  4458  0.489703  0.558246  0.485998  0.303813  0.515323         0.718151\n",
       "  4459  0.523115  0.529111  0.477629  0.425243  0.581458         0.211244,\n",
       "  0.4785849989263451),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4456  0.427958  0.469710  0.450792  0.267450  0.468268         0.606892\n",
       "  4457  0.471038  0.499619  0.499372  0.155800  0.500855         0.546798\n",
       "  4458  0.489703  0.558246  0.485998  0.303813  0.515323         0.718151\n",
       "  4459  0.523115  0.529111  0.477629  0.425243  0.581458         0.211244\n",
       "  4460  0.452259  0.472136  0.461983  0.329410  0.494748         0.445209,\n",
       "  0.4402624424563757),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4457  0.471038  0.499619  0.499372  0.155800  0.500855         0.546798\n",
       "  4458  0.489703  0.558246  0.485998  0.303813  0.515323         0.718151\n",
       "  4459  0.523115  0.529111  0.477629  0.425243  0.581458         0.211244\n",
       "  4460  0.452259  0.472136  0.461983  0.329410  0.494748         0.445209\n",
       "  4461  0.418870  0.442607  0.438028  0.319678  0.478585         0.371719,\n",
       "  0.4625850440326382),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4458  0.489703  0.558246  0.485998  0.303813  0.515323         0.718151\n",
       "  4459  0.523115  0.529111  0.477629  0.425243  0.581458         0.211244\n",
       "  4460  0.452259  0.472136  0.461983  0.329410  0.494748         0.445209\n",
       "  4461  0.418870  0.442607  0.438028  0.319678  0.478585         0.371719\n",
       "  4462  0.460119  0.459817  0.450548  0.278265  0.440262         0.572849,\n",
       "  0.45330933357207526),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4459  0.523115  0.529111  0.477629  0.425243  0.581458         0.211244\n",
       "  4460  0.452259  0.472136  0.461983  0.329410  0.494748         0.445209\n",
       "  4461  0.418870  0.442607  0.438028  0.319678  0.478585         0.371719\n",
       "  4462  0.460119  0.459817  0.450548  0.278265  0.440262         0.572849\n",
       "  4463  0.425004  0.459512  0.454563  0.344093  0.462585         0.468053,\n",
       "  0.4414079965067019),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4460  0.452259  0.472136  0.461983  0.329410  0.494748         0.445209\n",
       "  4461  0.418870  0.442607  0.438028  0.319678  0.478585         0.371719\n",
       "  4462  0.460119  0.459817  0.450548  0.278265  0.440262         0.572849\n",
       "  4463  0.425004  0.459512  0.454563  0.344093  0.462585         0.468053\n",
       "  4464  0.398291  0.431831  0.401332  0.405214  0.453309         0.459345,\n",
       "  0.4913120400091626),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4461  0.418870  0.442607  0.438028  0.319678  0.478585         0.371719\n",
       "  4462  0.460119  0.459817  0.450548  0.278265  0.440262         0.572849\n",
       "  4463  0.425004  0.459512  0.454563  0.344093  0.462585         0.468053\n",
       "  4464  0.398291  0.431831  0.401332  0.405214  0.453309         0.459345\n",
       "  4465  0.442109  0.481083  0.459831  0.260774  0.441408         0.664323,\n",
       "  0.46567940714145073),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4462  0.460119  0.459817  0.450548  0.278265  0.440262         0.572849\n",
       "  4463  0.425004  0.459512  0.454563  0.344093  0.462585         0.468053\n",
       "  4464  0.398291  0.431831  0.401332  0.405214  0.453309         0.459345\n",
       "  4465  0.442109  0.481083  0.459831  0.260774  0.441408         0.664323\n",
       "  4466  0.450225  0.458543  0.464873  0.181249  0.491312         0.413805,\n",
       "  0.49407907008199836),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4463  0.425004  0.459512  0.454563  0.344093  0.462585         0.468053\n",
       "  4464  0.398291  0.431831  0.401332  0.405214  0.453309         0.459345\n",
       "  4465  0.442109  0.481083  0.459831  0.260774  0.441408         0.664323\n",
       "  4466  0.450225  0.458543  0.464873  0.181249  0.491312         0.413805\n",
       "  4467  0.478008  0.468001  0.479962  0.160235  0.465679         0.593003,\n",
       "  0.43010164000947215),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4464  0.398291  0.431831  0.401332  0.405214  0.453309         0.459345\n",
       "  4465  0.442109  0.481083  0.459831  0.260774  0.441408         0.664323\n",
       "  4466  0.450225  0.458543  0.464873  0.181249  0.491312         0.413805\n",
       "  4467  0.478008  0.468001  0.479962  0.160235  0.465679         0.593003\n",
       "  4468  0.448017  0.444632  0.431865  0.239144  0.494079         0.286634,\n",
       "  0.40547315772406134),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4465  0.442109  0.481083  0.459831  0.260774  0.441408         0.664323\n",
       "  4466  0.450225  0.458543  0.464873  0.181249  0.491312         0.413805\n",
       "  4467  0.478008  0.468001  0.479962  0.160235  0.465679         0.593003\n",
       "  4468  0.448017  0.444632  0.431865  0.239144  0.494079         0.286634\n",
       "  4469  0.396829  0.412471  0.406668  0.254048  0.430102         0.417135,\n",
       "  0.3841622938704148),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4466  0.450225  0.458543  0.464873  0.181249  0.491312         0.413805\n",
       "  4467  0.478008  0.468001  0.479962  0.160235  0.465679         0.593003\n",
       "  4468  0.448017  0.444632  0.431865  0.239144  0.494079         0.286634\n",
       "  4469  0.396829  0.412471  0.406668  0.254048  0.430102         0.417135\n",
       "  4470  0.400514  0.396644  0.345031  0.357445  0.405473         0.428138,\n",
       "  0.4191745958977491),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4467  0.478008  0.468001  0.479962  0.160235  0.465679         0.593003\n",
       "  4468  0.448017  0.444632  0.431865  0.239144  0.494079         0.286634\n",
       "  4469  0.396829  0.412471  0.406668  0.254048  0.430102         0.417135\n",
       "  4470  0.400514  0.396644  0.345031  0.357445  0.405473         0.428138\n",
       "  4471  0.370651  0.388337  0.386894  0.283193  0.384162         0.614934,\n",
       "  0.33377474125545614),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4468  0.448017  0.444632  0.431865  0.239144  0.494079         0.286634\n",
       "  4469  0.396829  0.412471  0.406668  0.254048  0.430102         0.417135\n",
       "  4470  0.400514  0.396644  0.345031  0.357445  0.405473         0.428138\n",
       "  4471  0.370651  0.388337  0.386894  0.283193  0.384162         0.614934\n",
       "  4472  0.314490  0.302746  0.282341  0.638295  0.419175         0.215586,\n",
       "  0.33248785812083304),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4469  0.396829  0.412471  0.406668  0.254048  0.430102         0.417135\n",
       "  4470  0.400514  0.396644  0.345031  0.357445  0.405473         0.428138\n",
       "  4471  0.370651  0.388337  0.386894  0.283193  0.384162         0.614934\n",
       "  4472  0.314490  0.302746  0.282341  0.638295  0.419175         0.215586\n",
       "  4473  0.295870  0.305030  0.312493  0.361313  0.333775         0.494548,\n",
       "  0.3690325542173327),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4470  0.400514  0.396644  0.345031  0.357445  0.405473         0.428138\n",
       "  4471  0.370651  0.388337  0.386894  0.283193  0.384162         0.614934\n",
       "  4472  0.314490  0.302746  0.282341  0.638295  0.419175         0.215586\n",
       "  4473  0.295870  0.305030  0.312493  0.361313  0.333775         0.494548\n",
       "  4474  0.309562  0.341804  0.331710  0.267549  0.332488         0.620017,\n",
       "  0.43602262943445647),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4471  0.370651  0.388337  0.386894  0.283193  0.384162         0.614934\n",
       "  4472  0.314490  0.302746  0.282341  0.638295  0.419175         0.215586\n",
       "  4473  0.295870  0.305030  0.312493  0.361313  0.333775         0.494548\n",
       "  4474  0.309562  0.341804  0.331710  0.267549  0.332488         0.620017\n",
       "  4475  0.365896  0.401495  0.392851  0.263065  0.369033         0.720989,\n",
       "  0.4544697494913714),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4472  0.314490  0.302746  0.282341  0.638295  0.419175         0.215586\n",
       "  4473  0.295870  0.305030  0.312493  0.361313  0.333775         0.494548\n",
       "  4474  0.309562  0.341804  0.331710  0.267549  0.332488         0.620017\n",
       "  4475  0.365896  0.401495  0.392851  0.263065  0.369033         0.720989\n",
       "  4476  0.415487  0.455889  0.438344  0.526399  0.436023         0.559996,\n",
       "  0.455927670571485),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4473  0.295870  0.305030  0.312493  0.361313  0.333775         0.494548\n",
       "  4474  0.309562  0.341804  0.331710  0.267549  0.332488         0.620017\n",
       "  4475  0.365896  0.401495  0.392851  0.263065  0.369033         0.720989\n",
       "  4476  0.415487  0.455889  0.438344  0.526399  0.436023         0.559996\n",
       "  4477  0.443405  0.470618  0.453065  0.247126  0.454470         0.503651,\n",
       "  0.5096178607352724),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4474  0.309562  0.341804  0.331710  0.267549  0.332488         0.620017\n",
       "  4475  0.365896  0.401495  0.392851  0.263065  0.369033         0.720989\n",
       "  4476  0.415487  0.455889  0.438344  0.526399  0.436023         0.559996\n",
       "  4477  0.443405  0.470618  0.453065  0.247126  0.454470         0.503651\n",
       "  4478  0.432200  0.478230  0.449415  0.232861  0.455928         0.676880,\n",
       "  0.4622800707454715),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4475  0.365896  0.401495  0.392851  0.263065  0.369033         0.720989\n",
       "  4476  0.415487  0.455889  0.438344  0.526399  0.436023         0.559996\n",
       "  4477  0.443405  0.470618  0.453065  0.247126  0.454470         0.503651\n",
       "  4478  0.432200  0.478230  0.449415  0.232861  0.455928         0.676880\n",
       "  4479  0.459396  0.454836  0.454834  0.203052  0.509618         0.341820,\n",
       "  0.4986759547545796),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4476  0.415487  0.455889  0.438344  0.526399  0.436023         0.559996\n",
       "  4477  0.443405  0.470618  0.453065  0.247126  0.454470         0.503651\n",
       "  4478  0.432200  0.478230  0.449415  0.232861  0.455928         0.676880\n",
       "  4479  0.459396  0.454836  0.454834  0.203052  0.509618         0.341820\n",
       "  4480  0.470699  0.501121  0.497886  0.187457  0.462280         0.619523,\n",
       "  0.5019339918197534),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4477  0.443405  0.470618  0.453065  0.247126  0.454470         0.503651\n",
       "  4478  0.432200  0.478230  0.449415  0.232861  0.455928         0.676880\n",
       "  4479  0.459396  0.454836  0.454834  0.203052  0.509618         0.341820\n",
       "  4480  0.470699  0.501121  0.497886  0.187457  0.462280         0.619523\n",
       "  4481  0.454204  0.475952  0.476772  0.219829  0.498676         0.509621,\n",
       "  0.5020604292815891),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4478  0.432200  0.478230  0.449415  0.232861  0.455928         0.676880\n",
       "  4479  0.459396  0.454836  0.454834  0.203052  0.509618         0.341820\n",
       "  4480  0.470699  0.501121  0.497886  0.187457  0.462280         0.619523\n",
       "  4481  0.454204  0.475952  0.476772  0.219829  0.498676         0.509621\n",
       "  4482  0.472718  0.489641  0.500421  0.172158  0.501934         0.499235,\n",
       "  0.46754637922175624),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4479  0.459396  0.454836  0.454834  0.203052  0.509618         0.341820\n",
       "  4480  0.470699  0.501121  0.497886  0.187457  0.462280         0.619523\n",
       "  4481  0.454204  0.475952  0.476772  0.219829  0.498676         0.509621\n",
       "  4482  0.472718  0.489641  0.500421  0.172158  0.501934         0.499235\n",
       "  4483  0.464655  0.485126  0.472881  0.179525  0.502060         0.384350,\n",
       "  0.4156190834252491),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4480  0.470699  0.501121  0.497886  0.187457  0.462280         0.619523\n",
       "  4481  0.454204  0.475952  0.476772  0.219829  0.498676         0.509621\n",
       "  4482  0.472718  0.489641  0.500421  0.172158  0.501934         0.499235\n",
       "  4483  0.464655  0.485126  0.472881  0.179525  0.502060         0.384350\n",
       "  4484  0.410999  0.411091  0.412583  0.280310  0.467546         0.326598,\n",
       "  0.3479150880579869),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4481  0.454204  0.475952  0.476772  0.219829  0.498676         0.509621\n",
       "  4482  0.472718  0.489641  0.500421  0.172158  0.501934         0.499235\n",
       "  4483  0.464655  0.485126  0.472881  0.179525  0.502060         0.384350\n",
       "  4484  0.410999  0.411091  0.412583  0.280310  0.467546         0.326598\n",
       "  4485  0.330593  0.345820  0.348268  0.347221  0.415619         0.274275,\n",
       "  0.3526384101924036),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4482  0.472718  0.489641  0.500421  0.172158  0.501934         0.499235\n",
       "  4483  0.464655  0.485126  0.472881  0.179525  0.502060         0.384350\n",
       "  4484  0.410999  0.411091  0.412583  0.280310  0.467546         0.326598\n",
       "  4485  0.330593  0.345820  0.348268  0.347221  0.415619         0.274275\n",
       "  4486  0.322214  0.334210  0.344715  0.208324  0.347915         0.514481,\n",
       "  0.40019197250206107),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4483  0.464655  0.485126  0.472881  0.179525  0.502060         0.384350\n",
       "  4484  0.410999  0.411091  0.412583  0.280310  0.467546         0.326598\n",
       "  4485  0.330593  0.345820  0.348268  0.347221  0.415619         0.274275\n",
       "  4486  0.322214  0.334210  0.344715  0.208324  0.347915         0.514481\n",
       "  4487  0.352498  0.389214  0.371308  0.303320  0.352638         0.656528,\n",
       "  0.34433721583667665),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4484  0.410999  0.411091  0.412583  0.280310  0.467546         0.326598\n",
       "  4485  0.330593  0.345820  0.348268  0.347221  0.415619         0.274275\n",
       "  4486  0.322214  0.334210  0.344715  0.208324  0.347915         0.514481\n",
       "  4487  0.352498  0.389214  0.371308  0.303320  0.352638         0.656528\n",
       "  4488  0.341195  0.346918  0.336294  0.329459  0.400192         0.313573,\n",
       "  0.36262815981707064),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4485  0.330593  0.345820  0.348268  0.347221  0.415619         0.274275\n",
       "  4486  0.322214  0.334210  0.344715  0.208324  0.347915         0.514481\n",
       "  4487  0.352498  0.389214  0.371308  0.303320  0.352638         0.656528\n",
       "  4488  0.341195  0.346918  0.336294  0.329459  0.400192         0.313573\n",
       "  4489  0.316864  0.346293  0.333826  0.430491  0.344337         0.559478,\n",
       "  0.42435918640981907),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4486  0.322214  0.334210  0.344715  0.208324  0.347915         0.514481\n",
       "  4487  0.352498  0.389214  0.371308  0.303320  0.352638         0.656528\n",
       "  4488  0.341195  0.346918  0.336294  0.329459  0.400192         0.313573\n",
       "  4489  0.316864  0.346293  0.333826  0.430491  0.344337         0.559478\n",
       "  4490  0.364593  0.398520  0.388990  0.374986  0.362628         0.703547,\n",
       "  0.4246418148249209),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4487  0.352498  0.389214  0.371308  0.303320  0.352638         0.656528\n",
       "  4488  0.341195  0.346918  0.336294  0.329459  0.400192         0.313573\n",
       "  4489  0.316864  0.346293  0.333826  0.430491  0.344337         0.559478\n",
       "  4490  0.364593  0.398520  0.388990  0.374986  0.362628         0.703547\n",
       "  4491  0.386611  0.415248  0.410526  0.189133  0.424359         0.499753,\n",
       "  0.43431921229654646),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4488  0.341195  0.346918  0.336294  0.329459  0.400192         0.313573\n",
       "  4489  0.316864  0.346293  0.333826  0.430491  0.344337         0.559478\n",
       "  4490  0.364593  0.398520  0.388990  0.374986  0.362628         0.703547\n",
       "  4491  0.386611  0.415248  0.410526  0.189133  0.424359         0.499753\n",
       "  4492  0.413543  0.406506  0.415112  0.198839  0.424642         0.530911,\n",
       "  0.5214002433419049),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4489  0.316864  0.346293  0.333826  0.430491  0.344337         0.559478\n",
       "  4490  0.364593  0.398520  0.388990  0.374986  0.362628         0.703547\n",
       "  4491  0.386611  0.415248  0.410526  0.189133  0.424359         0.499753\n",
       "  4492  0.413543  0.406506  0.415112  0.198839  0.424642         0.530911\n",
       "  4493  0.423384  0.489512  0.453855  0.376366  0.434319         0.787621,\n",
       "  0.49290377741698665),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4490  0.364593  0.398520  0.388990  0.374986  0.362628         0.703547\n",
       "  4491  0.386611  0.415248  0.410526  0.189133  0.424359         0.499753\n",
       "  4492  0.413543  0.406506  0.415112  0.198839  0.424642         0.530911\n",
       "  4493  0.423384  0.489512  0.453855  0.376366  0.434319         0.787621\n",
       "  4494  0.503741  0.498856  0.489945  0.298984  0.521400         0.404307,\n",
       "  0.43263815490744717),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4491  0.386611  0.415248  0.410526  0.189133  0.424359         0.499753\n",
       "  4492  0.413543  0.406506  0.415112  0.198839  0.424642         0.530911\n",
       "  4493  0.423384  0.489512  0.453855  0.376366  0.434319         0.787621\n",
       "  4494  0.503741  0.498856  0.489945  0.298984  0.521400         0.404307\n",
       "  4495  0.464798  0.477597  0.436410  0.242223  0.492904         0.298944,\n",
       "  0.4279519353768446),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4492  0.413543  0.406506  0.415112  0.198839  0.424642         0.530911\n",
       "  4493  0.423384  0.489512  0.453855  0.376366  0.434319         0.787621\n",
       "  4494  0.503741  0.498856  0.489945  0.298984  0.521400         0.404307\n",
       "  4495  0.464798  0.477597  0.436410  0.242223  0.492904         0.298944\n",
       "  4496  0.401463  0.413646  0.423014  0.123921  0.432638         0.483274,\n",
       "  0.3850473709800093),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4493  0.423384  0.489512  0.453855  0.376366  0.434319         0.787621\n",
       "  4494  0.503741  0.498856  0.489945  0.298984  0.521400         0.404307\n",
       "  4495  0.464798  0.477597  0.436410  0.242223  0.492904         0.298944\n",
       "  4496  0.401463  0.413646  0.423014  0.123921  0.432638         0.483274\n",
       "  4497  0.376612  0.378029  0.359135  0.363112  0.427952         0.356522,\n",
       "  0.3807108442341942),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4494  0.503741  0.498856  0.489945  0.298984  0.521400         0.404307\n",
       "  4495  0.464798  0.477597  0.436410  0.242223  0.492904         0.298944\n",
       "  4496  0.401463  0.413646  0.423014  0.123921  0.432638         0.483274\n",
       "  4497  0.376612  0.378029  0.359135  0.363112  0.427952         0.356522\n",
       "  4498  0.344021  0.354788  0.356049  0.276664  0.385047         0.484434,\n",
       "  0.4522084841426237),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4495  0.464798  0.477597  0.436410  0.242223  0.492904         0.298944\n",
       "  4496  0.401463  0.413646  0.423014  0.123921  0.432638         0.483274\n",
       "  4497  0.376612  0.378029  0.359135  0.363112  0.427952         0.356522\n",
       "  4498  0.344021  0.354788  0.356049  0.276664  0.385047         0.484434\n",
       "  4499  0.331625  0.419567  0.343172  0.342812  0.380711         0.735938,\n",
       "  0.47194248733416155),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4496  0.401463  0.413646  0.423014  0.123921  0.432638         0.483274\n",
       "  4497  0.376612  0.378029  0.359135  0.363112  0.427952         0.356522\n",
       "  4498  0.344021  0.354788  0.356049  0.276664  0.385047         0.484434\n",
       "  4499  0.331625  0.419567  0.343172  0.342812  0.380711         0.735938\n",
       "  4500  0.430754  0.454645  0.433498  0.249860  0.452208         0.564264,\n",
       "  0.5328255542978753),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4497  0.376612  0.378029  0.359135  0.363112  0.427952         0.356522\n",
       "  4498  0.344021  0.354788  0.356049  0.276664  0.385047         0.484434\n",
       "  4499  0.331625  0.419567  0.343172  0.342812  0.380711         0.735938\n",
       "  4500  0.430754  0.454645  0.433498  0.249860  0.452208         0.564264\n",
       "  4501  0.459629  0.508192  0.482920  0.290904  0.471942         0.700735,\n",
       "  0.545656792107684),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4498  0.344021  0.354788  0.356049  0.276664  0.385047         0.484434\n",
       "  4499  0.331625  0.419567  0.343172  0.342812  0.380711         0.735938\n",
       "  4500  0.430754  0.454645  0.433498  0.249860  0.452208         0.564264\n",
       "  4501  0.459629  0.508192  0.482920  0.290904  0.471942         0.700735\n",
       "  4502  0.482868  0.516934  0.512723  0.252693  0.532826         0.541371,\n",
       "  0.4914161177221881),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4499  0.331625  0.419567  0.343172  0.342812  0.380711         0.735938\n",
       "  4500  0.430754  0.454645  0.433498  0.249860  0.452208         0.564264\n",
       "  4501  0.459629  0.508192  0.482920  0.290904  0.471942         0.700735\n",
       "  4502  0.482868  0.516934  0.512723  0.252693  0.532826         0.541371\n",
       "  4503  0.499446  0.491907  0.493464  0.223574  0.545657         0.318926,\n",
       "  0.46652738164723107),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4500  0.430754  0.454645  0.433498  0.249860  0.452208         0.564264\n",
       "  4501  0.459629  0.508192  0.482920  0.290904  0.471942         0.700735\n",
       "  4502  0.482868  0.516934  0.512723  0.252693  0.532826         0.541371\n",
       "  4503  0.499446  0.491907  0.493464  0.223574  0.545657         0.318926\n",
       "  4504  0.472145  0.477490  0.469088  0.202042  0.491416         0.416272,\n",
       "  0.42691794204263833),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4501  0.459629  0.508192  0.482920  0.290904  0.471942         0.700735\n",
       "  4502  0.482868  0.516934  0.512723  0.252693  0.532826         0.541371\n",
       "  4503  0.499446  0.491907  0.493464  0.223574  0.545657         0.318926\n",
       "  4504  0.472145  0.477490  0.469088  0.202042  0.491416         0.416272\n",
       "  4505  0.408856  0.436415  0.427063  0.374346  0.466527         0.367451,\n",
       "  0.4158051022536753),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4502  0.482868  0.516934  0.512723  0.252693  0.532826         0.541371\n",
       "  4503  0.499446  0.491907  0.493464  0.223574  0.545657         0.318926\n",
       "  4504  0.472145  0.477490  0.469088  0.202042  0.491416         0.416272\n",
       "  4505  0.408856  0.436415  0.427063  0.374346  0.466527         0.367451\n",
       "  4506  0.381532  0.388063  0.389471  0.293146  0.426918         0.461960,\n",
       "  0.4355464843110881),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4503  0.499446  0.491907  0.493464  0.223574  0.545657         0.318926\n",
       "  4504  0.472145  0.477490  0.469088  0.202042  0.491416         0.416272\n",
       "  4505  0.408856  0.436415  0.427063  0.374346  0.466527         0.367451\n",
       "  4506  0.381532  0.388063  0.389471  0.293146  0.426918         0.461960\n",
       "  4507  0.413739  0.418772  0.427056  0.317338  0.415805         0.564288,\n",
       "  0.39304371106253555),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4504  0.472145  0.477490  0.469088  0.202042  0.491416         0.416272\n",
       "  4505  0.408856  0.436415  0.427063  0.374346  0.466527         0.367451\n",
       "  4506  0.381532  0.388063  0.389471  0.293146  0.426918         0.461960\n",
       "  4507  0.413739  0.418772  0.427056  0.317338  0.415805         0.564288\n",
       "  4508  0.420927  0.430969  0.389088  0.305316  0.435546         0.357855,\n",
       "  0.4631131685055365),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4505  0.408856  0.436415  0.427063  0.374346  0.466527         0.367451\n",
       "  4506  0.381532  0.388063  0.389471  0.293146  0.426918         0.461960\n",
       "  4507  0.413739  0.418772  0.427056  0.317338  0.415805         0.564288\n",
       "  4508  0.420927  0.430969  0.389088  0.305316  0.435546         0.357855\n",
       "  4509  0.395156  0.431121  0.407150  0.276224  0.393044         0.731202,\n",
       "  0.46430321890229775),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4506  0.381532  0.388063  0.389471  0.293146  0.426918         0.461960\n",
       "  4507  0.413739  0.418772  0.427056  0.317338  0.415805         0.564288\n",
       "  4508  0.420927  0.430969  0.389088  0.305316  0.435546         0.357855\n",
       "  4509  0.395156  0.431121  0.407150  0.276224  0.393044         0.731202\n",
       "  4510  0.430271  0.454310  0.448692  0.224339  0.463113         0.502763,\n",
       "  0.469361371952532),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4507  0.413739  0.418772  0.427056  0.317338  0.415805         0.564288\n",
       "  4508  0.420927  0.430969  0.389088  0.305316  0.435546         0.357855\n",
       "  4509  0.395156  0.431121  0.407150  0.276224  0.393044         0.731202\n",
       "  4510  0.430271  0.454310  0.448692  0.224339  0.463113         0.502763\n",
       "  4511  0.445794  0.437223  0.428373  0.230346  0.464303         0.515591,\n",
       "  0.36998471057335824),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4508  0.420927  0.430969  0.389088  0.305316  0.435546         0.357855\n",
       "  4509  0.395156  0.431121  0.407150  0.276224  0.393044         0.731202\n",
       "  4510  0.430271  0.454310  0.448692  0.224339  0.463113         0.502763\n",
       "  4511  0.445794  0.437223  0.428373  0.230346  0.464303         0.515591\n",
       "  4512  0.396362  0.406255  0.363500  0.442061  0.469361         0.169232,\n",
       "  0.36775316896254995),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4509  0.395156  0.431121  0.407150  0.276224  0.393044         0.731202\n",
       "  4510  0.430271  0.454310  0.448692  0.224339  0.463113         0.502763\n",
       "  4511  0.445794  0.437223  0.428373  0.230346  0.464303         0.515591\n",
       "  4512  0.396362  0.406255  0.363500  0.442061  0.469361         0.169232\n",
       "  4513  0.352204  0.366590  0.354619  0.242936  0.369985         0.491415,\n",
       "  0.32654462796100425),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4510  0.430271  0.454310  0.448692  0.224339  0.463113         0.502763\n",
       "  4511  0.445794  0.437223  0.428373  0.230346  0.464303         0.515591\n",
       "  4512  0.396362  0.406255  0.363500  0.442061  0.469361         0.169232\n",
       "  4513  0.352204  0.366590  0.354619  0.242936  0.369985         0.491415\n",
       "  4514  0.330352  0.323265  0.320753  0.345627  0.367753         0.362147,\n",
       "  0.4479388134920539),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4511  0.445794  0.437223  0.428373  0.230346  0.464303         0.515591\n",
       "  4512  0.396362  0.406255  0.363500  0.442061  0.469361         0.169232\n",
       "  4513  0.352204  0.366590  0.354619  0.242936  0.369985         0.491415\n",
       "  4514  0.330352  0.323265  0.320753  0.345627  0.367753         0.362147\n",
       "  4515  0.363055  0.424866  0.375842  0.405345  0.326545         0.901421,\n",
       "  0.4626667619968532),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4512  0.396362  0.406255  0.363500  0.442061  0.469361         0.169232\n",
       "  4513  0.352204  0.366590  0.354619  0.242936  0.369985         0.491415\n",
       "  4514  0.330352  0.323265  0.320753  0.345627  0.367753         0.362147\n",
       "  4515  0.363055  0.424866  0.375842  0.405345  0.326545         0.901421\n",
       "  4516  0.411282  0.429443  0.427484  0.181518  0.447939         0.547661,\n",
       "  0.4931641353437528),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4513  0.352204  0.366590  0.354619  0.242936  0.369985         0.491415\n",
       "  4514  0.330352  0.323265  0.320753  0.345627  0.367753         0.362147\n",
       "  4515  0.363055  0.424866  0.375842  0.405345  0.326545         0.901421\n",
       "  4516  0.411282  0.429443  0.427484  0.181518  0.447939         0.547661\n",
       "  4517  0.420325  0.463005  0.448241  0.280460  0.462667         0.599961,\n",
       "  0.47590716982081926),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4514  0.330352  0.323265  0.320753  0.345627  0.367753         0.362147\n",
       "  4515  0.363055  0.424866  0.375842  0.405345  0.326545         0.901421\n",
       "  4516  0.411282  0.429443  0.427484  0.181518  0.447939         0.547661\n",
       "  4517  0.420325  0.463005  0.448241  0.280460  0.462667         0.599961\n",
       "  4518  0.452425  0.466362  0.470216  0.176004  0.493164         0.441583,\n",
       "  0.4821553881445606),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4515  0.363055  0.424866  0.375842  0.405345  0.326545         0.901421\n",
       "  4516  0.411282  0.429443  0.427484  0.181518  0.447939         0.547661\n",
       "  4517  0.420325  0.463005  0.448241  0.280460  0.462667         0.599961\n",
       "  4518  0.452425  0.466362  0.470216  0.176004  0.493164         0.441583\n",
       "  4519  0.435847  0.465751  0.463744  0.115089  0.475907         0.519538,\n",
       "  0.5250003711748055),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4516  0.411282  0.429443  0.427484  0.181518  0.447939         0.547661\n",
       "  4517  0.420325  0.463005  0.448241  0.280460  0.462667         0.599961\n",
       "  4518  0.452425  0.466362  0.470216  0.176004  0.493164         0.441583\n",
       "  4519  0.435847  0.465751  0.463744  0.115089  0.475907         0.519538\n",
       "  4520  0.464633  0.501449  0.492192  0.206141  0.482155         0.640911,\n",
       "  0.5263393377961785),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4517  0.420325  0.463005  0.448241  0.280460  0.462667         0.599961\n",
       "  4518  0.452425  0.466362  0.470216  0.176004  0.493164         0.441583\n",
       "  4519  0.435847  0.465751  0.463744  0.115089  0.475907         0.519538\n",
       "  4520  0.464633  0.501449  0.492192  0.206141  0.482155         0.640911\n",
       "  4521  0.494171  0.502670  0.515673  0.088565  0.525000         0.503256,\n",
       "  0.5166694233276476),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4518  0.452425  0.466362  0.470216  0.176004  0.493164         0.441583\n",
       "  4519  0.435847  0.465751  0.463744  0.115089  0.475907         0.519538\n",
       "  4520  0.464633  0.501449  0.492192  0.206141  0.482155         0.640911\n",
       "  4521  0.494171  0.502670  0.515673  0.088565  0.525000         0.503256\n",
       "  4522  0.473524  0.492906  0.500621  0.086816  0.526339         0.466745,\n",
       "  0.5154792539169206),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4519  0.435847  0.465751  0.463744  0.115089  0.475907         0.519538\n",
       "  4520  0.464633  0.501449  0.492192  0.206141  0.482155         0.640911\n",
       "  4521  0.494171  0.502670  0.515673  0.088565  0.525000         0.503256\n",
       "  4522  0.473524  0.492906  0.500621  0.086816  0.526339         0.466745\n",
       "  4523  0.506529  0.523417  0.517479  0.104620  0.516669         0.494869,\n",
       "  0.5059581515357814),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4520  0.464633  0.501449  0.492192  0.206141  0.482155         0.640911\n",
       "  4521  0.494171  0.502670  0.515673  0.088565  0.525000         0.503256\n",
       "  4522  0.473524  0.492906  0.500621  0.086816  0.526339         0.466745\n",
       "  4523  0.506529  0.523417  0.517479  0.104620  0.516669         0.494869\n",
       "  4524  0.489499  0.482837  0.498363  0.084458  0.515479         0.467239,\n",
       "  0.5528199302929251),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4521  0.494171  0.502670  0.515673  0.088565  0.525000         0.503256\n",
       "  4522  0.473524  0.492906  0.500621  0.086816  0.526339         0.466745\n",
       "  4523  0.506529  0.523417  0.517479  0.104620  0.516669         0.494869\n",
       "  4524  0.489499  0.482837  0.498363  0.084458  0.515479         0.467239\n",
       "  4525  0.513613  0.537452  0.540508  0.146944  0.505958         0.654233,\n",
       "  0.540472201595614),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4522  0.473524  0.492906  0.500621  0.086816  0.526339         0.466745\n",
       "  4523  0.506529  0.523417  0.517479  0.104620  0.516669         0.494869\n",
       "  4524  0.489499  0.482837  0.498363  0.084458  0.515479         0.467239\n",
       "  4525  0.513613  0.537452  0.540508  0.146944  0.505958         0.654233\n",
       "  4526  0.551139  0.545995  0.543368  0.099822  0.552820         0.457864,\n",
       "  0.5825734217238154),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4523  0.506529  0.523417  0.517479  0.104620  0.516669         0.494869\n",
       "  4524  0.489499  0.482837  0.498363  0.084458  0.515479         0.467239\n",
       "  4525  0.513613  0.537452  0.540508  0.146944  0.505958         0.654233\n",
       "  4526  0.551139  0.545995  0.543368  0.099822  0.552820         0.457864\n",
       "  4527  0.537274  0.550725  0.556162  0.093041  0.540472         0.638445,\n",
       "  0.5859949988546393),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4524  0.489499  0.482837  0.498363  0.084458  0.515479         0.467239\n",
       "  4525  0.513613  0.537452  0.540508  0.146944  0.505958         0.654233\n",
       "  4526  0.551139  0.545995  0.543368  0.099822  0.552820         0.457864\n",
       "  4527  0.537274  0.550725  0.556162  0.093041  0.540472         0.638445\n",
       "  4528  0.553098  0.560030  0.573772  0.085699  0.582573         0.510163,\n",
       "  0.5804906773236531),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4525  0.513613  0.537452  0.540508  0.146944  0.505958         0.654233\n",
       "  4526  0.551139  0.545995  0.543368  0.099822  0.552820         0.457864\n",
       "  4527  0.537274  0.550725  0.556162  0.093041  0.540472         0.638445\n",
       "  4528  0.553098  0.560030  0.573772  0.085699  0.582573         0.510163\n",
       "  4529  0.554756  0.559542  0.573246  0.086945  0.585995         0.480561,\n",
       "  0.5479105744533366),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4526  0.551139  0.545995  0.543368  0.099822  0.552820         0.457864\n",
       "  4527  0.537274  0.550725  0.556162  0.093041  0.540472         0.638445\n",
       "  4528  0.553098  0.560030  0.573772  0.085699  0.582573         0.510163\n",
       "  4529  0.554756  0.559542  0.573246  0.086945  0.585995         0.480561\n",
       "  4530  0.533808  0.543097  0.553151  0.111125  0.580491         0.390764,\n",
       "  0.5559440320164228),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4527  0.537274  0.550725  0.556162  0.093041  0.540472         0.638445\n",
       "  4528  0.553098  0.560030  0.573772  0.085699  0.582573         0.510163\n",
       "  4529  0.554756  0.559542  0.573246  0.086945  0.585995         0.480561\n",
       "  4530  0.533808  0.543097  0.553151  0.111125  0.580491         0.390764\n",
       "  4531  0.523258  0.536079  0.543368  0.087263  0.547911         0.525459,\n",
       "  0.5151817785095945),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4528  0.553098  0.560030  0.573772  0.085699  0.582573         0.510163\n",
       "  4529  0.554756  0.559542  0.573246  0.086945  0.585995         0.480561\n",
       "  4530  0.533808  0.543097  0.553151  0.111125  0.580491         0.390764\n",
       "  4531  0.523258  0.536079  0.543368  0.087263  0.547911         0.525459\n",
       "  4532  0.517833  0.510297  0.515071  0.143125  0.555944         0.363627,\n",
       "  0.46980765944724956),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4529  0.554756  0.559542  0.573246  0.086945  0.585995         0.480561\n",
       "  4530  0.533808  0.543097  0.553151  0.111125  0.580491         0.390764\n",
       "  4531  0.523258  0.536079  0.543368  0.087263  0.547911         0.525459\n",
       "  4532  0.517833  0.510297  0.515071  0.143125  0.555944         0.363627\n",
       "  4533  0.461016  0.456903  0.469915  0.132396  0.515182         0.348332,\n",
       "  0.465344591102379),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4530  0.533808  0.543097  0.553151  0.111125  0.580491         0.390764\n",
       "  4531  0.523258  0.536079  0.543368  0.087263  0.547911         0.525459\n",
       "  4532  0.517833  0.510297  0.515071  0.143125  0.555944         0.363627\n",
       "  4533  0.461016  0.456903  0.469915  0.132396  0.515182         0.348332\n",
       "  4534  0.434190  0.448207  0.464346  0.071726  0.469808         0.484014,\n",
       "  0.46430321890229775),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4531  0.523258  0.536079  0.543368  0.087263  0.547911         0.525459\n",
       "  4532  0.517833  0.510297  0.515071  0.143125  0.555944         0.363627\n",
       "  4533  0.461016  0.456903  0.469915  0.132396  0.515182         0.348332\n",
       "  4534  0.434190  0.448207  0.464346  0.071726  0.469808         0.484014\n",
       "  4535  0.436149  0.444958  0.456519  0.092180  0.465345         0.495362,\n",
       "  0.5089334560486332),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4532  0.517833  0.510297  0.515071  0.143125  0.555944         0.363627\n",
       "  4533  0.461016  0.456903  0.469915  0.132396  0.515182         0.348332\n",
       "  4534  0.434190  0.448207  0.464346  0.071726  0.469808         0.484014\n",
       "  4535  0.436149  0.444958  0.456519  0.092180  0.465345         0.495362\n",
       "  4536  0.446698  0.476430  0.475635  0.077688  0.464303         0.646832,\n",
       "  0.4137223727302586),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4533  0.461016  0.456903  0.469915  0.132396  0.515182         0.348332\n",
       "  4534  0.434190  0.448207  0.464346  0.071726  0.469808         0.484014\n",
       "  4535  0.436149  0.444958  0.456519  0.092180  0.465345         0.495362\n",
       "  4536  0.446698  0.476430  0.475635  0.077688  0.464303         0.646832\n",
       "  4537  0.456645  0.458429  0.417987  0.284926  0.508933         0.183047,\n",
       "  0.39944059270621124),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4534  0.434190  0.448207  0.464346  0.071726  0.469808         0.484014\n",
       "  4535  0.436149  0.444958  0.456519  0.092180  0.465345         0.495362\n",
       "  4536  0.446698  0.476430  0.475635  0.077688  0.464303         0.646832\n",
       "  4537  0.456645  0.458429  0.417987  0.284926  0.508933         0.183047\n",
       "  4538  0.380839  0.387643  0.396764  0.145577  0.413722         0.451450,\n",
       "  0.39304371106253555),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4535  0.436149  0.444958  0.456519  0.092180  0.465345         0.495362\n",
       "  4536  0.446698  0.476430  0.475635  0.077688  0.464303         0.646832\n",
       "  4537  0.456645  0.458429  0.417987  0.284926  0.508933         0.183047\n",
       "  4538  0.380839  0.387643  0.396764  0.145577  0.413722         0.451450\n",
       "  4539  0.384607  0.378642  0.381562  0.147574  0.399441         0.477600,\n",
       "  0.3817373545653058),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4536  0.446698  0.476430  0.475635  0.077688  0.464303         0.646832\n",
       "  4537  0.456645  0.458429  0.417987  0.284926  0.508933         0.183047\n",
       "  4538  0.380839  0.387643  0.396764  0.145577  0.413722         0.451450\n",
       "  4539  0.384607  0.378642  0.381562  0.147574  0.399441         0.477600\n",
       "  4540  0.393649  0.384744  0.385325  0.213473  0.393044         0.461318,\n",
       "  0.4025648134436748),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4537  0.456645  0.458429  0.417987  0.284926  0.508933         0.183047\n",
       "  4538  0.380839  0.387643  0.396764  0.145577  0.413722         0.451450\n",
       "  4539  0.384607  0.378642  0.381562  0.147574  0.399441         0.477600\n",
       "  4540  0.393649  0.384744  0.385325  0.213473  0.393044         0.461318\n",
       "  4541  0.357027  0.376354  0.372380  0.175120  0.381737         0.567890,\n",
       "  0.37474525432555517),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4538  0.380839  0.387643  0.396764  0.145577  0.413722         0.451450\n",
       "  4539  0.384607  0.378642  0.381562  0.147574  0.399441         0.477600\n",
       "  4540  0.393649  0.384744  0.385325  0.213473  0.393044         0.461318\n",
       "  4541  0.357027  0.376354  0.372380  0.175120  0.381737         0.567890\n",
       "  4542  0.389279  0.383295  0.371477  0.148435  0.402565         0.406552,\n",
       "  0.3568932040972579),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4539  0.384607  0.378642  0.381562  0.147574  0.399441         0.477600\n",
       "  4540  0.393649  0.384744  0.385325  0.213473  0.393044         0.461318\n",
       "  4541  0.357027  0.376354  0.372380  0.175120  0.381737         0.567890\n",
       "  4542  0.389279  0.383295  0.371477  0.148435  0.402565         0.406552\n",
       "  4543  0.340977  0.340656  0.347093  0.147782  0.374745         0.439609,\n",
       "  0.4015234412435935),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4540  0.393649  0.384744  0.385325  0.213473  0.393044         0.461318\n",
       "  4541  0.357027  0.376354  0.372380  0.175120  0.381737         0.567890\n",
       "  4542  0.389279  0.383295  0.371477  0.148435  0.402565         0.406552\n",
       "  4543  0.340977  0.340656  0.347093  0.147782  0.374745         0.439609\n",
       "  4544  0.335325  0.372845  0.363725  0.177618  0.356893         0.646832,\n",
       "  0.3857540164014932),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4541  0.357027  0.376354  0.372380  0.175120  0.381737         0.567890\n",
       "  4542  0.389279  0.383295  0.371477  0.148435  0.402565         0.406552\n",
       "  4543  0.340977  0.340656  0.347093  0.147782  0.374745         0.439609\n",
       "  4544  0.335325  0.372845  0.363725  0.177618  0.356893         0.646832\n",
       "  4545  0.356726  0.366590  0.370423  0.161278  0.401523         0.446516,\n",
       "  0.4208631511666894),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4542  0.389279  0.383295  0.371477  0.148435  0.402565         0.406552\n",
       "  4543  0.340977  0.340656  0.347093  0.147782  0.374745         0.439609\n",
       "  4544  0.335325  0.372845  0.363725  0.177618  0.356893         0.646832\n",
       "  4545  0.356726  0.366590  0.370423  0.161278  0.401523         0.446516\n",
       "  4546  0.369837  0.388253  0.399624  0.162167  0.385754         0.615255,\n",
       "  0.42220211778806216),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4543  0.340977  0.340656  0.347093  0.147782  0.374745         0.439609\n",
       "  4544  0.335325  0.372845  0.363725  0.177618  0.356893         0.646832\n",
       "  4545  0.356726  0.366590  0.370423  0.161278  0.401523         0.446516\n",
       "  4546  0.369837  0.388253  0.399624  0.162167  0.385754         0.615255\n",
       "  4547  0.399075  0.398017  0.413622  0.137531  0.420863         0.503256,\n",
       "  0.32461059151101335),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4544  0.335325  0.372845  0.363725  0.177618  0.356893         0.646832\n",
       "  4545  0.356726  0.366590  0.370423  0.161278  0.401523         0.446516\n",
       "  4546  0.369837  0.388253  0.399624  0.162167  0.385754         0.615255\n",
       "  4547  0.399075  0.398017  0.413622  0.137531  0.420863         0.503256\n",
       "  4548  0.352355  0.348131  0.324365  0.301140  0.422202         0.175153,\n",
       "  0.3329416434953911),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4545  0.356726  0.366590  0.370423  0.161278  0.401523         0.446516\n",
       "  4546  0.369837  0.388253  0.399624  0.162167  0.385754         0.615255\n",
       "  4547  0.399075  0.398017  0.413622  0.137531  0.420863         0.503256\n",
       "  4548  0.352355  0.348131  0.324365  0.301140  0.422202         0.175153\n",
       "  4549  0.300361  0.298246  0.316839  0.166884  0.324611         0.526446,\n",
       "  0.30363443955921854),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4546  0.369837  0.388253  0.399624  0.162167  0.385754         0.615255\n",
       "  4547  0.399075  0.398017  0.413622  0.137531  0.420863         0.503256\n",
       "  4548  0.352355  0.348131  0.324365  0.301140  0.422202         0.175153\n",
       "  4549  0.300361  0.298246  0.316839  0.166884  0.324611         0.526446\n",
       "  4550  0.293881  0.299924  0.298928  0.220826  0.332942         0.401618,\n",
       "  0.2996176438323199),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4547  0.399075  0.398017  0.413622  0.137531  0.420863         0.503256\n",
       "  4548  0.352355  0.348131  0.324365  0.301140  0.422202         0.175153\n",
       "  4549  0.300361  0.298246  0.316839  0.166884  0.324611         0.526446\n",
       "  4550  0.293881  0.299924  0.298928  0.220826  0.332942         0.401618\n",
       "  4551  0.263136  0.266667  0.277027  0.689465  0.303634         0.485494,\n",
       "  0.30289054276646343),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4548  0.352355  0.348131  0.324365  0.301140  0.422202         0.175153\n",
       "  4549  0.300361  0.298246  0.316839  0.166884  0.324611         0.526446\n",
       "  4550  0.293881  0.299924  0.298928  0.220826  0.332942         0.401618\n",
       "  4551  0.263136  0.266667  0.277027  0.689465  0.303634         0.485494\n",
       "  4552  0.256656  0.266514  0.284779  0.137602  0.299618         0.509670,\n",
       "  0.2728395759282469),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4549  0.300361  0.298246  0.316839  0.166884  0.324611         0.526446\n",
       "  4550  0.293881  0.299924  0.298928  0.220826  0.332942         0.401618\n",
       "  4551  0.263136  0.266667  0.277027  0.689465  0.303634         0.485494\n",
       "  4552  0.256656  0.266514  0.284779  0.137602  0.299618         0.509670\n",
       "  4553  0.261780  0.253547  0.265964  0.190114  0.302891         0.399151,\n",
       "  0.2457638987261368),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4550  0.293881  0.299924  0.298928  0.220826  0.332942         0.401618\n",
       "  4551  0.263136  0.266667  0.277027  0.689465  0.303634         0.485494\n",
       "  4552  0.256656  0.266514  0.284779  0.137602  0.299618         0.509670\n",
       "  4553  0.261780  0.253547  0.265964  0.190114  0.302891         0.399151\n",
       "  4554  0.252134  0.258429  0.248956  0.222081  0.272840         0.409019,\n",
       "  0.2540948465732946),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4551  0.263136  0.266667  0.277027  0.689465  0.303634         0.485494\n",
       "  4552  0.256656  0.266514  0.284779  0.137602  0.299618         0.509670\n",
       "  4553  0.261780  0.253547  0.265964  0.190114  0.302891         0.399151\n",
       "  4554  0.252134  0.258429  0.248956  0.222081  0.272840         0.409019\n",
       "  4555  0.210087  0.230816  0.240226  0.156499  0.245764         0.526445,\n",
       "  0.23326737281818022),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4552  0.256656  0.266514  0.284779  0.137602  0.299618         0.509670\n",
       "  4553  0.261780  0.253547  0.265964  0.190114  0.302891         0.399151\n",
       "  4554  0.252134  0.258429  0.248956  0.222081  0.272840         0.409019\n",
       "  4555  0.210087  0.230816  0.240226  0.156499  0.245764         0.526445\n",
       "  4556  0.219280  0.206865  0.219153  0.210504  0.254095         0.429741,\n",
       "  0.22791174436061978),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4553  0.261780  0.253547  0.265964  0.190114  0.302891         0.399151\n",
       "  4554  0.252134  0.258429  0.248956  0.222081  0.272840         0.409019\n",
       "  4555  0.210087  0.230816  0.240226  0.156499  0.245764         0.526445\n",
       "  4556  0.219280  0.206865  0.219153  0.210504  0.254095         0.429741\n",
       "  4557  0.197427  0.211899  0.224722  0.170851  0.233267         0.481054,\n",
       "  0.2172004576920079),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4554  0.252134  0.258429  0.248956  0.222081  0.272840         0.409019\n",
       "  4555  0.210087  0.230816  0.240226  0.156499  0.245764         0.526445\n",
       "  4556  0.219280  0.206865  0.219153  0.210504  0.254095         0.429741\n",
       "  4557  0.197427  0.211899  0.224722  0.170851  0.233267         0.481054\n",
       "  4558  0.217020  0.212204  0.209069  0.192865  0.227912         0.463292,\n",
       "  0.2566238635914291),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4555  0.210087  0.230816  0.240226  0.156499  0.245764         0.526445\n",
       "  4556  0.219280  0.206865  0.219153  0.210504  0.254095         0.429741\n",
       "  4557  0.197427  0.211899  0.224722  0.170851  0.233267         0.481054\n",
       "  4558  0.217020  0.212204  0.209069  0.192865  0.227912         0.463292\n",
       "  4559  0.188536  0.226545  0.215993  0.197694  0.217200         0.629564,\n",
       "  0.2172004576920079),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4556  0.219280  0.206865  0.219153  0.210504  0.254095         0.429741\n",
       "  4557  0.197427  0.211899  0.224722  0.170851  0.233267         0.481054\n",
       "  4558  0.217020  0.212204  0.209069  0.192865  0.227912         0.463292\n",
       "  4559  0.188536  0.226545  0.215993  0.197694  0.217200         0.629564\n",
       "  4560  0.207826  0.194508  0.196726  0.164490  0.256624         0.368068,\n",
       "  0.18833966026451865),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4557  0.197427  0.211899  0.224722  0.170851  0.233267         0.481054\n",
       "  4558  0.217020  0.212204  0.209069  0.192865  0.227912         0.463292\n",
       "  4559  0.188536  0.226545  0.215993  0.197694  0.217200         0.629564\n",
       "  4560  0.207826  0.194508  0.196726  0.164490  0.256624         0.368068\n",
       "  4561  0.184165  0.197468  0.189351  0.218151  0.217200         0.403099,\n",
       "  0.23520142414491652),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4558  0.217020  0.212204  0.209069  0.192865  0.227912         0.463292\n",
       "  4559  0.188536  0.226545  0.215993  0.197694  0.217200         0.629564\n",
       "  4560  0.207826  0.194508  0.196726  0.164490  0.256624         0.368068\n",
       "  4561  0.184165  0.197468  0.189351  0.218151  0.217200         0.403099\n",
       "  4562  0.176479  0.204729  0.204252  0.200440  0.188340         0.654233,\n",
       "  0.2814681181966967),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4559  0.188536  0.226545  0.215993  0.197694  0.217200         0.629564\n",
       "  4560  0.207826  0.194508  0.196726  0.164490  0.256624         0.368068\n",
       "  4561  0.184165  0.197468  0.189351  0.218151  0.217200         0.403099\n",
       "  4562  0.176479  0.204729  0.204252  0.200440  0.188340         0.654233\n",
       "  4563  0.234049  0.246682  0.264760  0.172613  0.235201         0.652260,\n",
       "  0.2786414918805249),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4560  0.207826  0.194508  0.196726  0.164490  0.256624         0.368068\n",
       "  4561  0.184165  0.197468  0.189351  0.218151  0.217200         0.403099\n",
       "  4562  0.176479  0.204729  0.204252  0.200440  0.188340         0.654233\n",
       "  4563  0.234049  0.246682  0.264760  0.172613  0.235201         0.652260\n",
       "  4564  0.228775  0.246987  0.245193  0.122043  0.281468         0.489441,\n",
       "  0.27893898216459667),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4561  0.184165  0.197468  0.189351  0.218151  0.217200         0.403099\n",
       "  4562  0.176479  0.204729  0.204252  0.200440  0.188340         0.654233\n",
       "  4563  0.234049  0.246682  0.264760  0.172613  0.235201         0.652260\n",
       "  4564  0.228775  0.246987  0.245193  0.122043  0.281468         0.489441\n",
       "  4565  0.240982  0.262090  0.271684  0.105792  0.278641         0.499802,\n",
       "  0.23921810085784956),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4562  0.176479  0.204729  0.204252  0.200440  0.188340         0.654233\n",
       "  4563  0.234049  0.246682  0.264760  0.172613  0.235201         0.652260\n",
       "  4564  0.228775  0.246987  0.245193  0.122043  0.281468         0.489441\n",
       "  4565  0.240982  0.262090  0.271684  0.105792  0.278641         0.499802\n",
       "  4566  0.228172  0.226850  0.237215  0.193172  0.278939         0.367081,\n",
       "  0.22642408466582098),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4563  0.234049  0.246682  0.264760  0.172613  0.235201         0.652260\n",
       "  4564  0.228775  0.246987  0.245193  0.122043  0.281468         0.489441\n",
       "  4565  0.240982  0.262090  0.271684  0.105792  0.278641         0.499802\n",
       "  4566  0.228172  0.226850  0.237215  0.193172  0.278939         0.367081\n",
       "  4567  0.216115  0.205034  0.217046  0.098077  0.239218         0.456384,\n",
       "  0.2166054920006102),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4564  0.228775  0.246987  0.245193  0.122043  0.281468         0.489441\n",
       "  4565  0.240982  0.262090  0.271684  0.105792  0.278641         0.499802\n",
       "  4566  0.228172  0.226850  0.237215  0.193172  0.278939         0.367081\n",
       "  4567  0.216115  0.205034  0.217046  0.098077  0.239218         0.456384\n",
       "  4568  0.192002  0.207018  0.207714  0.160748  0.226424         0.466252,\n",
       "  0.22032467842947145),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4565  0.240982  0.262090  0.271684  0.105792  0.278641         0.499802\n",
       "  4566  0.228172  0.226850  0.237215  0.193172  0.278939         0.367081\n",
       "  4567  0.216115  0.205034  0.217046  0.098077  0.239218         0.456384\n",
       "  4568  0.192002  0.207018  0.207714  0.160748  0.226424         0.466252\n",
       "  4569  0.192303  0.199817  0.214036  0.108098  0.216605         0.511150,\n",
       "  0.24130083038126604),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4566  0.228172  0.226850  0.237215  0.193172  0.278939         0.367081\n",
       "  4567  0.216115  0.205034  0.217046  0.098077  0.239218         0.456384\n",
       "  4568  0.192002  0.207018  0.207714  0.160748  0.226424         0.466252\n",
       "  4569  0.192303  0.199817  0.214036  0.108098  0.216605         0.511150\n",
       "  4570  0.157038  0.213272  0.177912  0.298642  0.220325         0.568383,\n",
       "  0.20366267859793585),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4567  0.216115  0.205034  0.217046  0.098077  0.239218         0.456384\n",
       "  4568  0.192002  0.207018  0.207714  0.160748  0.226424         0.466252\n",
       "  4569  0.192303  0.199817  0.214036  0.108098  0.216605         0.511150\n",
       "  4570  0.157038  0.213272  0.177912  0.298642  0.220325         0.568383\n",
       "  4571  0.227795  0.224866  0.204403  0.173154  0.241301         0.373989,\n",
       "  0.2572189482967926),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4568  0.192002  0.207018  0.207714  0.160748  0.226424         0.466252\n",
       "  4569  0.192303  0.199817  0.214036  0.108098  0.216605         0.511150\n",
       "  4570  0.157038  0.213272  0.177912  0.298642  0.220325         0.568383\n",
       "  4571  0.227795  0.224866  0.204403  0.173154  0.241301         0.373989\n",
       "  4572  0.211142  0.232189  0.241731  0.181614  0.203663         0.676436,\n",
       "  0.26629376318321407),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4569  0.192303  0.199817  0.214036  0.108098  0.216605         0.511150\n",
       "  4570  0.157038  0.213272  0.177912  0.298642  0.220325         0.568383\n",
       "  4571  0.227795  0.224866  0.204403  0.173154  0.241301         0.373989\n",
       "  4572  0.211142  0.232189  0.241731  0.181614  0.203663         0.676436\n",
       "  4573  0.277755  0.269565  0.258890  0.160660  0.257219         0.528912,\n",
       "  0.24992937264971582),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4570  0.157038  0.213272  0.177912  0.298642  0.220325         0.568383\n",
       "  4571  0.227795  0.224866  0.204403  0.173154  0.241301         0.373989\n",
       "  4572  0.211142  0.232189  0.241731  0.181614  0.203663         0.676436\n",
       "  4573  0.277755  0.269565  0.258890  0.160660  0.257219         0.528912\n",
       "  4574  0.228925  0.230496  0.243612  0.160206  0.266294         0.444543,\n",
       "  0.25349976186793133),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4571  0.227795  0.224866  0.204403  0.173154  0.241301         0.373989\n",
       "  4572  0.211142  0.232189  0.241731  0.181614  0.203663         0.676436\n",
       "  4573  0.277755  0.269565  0.258890  0.160660  0.257219         0.528912\n",
       "  4574  0.228925  0.230496  0.243612  0.160206  0.266294         0.444543\n",
       "  4575  0.230734  0.250953  0.248655  0.203952  0.249929         0.510657,\n",
       "  0.26763272980458686),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4572  0.211142  0.232189  0.241731  0.181614  0.203663         0.676436\n",
       "  4573  0.277755  0.269565  0.258890  0.160660  0.257219         0.528912\n",
       "  4574  0.228925  0.230496  0.243612  0.160206  0.266294         0.444543\n",
       "  4575  0.230734  0.250953  0.248655  0.203952  0.249929         0.510657\n",
       "  4576  0.195167  0.229901  0.222465  0.251544  0.253500         0.545688,\n",
       "  0.2897990511671089),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4573  0.277755  0.269565  0.258890  0.160660  0.257219         0.528912\n",
       "  4574  0.228925  0.230496  0.243612  0.160206  0.266294         0.444543\n",
       "  4575  0.230734  0.250953  0.248655  0.203952  0.249929         0.510657\n",
       "  4576  0.195167  0.229901  0.222465  0.251544  0.253500         0.545688\n",
       "  4577  0.249874  0.252479  0.253622  0.198479  0.267633         0.572331,\n",
       "  0.31895745789263574),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4574  0.228925  0.230496  0.243612  0.160206  0.266294         0.444543\n",
       "  4575  0.230734  0.250953  0.248655  0.203952  0.249929         0.510657\n",
       "  4576  0.195167  0.229901  0.222465  0.251544  0.253500         0.545688\n",
       "  4577  0.249874  0.252479  0.253622  0.198479  0.267633         0.572331\n",
       "  4578  0.268109  0.282990  0.294563  0.262895  0.289799         0.595520,\n",
       "  0.16855355870948485),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4575  0.230734  0.250953  0.248655  0.203952  0.249929         0.510657\n",
       "  4576  0.195167  0.229901  0.222465  0.251544  0.253500         0.545688\n",
       "  4577  0.249874  0.252479  0.253622  0.198479  0.267633         0.572331\n",
       "  4578  0.268109  0.282990  0.294563  0.262895  0.289799         0.595520\n",
       "  4579  0.169546  0.182914  0.167375  0.775250  0.318957         0.000000,\n",
       "  0.13552715346770539),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4576  0.195167  0.229901  0.222465  0.251544  0.253500         0.545688\n",
       "  4577  0.249874  0.252479  0.253622  0.198479  0.267633         0.572331\n",
       "  4578  0.268109  0.282990  0.294563  0.262895  0.289799         0.595520\n",
       "  4579  0.169546  0.182914  0.167375  0.775250  0.318957         0.000000\n",
       "  4580  0.132623  0.131503  0.127187  0.560082  0.168554         0.389283,\n",
       "  0.1947366609221599),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4577  0.249874  0.252479  0.253622  0.198479  0.267633         0.572331\n",
       "  4578  0.268109  0.282990  0.294563  0.262895  0.289799         0.595520\n",
       "  4579  0.169546  0.182914  0.167375  0.775250  0.318957         0.000000\n",
       "  4580  0.132623  0.131503  0.127187  0.560082  0.168554         0.389283\n",
       "  4581  0.105797  0.157284  0.133554  0.334175  0.135527         0.695185,\n",
       "  0.16617333890199681),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4578  0.268109  0.282990  0.294563  0.262895  0.289799         0.595520\n",
       "  4579  0.169546  0.182914  0.167375  0.775250  0.318957         0.000000\n",
       "  4580  0.132623  0.131503  0.127187  0.560082  0.168554         0.389283\n",
       "  4581  0.105797  0.157284  0.133554  0.334175  0.135527         0.695185\n",
       "  4582  0.154777  0.149504  0.164516  0.262384  0.194737         0.404085,\n",
       "  0.10428601721876185),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4579  0.169546  0.182914  0.167375  0.775250  0.318957         0.000000\n",
       "  4580  0.132623  0.131503  0.127187  0.560082  0.168554         0.389283\n",
       "  4581  0.105797  0.157284  0.133554  0.334175  0.135527         0.695185\n",
       "  4582  0.154777  0.149504  0.164516  0.262384  0.194737         0.404085\n",
       "  4583  0.151914  0.146682  0.105061  0.426852  0.166173         0.293566,\n",
       "  0.05325877941478496),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4580  0.132623  0.131503  0.127187  0.560082  0.168554         0.389283\n",
       "  4581  0.105797  0.157284  0.133554  0.334175  0.135527         0.695185\n",
       "  4582  0.154777  0.149504  0.164516  0.262384  0.194737         0.404085\n",
       "  4583  0.151914  0.146682  0.105061  0.426852  0.166173         0.293566\n",
       "  4584  0.081382  0.072464  0.053584  0.430954  0.104286         0.329583,\n",
       "  0.0),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4581  0.105797  0.157284  0.133554  0.334175  0.135527         0.695185\n",
       "  4582  0.154777  0.149504  0.164516  0.262384  0.194737         0.404085\n",
       "  4583  0.151914  0.146682  0.105061  0.426852  0.166173         0.293566\n",
       "  4584  0.081382  0.072464  0.053584  0.430954  0.104286         0.329583\n",
       "  4585  0.012584  0.000000  0.000000  0.492012  0.053259         0.322183,\n",
       "  0.04775433886983316),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4582  0.154777  0.149504  0.164516  0.262384  0.194737         0.404085\n",
       "  4583  0.151914  0.146682  0.105061  0.426852  0.166173         0.293566\n",
       "  4584  0.081382  0.072464  0.053584  0.430954  0.104286         0.329583\n",
       "  4585  0.012584  0.000000  0.000000  0.492012  0.053259         0.322183\n",
       "  4586  0.000000  0.002746  0.006472  0.389316  0.000000         0.657193,\n",
       "  0.0767640673986798),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4583  0.151914  0.146682  0.105061  0.426852  0.166173         0.293566\n",
       "  4584  0.081382  0.072464  0.053584  0.430954  0.104286         0.329583\n",
       "  4585  0.012584  0.000000  0.000000  0.492012  0.053259         0.322183\n",
       "  4586  0.000000  0.002746  0.006472  0.389316  0.000000         0.657193\n",
       "  4587  0.027579  0.036461  0.052832  0.225814  0.047754         0.595027,\n",
       "  0.08063205103818705),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4584  0.081382  0.072464  0.053584  0.430954  0.104286         0.329583\n",
       "  4585  0.012584  0.000000  0.000000  0.492012  0.053259         0.322183\n",
       "  4586  0.000000  0.002746  0.006472  0.389316  0.000000         0.657193\n",
       "  4587  0.027579  0.036461  0.052832  0.225814  0.047754         0.595027\n",
       "  4588  0.055008  0.058810  0.063217  0.266120  0.076764         0.511644,\n",
       "  0.058168135254373476),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4585  0.012584  0.000000  0.000000  0.492012  0.053259         0.322183\n",
       "  4586  0.000000  0.002746  0.006472  0.389316  0.000000         0.657193\n",
       "  4587  0.027579  0.036461  0.052832  0.225814  0.047754         0.595027\n",
       "  4588  0.055008  0.058810  0.063217  0.266120  0.076764         0.511644\n",
       "  4589  0.045740  0.044851  0.058852  0.223893  0.080632         0.424314,\n",
       "  0.15888364424095447),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4586  0.000000  0.002746  0.006472  0.389316  0.000000         0.657193\n",
       "  4587  0.027579  0.036461  0.052832  0.225814  0.047754         0.595027\n",
       "  4588  0.055008  0.058810  0.063217  0.266120  0.076764         0.511644\n",
       "  4589  0.045740  0.044851  0.058852  0.223893  0.080632         0.424314\n",
       "  4590  0.102933  0.122044  0.123424  0.416389  0.058168         0.832840,\n",
       "  0.19696818765622237),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4587  0.027579  0.036461  0.052832  0.225814  0.047754         0.595027\n",
       "  4588  0.055008  0.058810  0.063217  0.266120  0.076764         0.511644\n",
       "  4589  0.045740  0.044851  0.058852  0.223893  0.080632         0.424314\n",
       "  4590  0.102933  0.122044  0.123424  0.416389  0.058168         0.832840\n",
       "  4591  0.138651  0.164912  0.161204  0.271011  0.158884         0.625123,\n",
       "  0.1865544061484281),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4588  0.055008  0.058810  0.063217  0.266120  0.076764         0.511644\n",
       "  4589  0.045740  0.044851  0.058852  0.223893  0.080632         0.424314\n",
       "  4590  0.102933  0.122044  0.123424  0.416389  0.058168         0.832840\n",
       "  4591  0.138651  0.164912  0.161204  0.271011  0.158884         0.625123\n",
       "  4592  0.150557  0.162166  0.175548  0.192189  0.196968         0.464278,\n",
       "  0.22657288187646696),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4589  0.045740  0.044851  0.058852  0.223893  0.080632         0.424314\n",
       "  4590  0.102933  0.122044  0.123424  0.416389  0.058168         0.832840\n",
       "  4591  0.138651  0.164912  0.161204  0.271011  0.158884         0.625123\n",
       "  4592  0.150557  0.162166  0.175548  0.192189  0.196968         0.464278\n",
       "  4593  0.198332  0.211594  0.204252  0.286555  0.186554         0.631537,\n",
       "  0.2305895585894),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4590  0.102933  0.122044  0.123424  0.416389  0.058168         0.832840\n",
       "  4591  0.138651  0.164912  0.161204  0.271011  0.158884         0.625123\n",
       "  4592  0.150557  0.162166  0.175548  0.192189  0.196968         0.464278\n",
       "  4593  0.198332  0.211594  0.204252  0.286555  0.186554         0.631537\n",
       "  4594  0.188536  0.202898  0.217528  0.198226  0.226573         0.512137,\n",
       "  0.22329998294232323),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4591  0.138651  0.164912  0.161204  0.271011  0.158884         0.625123\n",
       "  4592  0.150557  0.162166  0.175548  0.192189  0.196968         0.464278\n",
       "  4593  0.198332  0.211594  0.204252  0.286555  0.186554         0.631537\n",
       "  4594  0.188536  0.202898  0.217528  0.198226  0.226573         0.512137\n",
       "  4595  0.175876  0.197254  0.205456  0.163224  0.230590         0.474640,\n",
       "  0.21288630557174892),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4592  0.150557  0.162166  0.175548  0.192189  0.196968         0.464278\n",
       "  4593  0.198332  0.211594  0.204252  0.286555  0.186554         0.631537\n",
       "  4594  0.188536  0.202898  0.217528  0.198226  0.226573         0.512137\n",
       "  4595  0.175876  0.197254  0.205456  0.163224  0.230590         0.474640\n",
       "  4596  0.203456  0.192372  0.200038  0.202041  0.223300         0.464279,\n",
       "  0.18357910163557611),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4593  0.198332  0.211594  0.204252  0.286555  0.186554         0.631537\n",
       "  4594  0.188536  0.202898  0.217528  0.198226  0.226573         0.512137\n",
       "  4595  0.175876  0.197254  0.205456  0.163224  0.230590         0.474640\n",
       "  4596  0.203456  0.192372  0.200038  0.202041  0.223300         0.464279\n",
       "  4597  0.181603  0.185660  0.183932  0.124770  0.212886         0.401618,\n",
       "  0.2058942202087437),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4594  0.188536  0.202898  0.217528  0.198226  0.226573         0.512137\n",
       "  4595  0.175876  0.197254  0.205456  0.163224  0.230590         0.474640\n",
       "  4596  0.203456  0.192372  0.200038  0.202041  0.223300         0.464279\n",
       "  4597  0.181603  0.185660  0.183932  0.124770  0.212886         0.401618\n",
       "  4598  0.160504  0.167780  0.164967  0.126889  0.183579         0.572824,\n",
       "  0.22806054157126554),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4595  0.175876  0.197254  0.205456  0.163224  0.230590         0.474640\n",
       "  4596  0.203456  0.192372  0.200038  0.202041  0.223300         0.464279\n",
       "  4597  0.181603  0.185660  0.183932  0.124770  0.212886         0.401618\n",
       "  4598  0.160504  0.167780  0.164967  0.126889  0.183579         0.572824\n",
       "  4599  0.178287  0.190984  0.209069  0.110875  0.205894         0.572331,\n",
       "  0.20991088204493114),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4596  0.203456  0.192372  0.200038  0.202041  0.223300         0.464279\n",
       "  4597  0.181603  0.185660  0.183932  0.124770  0.212886         0.401618\n",
       "  4598  0.160504  0.167780  0.164967  0.126889  0.183579         0.572824\n",
       "  4599  0.178287  0.190984  0.209069  0.110875  0.205894         0.572331\n",
       "  4600  0.195242  0.189016  0.211929  0.000000  0.228061         0.438622,\n",
       "  0.18982730508257162),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4597  0.181603  0.185660  0.183932  0.124770  0.212886         0.401618\n",
       "  4598  0.160504  0.167780  0.164967  0.126889  0.183579         0.572824\n",
       "  4599  0.178287  0.190984  0.209069  0.110875  0.205894         0.572331\n",
       "  4600  0.195242  0.189016  0.211929  0.000000  0.228061         0.438622\n",
       "  4601  0.176178  0.172082  0.187244  0.140507  0.209911         0.432208,\n",
       "  0.17777717080655253),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4598  0.160504  0.167780  0.164967  0.126889  0.183579         0.572824\n",
       "  4599  0.178287  0.190984  0.209069  0.110875  0.205894         0.572331\n",
       "  4600  0.195242  0.189016  0.211929  0.000000  0.228061         0.438622\n",
       "  4601  0.176178  0.172082  0.187244  0.140507  0.209911         0.432208\n",
       "  4602  0.158093  0.150114  0.164666  0.143531  0.189827         0.458851,\n",
       "  0.26718633817264936),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4599  0.178287  0.190984  0.209069  0.110875  0.205894         0.572331\n",
       "  4600  0.195242  0.189016  0.211929  0.000000  0.228061         0.438622\n",
       "  4601  0.176178  0.172082  0.187244  0.140507  0.209911         0.432208\n",
       "  4602  0.158093  0.150114  0.164666  0.143531  0.189827         0.458851\n",
       "  4603  0.144830  0.227307  0.168881  0.385803  0.177777         0.795342,\n",
       "  0.2646573211545151),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4600  0.195242  0.189016  0.211929  0.000000  0.228061         0.438622\n",
       "  4601  0.176178  0.172082  0.187244  0.140507  0.209911         0.432208\n",
       "  4602  0.158093  0.150114  0.164666  0.143531  0.189827         0.458851\n",
       "  4603  0.144830  0.227307  0.168881  0.385803  0.177777         0.795342\n",
       "  4604  0.239475  0.244699  0.259191  0.162643  0.267186         0.490428,\n",
       "  0.25796283021280186),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4601  0.176178  0.172082  0.187244  0.140507  0.209911         0.432208\n",
       "  4602  0.158093  0.150114  0.164666  0.143531  0.189827         0.458851\n",
       "  4603  0.144830  0.227307  0.168881  0.385803  0.177777         0.795342\n",
       "  4604  0.239475  0.244699  0.259191  0.162643  0.267186         0.490428\n",
       "  4605  0.208881  0.222731  0.236613  0.126305  0.264657         0.476613,\n",
       "  0.24368116920272032),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4602  0.158093  0.150114  0.164666  0.143531  0.189827         0.458851\n",
       "  4603  0.144830  0.227307  0.168881  0.385803  0.177777         0.795342\n",
       "  4604  0.239475  0.244699  0.259191  0.162643  0.267186         0.490428\n",
       "  4605  0.208881  0.222731  0.236613  0.126305  0.264657         0.476613\n",
       "  4606  0.215588  0.231884  0.239398  0.140273  0.257963         0.451451,\n",
       "  0.20559662578745197),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4603  0.144830  0.227307  0.168881  0.385803  0.177777         0.795342\n",
       "  4604  0.239475  0.244699  0.259191  0.162643  0.267186         0.490428\n",
       "  4605  0.208881  0.222731  0.236613  0.126305  0.264657         0.476613\n",
       "  4606  0.215588  0.231884  0.239398  0.140273  0.257963         0.451451\n",
       "  4607  0.213402  0.208390  0.200339  0.151632  0.243681         0.372508,\n",
       "  0.17346291454907337),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4604  0.239475  0.244699  0.259191  0.162643  0.267186         0.490428\n",
       "  4605  0.208881  0.222731  0.236613  0.126305  0.264657         0.476613\n",
       "  4606  0.215588  0.231884  0.239398  0.140273  0.257963         0.451451\n",
       "  4607  0.213402  0.208390  0.200339  0.151632  0.243681         0.372508\n",
       "  4608  0.169697  0.164149  0.174224  0.222710  0.205597         0.392244,\n",
       "  0.15561074530681052),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4605  0.208881  0.222731  0.236613  0.126305  0.264657         0.476613\n",
       "  4606  0.215588  0.231884  0.239398  0.140273  0.257963         0.451451\n",
       "  4607  0.213402  0.208390  0.200339  0.151632  0.243681         0.372508\n",
       "  4608  0.169697  0.164149  0.174224  0.222710  0.205597         0.392244\n",
       "  4609  0.153421  0.142182  0.155786  0.209718  0.173463         0.439609,\n",
       "  0.142519253707456),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4606  0.215588  0.231884  0.239398  0.140273  0.257963         0.451451\n",
       "  4607  0.213402  0.208390  0.200339  0.151632  0.243681         0.372508\n",
       "  4608  0.169697  0.164149  0.174224  0.222710  0.205597         0.392244\n",
       "  4609  0.153421  0.142182  0.155786  0.209718  0.173463         0.439609\n",
       "  4610  0.126444  0.121129  0.144045  0.164045  0.155611         0.455398,\n",
       "  0.14980882935453255),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4607  0.213402  0.208390  0.200339  0.151632  0.243681         0.372508\n",
       "  4608  0.169697  0.164149  0.174224  0.222710  0.205597         0.392244\n",
       "  4609  0.153421  0.142182  0.155786  0.209718  0.173463         0.439609\n",
       "  4610  0.126444  0.121129  0.144045  0.164045  0.155611         0.455398\n",
       "  4611  0.114237  0.111747  0.127187  0.231739  0.142519         0.522992,\n",
       "  0.18387657704290206),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4608  0.169697  0.164149  0.174224  0.222710  0.205597         0.392244\n",
       "  4609  0.153421  0.142182  0.155786  0.209718  0.173463         0.439609\n",
       "  4610  0.126444  0.121129  0.144045  0.164045  0.155611         0.455398\n",
       "  4611  0.114237  0.111747  0.127187  0.231739  0.142519         0.522992\n",
       "  4612  0.189289  0.202136  0.179567  0.322986  0.149809         0.611802,\n",
       "  0.17584313435656163),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4609  0.153421  0.142182  0.155786  0.209718  0.173463         0.439609\n",
       "  4610  0.126444  0.121129  0.144045  0.164045  0.155611         0.455398\n",
       "  4611  0.114237  0.111747  0.127187  0.231739  0.142519         0.522992\n",
       "  4612  0.189289  0.202136  0.179567  0.322986  0.149809         0.611802\n",
       "  4613  0.151160  0.162776  0.157893  0.220308  0.183877         0.472173,\n",
       "  0.11469969458933638),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4610  0.126444  0.121129  0.144045  0.164045  0.155611         0.455398\n",
       "  4611  0.114237  0.111747  0.127187  0.231739  0.142519         0.522992\n",
       "  4612  0.189289  0.202136  0.179567  0.322986  0.149809         0.611802\n",
       "  4613  0.151160  0.162776  0.157893  0.220308  0.183877         0.472173\n",
       "  4614  0.121018  0.114111  0.105061  0.243046  0.175843         0.296033,\n",
       "  0.1096416605530679),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4611  0.114237  0.111747  0.127187  0.231739  0.142519         0.522992\n",
       "  4612  0.189289  0.202136  0.179567  0.322986  0.149809         0.611802\n",
       "  4613  0.151160  0.162776  0.157893  0.220308  0.183877         0.472173\n",
       "  4614  0.121018  0.114111  0.105061  0.243046  0.175843         0.296033\n",
       "  4615  0.085753  0.079329  0.098739  0.491705  0.114700         0.482041,\n",
       "  0.08420244025640233),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4612  0.189289  0.202136  0.179567  0.322986  0.149809         0.611802\n",
       "  4613  0.151160  0.162776  0.157893  0.220308  0.183877         0.472173\n",
       "  4614  0.121018  0.114111  0.105061  0.243046  0.175843         0.296033\n",
       "  4615  0.085753  0.079329  0.098739  0.491705  0.114700         0.482041\n",
       "  4616  0.080930  0.070938  0.082408  0.178028  0.109642         0.414446,\n",
       "  0.09134320381608751),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4613  0.151160  0.162776  0.157893  0.220308  0.183877         0.472173\n",
       "  4614  0.121018  0.114111  0.105061  0.243046  0.175843         0.296033\n",
       "  4615  0.085753  0.079329  0.098739  0.491705  0.114700         0.482041\n",
       "  4616  0.080930  0.070938  0.082408  0.178028  0.109642         0.414446\n",
       "  4617  0.048528  0.049275  0.069088  0.165172  0.084202         0.522498,\n",
       "  0.10056683078990059),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4614  0.121018  0.114111  0.105061  0.243046  0.175843         0.296033\n",
       "  4615  0.085753  0.079329  0.098739  0.491705  0.114700         0.482041\n",
       "  4616  0.080930  0.070938  0.082408  0.178028  0.109642         0.414446\n",
       "  4617  0.048528  0.049275  0.069088  0.165172  0.084202         0.522498\n",
       "  4618  0.063599  0.066590  0.082183  0.144965  0.091343         0.529406,\n",
       "  0.07096213656965622),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4615  0.085753  0.079329  0.098739  0.491705  0.114700         0.482041\n",
       "  4616  0.080930  0.070938  0.082408  0.178028  0.109642         0.414446\n",
       "  4617  0.048528  0.049275  0.069088  0.165172  0.084202         0.522498\n",
       "  4618  0.063599  0.066590  0.082183  0.144965  0.091343         0.529406\n",
       "  4619  0.051542  0.040122  0.052531  0.185857  0.100567         0.400632,\n",
       "  0.09402103292161335),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4616  0.080930  0.070938  0.082408  0.178028  0.109642         0.414446\n",
       "  4617  0.048528  0.049275  0.069088  0.165172  0.084202         0.522498\n",
       "  4618  0.063599  0.066590  0.082183  0.144965  0.091343         0.529406\n",
       "  4619  0.051542  0.040122  0.052531  0.185857  0.100567         0.400632\n",
       "  4620  0.031799  0.054157  0.062766  0.113907  0.070962         0.575291,\n",
       "  0.06605278073006793),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4617  0.048528  0.049275  0.069088  0.165172  0.084202         0.522498\n",
       "  4618  0.063599  0.066590  0.082183  0.144965  0.091343         0.529406\n",
       "  4619  0.051542  0.040122  0.052531  0.185857  0.100567         0.400632\n",
       "  4620  0.031799  0.054157  0.062766  0.113907  0.070962         0.575291\n",
       "  4621  0.057269  0.045004  0.061486  0.085032  0.094021         0.406059,\n",
       "  0.04418394965161765),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4618  0.063599  0.066590  0.082183  0.144965  0.091343         0.529406\n",
       "  4619  0.051542  0.040122  0.052531  0.185857  0.100567         0.400632\n",
       "  4620  0.031799  0.054157  0.062766  0.113907  0.070962         0.575291\n",
       "  4621  0.057269  0.045004  0.061486  0.085032  0.094021         0.406059\n",
       "  4622  0.029991  0.030053  0.043951  0.114702  0.066053         0.426288,\n",
       "  0.08122701672958477),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4619  0.051542  0.040122  0.052531  0.185857  0.100567         0.400632\n",
       "  4620  0.031799  0.054157  0.062766  0.113907  0.070962         0.575291\n",
       "  4621  0.057269  0.045004  0.061486  0.085032  0.094021         0.406059\n",
       "  4622  0.029991  0.030053  0.043951  0.114702  0.066053         0.426288\n",
       "  4623  0.022908  0.042944  0.053283  0.119643  0.044184         0.621669,\n",
       "  0.077954221932661),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4620  0.031799  0.054157  0.062766  0.113907  0.070962         0.575291\n",
       "  4621  0.057269  0.045004  0.061486  0.085032  0.094021         0.406059\n",
       "  4622  0.029991  0.030053  0.043951  0.114702  0.066053         0.426288\n",
       "  4623  0.022908  0.042944  0.053283  0.119643  0.044184         0.621669\n",
       "  4624  0.027956  0.034783  0.053885  0.130712  0.081227         0.487961,\n",
       "  0.09238457601616856),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4621  0.057269  0.045004  0.061486  0.085032  0.094021         0.406059\n",
       "  4622  0.029991  0.030053  0.043951  0.114702  0.066053         0.426288\n",
       "  4623  0.022908  0.042944  0.053283  0.119643  0.044184         0.621669\n",
       "  4624  0.027956  0.034783  0.053885  0.130712  0.081227         0.487961\n",
       "  4625  0.065106  0.076278  0.083838  0.149918  0.077954         0.546674,\n",
       "  0.07765662751136926),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4622  0.029991  0.030053  0.043951  0.114702  0.066053         0.426288\n",
       "  4623  0.022908  0.042944  0.053283  0.119643  0.044184         0.621669\n",
       "  4624  0.027956  0.034783  0.053885  0.130712  0.081227         0.487961\n",
       "  4625  0.065106  0.076278  0.083838  0.149918  0.077954         0.546674\n",
       "  4626  0.082889  0.071548  0.065475  0.227620  0.092385         0.449970,\n",
       "  0.04879571106991443),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4623  0.022908  0.042944  0.053283  0.119643  0.044184         0.621669\n",
       "  4624  0.027956  0.034783  0.053885  0.130712  0.081227         0.487961\n",
       "  4625  0.065106  0.076278  0.083838  0.149918  0.077954         0.546674\n",
       "  4626  0.082889  0.071548  0.065475  0.227620  0.092385         0.449970\n",
       "  4627  0.038581  0.025324  0.046811  0.179452  0.077657         0.403098,\n",
       "  0.06947449175160303),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4624  0.027956  0.034783  0.053885  0.130712  0.081227         0.487961\n",
       "  4625  0.065106  0.076278  0.083838  0.149918  0.077954         0.546674\n",
       "  4626  0.082889  0.071548  0.065475  0.227620  0.092385         0.449970\n",
       "  4627  0.038581  0.025324  0.046811  0.179452  0.077657         0.403098\n",
       "  4628  0.027881  0.029291  0.031910  0.222174  0.048796         0.567397,\n",
       "  0.07899559413274226),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4625  0.065106  0.076278  0.083838  0.149918  0.077954         0.546674\n",
       "  4626  0.082889  0.071548  0.065475  0.227620  0.092385         0.449970\n",
       "  4627  0.038581  0.025324  0.046811  0.179452  0.077657         0.403098\n",
       "  4628  0.027881  0.029291  0.031910  0.222174  0.048796         0.567397\n",
       "  4629  0.055536  0.065294  0.077216  0.177735  0.069474         0.530393,\n",
       "  0.08554128786380955),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4626  0.082889  0.071548  0.065475  0.227620  0.092385         0.449970\n",
       "  4627  0.038581  0.025324  0.046811  0.179452  0.077657         0.403098\n",
       "  4628  0.027881  0.029291  0.031910  0.222174  0.048796         0.567397\n",
       "  4629  0.055536  0.065294  0.077216  0.177735  0.069474         0.530393\n",
       "  4630  0.018236  0.044622  0.048918  0.175997  0.078996         0.520525,\n",
       "  0.1304691194314369),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4627  0.038581  0.025324  0.046811  0.179452  0.077657         0.403098\n",
       "  4628  0.027881  0.029291  0.031910  0.222174  0.048796         0.567397\n",
       "  4629  0.055536  0.065294  0.077216  0.177735  0.069474         0.530393\n",
       "  4630  0.018236  0.044622  0.048918  0.175997  0.078996         0.520525\n",
       "  4631  0.068572  0.090008  0.094675  0.214715  0.085541         0.647819,\n",
       "  0.12526228818452245),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4628  0.027881  0.029291  0.031910  0.222174  0.048796         0.567397\n",
       "  4629  0.055536  0.065294  0.077216  0.177735  0.069474         0.530393\n",
       "  4630  0.018236  0.044622  0.048918  0.175997  0.078996         0.520525\n",
       "  4631  0.068572  0.090008  0.094675  0.214715  0.085541         0.647819\n",
       "  4632  0.103838  0.092601  0.107168  0.174748  0.130469         0.481547,\n",
       "  0.13850257699452295),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  4629  0.055536  0.065294  0.077216  0.177735  0.069474         0.530393\n",
       "  4630  0.018236  0.044622  0.048918  0.175997  0.078996         0.520525\n",
       "  4631  0.068572  0.090008  0.094675  0.214715  0.085541         0.647819\n",
       "  4632  0.103838  0.092601  0.107168  0.174748  0.130469         0.481547\n",
       "  4633  0.090696  0.098093  0.112587  0.123796  0.125262         0.542727,\n",
       "  0.12898147461338372)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0689bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class akbankDataset(Dataset):\n",
    "    def __init__(self,sequences):\n",
    "        self.sequences = sequences\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self,index):\n",
    "        sequence, label = self.sequences[index]\n",
    "        return dict(\n",
    "            sequence = torch.Tensor(sequence.to_numpy()),\n",
    "            label = torch.tensor(label).float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0e5ab33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = akbankDataset(train_sequences)\n",
    "test_dataset = akbankDataset(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bb13759c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('sequence', tensor([[0.0029, 0.0023, 0.0018, 0.1855, 0.0256, 0.4790],\n",
       "        [0.0014, 0.0015, 0.0019, 0.1118, 0.0245, 0.4891],\n",
       "        [0.0014, 0.0015, 0.0021, 0.0863, 0.0248, 0.4905],\n",
       "        [0.0021, 0.0016, 0.0024, 0.0755, 0.0253, 0.4840],\n",
       "        [0.0015, 0.0009, 0.0015, 0.0632, 0.0248, 0.4797]])), ('label', tensor(0.0006))])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(3).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c54235ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class akbankPriceDataModule_normal(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "            self,train_sequences,test_sequences,batch_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_sequences = train_sequences\n",
    "        self.test_sequences = test_sequences\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def setup(self):\n",
    "        self.train_dataset = akbankDataset(self.train_sequences)\n",
    "        self.test_dataset = akbankDataset(self.test_sequences)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,self.batch_size,shuffle = False)\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset,self.batch_size,shuffle = False)\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset,self.batch_size,shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "04485a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 200\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8165772c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(       Open      High       Low    Volume  Previous  last_difference\n",
       "  0  0.000195  0.000550  0.000000  0.543577  0.000000         0.666940\n",
       "  1  0.000438  0.001739  0.001100  0.277886  0.023485         0.501404\n",
       "  2  0.002782  0.002782  0.003171  0.222151  0.025350         0.489074\n",
       "  3  0.002900  0.002337  0.001843  0.185523  0.025605         0.479002\n",
       "  4  0.001389  0.001484  0.001918  0.111765  0.024545         0.489146,\n",
       "  0.0018933248748919732),\n",
       " (       Open      High       Low    Volume  Previous  last_difference\n",
       "  1  0.000438  0.001739  0.001100  0.277886  0.023485         0.501404\n",
       "  2  0.002782  0.002782  0.003171  0.222151  0.025350         0.489074\n",
       "  3  0.002900  0.002337  0.001843  0.185523  0.025605         0.479002\n",
       "  4  0.001389  0.001484  0.001918  0.111765  0.024545         0.489146\n",
       "  5  0.001386  0.001472  0.002107  0.086276  0.024809         0.490543,\n",
       "  0.0014715190007399993),\n",
       " (       Open      High       Low    Volume  Previous  last_difference\n",
       "  2  0.002782  0.002782  0.003171  0.222151  0.025350         0.489074\n",
       "  3  0.002900  0.002337  0.001843  0.185523  0.025605         0.479002\n",
       "  4  0.001389  0.001484  0.001918  0.111765  0.024545         0.489146\n",
       "  5  0.001386  0.001472  0.002107  0.086276  0.024809         0.490543\n",
       "  6  0.002144  0.001630  0.002357  0.075525  0.025256         0.483966,\n",
       "  0.0004793283560317156),\n",
       " (       Open      High       Low    Volume  Previous  last_difference\n",
       "  3  0.002900  0.002337  0.001843  0.185523  0.025605         0.479002\n",
       "  4  0.001389  0.001484  0.001918  0.111765  0.024545         0.489146\n",
       "  5  0.001386  0.001472  0.002107  0.086276  0.024809         0.490543\n",
       "  6  0.002144  0.001630  0.002357  0.075525  0.025256         0.483966\n",
       "  7  0.001466  0.000889  0.001465  0.063169  0.024845         0.479701,\n",
       "  0.0005656024564890255),\n",
       " (       Open      High       Low    Volume  Previous  last_difference\n",
       "  4  0.001389  0.001484  0.001918  0.111765  0.024545         0.489146\n",
       "  5  0.001386  0.001472  0.002107  0.086276  0.024809         0.490543\n",
       "  6  0.002144  0.001630  0.002357  0.075525  0.025256         0.483966\n",
       "  7  0.001466  0.000889  0.001465  0.063169  0.024845         0.479701\n",
       "  8  0.000748  0.000467  0.001502  0.059774  0.023876         0.487766,\n",
       "  5.752248187974526e-05),\n",
       " (       Open      High       Low    Volume  Previous  last_difference\n",
       "  5  0.001386  0.001472  0.002107  0.086276  0.024809         0.490543\n",
       "  6  0.002144  0.001630  0.002357  0.075525  0.025256         0.483966\n",
       "  7  0.001466  0.000889  0.001465  0.063169  0.024845         0.479701\n",
       "  8  0.000748  0.000467  0.001502  0.059774  0.023876         0.487766\n",
       "  9  0.000845  0.000291  0.000899  0.111152  0.023960         0.483321,\n",
       "  0.00035949145584263445),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  6   0.002144  0.001630  0.002357  0.075525  0.025256         0.483966\n",
       "  7   0.001466  0.000889  0.001465  0.063169  0.024845         0.479701\n",
       "  8   0.000748  0.000467  0.001502  0.059774  0.023876         0.487766\n",
       "  9   0.000845  0.000291  0.000899  0.111152  0.023960         0.483321\n",
       "  10  0.000000  0.000149  0.000722  0.183956  0.023464         0.489379,\n",
       "  0.0),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  7   0.001466  0.000889  0.001465  0.063169  0.024845         0.479701\n",
       "  8   0.000748  0.000467  0.001502  0.059774  0.023876         0.487766\n",
       "  9   0.000845  0.000291  0.000899  0.111152  0.023960         0.483321\n",
       "  10  0.000000  0.000149  0.000722  0.183956  0.023464         0.489379\n",
       "  11  0.000424  0.000000  0.000814  0.062629  0.023759         0.484432,\n",
       "  0.0003762728557085218),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  8   0.000748  0.000467  0.001502  0.059774  0.023876         0.487766\n",
       "  9   0.000845  0.000291  0.000899  0.111152  0.023960         0.483321\n",
       "  10  0.000000  0.000149  0.000722  0.183956  0.023464         0.489379\n",
       "  11  0.000424  0.000000  0.000814  0.062629  0.023759         0.484432\n",
       "  12  0.000438  0.000062  0.000884  0.071091  0.023407         0.489934,\n",
       "  0.0005488306789854479),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  9   0.000845  0.000291  0.000899  0.111152  0.023960         0.483321\n",
       "  10  0.000000  0.000149  0.000722  0.183956  0.023464         0.489379\n",
       "  11  0.000424  0.000000  0.000814  0.062629  0.023759         0.484432\n",
       "  12  0.000438  0.000062  0.000884  0.071091  0.023407         0.489934\n",
       "  13  0.000373  0.000306  0.001100  0.060598  0.023775         0.488411,\n",
       "  0.0005512266472002432),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  10  0.000000  0.000149  0.000722  0.183956  0.023464         0.489379\n",
       "  11  0.000424  0.000000  0.000814  0.062629  0.023759         0.484432\n",
       "  12  0.000438  0.000062  0.000884  0.071091  0.023407         0.489934\n",
       "  13  0.000373  0.000306  0.001100  0.060598  0.023775         0.488411\n",
       "  14  0.000804  0.000230  0.001221  0.049351  0.023943         0.487138,\n",
       "  0.0012750014953151027),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  11  0.000424  0.000000  0.000814  0.062629  0.023759         0.484432\n",
       "  12  0.000438  0.000062  0.000884  0.071091  0.023407         0.489934\n",
       "  13  0.000373  0.000306  0.001100  0.060598  0.023775         0.488411\n",
       "  14  0.000804  0.000230  0.001221  0.049351  0.023943         0.487138\n",
       "  15  0.000580  0.001142  0.001293  0.105803  0.023946         0.492533,\n",
       "  0.00179506131099837),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  12  0.000438  0.000062  0.000884  0.071091  0.023407         0.489934\n",
       "  13  0.000373  0.000306  0.001100  0.060598  0.023775         0.488411\n",
       "  14  0.000804  0.000230  0.001221  0.049351  0.023943         0.487138\n",
       "  15  0.000580  0.001142  0.001293  0.105803  0.023946         0.492533\n",
       "  16  0.001791  0.001581  0.002543  0.095397  0.024653         0.491009,\n",
       "  0.0027513124324846927),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  13  0.000373  0.000306  0.001100  0.060598  0.023775         0.488411\n",
       "  14  0.000804  0.000230  0.001221  0.049351  0.023943         0.487138\n",
       "  15  0.000580  0.001142  0.001293  0.105803  0.023946         0.492533\n",
       "  16  0.001791  0.001581  0.002543  0.095397  0.024653         0.491009\n",
       "  17  0.001988  0.002431  0.002623  0.131734  0.025160         0.494271,\n",
       "  0.0028735356785262664),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  14  0.000804  0.000230  0.001221  0.049351  0.023943         0.487138\n",
       "  15  0.000580  0.001142  0.001293  0.105803  0.023946         0.492533\n",
       "  16  0.001791  0.001581  0.002543  0.095397  0.024653         0.491009\n",
       "  17  0.001988  0.002431  0.002623  0.131734  0.025160         0.494271\n",
       "  18  0.002736  0.002960  0.003449  0.130323  0.026094         0.488035,\n",
       "  0.0033456665075512863),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  15  0.000580  0.001142  0.001293  0.105803  0.023946         0.492533\n",
       "  16  0.001791  0.001581  0.002543  0.095397  0.024653         0.491009\n",
       "  17  0.001988  0.002431  0.002623  0.131734  0.025160         0.494271\n",
       "  18  0.002736  0.002960  0.003449  0.130323  0.026094         0.488035\n",
       "  19  0.003165  0.003332  0.003800  0.112711  0.026214         0.490651,\n",
       "  0.004189278255855227),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  16  0.001791  0.001581  0.002543  0.095397  0.024653         0.491009\n",
       "  17  0.001988  0.002431  0.002623  0.131734  0.025160         0.494271\n",
       "  18  0.002736  0.002960  0.003449  0.130323  0.026094         0.488035\n",
       "  19  0.003165  0.003332  0.003800  0.112711  0.026214         0.490651\n",
       "  20  0.003665  0.003732  0.004261  0.115221  0.026675         0.493429,\n",
       "  0.004637439780369963),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  17  0.001988  0.002431  0.002623  0.131734  0.025160         0.494271\n",
       "  18  0.002736  0.002960  0.003449  0.130323  0.026094         0.488035\n",
       "  19  0.003165  0.003332  0.003800  0.112711  0.026214         0.490651\n",
       "  20  0.003665  0.003732  0.004261  0.115221  0.026675         0.493429\n",
       "  21  0.004274  0.004706  0.005041  0.129297  0.027499         0.490472,\n",
       "  0.004273156388097738),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  18  0.002736  0.002960  0.003449  0.130323  0.026094         0.488035\n",
       "  19  0.003165  0.003332  0.003800  0.112711  0.026214         0.490651\n",
       "  20  0.003665  0.003732  0.004261  0.115221  0.026675         0.493429\n",
       "  21  0.004274  0.004706  0.005041  0.129297  0.027499         0.490472\n",
       "  22  0.004962  0.004427  0.005220  0.087906  0.027936         0.484397,\n",
       "  0.004402567538783701),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  19  0.003165  0.003332  0.003800  0.112711  0.026214         0.490651\n",
       "  20  0.003665  0.003732  0.004261  0.115221  0.026675         0.493429\n",
       "  21  0.004274  0.004706  0.005041  0.129297  0.027499         0.490472\n",
       "  22  0.004962  0.004427  0.005220  0.087906  0.027936         0.484397\n",
       "  23  0.004382  0.004249  0.005050  0.092197  0.027581         0.488088,\n",
       "  0.004987347363353413),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  20  0.003665  0.003732  0.004261  0.115221  0.026675         0.493429\n",
       "  21  0.004274  0.004706  0.005041  0.129297  0.027499         0.490472\n",
       "  22  0.004962  0.004427  0.005220  0.087906  0.027936         0.484397\n",
       "  23  0.004382  0.004249  0.005050  0.092197  0.027581         0.488088\n",
       "  24  0.004729  0.004951  0.005101  0.103816  0.027707         0.491493,\n",
       "  0.004750079153552349),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  21  0.004274  0.004706  0.005041  0.129297  0.027499         0.490472\n",
       "  22  0.004962  0.004427  0.005220  0.087906  0.027936         0.484397\n",
       "  23  0.004382  0.004249  0.005050  0.092197  0.027581         0.488088\n",
       "  24  0.004729  0.004951  0.005101  0.103816  0.027707         0.491493\n",
       "  25  0.005234  0.005299  0.005765  0.110972  0.028278         0.485346,\n",
       "  0.00437381592020614),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  22  0.004962  0.004427  0.005220  0.087906  0.027936         0.484397\n",
       "  23  0.004382  0.004249  0.005050  0.092197  0.027581         0.488088\n",
       "  24  0.004729  0.004951  0.005101  0.103816  0.027707         0.491493\n",
       "  25  0.005234  0.005299  0.005765  0.110972  0.028278         0.485346\n",
       "  26  0.004902  0.004536  0.005290  0.085926  0.028046         0.484307,\n",
       "  0.006434897059583132),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  23  0.004382  0.004249  0.005050  0.092197  0.027581         0.488088\n",
       "  24  0.004729  0.004951  0.005101  0.103816  0.027707         0.491493\n",
       "  25  0.005234  0.005299  0.005765  0.110972  0.028278         0.485346\n",
       "  26  0.004902  0.004536  0.005290  0.085926  0.028046         0.484307\n",
       "  27  0.005321  0.006081  0.005874  0.205996  0.027679         0.502533,\n",
       "  0.007446265072372098),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  24  0.004729  0.004951  0.005101  0.103816  0.027707         0.491493\n",
       "  25  0.005234  0.005299  0.005765  0.110972  0.028278         0.485346\n",
       "  26  0.004902  0.004536  0.005290  0.085926  0.028046         0.484307\n",
       "  27  0.005321  0.006081  0.005874  0.205996  0.027679         0.502533\n",
       "  28  0.006621  0.007887  0.007332  0.371407  0.029692         0.494683,\n",
       "  0.007091565552959058),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  25  0.005234  0.005299  0.005765  0.110972  0.028278         0.485346\n",
       "  26  0.004902  0.004536  0.005290  0.085926  0.028046         0.484307\n",
       "  27  0.005321  0.006081  0.005874  0.205996  0.027679         0.502533\n",
       "  28  0.006621  0.007887  0.007332  0.371407  0.029692         0.494683\n",
       "  29  0.007391  0.007242  0.008003  0.167392  0.030679         0.484468,\n",
       "  0.007805756528214729),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  26  0.004902  0.004536  0.005290  0.085926  0.028046         0.484307\n",
       "  27  0.005321  0.006081  0.005874  0.205996  0.027679         0.502533\n",
       "  28  0.006621  0.007887  0.007332  0.371407  0.029692         0.494683\n",
       "  29  0.007391  0.007242  0.008003  0.167392  0.030679         0.484468\n",
       "  30  0.007607  0.007702  0.007979  0.184032  0.030333         0.492461,\n",
       "  0.00840011060328132),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  27  0.005321  0.006081  0.005874  0.205996  0.027679         0.502533\n",
       "  28  0.006621  0.007887  0.007332  0.371407  0.029692         0.494683\n",
       "  29  0.007391  0.007242  0.008003  0.167392  0.030679         0.484468\n",
       "  30  0.007607  0.007702  0.007979  0.184032  0.030333         0.492461\n",
       "  31  0.008678  0.008325  0.009221  0.158441  0.031030         0.491565,\n",
       "  0.009193387774349904),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  28  0.006621  0.007887  0.007332  0.371407  0.029692         0.494683\n",
       "  29  0.007391  0.007242  0.008003  0.167392  0.030679         0.484468\n",
       "  30  0.007607  0.007702  0.007979  0.184032  0.030333         0.492461\n",
       "  31  0.008678  0.008325  0.009221  0.158441  0.031030         0.491565\n",
       "  32  0.008536  0.008719  0.008788  0.182186  0.031611         0.493052,\n",
       "  0.00888423089574263),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  29  0.007391  0.007242  0.008003  0.167392  0.030679         0.484468\n",
       "  30  0.007607  0.007702  0.007979  0.184032  0.030333         0.492461\n",
       "  31  0.008678  0.008325  0.009221  0.158441  0.031030         0.491565\n",
       "  32  0.008536  0.008719  0.008788  0.182186  0.031611         0.493052\n",
       "  33  0.009232  0.008700  0.009698  0.162807  0.032386         0.484809,\n",
       "  0.009308423115747085),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  30  0.007607  0.007702  0.007979  0.184032  0.030333         0.492461\n",
       "  31  0.008678  0.008325  0.009221  0.158441  0.031030         0.491565\n",
       "  32  0.008536  0.008719  0.008788  0.182186  0.031611         0.493052\n",
       "  33  0.009232  0.008700  0.009698  0.162807  0.032386         0.484809\n",
       "  34  0.009080  0.009038  0.009832  0.171739  0.032084         0.490293,\n",
       "  0.009040007319153668),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  31  0.008678  0.008325  0.009221  0.158441  0.031030         0.491565\n",
       "  32  0.008536  0.008719  0.008788  0.182186  0.031611         0.493052\n",
       "  33  0.009232  0.008700  0.009698  0.162807  0.032386         0.484809\n",
       "  34  0.009080  0.009038  0.009832  0.171739  0.032084         0.490293\n",
       "  35  0.009513  0.008991  0.009945  0.134663  0.032498         0.485113,\n",
       "  0.008448049212301873),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  32  0.008536  0.008719  0.008788  0.182186  0.031611         0.493052\n",
       "  33  0.009232  0.008700  0.009698  0.162807  0.032386         0.484809\n",
       "  34  0.009080  0.009038  0.009832  0.171739  0.032084         0.490293\n",
       "  35  0.009513  0.008991  0.009945  0.134663  0.032498         0.485113\n",
       "  36  0.009099  0.008797  0.009178  0.127390  0.032236         0.482694,\n",
       "  0.008960921123340748),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  33  0.009232  0.008700  0.009698  0.162807  0.032386         0.484809\n",
       "  34  0.009080  0.009038  0.009832  0.171739  0.032084         0.490293\n",
       "  35  0.009513  0.008991  0.009945  0.134663  0.032498         0.485113\n",
       "  36  0.009099  0.008797  0.009178  0.127390  0.032236         0.482694\n",
       "  37  0.008483  0.008500  0.009069  0.141918  0.031658         0.490956,\n",
       "  0.009799731312852788),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  34  0.009080  0.009038  0.009832  0.171739  0.032084         0.490293\n",
       "  35  0.009513  0.008991  0.009945  0.134663  0.032498         0.485113\n",
       "  36  0.009099  0.008797  0.009178  0.127390  0.032236         0.482694\n",
       "  37  0.008483  0.008500  0.009069  0.141918  0.031658         0.490956\n",
       "  38  0.010620  0.009908  0.010687  0.240535  0.032159         0.493393,\n",
       "  0.010063355173016614),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  35  0.009513  0.008991  0.009945  0.134663  0.032498         0.485113\n",
       "  36  0.009099  0.008797  0.009178  0.127390  0.032236         0.482694\n",
       "  37  0.008483  0.008500  0.009069  0.141918  0.031658         0.490956\n",
       "  38  0.010620  0.009908  0.010687  0.240535  0.032159         0.493393\n",
       "  39  0.010064  0.009631  0.010318  0.127025  0.032978         0.489092,\n",
       "  0.010569039179411103),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  36  0.009099  0.008797  0.009178  0.127390  0.032236         0.482694\n",
       "  37  0.008483  0.008500  0.009069  0.141918  0.031658         0.490956\n",
       "  38  0.010620  0.009908  0.010687  0.240535  0.032159         0.493393\n",
       "  39  0.010064  0.009631  0.010318  0.127025  0.032978         0.489092\n",
       "  40  0.011008  0.010370  0.011140  0.160535  0.033235         0.490902,\n",
       "  0.011779330288202067),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  37  0.008483  0.008500  0.009069  0.141918  0.031658         0.490956\n",
       "  38  0.010620  0.009908  0.010687  0.240535  0.032159         0.493393\n",
       "  39  0.010064  0.009631  0.010318  0.127025  0.032978         0.489092\n",
       "  40  0.011008  0.010370  0.011140  0.160535  0.033235         0.490902\n",
       "  41  0.010591  0.011247  0.010960  0.170780  0.033729         0.496171,\n",
       "  0.011486945187098366),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  38  0.010620  0.009908  0.010687  0.240535  0.032159         0.493393\n",
       "  39  0.010064  0.009631  0.010318  0.127025  0.032978         0.489092\n",
       "  40  0.011008  0.010370  0.011140  0.160535  0.033235         0.490902\n",
       "  41  0.010591  0.011247  0.010960  0.170780  0.033729         0.496171\n",
       "  42  0.012348  0.012005  0.012448  0.220371  0.034911         0.484934,\n",
       "  0.009701467748959188),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  39  0.010064  0.009631  0.010318  0.127025  0.032978         0.489092\n",
       "  40  0.011008  0.010370  0.011140  0.160535  0.033235         0.490902\n",
       "  41  0.010591  0.011247  0.010960  0.170780  0.033729         0.496171\n",
       "  42  0.012348  0.012005  0.012448  0.220371  0.034911         0.484934\n",
       "  43  0.011754  0.011197  0.010570  0.276526  0.034625         0.473769,\n",
       "  0.011832051211289909),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  40  0.011008  0.010370  0.011140  0.160535  0.033235         0.490902\n",
       "  41  0.010591  0.011247  0.010960  0.170780  0.033729         0.496171\n",
       "  42  0.012348  0.012005  0.012448  0.220371  0.034911         0.484934\n",
       "  43  0.011754  0.011197  0.010570  0.276526  0.034625         0.473769\n",
       "  44  0.010880  0.011467  0.011060  0.354770  0.032882         0.503052,\n",
       "  0.017356239790192473),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  41  0.010591  0.011247  0.010960  0.170780  0.033729         0.496171\n",
       "  42  0.012348  0.012005  0.012448  0.220371  0.034911         0.484934\n",
       "  43  0.011754  0.011197  0.010570  0.276526  0.034625         0.473769\n",
       "  44  0.010880  0.011467  0.011060  0.354770  0.032882         0.503052\n",
       "  45  0.017171  0.018586  0.016500  0.897239  0.034963         0.528429,\n",
       "  0.020943956821612108),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  42  0.012348  0.012005  0.012448  0.220371  0.034911         0.484934\n",
       "  43  0.011754  0.011197  0.010570  0.276526  0.034625         0.473769\n",
       "  44  0.010880  0.011467  0.011060  0.354770  0.032882         0.503052\n",
       "  45  0.017171  0.018586  0.016500  0.897239  0.034963         0.528429\n",
       "  46  0.018581  0.021966  0.018551  0.796839  0.040357         0.513948,\n",
       "  0.019601858593920374),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  43  0.011754  0.011197  0.010570  0.276526  0.034625         0.473769\n",
       "  44  0.010880  0.011467  0.011060  0.354770  0.032882         0.503052\n",
       "  45  0.017171  0.018586  0.016500  0.897239  0.034963         0.528429\n",
       "  46  0.018581  0.021966  0.018551  0.796839  0.040357         0.513948\n",
       "  47  0.020974  0.021542  0.020356  0.542487  0.043861         0.477085,\n",
       "  0.02060124676563536),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  44  0.010880  0.011467  0.011060  0.354770  0.032882         0.503052\n",
       "  45  0.017171  0.018586  0.016500  0.897239  0.034963         0.528429\n",
       "  46  0.018581  0.021966  0.018551  0.796839  0.040357         0.513948\n",
       "  47  0.020974  0.021542  0.020356  0.542487  0.043861         0.477085\n",
       "  48  0.020102  0.020802  0.020785  0.324779  0.042550         0.494594,\n",
       "  0.022357953340472355),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  45  0.017171  0.018586  0.016500  0.897239  0.034963         0.528429\n",
       "  46  0.018581  0.021966  0.018551  0.796839  0.040357         0.513948\n",
       "  47  0.020974  0.021542  0.020356  0.542487  0.043861         0.477085\n",
       "  48  0.020102  0.020802  0.020785  0.324779  0.042550         0.494594\n",
       "  49  0.021055  0.021956  0.021712  0.361028  0.043526         0.500257,\n",
       "  0.02172046221517712),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  46  0.018581  0.021966  0.018551  0.796839  0.040357         0.513948\n",
       "  47  0.020974  0.021542  0.020356  0.542487  0.043861         0.477085\n",
       "  48  0.020102  0.020802  0.020785  0.324779  0.042550         0.494594\n",
       "  49  0.021055  0.021956  0.021712  0.361028  0.043526         0.500257\n",
       "  50  0.023994  0.023274  0.022923  0.514647  0.045242         0.482354,\n",
       "  0.02301222586563349),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  47  0.020974  0.021542  0.020356  0.542487  0.043861         0.477085\n",
       "  48  0.020102  0.020802  0.020785  0.324779  0.042550         0.494594\n",
       "  49  0.021055  0.021956  0.021712  0.361028  0.043526         0.500257\n",
       "  50  0.023994  0.023274  0.022923  0.514647  0.045242         0.482354\n",
       "  51  0.022709  0.022734  0.023086  0.297255  0.044619         0.496780,\n",
       "  0.022734226196180877),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  48  0.020102  0.020802  0.020785  0.324779  0.042550         0.494594\n",
       "  49  0.021055  0.021956  0.021712  0.361028  0.043526         0.500257\n",
       "  50  0.023994  0.023274  0.022923  0.514647  0.045242         0.482354\n",
       "  51  0.022709  0.022734  0.023086  0.297255  0.044619         0.496780\n",
       "  52  0.023967  0.023108  0.023587  0.275884  0.045881         0.485042,\n",
       "  0.02196730467547506),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  49  0.021055  0.021956  0.021712  0.361028  0.043526         0.500257\n",
       "  50  0.023994  0.023274  0.022923  0.514647  0.045242         0.482354\n",
       "  51  0.022709  0.022734  0.023086  0.297255  0.044619         0.496780\n",
       "  52  0.023967  0.023108  0.023587  0.275884  0.045881         0.485042\n",
       "  53  0.023823  0.023665  0.022960  0.337724  0.045610         0.481386,\n",
       "  0.020296872201095367),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  50  0.023994  0.023274  0.022923  0.514647  0.045242         0.482354\n",
       "  51  0.022709  0.022734  0.023086  0.297255  0.044619         0.496780\n",
       "  52  0.023967  0.023108  0.023587  0.275884  0.045881         0.485042\n",
       "  53  0.023823  0.023665  0.022960  0.337724  0.045610         0.481386\n",
       "  54  0.021479  0.021011  0.021167  0.350394  0.044861         0.474630,\n",
       "  0.016618079510426518),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  51  0.022709  0.022734  0.023086  0.297255  0.044619         0.496780\n",
       "  52  0.023967  0.023108  0.023587  0.275884  0.045881         0.485042\n",
       "  53  0.023823  0.023665  0.022960  0.337724  0.045610         0.481386\n",
       "  54  0.021479  0.021011  0.021167  0.350394  0.044861         0.474630\n",
       "  55  0.019924  0.019091  0.017583  0.482311  0.043229         0.459612,\n",
       "  0.01738499140877003),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  52  0.023967  0.023108  0.023587  0.275884  0.045881         0.485042\n",
       "  53  0.023823  0.023665  0.022960  0.337724  0.045610         0.481386\n",
       "  54  0.021479  0.021011  0.021167  0.350394  0.044861         0.474630\n",
       "  55  0.019924  0.019091  0.017583  0.482311  0.043229         0.459612\n",
       "  56  0.017265  0.017465  0.017788  0.272126  0.039637         0.492855,\n",
       "  0.016462303087015486),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  53  0.023823  0.023665  0.022960  0.337724  0.045610         0.481386\n",
       "  54  0.021479  0.021011  0.021167  0.350394  0.044861         0.474630\n",
       "  55  0.019924  0.019091  0.017583  0.482311  0.043229         0.459612\n",
       "  56  0.017265  0.017465  0.017788  0.272126  0.039637         0.492855\n",
       "  57  0.018028  0.017409  0.016788  0.269022  0.040385         0.480221,\n",
       "  0.01626098402279869),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  54  0.021479  0.021011  0.021167  0.350394  0.044861         0.474630\n",
       "  55  0.019924  0.019091  0.017583  0.482311  0.043229         0.459612\n",
       "  56  0.017265  0.017465  0.017788  0.272126  0.039637         0.492855\n",
       "  57  0.018028  0.017409  0.016788  0.269022  0.040385         0.480221\n",
       "  58  0.017202  0.016773  0.017045  0.258802  0.039484         0.485615,\n",
       "  0.019894243695024075),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  55  0.019924  0.019091  0.017583  0.482311  0.043229         0.459612\n",
       "  56  0.017265  0.017465  0.017788  0.272126  0.039637         0.492855\n",
       "  57  0.018028  0.017409  0.016788  0.269022  0.040385         0.480221\n",
       "  58  0.017202  0.016773  0.017045  0.258802  0.039484         0.485615\n",
       "  59  0.016832  0.019435  0.017345  0.364402  0.039288         0.514289,\n",
       "  0.019649787580578625),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  56  0.017265  0.017465  0.017788  0.272126  0.039637         0.492855\n",
       "  57  0.018028  0.017409  0.016788  0.269022  0.040385         0.480221\n",
       "  58  0.017202  0.016773  0.017045  0.258802  0.039484         0.485615\n",
       "  59  0.016832  0.019435  0.017345  0.364402  0.039288         0.514289\n",
       "  60  0.020706  0.020868  0.019726  0.407225  0.042836         0.485293,\n",
       "  0.020337613283109224),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  57  0.018028  0.017409  0.016788  0.269022  0.040385         0.480221\n",
       "  58  0.017202  0.016773  0.017045  0.258802  0.039484         0.485615\n",
       "  59  0.016832  0.019435  0.017345  0.364402  0.039288         0.514289\n",
       "  60  0.020706  0.020868  0.019726  0.407225  0.042836         0.485293\n",
       "  61  0.019556  0.020518  0.020053  0.289388  0.042597         0.492264,\n",
       "  0.017382595440555242),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  58  0.017202  0.016773  0.017045  0.258802  0.039484         0.485615\n",
       "  59  0.016832  0.019435  0.017345  0.364402  0.039288         0.514289\n",
       "  60  0.020706  0.020868  0.019726  0.407225  0.042836         0.485293\n",
       "  61  0.019556  0.020518  0.020053  0.289388  0.042597         0.492264\n",
       "  62  0.018846  0.018420  0.018135  0.508685  0.043269         0.465024,\n",
       "  0.017373011567696047),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  59  0.016832  0.019435  0.017345  0.364402  0.039288         0.514289\n",
       "  60  0.020706  0.020868  0.019726  0.407225  0.042836         0.485293\n",
       "  61  0.019556  0.020518  0.020053  0.289388  0.042597         0.492264\n",
       "  62  0.018846  0.018420  0.018135  0.508685  0.043269         0.465024\n",
       "  63  0.016805  0.017954  0.017692  0.440957  0.040383         0.487049,\n",
       "  0.016184293795200564),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  60  0.020706  0.020868  0.019726  0.407225  0.042836         0.485293\n",
       "  61  0.019556  0.020518  0.020053  0.289388  0.042597         0.492264\n",
       "  62  0.018846  0.018420  0.018135  0.508685  0.043269         0.465024\n",
       "  63  0.016805  0.017954  0.017692  0.440957  0.040383         0.487049\n",
       "  64  0.017111  0.017224  0.016899  0.404392  0.040374         0.478232,\n",
       "  0.016630059351500515),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  61  0.019556  0.020518  0.020053  0.289388  0.042597         0.492264\n",
       "  62  0.018846  0.018420  0.018135  0.508685  0.043269         0.465024\n",
       "  63  0.016805  0.017954  0.017692  0.440957  0.040383         0.487049\n",
       "  64  0.017111  0.017224  0.016899  0.404392  0.040374         0.478232\n",
       "  65  0.016825  0.016172  0.017091  0.213203  0.039213         0.490454,\n",
       "  0.015599523592993162),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  62  0.018846  0.018420  0.018135  0.508685  0.043269         0.465024\n",
       "  63  0.016805  0.017954  0.017692  0.440957  0.040383         0.487049\n",
       "  64  0.017111  0.017224  0.016899  0.404392  0.040374         0.478232\n",
       "  65  0.016825  0.016172  0.017091  0.213203  0.039213         0.490454\n",
       "  66  0.015710  0.016058  0.015829  0.300740  0.039648         0.479415,\n",
       "  0.016179501858770973),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  63  0.016805  0.017954  0.017692  0.440957  0.040383         0.487049\n",
       "  64  0.017111  0.017224  0.016899  0.404392  0.040374         0.478232\n",
       "  65  0.016825  0.016172  0.017091  0.213203  0.039213         0.490454\n",
       "  66  0.015710  0.016058  0.015829  0.300740  0.039648         0.479415\n",
       "  67  0.016553  0.016373  0.017086  0.301837  0.038642         0.491457,\n",
       "  0.017914644719674803),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  64  0.017111  0.017224  0.016899  0.404392  0.040374         0.478232\n",
       "  65  0.016825  0.016172  0.017091  0.213203  0.039213         0.490454\n",
       "  66  0.015710  0.016058  0.015829  0.300740  0.039648         0.479415\n",
       "  67  0.016553  0.016373  0.017086  0.301837  0.038642         0.491457\n",
       "  68  0.018201  0.017885  0.018542  0.371589  0.039208         0.500095,\n",
       "  0.019024276296357372),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  65  0.016825  0.016172  0.017091  0.213203  0.039213         0.490454\n",
       "  66  0.015710  0.016058  0.015829  0.300740  0.039648         0.479415\n",
       "  67  0.016553  0.016373  0.017086  0.301837  0.038642         0.491457\n",
       "  68  0.018201  0.017885  0.018542  0.371589  0.039208         0.500095\n",
       "  69  0.018437  0.018553  0.019222  0.157522  0.040903         0.495418,\n",
       "  0.01942211286599906),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  66  0.015710  0.016058  0.015829  0.300740  0.039648         0.479415\n",
       "  67  0.016553  0.016373  0.017086  0.301837  0.038642         0.491457\n",
       "  68  0.018201  0.017885  0.018542  0.371589  0.039208         0.500095\n",
       "  69  0.018437  0.018553  0.019222  0.157522  0.040903         0.495418\n",
       "  70  0.019534  0.019245  0.019753  0.259351  0.041986         0.490095,\n",
       "  0.019644995644149035),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  67  0.016553  0.016373  0.017086  0.301837  0.038642         0.491457\n",
       "  68  0.018201  0.017885  0.018542  0.371589  0.039208         0.500095\n",
       "  69  0.018437  0.018553  0.019222  0.157522  0.040903         0.495418\n",
       "  70  0.019534  0.019245  0.019753  0.259351  0.041986         0.490095\n",
       "  71  0.019619  0.019257  0.020416  0.187194  0.042375         0.488787,\n",
       "  0.019160884974050028),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  68  0.018201  0.017885  0.018542  0.371589  0.039208         0.500095\n",
       "  69  0.018437  0.018553  0.019222  0.157522  0.040903         0.495418\n",
       "  70  0.019534  0.019245  0.019753  0.259351  0.041986         0.490095\n",
       "  71  0.019619  0.019257  0.020416  0.187194  0.042375         0.488787\n",
       "  72  0.019917  0.019139  0.020247  0.191185  0.042593         0.483501,\n",
       "  0.01902667226457216),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  69  0.018437  0.018553  0.019222  0.157522  0.040903         0.495418\n",
       "  70  0.019534  0.019245  0.019753  0.259351  0.041986         0.490095\n",
       "  71  0.019619  0.019257  0.020416  0.187194  0.042375         0.488787\n",
       "  72  0.019917  0.019139  0.020247  0.191185  0.042593         0.483501\n",
       "  73  0.019424  0.018904  0.020005  0.152190  0.042120         0.486117,\n",
       "  0.019266336442588027),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  70  0.019534  0.019245  0.019753  0.259351  0.041986         0.490095\n",
       "  71  0.019619  0.019257  0.020416  0.187194  0.042375         0.488787\n",
       "  72  0.019917  0.019139  0.020247  0.191185  0.042593         0.483501\n",
       "  73  0.019424  0.018904  0.020005  0.152190  0.042120         0.486117\n",
       "  74  0.019436  0.018797  0.019775  0.142663  0.041989         0.488913,\n",
       "  0.018281324080161827),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  71  0.019619  0.019257  0.020416  0.187194  0.042375         0.488787\n",
       "  72  0.019917  0.019139  0.020247  0.191185  0.042593         0.483501\n",
       "  73  0.019424  0.018904  0.020005  0.152190  0.042120         0.486117\n",
       "  74  0.019436  0.018797  0.019775  0.142663  0.041989         0.488913\n",
       "  75  0.019238  0.018712  0.019392  0.152022  0.042223         0.479755,\n",
       "  0.017116575612176613),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  72  0.019917  0.019139  0.020247  0.191185  0.042593         0.483501\n",
       "  73  0.019424  0.018904  0.020005  0.152190  0.042120         0.486117\n",
       "  74  0.019436  0.018797  0.019775  0.142663  0.041989         0.488913\n",
       "  75  0.019238  0.018712  0.019392  0.152022  0.042223         0.479755\n",
       "  76  0.018485  0.017646  0.018067  0.167027  0.041261         0.478411,\n",
       "  0.016769063997407966),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  73  0.019424  0.018904  0.020005  0.152190  0.042120         0.486117\n",
       "  74  0.019436  0.018797  0.019775  0.142663  0.041989         0.488913\n",
       "  75  0.019238  0.018712  0.019392  0.152022  0.042223         0.479755\n",
       "  76  0.018485  0.017646  0.018067  0.167027  0.041261         0.478411\n",
       "  77  0.017125  0.017048  0.017626  0.183346  0.040123         0.484522,\n",
       "  0.017595894345846023),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  74  0.019436  0.018797  0.019775  0.142663  0.041989         0.488913\n",
       "  75  0.019238  0.018712  0.019392  0.152022  0.042223         0.479755\n",
       "  76  0.018485  0.017646  0.018067  0.167027  0.041261         0.478411\n",
       "  77  0.017125  0.017048  0.017626  0.183346  0.040123         0.484522\n",
       "  78  0.017101  0.017006  0.017563  0.186075  0.039784         0.493303,\n",
       "  0.017169296535264455),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  75  0.019238  0.018712  0.019392  0.152022  0.042223         0.479755\n",
       "  76  0.018485  0.017646  0.018067  0.167027  0.041261         0.478411\n",
       "  77  0.017125  0.017048  0.017626  0.183346  0.040123         0.484522\n",
       "  78  0.017101  0.017006  0.017563  0.186075  0.039784         0.493303\n",
       "  79  0.017867  0.017333  0.018246  0.104913  0.040591         0.483931,\n",
       "  0.01688170337059035),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  76  0.018485  0.017646  0.018067  0.167027  0.041261         0.478411\n",
       "  77  0.017125  0.017048  0.017626  0.183346  0.040123         0.484522\n",
       "  78  0.017101  0.017006  0.017563  0.186075  0.039784         0.493303\n",
       "  79  0.017867  0.017333  0.018246  0.104913  0.040591         0.483931\n",
       "  80  0.017563  0.016930  0.017800  0.117109  0.040175         0.484970,\n",
       "  0.018856510409510033),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  77  0.017125  0.017048  0.017626  0.183346  0.040123         0.484522\n",
       "  78  0.017101  0.017006  0.017563  0.186075  0.039784         0.493303\n",
       "  79  0.017867  0.017333  0.018246  0.104913  0.040591         0.483931\n",
       "  80  0.017563  0.016930  0.017800  0.117109  0.040175         0.484970\n",
       "  81  0.017282  0.018266  0.017837  0.269611  0.039894         0.501888,\n",
       "  0.019117738301459065),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  78  0.017101  0.017006  0.017563  0.186075  0.039784         0.493303\n",
       "  79  0.017867  0.017333  0.018246  0.104913  0.040591         0.483931\n",
       "  80  0.017563  0.016930  0.017800  0.117109  0.040175         0.484970\n",
       "  81  0.017282  0.018266  0.017837  0.269611  0.039894         0.501888\n",
       "  82  0.018964  0.018710  0.019547  0.278917  0.041823         0.489074,\n",
       "  0.018324461130390473),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  79  0.017867  0.017333  0.018246  0.104913  0.040591         0.483931\n",
       "  80  0.017563  0.016930  0.017800  0.117109  0.040175         0.484970\n",
       "  81  0.017282  0.018266  0.017837  0.269611  0.039894         0.501888\n",
       "  82  0.018964  0.018710  0.019547  0.278917  0.041823         0.489074\n",
       "  83  0.018714  0.018662  0.019375  0.208423  0.042078         0.481189,\n",
       "  0.019189636592627592),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  80  0.017563  0.016930  0.017800  0.117109  0.040175         0.484970\n",
       "  81  0.017282  0.018266  0.017837  0.269611  0.039894         0.501888\n",
       "  82  0.018964  0.018710  0.019547  0.278917  0.041823         0.489074\n",
       "  83  0.018714  0.018662  0.019375  0.208423  0.042078         0.481189\n",
       "  84  0.018668  0.018665  0.019520  0.179561  0.041303         0.493590,\n",
       "  0.020373562428693485),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  81  0.017282  0.018266  0.017837  0.269611  0.039894         0.501888\n",
       "  82  0.018964  0.018710  0.019547  0.278917  0.041823         0.489074\n",
       "  83  0.018714  0.018662  0.019375  0.208423  0.042078         0.481189\n",
       "  84  0.018668  0.018665  0.019520  0.179561  0.041303         0.493590\n",
       "  85  0.019929  0.020551  0.020809  0.239112  0.042148         0.495974,\n",
       "  0.0200691974865158),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  82  0.018964  0.018710  0.019547  0.278917  0.041823         0.489074\n",
       "  83  0.018714  0.018662  0.019375  0.208423  0.042078         0.481189\n",
       "  84  0.018668  0.018665  0.019520  0.179561  0.041303         0.493590\n",
       "  85  0.019929  0.020551  0.020809  0.239112  0.042148         0.495974\n",
       "  86  0.020966  0.020413  0.021179  0.134079  0.043304         0.484845,\n",
       "  0.020680332961448274),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  83  0.018714  0.018662  0.019375  0.208423  0.042078         0.481189\n",
       "  84  0.018668  0.018665  0.019520  0.179561  0.041303         0.493590\n",
       "  85  0.019929  0.020551  0.020809  0.239112  0.042148         0.495974\n",
       "  86  0.020966  0.020413  0.021179  0.134079  0.043304         0.484845\n",
       "  87  0.020386  0.020169  0.021085  0.094935  0.043007         0.491690,\n",
       "  0.02106378409943888),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  84  0.018668  0.018665  0.019520  0.179561  0.041303         0.493590\n",
       "  85  0.019929  0.020551  0.020809  0.239112  0.042148         0.495974\n",
       "  86  0.020966  0.020413  0.021179  0.134079  0.043304         0.484845\n",
       "  87  0.020386  0.020169  0.021085  0.094935  0.043007         0.491690\n",
       "  88  0.021241  0.020584  0.021809  0.087823  0.043604         0.489988,\n",
       "  0.022024827157354803),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  85  0.019929  0.020551  0.020809  0.239112  0.042148         0.495974\n",
       "  86  0.020966  0.020413  0.021179  0.134079  0.043304         0.484845\n",
       "  87  0.020386  0.020169  0.021085  0.094935  0.043007         0.491690\n",
       "  88  0.021241  0.020584  0.021809  0.087823  0.043604         0.489988\n",
       "  89  0.021650  0.021698  0.022560  0.148376  0.043978         0.494307,\n",
       "  0.022228542189786395),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  86  0.020966  0.020413  0.021179  0.134079  0.043304         0.484845\n",
       "  87  0.020386  0.020169  0.021085  0.094935  0.043007         0.491690\n",
       "  88  0.021241  0.020584  0.021809  0.087823  0.043604         0.489988\n",
       "  89  0.021650  0.021698  0.022560  0.148376  0.043978         0.494307\n",
       "  90  0.022362  0.021757  0.023023  0.100744  0.044917         0.488644,\n",
       "  0.02226209536715586),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  87  0.020386  0.020169  0.021085  0.094935  0.043007         0.491690\n",
       "  88  0.021241  0.020584  0.021809  0.087823  0.043604         0.489988\n",
       "  89  0.021650  0.021698  0.022560  0.148376  0.043978         0.494307\n",
       "  90  0.022362  0.021757  0.023023  0.100744  0.044917         0.488644\n",
       "  91  0.022283  0.021750  0.023209  0.065044  0.045116         0.487371,\n",
       "  0.02338849872134201),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  88  0.021241  0.020584  0.021809  0.087823  0.043604         0.489988\n",
       "  89  0.021650  0.021698  0.022560  0.148376  0.043978         0.494307\n",
       "  90  0.022362  0.021757  0.023023  0.100744  0.044917         0.488644\n",
       "  91  0.022283  0.021750  0.023209  0.065044  0.045116         0.487371\n",
       "  92  0.022569  0.022866  0.023226  0.143516  0.045148         0.495543,\n",
       "  0.02223573009443078),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  89  0.021650  0.021698  0.022560  0.148376  0.043978         0.494307\n",
       "  90  0.022362  0.021757  0.023023  0.100744  0.044917         0.488644\n",
       "  91  0.022283  0.021750  0.023209  0.065044  0.045116         0.487371\n",
       "  92  0.022569  0.022866  0.023226  0.143516  0.045148         0.495543\n",
       "  93  0.024076  0.023257  0.023398  0.186428  0.046248         0.478501,\n",
       "  0.024613165639421754),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  90  0.022362  0.021757  0.023023  0.100744  0.044917         0.488644\n",
       "  91  0.022283  0.021750  0.023209  0.065044  0.045116         0.487371\n",
       "  92  0.022569  0.022866  0.023226  0.143516  0.045148         0.495543\n",
       "  93  0.024076  0.023257  0.023398  0.186428  0.046248         0.478501\n",
       "  94  0.023635  0.024148  0.024101  0.385288  0.045123         0.504898,\n",
       "  0.022645546505146465),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  91  0.022283  0.021750  0.023209  0.065044  0.045116         0.487371\n",
       "  92  0.022569  0.022866  0.023226  0.143516  0.045148         0.495543\n",
       "  93  0.024076  0.023257  0.023398  0.186428  0.046248         0.478501\n",
       "  94  0.023635  0.024148  0.024101  0.385288  0.045123         0.504898\n",
       "  95  0.024598  0.023980  0.023621  0.334494  0.047444         0.472407,\n",
       "  0.02240828791770771),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  92  0.022569  0.022866  0.023226  0.143516  0.045148         0.495543\n",
       "  93  0.024076  0.023257  0.023398  0.186428  0.046248         0.478501\n",
       "  94  0.023635  0.024148  0.024101  0.385288  0.045123         0.504898\n",
       "  95  0.024598  0.023980  0.023621  0.334494  0.047444         0.472407\n",
       "  96  0.022685  0.022551  0.023318  0.200246  0.045523         0.485346,\n",
       "  0.021219570145212228),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  93  0.024076  0.023257  0.023398  0.186428  0.046248         0.478501\n",
       "  94  0.023635  0.024148  0.024101  0.385288  0.045123         0.504898\n",
       "  95  0.024598  0.023980  0.023621  0.334494  0.047444         0.472407\n",
       "  96  0.022685  0.022551  0.023318  0.200246  0.045523         0.485346\n",
       "  97  0.023077  0.022314  0.022226  0.252553  0.045291         0.478232,\n",
       "  0.022489770081735427),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  94  0.023635  0.024148  0.024101  0.385288  0.045123         0.504898\n",
       "  95  0.024598  0.023980  0.023621  0.334494  0.047444         0.472407\n",
       "  96  0.022685  0.022551  0.023318  0.200246  0.045523         0.485346\n",
       "  97  0.023077  0.022314  0.022226  0.252553  0.045291         0.478232\n",
       "  98  0.022008  0.021923  0.022483  0.234938  0.044130         0.496619,\n",
       "  0.022779759214624332),\n",
       " (        Open      High       Low    Volume  Previous  last_difference\n",
       "  95  0.024598  0.023980  0.023621  0.334494  0.047444         0.472407\n",
       "  96  0.022685  0.022551  0.023318  0.200246  0.045523         0.485346\n",
       "  97  0.023077  0.022314  0.022226  0.252553  0.045291         0.478232\n",
       "  98  0.022008  0.021923  0.022483  0.234938  0.044130         0.496619\n",
       "  99  0.022937  0.022835  0.023221  0.183292  0.045371         0.489289,\n",
       "  0.022415475822352104),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  96   0.022685  0.022551  0.023318  0.200246  0.045523         0.485346\n",
       "  97   0.023077  0.022314  0.022226  0.252553  0.045291         0.478232\n",
       "  98   0.022008  0.021923  0.022483  0.234938  0.044130         0.496619\n",
       "  99   0.022937  0.022835  0.023221  0.183292  0.045371         0.489289\n",
       "  100  0.023207  0.022743  0.023548  0.169163  0.045654         0.484397,\n",
       "  0.022856449442222444),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  97   0.023077  0.022314  0.022226  0.252553  0.045291         0.478232\n",
       "  98   0.022008  0.021923  0.022483  0.234938  0.044130         0.496619\n",
       "  99   0.022937  0.022835  0.023221  0.183292  0.045371         0.489289\n",
       "  100  0.023207  0.022743  0.023548  0.169163  0.045654         0.484397\n",
       "  101  0.022896  0.022321  0.022899  0.198816  0.045298         0.490418,\n",
       "  0.02284446960114846),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  98   0.022008  0.021923  0.022483  0.234938  0.044130         0.496619\n",
       "  99   0.022937  0.022835  0.023221  0.183292  0.045371         0.489289\n",
       "  100  0.023207  0.022743  0.023548  0.169163  0.045654         0.484397\n",
       "  101  0.022896  0.022321  0.022899  0.198816  0.045298         0.490418\n",
       "  102  0.023149  0.022667  0.023759  0.166504  0.045729         0.487031,\n",
       "  0.023956497146045832),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  99   0.022937  0.022835  0.023221  0.183292  0.045371         0.489289\n",
       "  100  0.023207  0.022743  0.023548  0.169163  0.045654         0.484397\n",
       "  101  0.022896  0.022321  0.022899  0.198816  0.045298         0.490418\n",
       "  102  0.023149  0.022667  0.023759  0.166504  0.045729         0.487031\n",
       "  103  0.023298  0.023288  0.023778  0.234388  0.045717         0.495436,\n",
       "  0.024898362835881063),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  100  0.023207  0.022743  0.023548  0.169163  0.045654         0.484397\n",
       "  101  0.022896  0.022321  0.022899  0.198816  0.045298         0.490418\n",
       "  102  0.023149  0.022667  0.023759  0.166504  0.045729         0.487031\n",
       "  103  0.023298  0.023288  0.023778  0.234388  0.045717         0.495436\n",
       "  104  0.024494  0.024475  0.024876  0.320306  0.046803         0.494164,\n",
       "  0.023316600430173483),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  101  0.022896  0.022321  0.022899  0.198816  0.045298         0.490418\n",
       "  102  0.023149  0.022667  0.023759  0.166504  0.045729         0.487031\n",
       "  103  0.023298  0.023288  0.023778  0.234388  0.045717         0.495436\n",
       "  104  0.024494  0.024475  0.024876  0.320306  0.046803         0.494164\n",
       "  105  0.025380  0.024542  0.024403  0.273729  0.047723         0.475293,\n",
       "  0.022506541859239),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  102  0.023149  0.022667  0.023759  0.166504  0.045729         0.487031\n",
       "  103  0.023298  0.023288  0.023778  0.234388  0.045717         0.495436\n",
       "  104  0.024494  0.024475  0.024876  0.320306  0.046803         0.494164\n",
       "  105  0.025380  0.024542  0.024403  0.273729  0.047723         0.475293\n",
       "  106  0.022456  0.022397  0.023262  0.218853  0.046178         0.481063,\n",
       "  0.021154859758688086),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  103  0.023298  0.023288  0.023778  0.234388  0.045717         0.495436\n",
       "  104  0.024494  0.024475  0.024876  0.320306  0.046803         0.494164\n",
       "  105  0.025380  0.024542  0.024403  0.273729  0.047723         0.475293\n",
       "  106  0.022456  0.022397  0.023262  0.218853  0.046178         0.481063\n",
       "  107  0.022947  0.022186  0.022323  0.225099  0.045387         0.477013,\n",
       "  0.01934302667018614),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  104  0.024494  0.024475  0.024876  0.320306  0.046803         0.494164\n",
       "  105  0.025380  0.024542  0.024403  0.273729  0.047723         0.475293\n",
       "  106  0.022456  0.022397  0.023262  0.218853  0.046178         0.481063\n",
       "  107  0.022947  0.022186  0.022323  0.225099  0.045387         0.477013\n",
       "  108  0.021539  0.020757  0.020433  0.340983  0.044067         0.473572,\n",
       "  0.01848024717616382),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  105  0.025380  0.024542  0.024403  0.273729  0.047723         0.475293\n",
       "  106  0.022456  0.022397  0.023262  0.218853  0.046178         0.481063\n",
       "  107  0.022947  0.022186  0.022323  0.225099  0.045387         0.477013\n",
       "  108  0.021539  0.020757  0.020433  0.340983  0.044067         0.473572\n",
       "  109  0.019915  0.019077  0.019457  0.259171  0.042298         0.480669,\n",
       "  0.021384930441482455),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  106  0.022456  0.022397  0.023262  0.218853  0.046178         0.481063\n",
       "  107  0.022947  0.022186  0.022323  0.225099  0.045387         0.477013\n",
       "  108  0.021539  0.020757  0.020433  0.340983  0.044067         0.473572\n",
       "  109  0.019915  0.019077  0.019457  0.259171  0.042298         0.480669\n",
       "  110  0.019272  0.020776  0.020150  0.299273  0.041455         0.508841,\n",
       "  0.021106930772029842),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  107  0.022947  0.022186  0.022323  0.225099  0.045387         0.477013\n",
       "  108  0.021539  0.020757  0.020433  0.340983  0.044067         0.473572\n",
       "  109  0.019915  0.019077  0.019457  0.259171  0.042298         0.480669\n",
       "  110  0.019272  0.020776  0.020150  0.299273  0.041455         0.508841\n",
       "  111  0.021556  0.020646  0.021615  0.161105  0.044292         0.485042,\n",
       "  0.02164856392400859),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  108  0.021539  0.020757  0.020433  0.340983  0.044067         0.473572\n",
       "  109  0.019915  0.019077  0.019457  0.259171  0.042298         0.480669\n",
       "  110  0.019272  0.020776  0.020150  0.299273  0.041455         0.508841\n",
       "  111  0.021556  0.020646  0.021615  0.161105  0.044292         0.485042\n",
       "  112  0.021859  0.022030  0.021892  0.296848  0.044020         0.491171,\n",
       "  0.0229139719241022),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  109  0.019915  0.019077  0.019457  0.259171  0.042298         0.480669\n",
       "  110  0.019272  0.020776  0.020150  0.299273  0.041455         0.508841\n",
       "  111  0.021556  0.020646  0.021615  0.161105  0.044292         0.485042\n",
       "  112  0.021859  0.022030  0.021892  0.296848  0.044020         0.491171\n",
       "  113  0.022742  0.022423  0.023195  0.233328  0.044549         0.496583,\n",
       "  0.02202243118914),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  110  0.019272  0.020776  0.020150  0.299273  0.041455         0.508841\n",
       "  111  0.021556  0.020646  0.021615  0.161105  0.044292         0.485042\n",
       "  112  0.021859  0.022030  0.021892  0.296848  0.044020         0.491171\n",
       "  113  0.022742  0.022423  0.023195  0.233328  0.044549         0.496583\n",
       "  114  0.022908  0.022494  0.022931  0.458132  0.045785         0.480454,\n",
       "  0.025392057378839258),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  111  0.021556  0.020646  0.021615  0.161105  0.044292         0.485042\n",
       "  112  0.021859  0.022030  0.021892  0.296848  0.044020         0.491171\n",
       "  113  0.022742  0.022423  0.023195  0.233328  0.044549         0.496583\n",
       "  114  0.022908  0.022494  0.022931  0.458132  0.045785         0.480454\n",
       "  115  0.028003  0.027267  0.026087  0.797693  0.044914         0.512317,\n",
       "  0.026566399342045955),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  112  0.021859  0.022030  0.021892  0.296848  0.044020         0.491171\n",
       "  113  0.022742  0.022423  0.023195  0.233328  0.044549         0.496583\n",
       "  114  0.022908  0.022494  0.022931  0.458132  0.045785         0.480454\n",
       "  115  0.028003  0.027267  0.026087  0.797693  0.044914         0.512317\n",
       "  116  0.025703  0.026454  0.026607  0.315818  0.048205         0.495902,\n",
       "  0.025008606240848653),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  113  0.022742  0.022423  0.023195  0.233328  0.044549         0.496583\n",
       "  114  0.022908  0.022494  0.022931  0.458132  0.045785         0.480454\n",
       "  115  0.028003  0.027267  0.026087  0.797693  0.044914         0.512317\n",
       "  116  0.025703  0.026454  0.026607  0.315818  0.048205         0.495902\n",
       "  117  0.025818  0.025122  0.025830  0.360359  0.049352         0.475472,\n",
       "  0.02301222586563349),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  114  0.022908  0.022494  0.022931  0.458132  0.045785         0.480454\n",
       "  115  0.028003  0.027267  0.026087  0.797693  0.044914         0.512317\n",
       "  116  0.025703  0.026454  0.026607  0.315818  0.048205         0.495902\n",
       "  117  0.025818  0.025122  0.025830  0.360359  0.049352         0.475472\n",
       "  118  0.025527  0.024802  0.024113  0.315144  0.047831         0.472192,\n",
       "  0.023637746772217052),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  115  0.028003  0.027267  0.026087  0.797693  0.044914         0.512317\n",
       "  116  0.025703  0.026454  0.026607  0.315818  0.048205         0.495902\n",
       "  117  0.025818  0.025122  0.025830  0.360359  0.049352         0.475472\n",
       "  118  0.025527  0.024802  0.024113  0.315144  0.047831         0.472192\n",
       "  119  0.023529  0.023290  0.023875  0.279136  0.045881         0.491798,\n",
       "  0.021945740961541883),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  116  0.025703  0.026454  0.026607  0.315818  0.048205         0.495902\n",
       "  117  0.025818  0.025122  0.025830  0.360359  0.049352         0.475472\n",
       "  118  0.025527  0.024802  0.024113  0.315144  0.047831         0.472192\n",
       "  119  0.023529  0.023290  0.023875  0.279136  0.045881         0.491798\n",
       "  120  0.024444  0.023665  0.022647  0.417572  0.046492         0.474468,\n",
       "  0.02108296146751956),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  117  0.025818  0.025122  0.025830  0.360359  0.049352         0.475472\n",
       "  118  0.025527  0.024802  0.024113  0.315144  0.047831         0.472192\n",
       "  119  0.023529  0.023290  0.023875  0.279136  0.045881         0.491798\n",
       "  120  0.024444  0.023665  0.022647  0.417572  0.046492         0.474468\n",
       "  121  0.022328  0.021440  0.021628  0.461627  0.044839         0.480669,\n",
       "  0.020943956821612108),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  118  0.025527  0.024802  0.024113  0.315144  0.047831         0.472192\n",
       "  119  0.023529  0.023290  0.023875  0.279136  0.045881         0.491798\n",
       "  120  0.024444  0.023665  0.022647  0.417572  0.046492         0.474468\n",
       "  121  0.022328  0.021440  0.021628  0.461627  0.044839         0.480669\n",
       "  122  0.021051  0.021466  0.021826  0.318929  0.043997         0.486081,\n",
       "  0.02228365908108904),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  119  0.023529  0.023290  0.023875  0.279136  0.045881         0.491798\n",
       "  120  0.024444  0.023665  0.022647  0.417572  0.046492         0.474468\n",
       "  121  0.022328  0.021440  0.021628  0.461627  0.044839         0.480669\n",
       "  122  0.021051  0.021466  0.021826  0.318929  0.043997         0.486081\n",
       "  123  0.020134  0.021646  0.020598  0.937866  0.043861         0.497138,\n",
       "  0.022820500296638183),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  120  0.024444  0.023665  0.022647  0.417572  0.046492         0.474468\n",
       "  121  0.022328  0.021440  0.021628  0.461627  0.044839         0.480669\n",
       "  122  0.021051  0.021466  0.021826  0.318929  0.043997         0.486081\n",
       "  123  0.020134  0.021646  0.020598  0.937866  0.043861         0.497138\n",
       "  124  0.022721  0.023248  0.023524  0.627028  0.045169         0.491135,\n",
       "  0.023582620258552105),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  121  0.022328  0.021440  0.021628  0.461627  0.044839         0.480669\n",
       "  122  0.021051  0.021466  0.021826  0.318929  0.043997         0.486081\n",
       "  123  0.020134  0.021646  0.020598  0.937866  0.043861         0.497138\n",
       "  124  0.022721  0.023248  0.023524  0.627028  0.045169         0.491135\n",
       "  125  0.022985  0.023127  0.023820  0.402025  0.045694         0.492819,\n",
       "  0.02346039701251054),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  122  0.021051  0.021466  0.021826  0.318929  0.043997         0.486081\n",
       "  123  0.020134  0.021646  0.020598  0.937866  0.043861         0.497138\n",
       "  124  0.022721  0.023248  0.023524  0.627028  0.045169         0.491135\n",
       "  125  0.022985  0.023127  0.023820  0.402025  0.045694         0.492819\n",
       "  126  0.023739  0.023226  0.024428  0.253217  0.046438         0.486207,\n",
       "  0.023472376853584522),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  123  0.020134  0.021646  0.020598  0.937866  0.043861         0.497138\n",
       "  124  0.022721  0.023248  0.023524  0.627028  0.045169         0.491135\n",
       "  125  0.022985  0.023127  0.023820  0.402025  0.045694         0.492819\n",
       "  126  0.023739  0.023226  0.024428  0.253217  0.046438         0.486207\n",
       "  127  0.023902  0.023011  0.024391  0.206310  0.046319         0.487210,\n",
       "  0.021895406384306534),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  124  0.022721  0.023248  0.023524  0.627028  0.045169         0.491135\n",
       "  125  0.022985  0.023127  0.023820  0.402025  0.045694         0.492819\n",
       "  126  0.023739  0.023226  0.024428  0.253217  0.046438         0.486207\n",
       "  127  0.023902  0.023011  0.024391  0.206310  0.046319         0.487210\n",
       "  128  0.023419  0.023025  0.022873  0.327873  0.046330         0.475328,\n",
       "  0.02251373938624571),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  125  0.022985  0.023127  0.023820  0.402025  0.045694         0.492819\n",
       "  126  0.023739  0.023226  0.024428  0.253217  0.046438         0.486207\n",
       "  127  0.023902  0.023011  0.024391  0.206310  0.046319         0.487210\n",
       "  128  0.023419  0.023025  0.022873  0.327873  0.046330         0.475328\n",
       "  129  0.022649  0.021978  0.022453  0.379008  0.044790         0.491744,\n",
       "  0.021301052309239937),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  126  0.023739  0.023226  0.024428  0.253217  0.046438         0.486207\n",
       "  127  0.023902  0.023011  0.024391  0.206310  0.046319         0.487210\n",
       "  128  0.023419  0.023025  0.022873  0.327873  0.046330         0.475328\n",
       "  129  0.022649  0.021978  0.022453  0.379008  0.044790         0.491744\n",
       "  130  0.020259  0.020880  0.020896  0.627794  0.045394         0.478052,\n",
       "  0.020577277461125084),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  127  0.023902  0.023011  0.024391  0.206310  0.046319         0.487210\n",
       "  128  0.023419  0.023025  0.022873  0.327873  0.046330         0.475328\n",
       "  129  0.022649  0.021978  0.022453  0.379008  0.044790         0.491744\n",
       "  130  0.020259  0.020880  0.020896  0.627794  0.045394         0.478052\n",
       "  131  0.021650  0.020897  0.021690  0.242493  0.044210         0.481708,\n",
       "  0.02108535743573436),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  128  0.023419  0.023025  0.022873  0.327873  0.046330         0.475328\n",
       "  129  0.022649  0.021978  0.022453  0.379008  0.044790         0.491744\n",
       "  130  0.020259  0.020880  0.020896  0.627794  0.045394         0.478052\n",
       "  131  0.021650  0.020897  0.021690  0.242493  0.044210         0.481708\n",
       "  132  0.020892  0.020885  0.021773  0.190074  0.043503         0.490920,\n",
       "  0.020622810479568533),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  129  0.022649  0.021978  0.022453  0.379008  0.044790         0.491744\n",
       "  130  0.020259  0.020880  0.020896  0.627794  0.045394         0.478052\n",
       "  131  0.021650  0.020897  0.021690  0.242493  0.044210         0.481708\n",
       "  132  0.020892  0.020885  0.021773  0.190074  0.043503         0.490920\n",
       "  133  0.021683  0.020857  0.020840  0.226384  0.043999         0.483662,\n",
       "  0.020411907542492547),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  130  0.020259  0.020880  0.020896  0.627794  0.045394         0.478052\n",
       "  131  0.021650  0.020897  0.021690  0.242493  0.044210         0.481708\n",
       "  132  0.020892  0.020885  0.021773  0.190074  0.043503         0.490920\n",
       "  133  0.021683  0.020857  0.020840  0.226384  0.043999         0.483662\n",
       "  134  0.020880  0.020364  0.021412  0.177112  0.043548         0.485543,\n",
       "  0.020850485194148092),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  131  0.021650  0.020897  0.021690  0.242493  0.044210         0.481708\n",
       "  132  0.020892  0.020885  0.021773  0.190074  0.043503         0.490920\n",
       "  133  0.021683  0.020857  0.020840  0.226384  0.043999         0.483662\n",
       "  134  0.020880  0.020364  0.021412  0.177112  0.043548         0.485543\n",
       "  135  0.020923  0.020383  0.021400  0.184971  0.043342         0.490400,\n",
       "  0.02058446536576947),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  132  0.020892  0.020885  0.021773  0.190074  0.043503         0.490920\n",
       "  133  0.021683  0.020857  0.020840  0.226384  0.043999         0.483662\n",
       "  134  0.020880  0.020364  0.021412  0.177112  0.043548         0.485543\n",
       "  135  0.020923  0.020383  0.021400  0.184971  0.043342         0.490400\n",
       "  136  0.021060  0.020264  0.021584  0.164673  0.043770         0.485131,\n",
       "  0.021281874941159257),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  133  0.021683  0.020857  0.020840  0.226384  0.043999         0.483662\n",
       "  134  0.020880  0.020364  0.021412  0.177112  0.043548         0.485543\n",
       "  135  0.020923  0.020383  0.021400  0.184971  0.043342         0.490400\n",
       "  136  0.021060  0.020264  0.021584  0.164673  0.043770         0.485131\n",
       "  137  0.021320  0.020821  0.022059  0.210724  0.043510         0.492336,\n",
       "  0.020416699478922138),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  134  0.020880  0.020364  0.021412  0.177112  0.043548         0.485543\n",
       "  135  0.020923  0.020383  0.021400  0.184971  0.043342         0.490400\n",
       "  136  0.021060  0.020264  0.021584  0.164673  0.043770         0.485131\n",
       "  137  0.021320  0.020821  0.022059  0.210724  0.043510         0.492336\n",
       "  138  0.021638  0.020880  0.021560  0.195612  0.044191         0.480651,\n",
       "  0.019494011157167587),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  135  0.020923  0.020383  0.021400  0.184971  0.043342         0.490400\n",
       "  136  0.021060  0.020264  0.021584  0.164673  0.043770         0.485131\n",
       "  137  0.021320  0.020821  0.022059  0.210724  0.043510         0.492336\n",
       "  138  0.021638  0.020880  0.021560  0.195612  0.044191         0.480651\n",
       "  139  0.020461  0.019648  0.020395  0.276227  0.043346         0.480221,\n",
       "  0.019165676910479625),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  136  0.021060  0.020264  0.021584  0.164673  0.043770         0.485131\n",
       "  137  0.021320  0.020821  0.022059  0.210724  0.043510         0.492336\n",
       "  138  0.021638  0.020880  0.021560  0.195612  0.044191         0.480651\n",
       "  139  0.020461  0.019648  0.020395  0.276227  0.043346         0.480221\n",
       "  140  0.019691  0.018830  0.019726  0.266500  0.042445         0.484665,\n",
       "  0.018643211504219253),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  137  0.021320  0.020821  0.022059  0.210724  0.043510         0.492336\n",
       "  138  0.021638  0.020880  0.021560  0.195612  0.044191         0.480651\n",
       "  139  0.020461  0.019648  0.020395  0.276227  0.043346         0.480221\n",
       "  140  0.019691  0.018830  0.019726  0.266500  0.042445         0.484665\n",
       "  141  0.019554  0.018771  0.019666  0.195179  0.042124         0.483214,\n",
       "  0.017969771233339742),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  138  0.021638  0.020880  0.021560  0.195612  0.044191         0.480651\n",
       "  139  0.020461  0.019648  0.020395  0.276227  0.043346         0.480221\n",
       "  140  0.019691  0.018830  0.019726  0.266500  0.042445         0.484665\n",
       "  141  0.019554  0.018771  0.019666  0.195179  0.042124         0.483214\n",
       "  142  0.019046  0.018167  0.018556  0.271027  0.041614         0.482085,\n",
       "  0.018837342663791657),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  139  0.020461  0.019648  0.020395  0.276227  0.043346         0.480221\n",
       "  140  0.019691  0.018830  0.019726  0.266500  0.042445         0.484665\n",
       "  141  0.019554  0.018771  0.019666  0.195179  0.042124         0.483214\n",
       "  142  0.019046  0.018167  0.018556  0.271027  0.041614         0.482085\n",
       "  143  0.018317  0.018546  0.018953  0.253404  0.040957         0.493608,\n",
       "  0.018115963783891593),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  140  0.019691  0.018830  0.019726  0.266500  0.042445         0.484665\n",
       "  141  0.019554  0.018771  0.019666  0.195179  0.042124         0.483214\n",
       "  142  0.019046  0.018167  0.018556  0.271027  0.041614         0.482085\n",
       "  143  0.018317  0.018546  0.018953  0.253404  0.040957         0.493608\n",
       "  144  0.018654  0.018217  0.019147  0.172753  0.041804         0.481726,\n",
       "  0.019000306991847096),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  141  0.019554  0.018771  0.019666  0.195179  0.042124         0.483214\n",
       "  142  0.019046  0.018167  0.018556  0.271027  0.041614         0.482085\n",
       "  143  0.018317  0.018546  0.018953  0.253404  0.040957         0.493608\n",
       "  144  0.018654  0.018217  0.019147  0.172753  0.041804         0.481726\n",
       "  145  0.018757  0.018461  0.019339  0.200830  0.041099         0.493733,\n",
       "  0.019180052719768404),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  142  0.019046  0.018167  0.018556  0.271027  0.041614         0.482085\n",
       "  143  0.018317  0.018546  0.018953  0.253404  0.040957         0.493608\n",
       "  144  0.018654  0.018217  0.019147  0.172753  0.041804         0.481726\n",
       "  145  0.018757  0.018461  0.019339  0.200830  0.041099         0.493733\n",
       "  146  0.019161  0.018641  0.019947  0.172357  0.041963         0.488465,\n",
       "  0.0193813717839852),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  143  0.018317  0.018546  0.018953  0.253404  0.040957         0.493608\n",
       "  144  0.018654  0.018217  0.019147  0.172753  0.041804         0.481726\n",
       "  145  0.018757  0.018461  0.019339  0.200830  0.041099         0.493733\n",
       "  146  0.019161  0.018641  0.019947  0.172357  0.041963         0.488465\n",
       "  147  0.019272  0.019060  0.019690  0.181933  0.042139         0.488626,\n",
       "  0.01883494669557686),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  144  0.018654  0.018217  0.019147  0.172753  0.041804         0.481726\n",
       "  145  0.018757  0.018461  0.019339  0.200830  0.041099         0.493733\n",
       "  146  0.019161  0.018641  0.019947  0.172357  0.041963         0.488465\n",
       "  147  0.019272  0.019060  0.019690  0.181933  0.042139         0.488626\n",
       "  148  0.019732  0.019006  0.019835  0.136886  0.042335         0.483035,\n",
       "  0.018926012732463766),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  145  0.018757  0.018461  0.019339  0.200830  0.041099         0.493733\n",
       "  146  0.019161  0.018641  0.019947  0.172357  0.041963         0.488465\n",
       "  147  0.019272  0.019060  0.019690  0.181933  0.042139         0.488626\n",
       "  148  0.019732  0.019006  0.019835  0.136886  0.042335         0.483035\n",
       "  149  0.018959  0.018603  0.019864  0.117751  0.041801         0.487802,\n",
       "  0.0189907231189879),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  146  0.019161  0.018641  0.019947  0.172357  0.041963         0.488465\n",
       "  147  0.019272  0.019060  0.019690  0.181933  0.042139         0.488626\n",
       "  148  0.019732  0.019006  0.019835  0.136886  0.042335         0.483035\n",
       "  149  0.018959  0.018603  0.019864  0.117751  0.041801         0.487802\n",
       "  150  0.019616  0.018750  0.020162  0.090027  0.041890         0.487604,\n",
       "  0.019510782934671168),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  147  0.019272  0.019060  0.019690  0.181933  0.042139         0.488626\n",
       "  148  0.019732  0.019006  0.019835  0.136886  0.042335         0.483035\n",
       "  149  0.018959  0.018603  0.019864  0.117751  0.041801         0.487802\n",
       "  150  0.019616  0.018750  0.020162  0.090027  0.041890         0.487604\n",
       "  151  0.019852  0.019684  0.020586  0.212441  0.041954         0.491009,\n",
       "  0.01906741334658602),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  148  0.019732  0.019006  0.019835  0.136886  0.042335         0.483035\n",
       "  149  0.018959  0.018603  0.019864  0.117751  0.041801         0.487802\n",
       "  150  0.019616  0.018750  0.020162  0.090027  0.041890         0.487604\n",
       "  151  0.019852  0.019684  0.020586  0.212441  0.041954         0.491009\n",
       "  152  0.019700  0.019323  0.019888  0.157349  0.042462         0.483805,\n",
       "  0.01927831628366201),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  149  0.018959  0.018603  0.019864  0.117751  0.041801         0.487802\n",
       "  150  0.019616  0.018750  0.020162  0.090027  0.041890         0.487604\n",
       "  151  0.019852  0.019684  0.020586  0.212441  0.041954         0.491009\n",
       "  152  0.019700  0.019323  0.019888  0.157349  0.042462         0.483805\n",
       "  153  0.019602  0.018890  0.020259  0.151587  0.042029         0.488698,\n",
       "  0.01929269209295079),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  150  0.019616  0.018750  0.020162  0.090027  0.041890         0.487604\n",
       "  151  0.019852  0.019684  0.020586  0.212441  0.041954         0.491009\n",
       "  152  0.019700  0.019323  0.019888  0.157349  0.042462         0.483805\n",
       "  153  0.019602  0.018890  0.020259  0.151587  0.042029         0.488698\n",
       "  154  0.018954  0.018875  0.019784  0.164539  0.042234         0.487228,\n",
       "  0.019180052719768404),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  151  0.019852  0.019684  0.020586  0.212441  0.041954         0.491009\n",
       "  152  0.019700  0.019323  0.019888  0.157349  0.042462         0.483805\n",
       "  153  0.019602  0.018890  0.020259  0.151587  0.042029         0.488698\n",
       "  154  0.018954  0.018875  0.019784  0.164539  0.042234         0.487228\n",
       "  155  0.019871  0.019245  0.020353  0.150271  0.042249         0.486278,\n",
       "  0.02043827281521762),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  152  0.019700  0.019323  0.019888  0.157349  0.042462         0.483805\n",
       "  153  0.019602  0.018890  0.020259  0.151587  0.042029         0.488698\n",
       "  154  0.018954  0.018875  0.019784  0.164539  0.042234         0.487228\n",
       "  155  0.019871  0.019245  0.020353  0.150271  0.042249         0.486278\n",
       "  156  0.019436  0.019807  0.020317  0.196349  0.042139         0.496529,\n",
       "  0.021224362081641825),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  153  0.019602  0.018890  0.020259  0.151587  0.042029         0.488698\n",
       "  154  0.018954  0.018875  0.019784  0.164539  0.042234         0.487228\n",
       "  155  0.019871  0.019245  0.020353  0.150271  0.042249         0.486278\n",
       "  156  0.019436  0.019807  0.020317  0.196349  0.042139         0.496529\n",
       "  157  0.021308  0.020978  0.022189  0.212410  0.043367         0.492999,\n",
       "  0.021380138505052857),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  154  0.018954  0.018875  0.019784  0.164539  0.042234         0.487228\n",
       "  155  0.019871  0.019245  0.020353  0.150271  0.042249         0.486278\n",
       "  156  0.019436  0.019807  0.020317  0.196349  0.042139         0.496529\n",
       "  157  0.021308  0.020978  0.022189  0.212410  0.043367         0.492999\n",
       "  158  0.021671  0.020833  0.022192  0.127665  0.044135         0.488285,\n",
       "  0.022468196745439938),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  155  0.019871  0.019245  0.020353  0.150271  0.042249         0.486278\n",
       "  156  0.019436  0.019807  0.020317  0.196349  0.042139         0.496529\n",
       "  157  0.021308  0.020978  0.022189  0.212410  0.043367         0.492999\n",
       "  158  0.021671  0.020833  0.022192  0.127665  0.044135         0.488285\n",
       "  159  0.021561  0.022011  0.022449  0.235651  0.044287         0.495257,\n",
       "  0.022058380334724268),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  156  0.019436  0.019807  0.020317  0.196349  0.042139         0.496529\n",
       "  157  0.021308  0.020978  0.022189  0.212410  0.043367         0.492999\n",
       "  158  0.021671  0.020833  0.022192  0.127665  0.044135         0.488285\n",
       "  159  0.021561  0.022011  0.022449  0.235651  0.044287         0.495257\n",
       "  160  0.022742  0.022124  0.023129  0.124357  0.045350         0.484056,\n",
       "  0.02234118156296878),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  157  0.021308  0.020978  0.022189  0.212410  0.043367         0.492999\n",
       "  158  0.021671  0.020833  0.022192  0.127665  0.044135         0.488285\n",
       "  159  0.021561  0.022011  0.022449  0.235651  0.044287         0.495257\n",
       "  160  0.022742  0.022124  0.023129  0.124357  0.045350         0.484056\n",
       "  161  0.022598  0.022053  0.023340  0.131506  0.044949         0.489235,\n",
       "  0.0225161353544605),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  158  0.021671  0.020833  0.022192  0.127665  0.044135         0.488285\n",
       "  159  0.021561  0.022011  0.022449  0.235651  0.044287         0.495257\n",
       "  160  0.022742  0.022124  0.023129  0.124357  0.045350         0.484056\n",
       "  161  0.022598  0.022053  0.023340  0.131506  0.044949         0.489235\n",
       "  162  0.022576  0.021963  0.022635  0.177941  0.045226         0.488429,\n",
       "  0.022269283271800253),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  159  0.021561  0.022011  0.022449  0.235651  0.044287         0.495257\n",
       "  160  0.022742  0.022124  0.023129  0.124357  0.045350         0.484056\n",
       "  161  0.022598  0.022053  0.023340  0.131506  0.044949         0.489235\n",
       "  162  0.022576  0.021963  0.022635  0.177941  0.045226         0.488429\n",
       "  163  0.022689  0.021940  0.022713  0.159363  0.045397         0.485275,\n",
       "  0.021914583752387213),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  160  0.022742  0.022124  0.023129  0.124357  0.045350         0.484056\n",
       "  161  0.022598  0.022053  0.023340  0.131506  0.044949         0.489235\n",
       "  162  0.022576  0.021963  0.022635  0.177941  0.045226         0.488429\n",
       "  163  0.022689  0.021940  0.022713  0.159363  0.045397         0.485275\n",
       "  164  0.022641  0.021949  0.022802  0.149558  0.045155         0.484468,\n",
       "  0.020368770492263887),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  161  0.022598  0.022053  0.023340  0.131506  0.044949         0.489235\n",
       "  162  0.022576  0.021963  0.022635  0.177941  0.045226         0.488429\n",
       "  163  0.022689  0.021940  0.022713  0.159363  0.045397         0.485275\n",
       "  164  0.022641  0.021949  0.022802  0.149558  0.045155         0.484468\n",
       "  165  0.021878  0.020996  0.021485  0.281505  0.044809         0.475561,\n",
       "  0.020840901321288904),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  162  0.022576  0.021963  0.022635  0.177941  0.045226         0.488429\n",
       "  163  0.022689  0.021940  0.022713  0.159363  0.045397         0.485275\n",
       "  164  0.022641  0.021949  0.022802  0.149558  0.045155         0.484468\n",
       "  165  0.021878  0.020996  0.021485  0.281505  0.044809         0.475561\n",
       "  166  0.020550  0.020413  0.021201  0.159229  0.043299         0.490651,\n",
       "  0.02190260391131323),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  163  0.022689  0.021940  0.022713  0.159363  0.045397         0.485275\n",
       "  164  0.022641  0.021949  0.022802  0.149558  0.045155         0.484468\n",
       "  165  0.021878  0.020996  0.021485  0.281505  0.044809         0.475561\n",
       "  166  0.020550  0.020413  0.021201  0.159229  0.043299         0.490651\n",
       "  167  0.021693  0.021390  0.022301  0.204950  0.043760         0.495060,\n",
       "  0.023508325999168782),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  164  0.022641  0.021949  0.022802  0.149558  0.045155         0.484468\n",
       "  165  0.021878  0.020996  0.021485  0.281505  0.044809         0.475561\n",
       "  166  0.020550  0.020413  0.021201  0.159229  0.043299         0.490651\n",
       "  167  0.021693  0.021390  0.022301  0.204950  0.043760         0.495060\n",
       "  168  0.023919  0.023404  0.024210  0.375736  0.044797         0.499128,\n",
       "  0.024975053063479188),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  165  0.021878  0.020996  0.021485  0.281505  0.044809         0.475561\n",
       "  166  0.020550  0.020413  0.021201  0.159229  0.043299         0.490651\n",
       "  167  0.021693  0.021390  0.022301  0.204950  0.043760         0.495060\n",
       "  168  0.023919  0.023404  0.024210  0.375736  0.044797         0.499128\n",
       "  169  0.024362  0.024471  0.025036  0.431690  0.046365         0.498088,\n",
       "  0.02775272114632665),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  166  0.020550  0.020413  0.021201  0.159229  0.043299         0.490651\n",
       "  167  0.021693  0.021390  0.022301  0.204950  0.043760         0.495060\n",
       "  168  0.023919  0.023404  0.024210  0.375736  0.044797         0.499128\n",
       "  169  0.024362  0.024471  0.025036  0.431690  0.046365         0.498088\n",
       "  170  0.029772  0.028973  0.028654  0.807563  0.047798         0.507891,\n",
       "  0.02960289934862766),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  167  0.021693  0.021390  0.022301  0.204950  0.043760         0.495060\n",
       "  168  0.023919  0.023404  0.024210  0.375736  0.044797         0.499128\n",
       "  169  0.024362  0.024471  0.025036  0.431690  0.046365         0.498088\n",
       "  170  0.029772  0.028973  0.028654  0.807563  0.047798         0.507891\n",
       "  171  0.028550  0.029149  0.029444  0.482479  0.050511         0.500956,\n",
       "  0.028457318626360823),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  168  0.023919  0.023404  0.024210  0.375736  0.044797         0.499128\n",
       "  169  0.024362  0.024471  0.025036  0.431690  0.046365         0.498088\n",
       "  170  0.029772  0.028973  0.028654  0.807563  0.047798         0.507891\n",
       "  171  0.028550  0.029149  0.029444  0.482479  0.050511         0.500956\n",
       "  172  0.029127  0.028499  0.029630  0.420017  0.052317         0.478554,\n",
       "  0.028704170709021076),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  169  0.024362  0.024471  0.025036  0.431690  0.046365         0.498088\n",
       "  170  0.029772  0.028973  0.028654  0.807563  0.047798         0.507891\n",
       "  171  0.028550  0.029149  0.029444  0.482479  0.050511         0.500956\n",
       "  172  0.029127  0.028499  0.029630  0.420017  0.052317         0.478554\n",
       "  173  0.028590  0.028227  0.029255  0.249578  0.051199         0.488966,\n",
       "  0.028625084513208155),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  170  0.029772  0.028973  0.028654  0.807563  0.047798         0.507891\n",
       "  171  0.028550  0.029149  0.029444  0.482479  0.050511         0.500956\n",
       "  172  0.029127  0.028499  0.029630  0.420017  0.052317         0.478554\n",
       "  173  0.028590  0.028227  0.029255  0.249578  0.051199         0.488966\n",
       "  174  0.028954  0.028518  0.029490  0.211099  0.051440         0.486529,\n",
       "  0.02875690125447122),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  171  0.028550  0.029149  0.029444  0.482479  0.050511         0.500956\n",
       "  172  0.029127  0.028499  0.029630  0.420017  0.052317         0.478554\n",
       "  173  0.028590  0.028227  0.029255  0.249578  0.051199         0.488966\n",
       "  174  0.028954  0.028518  0.029490  0.211099  0.051440         0.486529\n",
       "  175  0.029534  0.028559  0.029517  0.222954  0.051362         0.488106,\n",
       "  0.02930572231109436),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  172  0.029127  0.028499  0.029630  0.420017  0.052317         0.478554\n",
       "  173  0.028590  0.028227  0.029255  0.249578  0.051199         0.488966\n",
       "  174  0.028954  0.028518  0.029490  0.211099  0.051440         0.486529\n",
       "  175  0.029534  0.028559  0.029517  0.222954  0.051362         0.488106\n",
       "  176  0.029568  0.028902  0.030095  0.237480  0.051491         0.491224,\n",
       "  0.030240400096285206),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  173  0.028590  0.028227  0.029255  0.249578  0.051199         0.488966\n",
       "  174  0.028954  0.028518  0.029490  0.211099  0.051440         0.486529\n",
       "  175  0.029534  0.028559  0.029517  0.222954  0.051362         0.488106\n",
       "  176  0.029568  0.028902  0.030095  0.237480  0.051491         0.491224\n",
       "  177  0.029519  0.029957  0.030364  0.432378  0.052027         0.494110,\n",
       "  0.03079401308933794),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  174  0.028954  0.028518  0.029490  0.211099  0.051440         0.486529\n",
       "  175  0.029534  0.028559  0.029517  0.222954  0.051362         0.488106\n",
       "  176  0.029568  0.028902  0.030095  0.237480  0.051491         0.491224\n",
       "  177  0.029519  0.029957  0.030364  0.432378  0.052027         0.494110\n",
       "  178  0.030814  0.030367  0.031740  0.293815  0.052940         0.491260,\n",
       "  0.030429729697065717),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  175  0.029534  0.028559  0.029517  0.222954  0.051362         0.488106\n",
       "  176  0.029568  0.028902  0.030095  0.237480  0.051491         0.491224\n",
       "  177  0.029519  0.029957  0.030364  0.432378  0.052027         0.494110\n",
       "  178  0.030814  0.030367  0.031740  0.293815  0.052940         0.491260\n",
       "  179  0.031149  0.030068  0.031469  0.182563  0.053481         0.484397,\n",
       "  0.030678977747940758),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  176  0.029568  0.028902  0.030095  0.237480  0.051491         0.491224\n",
       "  177  0.029519  0.029957  0.030364  0.432378  0.052027         0.494110\n",
       "  178  0.030814  0.030367  0.031740  0.293815  0.052940         0.491260\n",
       "  179  0.031149  0.030068  0.031469  0.182563  0.053481         0.484397\n",
       "  180  0.031096  0.030217  0.031612  0.164425  0.053125         0.488984,\n",
       "  0.030199659014271355),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  177  0.029519  0.029957  0.030364  0.432378  0.052027         0.494110\n",
       "  178  0.030814  0.030367  0.031740  0.293815  0.052940         0.491260\n",
       "  179  0.031149  0.030068  0.031469  0.182563  0.053481         0.484397\n",
       "  180  0.031096  0.030217  0.031612  0.164425  0.053125         0.488984\n",
       "  181  0.030999  0.030040  0.031360  0.134578  0.053368         0.483536,\n",
       "  0.030626247202490613),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  178  0.030814  0.030367  0.031740  0.293815  0.052940         0.491260\n",
       "  179  0.031149  0.030068  0.031469  0.182563  0.053481         0.484397\n",
       "  180  0.031096  0.030217  0.031612  0.164425  0.053125         0.488984\n",
       "  181  0.030999  0.030040  0.031360  0.134578  0.053368         0.483536\n",
       "  182  0.030391  0.029874  0.031188  0.154255  0.052900         0.490310,\n",
       "  0.031462671046150165),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  179  0.031149  0.030068  0.031469  0.182563  0.053481         0.484397\n",
       "  180  0.031096  0.030217  0.031612  0.164425  0.053125         0.488984\n",
       "  181  0.030999  0.030040  0.031360  0.134578  0.053368         0.483536\n",
       "  182  0.030391  0.029874  0.031188  0.154255  0.052900         0.490310\n",
       "  183  0.031233  0.030864  0.031965  0.279107  0.053317         0.493375,\n",
       "  0.03084674363478809),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  180  0.031096  0.030217  0.031612  0.164425  0.053125         0.488984\n",
       "  181  0.030999  0.030040  0.031360  0.134578  0.053368         0.483536\n",
       "  182  0.030391  0.029874  0.031188  0.154255  0.052900         0.490310\n",
       "  183  0.031233  0.030864  0.031965  0.279107  0.053317         0.493375\n",
       "  184  0.031676  0.030924  0.032031  0.217554  0.054134         0.482515,\n",
       "  0.03097136284904446),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  181  0.030999  0.030040  0.031360  0.134578  0.053368         0.483536\n",
       "  182  0.030391  0.029874  0.031188  0.154255  0.052900         0.490310\n",
       "  183  0.031233  0.030864  0.031965  0.279107  0.053317         0.493375\n",
       "  184  0.031676  0.030924  0.032031  0.217554  0.054134         0.482515\n",
       "  185  0.031283  0.030653  0.031818  0.180274  0.053532         0.488052,\n",
       "  0.031405148564270416),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  182  0.030391  0.029874  0.031188  0.154255  0.052900         0.490310\n",
       "  183  0.031233  0.030864  0.031965  0.279107  0.053317         0.493375\n",
       "  184  0.031676  0.030924  0.032031  0.217554  0.054134         0.482515\n",
       "  185  0.031283  0.030653  0.031818  0.180274  0.053532         0.488052\n",
       "  186  0.031404  0.030779  0.032120  0.138095  0.053654         0.490364,\n",
       "  0.0319036446660205),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  183  0.031233  0.030864  0.031965  0.279107  0.053317         0.493375\n",
       "  184  0.031676  0.030924  0.032031  0.217554  0.054134         0.482515\n",
       "  185  0.031283  0.030653  0.031818  0.180274  0.053532         0.488052\n",
       "  186  0.031404  0.030779  0.032120  0.138095  0.053654         0.490364\n",
       "  187  0.031616  0.031213  0.032515  0.189843  0.054077         0.490848,\n",
       "  0.03334879839403543),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  184  0.031676  0.030924  0.032031  0.217554  0.054134         0.482515\n",
       "  185  0.031283  0.030653  0.031818  0.180274  0.053532         0.488052\n",
       "  186  0.031404  0.030779  0.032120  0.138095  0.053654         0.490364\n",
       "  187  0.031616  0.031213  0.032515  0.189843  0.054077         0.490848\n",
       "  188  0.032350  0.032758  0.033319  0.299373  0.054564         0.497927,\n",
       "  0.03335359033046502),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  185  0.031283  0.030653  0.031818  0.180274  0.053532         0.488052\n",
       "  186  0.031404  0.030779  0.032120  0.138095  0.053654         0.490364\n",
       "  187  0.031616  0.031213  0.032515  0.189843  0.054077         0.490848\n",
       "  188  0.032350  0.032758  0.033319  0.299373  0.054564         0.497927\n",
       "  189  0.033969  0.033042  0.034470  0.236242  0.055976         0.487156,\n",
       "  0.03393596456445762),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  186  0.031404  0.030779  0.032120  0.138095  0.053654         0.490364\n",
       "  187  0.031616  0.031213  0.032515  0.189843  0.054077         0.490848\n",
       "  188  0.032350  0.032758  0.033319  0.299373  0.054564         0.497927\n",
       "  189  0.033969  0.033042  0.034470  0.236242  0.055976         0.487156\n",
       "  190  0.034179  0.033161  0.034804  0.198468  0.055980         0.491475,\n",
       "  0.03725286983106904),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  187  0.031616  0.031213  0.032515  0.189843  0.054077         0.490848\n",
       "  188  0.032350  0.032758  0.033319  0.299373  0.054564         0.497927\n",
       "  189  0.033969  0.033042  0.034470  0.236242  0.055976         0.487156\n",
       "  190  0.034179  0.033161  0.034804  0.198468  0.055980         0.491475\n",
       "  191  0.034648  0.037054  0.035545  0.520139  0.056549         0.511923,\n",
       "  0.03738468657233211),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  188  0.032350  0.032758  0.033319  0.299373  0.054564         0.497927\n",
       "  189  0.033969  0.033042  0.034470  0.236242  0.055976         0.487156\n",
       "  190  0.034179  0.033161  0.034804  0.198468  0.055980         0.491475\n",
       "  191  0.034648  0.037054  0.035545  0.520139  0.056549         0.511923\n",
       "  192  0.037969  0.038794  0.038159  0.706330  0.059788         0.488106,\n",
       "  0.03853745519924333),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  189  0.033969  0.033042  0.034470  0.236242  0.055976         0.487156\n",
       "  190  0.034179  0.033161  0.034804  0.198468  0.055980         0.491475\n",
       "  191  0.034648  0.037054  0.035545  0.520139  0.056549         0.511923\n",
       "  192  0.037969  0.038794  0.038159  0.706330  0.059788         0.488106\n",
       "  193  0.036951  0.037737  0.037464  0.439133  0.059917         0.495741,\n",
       "  0.03815159847067562),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  190  0.034179  0.033161  0.034804  0.198468  0.055980         0.491475\n",
       "  191  0.034648  0.037054  0.035545  0.520139  0.056549         0.511923\n",
       "  192  0.037969  0.038794  0.038159  0.706330  0.059788         0.488106\n",
       "  193  0.036951  0.037737  0.037464  0.439133  0.059917         0.495741\n",
       "  194  0.038932  0.038396  0.039321  0.329403  0.061043         0.484235,\n",
       "  0.039781289863041455),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  191  0.034648  0.037054  0.035545  0.520139  0.056549         0.511923\n",
       "  192  0.037969  0.038794  0.038159  0.706330  0.059788         0.488106\n",
       "  193  0.036951  0.037737  0.037464  0.439133  0.059917         0.495741\n",
       "  194  0.038932  0.038396  0.039321  0.329403  0.061043         0.484235\n",
       "  195  0.038811  0.038938  0.039552  0.296262  0.060666         0.499307,\n",
       "  0.04248226771829079),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  192  0.037969  0.038794  0.038159  0.706330  0.059788         0.488106\n",
       "  193  0.036951  0.037737  0.037464  0.439133  0.059917         0.495741\n",
       "  194  0.038932  0.038396  0.039321  0.329403  0.061043         0.484235\n",
       "  195  0.038811  0.038938  0.039552  0.296262  0.060666         0.499307\n",
       "  196  0.040970  0.041865  0.042003  0.540778  0.062258         0.507318,\n",
       "  0.04505382480049186),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  193  0.036951  0.037737  0.037464  0.439133  0.059917         0.495741\n",
       "  194  0.038932  0.038396  0.039321  0.329403  0.061043         0.484235\n",
       "  195  0.038811  0.038938  0.039552  0.296262  0.060666         0.499307\n",
       "  196  0.040970  0.041865  0.042003  0.540778  0.062258         0.507318\n",
       "  197  0.044284  0.045299  0.045067  0.855884  0.064895         0.506350,\n",
       "  0.04502986511834389),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  194  0.038932  0.038396  0.039321  0.329403  0.061043         0.484235\n",
       "  195  0.038811  0.038938  0.039552  0.296262  0.060666         0.499307\n",
       "  196  0.040970  0.041865  0.042003  0.540778  0.062258         0.507318\n",
       "  197  0.044284  0.045299  0.045067  0.855884  0.064895         0.506350\n",
       "  198  0.045615  0.044562  0.045692  0.437094  0.067407         0.486941,\n",
       "  0.043198854661761266),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  195  0.038811  0.038938  0.039552  0.296262  0.060666         0.499307\n",
       "  196  0.040970  0.041865  0.042003  0.540778  0.062258         0.507318\n",
       "  197  0.044284  0.045299  0.045067  0.855884  0.064895         0.506350\n",
       "  198  0.045615  0.044562  0.045692  0.437094  0.067407         0.486941\n",
       "  199  0.045148  0.044448  0.043950  0.456753  0.067383         0.473429,\n",
       "  0.04575843190288835),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  196  0.040970  0.041865  0.042003  0.540778  0.062258         0.507318\n",
       "  197  0.044284  0.045299  0.045067  0.855884  0.064895         0.506350\n",
       "  198  0.045615  0.044562  0.045692  0.437094  0.067407         0.486941\n",
       "  199  0.045148  0.044448  0.043950  0.456753  0.067383         0.473429\n",
       "  200  0.044089  0.045503  0.045021  0.547809  0.065595         0.506260,\n",
       "  0.04628088768678641),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  197  0.044284  0.045299  0.045067  0.855884  0.064895         0.506350\n",
       "  198  0.045615  0.044562  0.045692  0.437094  0.067407         0.486941\n",
       "  199  0.045148  0.044448  0.043950  0.456753  0.067383         0.473429\n",
       "  200  0.044089  0.045503  0.045021  0.547809  0.065595         0.506260\n",
       "  201  0.047629  0.046887  0.047072  0.591521  0.068095         0.491027,\n",
       "  0.04303108877491392),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  198  0.045615  0.044562  0.045692  0.437094  0.067407         0.486941\n",
       "  199  0.045148  0.044448  0.043950  0.456753  0.067383         0.473429\n",
       "  200  0.044089  0.045503  0.045021  0.547809  0.065595         0.506260\n",
       "  201  0.047629  0.046887  0.047072  0.591521  0.068095         0.491027\n",
       "  202  0.046606  0.045370  0.044093  0.625036  0.068605         0.462820,\n",
       "  0.04464880032620577),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  199  0.045148  0.044448  0.043950  0.456753  0.067383         0.473429\n",
       "  200  0.044089  0.045503  0.045021  0.547809  0.065595         0.506260\n",
       "  201  0.047629  0.046887  0.047072  0.591521  0.068095         0.491027\n",
       "  202  0.046606  0.045370  0.044093  0.625036  0.068605         0.462820\n",
       "  203  0.044650  0.044258  0.044713  0.399807  0.065431         0.499217,\n",
       "  0.04373569587731041),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  200  0.044089  0.045503  0.045021  0.547809  0.065595         0.506260\n",
       "  201  0.047629  0.046887  0.047072  0.591521  0.068095         0.491027\n",
       "  202  0.046606  0.045370  0.044093  0.625036  0.068605         0.462820\n",
       "  203  0.044650  0.044258  0.044713  0.399807  0.065431         0.499217\n",
       "  204  0.045196  0.043969  0.044582  0.308728  0.067011         0.480293,\n",
       "  0.04379561432740495),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  201  0.047629  0.046887  0.047072  0.591521  0.068095         0.491027\n",
       "  202  0.046606  0.045370  0.044093  0.625036  0.068605         0.462820\n",
       "  203  0.044650  0.044258  0.044713  0.399807  0.065431         0.499217\n",
       "  204  0.045196  0.043969  0.044582  0.308728  0.067011         0.480293\n",
       "  205  0.043470  0.043237  0.043735  0.311321  0.066119         0.487569,\n",
       "  0.04274109964202502),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  202  0.046606  0.045370  0.044093  0.625036  0.068605         0.462820\n",
       "  203  0.044650  0.044258  0.044713  0.399807  0.065431         0.499217\n",
       "  204  0.045196  0.043969  0.044582  0.308728  0.067011         0.480293\n",
       "  205  0.043470  0.043237  0.043735  0.311321  0.066119         0.487569\n",
       "  206  0.043174  0.042538  0.044033  0.245373  0.066178         0.479235,\n",
       "  0.041890309611439),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  203  0.044650  0.044258  0.044713  0.399807  0.065431         0.499217\n",
       "  204  0.045196  0.043969  0.044582  0.308728  0.067011         0.480293\n",
       "  205  0.043470  0.043237  0.043735  0.311321  0.066119         0.487569\n",
       "  206  0.043174  0.042538  0.044033  0.245373  0.066178         0.479235\n",
       "  207  0.042310  0.041604  0.041533  0.507851  0.065148         0.480759,\n",
       "  0.04252300880030464),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  204  0.045196  0.043969  0.044582  0.308728  0.067011         0.480293\n",
       "  205  0.043470  0.043237  0.043735  0.311321  0.066119         0.487569\n",
       "  206  0.043174  0.042538  0.044033  0.245373  0.066178         0.479235\n",
       "  207  0.042310  0.041604  0.041533  0.507851  0.065148         0.480759\n",
       "  208  0.042132  0.041841  0.042899  0.303031  0.064317         0.491852,\n",
       "  0.04320843853462045),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  205  0.043470  0.043237  0.043735  0.311321  0.066119         0.487569\n",
       "  206  0.043174  0.042538  0.044033  0.245373  0.066178         0.479235\n",
       "  207  0.042310  0.041604  0.041533  0.507851  0.065148         0.480759\n",
       "  208  0.042132  0.041841  0.042899  0.303031  0.064317         0.491852\n",
       "  209  0.043273  0.042315  0.043584  0.253704  0.064935         0.492246,\n",
       "  0.04474227195366978),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  206  0.043174  0.042538  0.044033  0.245373  0.066178         0.479235\n",
       "  207  0.042310  0.041604  0.041533  0.507851  0.065148         0.480759\n",
       "  208  0.042132  0.041841  0.042899  0.303031  0.064317         0.491852\n",
       "  209  0.043273  0.042315  0.043584  0.253704  0.064935         0.492246\n",
       "  210  0.042573  0.044062  0.042574  0.511295  0.065604         0.498590,\n",
       "  0.0450154796866928),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  207  0.042310  0.041604  0.041533  0.507851  0.065148         0.480759\n",
       "  208  0.042132  0.041841  0.042899  0.303031  0.064317         0.491852\n",
       "  209  0.043273  0.042315  0.043584  0.253704  0.064935         0.492246\n",
       "  210  0.042573  0.044062  0.042574  0.511295  0.065604         0.498590\n",
       "  211  0.045456  0.044685  0.045781  0.367972  0.067102         0.489164,\n",
       "  0.04536538726967625),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  208  0.042132  0.041841  0.042899  0.303031  0.064317         0.491852\n",
       "  209  0.043273  0.042315  0.043584  0.253704  0.064935         0.492246\n",
       "  210  0.042573  0.044062  0.042574  0.511295  0.065604         0.498590\n",
       "  211  0.045456  0.044685  0.045781  0.367972  0.067102         0.489164\n",
       "  212  0.045841  0.045164  0.046678  0.254667  0.067369         0.489737,\n",
       "  0.045463650833569846),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  209  0.043273  0.042315  0.043584  0.253704  0.064935         0.492246\n",
       "  210  0.042573  0.044062  0.042574  0.511295  0.065604         0.498590\n",
       "  211  0.045456  0.044685  0.045781  0.367972  0.067102         0.489164\n",
       "  212  0.045841  0.045164  0.046678  0.254667  0.067369         0.489737\n",
       "  213  0.045439  0.045754  0.046152  0.341803  0.067711         0.487855,\n",
       "  0.04727069198564221),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  210  0.042573  0.044062  0.042574  0.511295  0.065604         0.498590\n",
       "  211  0.045456  0.044685  0.045781  0.367972  0.067102         0.489164\n",
       "  212  0.045841  0.045164  0.046678  0.254667  0.067369         0.489737\n",
       "  213  0.045439  0.045754  0.046152  0.341803  0.067711         0.487855\n",
       "  214  0.046137  0.046510  0.046898  0.432159  0.067807         0.500633,\n",
       "  0.04891236321908202),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  211  0.045456  0.044685  0.045781  0.367972  0.067102         0.489164\n",
       "  212  0.045841  0.045164  0.046678  0.254667  0.067369         0.489737\n",
       "  213  0.045439  0.045754  0.046152  0.341803  0.067711         0.487855\n",
       "  214  0.046137  0.046510  0.046898  0.432159  0.067807         0.500633\n",
       "  215  0.048062  0.048043  0.047935  0.432930  0.069572         0.499396,\n",
       "  0.04840907518090233),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  212  0.045841  0.045164  0.046678  0.254667  0.067369         0.489737\n",
       "  213  0.045439  0.045754  0.046152  0.341803  0.067711         0.487855\n",
       "  214  0.046137  0.046510  0.046898  0.432159  0.067807         0.500633\n",
       "  215  0.048062  0.048043  0.047935  0.432930  0.069572         0.499396\n",
       "  216  0.049838  0.049176  0.049906  0.462936  0.071175         0.483357,\n",
       "  0.04618502971346991),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  213  0.045439  0.045754  0.046152  0.341803  0.067711         0.487855\n",
       "  214  0.046137  0.046510  0.046898  0.432159  0.067807         0.500633\n",
       "  215  0.048062  0.048043  0.047935  0.432930  0.069572         0.499396\n",
       "  216  0.049838  0.049176  0.049906  0.462936  0.071175         0.483357\n",
       "  217  0.048929  0.048022  0.047520  0.444990  0.070683         0.470490,\n",
       "  0.04652773976944666),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  214  0.046137  0.046510  0.046898  0.432159  0.067807         0.500633\n",
       "  215  0.048062  0.048043  0.047935  0.432930  0.069572         0.499396\n",
       "  216  0.049838  0.049176  0.049906  0.462936  0.071175         0.483357\n",
       "  217  0.048929  0.048022  0.047520  0.444990  0.070683         0.470490\n",
       "  218  0.046965  0.046730  0.047252  0.367050  0.068511         0.489683,\n",
       "  0.04583272616227167),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  215  0.048062  0.048043  0.047935  0.432930  0.069572         0.499396\n",
       "  216  0.049838  0.049176  0.049906  0.462936  0.071175         0.483357\n",
       "  217  0.048929  0.048022  0.047520  0.444990  0.070683         0.470490\n",
       "  218  0.046965  0.046730  0.047252  0.367050  0.068511         0.489683\n",
       "  219  0.047133  0.046093  0.046811  0.224350  0.068846         0.481923,\n",
       "  0.046901616656940384),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  216  0.049838  0.049176  0.049906  0.462936  0.071175         0.483357\n",
       "  217  0.048929  0.048022  0.047520  0.444990  0.070683         0.470490\n",
       "  218  0.046965  0.046730  0.047252  0.367050  0.068511         0.489683\n",
       "  219  0.047133  0.046093  0.046811  0.224350  0.068846         0.481923\n",
       "  220  0.046426  0.046031  0.047055  0.182183  0.068167         0.495113,\n",
       "  0.045897436548795814),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  217  0.048929  0.048022  0.047520  0.444990  0.070683         0.470490\n",
       "  218  0.046965  0.046730  0.047252  0.367050  0.068511         0.489683\n",
       "  219  0.047133  0.046093  0.046811  0.224350  0.068846         0.481923\n",
       "  220  0.046426  0.046031  0.047055  0.182183  0.068167         0.495113\n",
       "  221  0.047677  0.046415  0.047334  0.194498  0.069211         0.479612,\n",
       "  0.04686087557492652),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  218  0.046965  0.046730  0.047252  0.367050  0.068511         0.489683\n",
       "  219  0.047133  0.046093  0.046811  0.224350  0.068846         0.481923\n",
       "  220  0.046426  0.046031  0.047055  0.182183  0.068167         0.495113\n",
       "  221  0.047677  0.046415  0.047334  0.194498  0.069211         0.479612\n",
       "  222  0.045774  0.045988  0.046639  0.259485  0.068231         0.494325,\n",
       "  0.04702623587119675),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  219  0.047133  0.046093  0.046811  0.224350  0.068846         0.481923\n",
       "  220  0.046426  0.046031  0.047055  0.182183  0.068167         0.495113\n",
       "  221  0.047677  0.046415  0.047334  0.194498  0.069211         0.479612\n",
       "  222  0.045774  0.045988  0.046639  0.259485  0.068231         0.494325\n",
       "  223  0.047424  0.046391  0.047981  0.181298  0.069171         0.488357,\n",
       "  0.04633601420045136),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  220  0.046426  0.046031  0.047055  0.182183  0.068167         0.495113\n",
       "  221  0.047677  0.046415  0.047334  0.194498  0.069211         0.479612\n",
       "  222  0.045774  0.045988  0.046639  0.259485  0.068231         0.494325\n",
       "  223  0.047424  0.046391  0.047981  0.181298  0.069171         0.488357\n",
       "  224  0.047461  0.046178  0.047247  0.203984  0.069333         0.481959,\n",
       "  0.045959750967105154),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  221  0.047677  0.046415  0.047334  0.194498  0.069211         0.479612\n",
       "  222  0.045774  0.045988  0.046639  0.259485  0.068231         0.494325\n",
       "  223  0.047424  0.046391  0.047981  0.181298  0.069171         0.488357\n",
       "  224  0.047461  0.046178  0.047247  0.203984  0.069333         0.481959\n",
       "  225  0.046736  0.045657  0.047225  0.142558  0.068659         0.484307,\n",
       "  0.047656548714209906),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  222  0.045774  0.045988  0.046639  0.259485  0.068231         0.494325\n",
       "  223  0.047424  0.046391  0.047981  0.181298  0.069171         0.488357\n",
       "  224  0.047461  0.046178  0.047247  0.203984  0.069333         0.481959\n",
       "  225  0.046736  0.045657  0.047225  0.142558  0.068659         0.484307\n",
       "  226  0.046524  0.046804  0.047508  0.278112  0.068291         0.499809,\n",
       "  0.04814305535252371),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  223  0.047424  0.046391  0.047981  0.181298  0.069171         0.488357\n",
       "  224  0.047461  0.046178  0.047247  0.203984  0.069333         0.481959\n",
       "  225  0.046736  0.045657  0.047225  0.142558  0.068659         0.484307\n",
       "  226  0.046524  0.046804  0.047508  0.278112  0.068291         0.499809\n",
       "  227  0.049612  0.048583  0.049439  0.259378  0.069948         0.490758,\n",
       "  0.04821495364369223),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  224  0.047461  0.046178  0.047247  0.203984  0.069333         0.481959\n",
       "  225  0.046736  0.045657  0.047225  0.142558  0.068659         0.484307\n",
       "  226  0.046524  0.046804  0.047508  0.278112  0.068291         0.499809\n",
       "  227  0.049612  0.048583  0.049439  0.259378  0.069948         0.490758\n",
       "  228  0.048625  0.047789  0.049369  0.205154  0.070424         0.487658,\n",
       "  0.04781951304226534),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  225  0.046736  0.045657  0.047225  0.142558  0.068659         0.484307\n",
       "  226  0.046524  0.046804  0.047508  0.278112  0.068291         0.499809\n",
       "  227  0.049612  0.048583  0.049439  0.259378  0.069948         0.490758\n",
       "  228  0.048625  0.047789  0.049369  0.205154  0.070424         0.487658\n",
       "  229  0.048327  0.047434  0.048877  0.150899  0.070494         0.484164,\n",
       "  0.0503024000557943),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  226  0.046524  0.046804  0.047508  0.278112  0.068291         0.499809\n",
       "  227  0.049612  0.048583  0.049439  0.259378  0.069948         0.490758\n",
       "  228  0.048625  0.047789  0.049369  0.205154  0.070424         0.487658\n",
       "  229  0.048327  0.047434  0.048877  0.150899  0.070494         0.484164\n",
       "  230  0.048832  0.049436  0.049858  0.306899  0.070108         0.505687,\n",
       "  0.05080568809397398),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  227  0.049612  0.048583  0.049439  0.259378  0.069948         0.490758\n",
       "  228  0.048625  0.047789  0.049369  0.205154  0.070424         0.487658\n",
       "  229  0.048327  0.047434  0.048877  0.150899  0.070494         0.484164\n",
       "  230  0.048832  0.049436  0.049858  0.306899  0.070108         0.505687\n",
       "  231  0.049667  0.049972  0.049858  0.347981  0.072532         0.490884,\n",
       "  0.05127063101835462),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  228  0.048625  0.047789  0.049369  0.205154  0.070424         0.487658\n",
       "  229  0.048327  0.047434  0.048877  0.150899  0.070494         0.484164\n",
       "  230  0.048832  0.049436  0.049858  0.306899  0.070108         0.505687\n",
       "  231  0.049667  0.049972  0.049858  0.347981  0.072532         0.490884\n",
       "  232  0.051708  0.051202  0.052137  0.481249  0.073024         0.490597,\n",
       "  0.04850494277658114),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  229  0.048327  0.047434  0.048877  0.150899  0.070494         0.484164\n",
       "  230  0.048832  0.049436  0.049858  0.306899  0.070108         0.505687\n",
       "  231  0.049667  0.049972  0.049858  0.347981  0.072532         0.490884\n",
       "  232  0.051708  0.051202  0.052137  0.481249  0.073024         0.490597\n",
       "  233  0.049860  0.049176  0.048533  0.568749  0.073478         0.466440,\n",
       "  0.04693516983430984),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  230  0.048832  0.049436  0.049858  0.306899  0.070108         0.505687\n",
       "  231  0.049667  0.049972  0.049858  0.347981  0.072532         0.490884\n",
       "  232  0.051708  0.051202  0.052137  0.481249  0.073024         0.490597\n",
       "  233  0.049860  0.049176  0.048533  0.568749  0.073478         0.466440\n",
       "  234  0.048902  0.047763  0.048201  0.234838  0.070777         0.475382,\n",
       "  0.04699268269382728),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  231  0.049667  0.049972  0.049858  0.347981  0.072532         0.490884\n",
       "  232  0.051708  0.051202  0.052137  0.481249  0.073024         0.490597\n",
       "  233  0.049860  0.049176  0.048533  0.568749  0.073478         0.466440\n",
       "  234  0.048902  0.047763  0.048201  0.234838  0.070777         0.475382\n",
       "  235  0.047126  0.046510  0.047506  0.238684  0.069244         0.487551,\n",
       "  0.047194001758044085),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  232  0.051708  0.051202  0.052137  0.481249  0.073024         0.490597\n",
       "  233  0.049860  0.049176  0.048533  0.568749  0.073478         0.466440\n",
       "  234  0.048902  0.047763  0.048201  0.234838  0.070777         0.475382\n",
       "  235  0.047126  0.046510  0.047506  0.238684  0.069244         0.487551\n",
       "  236  0.047783  0.046564  0.047581  0.175468  0.069300         0.488626,\n",
       "  0.04637196334603562),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  233  0.049860  0.049176  0.048533  0.568749  0.073478         0.466440\n",
       "  234  0.048902  0.047763  0.048201  0.234838  0.070777         0.475382\n",
       "  235  0.047126  0.046510  0.047506  0.238684  0.069244         0.487551\n",
       "  236  0.047783  0.046564  0.047581  0.175468  0.069300         0.488626\n",
       "  237  0.047704  0.046370  0.047794  0.144034  0.069497         0.480974,\n",
       "  0.044996311940974434),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  234  0.048902  0.047763  0.048201  0.234838  0.070777         0.475382\n",
       "  235  0.047126  0.046510  0.047506  0.238684  0.069244         0.487551\n",
       "  236  0.047783  0.046564  0.047581  0.175468  0.069300         0.488626\n",
       "  237  0.047704  0.046370  0.047794  0.144034  0.069497         0.480974\n",
       "  238  0.046435  0.045287  0.046271  0.203328  0.068694         0.476834,\n",
       "  0.04591900026272898),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  235  0.047126  0.046510  0.047506  0.238684  0.069244         0.487551\n",
       "  236  0.047783  0.046564  0.047581  0.175468  0.069300         0.488626\n",
       "  237  0.047704  0.046370  0.047794  0.144034  0.069497         0.480974\n",
       "  238  0.046435  0.045287  0.046271  0.203328  0.068694         0.476834\n",
       "  239  0.045468  0.045206  0.046540  0.137633  0.067350         0.494020,\n",
       "  0.047735634910022834),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  236  0.047783  0.046564  0.047581  0.175468  0.069300         0.488626\n",
       "  237  0.047704  0.046370  0.047794  0.144034  0.069497         0.480974\n",
       "  238  0.046435  0.045287  0.046271  0.203328  0.068694         0.476834\n",
       "  239  0.045468  0.045206  0.046540  0.137633  0.067350         0.494020\n",
       "  240  0.046305  0.046870  0.047271  0.177226  0.068252         0.500705,\n",
       "  0.047282671826716194),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  237  0.047704  0.046370  0.047794  0.144034  0.069497         0.480974\n",
       "  238  0.046435  0.045287  0.046271  0.203328  0.068694         0.476834\n",
       "  239  0.045468  0.045206  0.046540  0.137633  0.067350         0.494020\n",
       "  240  0.046305  0.046870  0.047271  0.177226  0.068252         0.500705\n",
       "  241  0.047846  0.046917  0.048356  0.144156  0.070026         0.483733,\n",
       "  0.047385727327039384),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  238  0.046435  0.045287  0.046271  0.203328  0.068694         0.476834\n",
       "  239  0.045468  0.045206  0.046540  0.137633  0.067350         0.494020\n",
       "  240  0.046305  0.046870  0.047271  0.177226  0.068252         0.500705\n",
       "  241  0.047846  0.046917  0.048356  0.144156  0.070026         0.483733\n",
       "  242  0.047256  0.046747  0.048271  0.127273  0.069583         0.487891,\n",
       "  0.04609635002243549),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  239  0.045468  0.045206  0.046540  0.137633  0.067350         0.494020\n",
       "  240  0.046305  0.046870  0.047271  0.177226  0.068252         0.500705\n",
       "  241  0.047846  0.046917  0.048356  0.144156  0.070026         0.483733\n",
       "  242  0.047256  0.046747  0.048271  0.127273  0.069583         0.487891\n",
       "  243  0.047725  0.046631  0.047317  0.144377  0.069684         0.477479,\n",
       "  0.04583272616227167),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  240  0.046305  0.046870  0.047271  0.177226  0.068252         0.500705\n",
       "  241  0.047846  0.046917  0.048356  0.144156  0.070026         0.483733\n",
       "  242  0.047256  0.046747  0.048271  0.127273  0.069583         0.487891\n",
       "  243  0.047725  0.046631  0.047317  0.144377  0.069684         0.477479\n",
       "  244  0.046787  0.045953  0.047118  0.108916  0.068425         0.485149,\n",
       "  0.0459094163898698),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  241  0.047846  0.046917  0.048356  0.144156  0.070026         0.483733\n",
       "  242  0.047256  0.046747  0.048271  0.127273  0.069583         0.487891\n",
       "  243  0.047725  0.046631  0.047317  0.144377  0.069684         0.477479\n",
       "  244  0.046787  0.045953  0.047118  0.108916  0.068425         0.485149\n",
       "  245  0.046392  0.045249  0.046639  0.140476  0.068167         0.487694,\n",
       "  0.04449781583922434),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  242  0.047256  0.046747  0.048271  0.127273  0.069583         0.487891\n",
       "  243  0.047725  0.046631  0.047317  0.144377  0.069684         0.477479\n",
       "  244  0.046787  0.045953  0.047118  0.108916  0.068425         0.485149\n",
       "  245  0.046392  0.045249  0.046639  0.140476  0.068167         0.487694\n",
       "  246  0.046233  0.045166  0.045760  0.167224  0.068242         0.476565,\n",
       "  0.04410716717422704),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  243  0.047725  0.046631  0.047317  0.144377  0.069684         0.477479\n",
       "  244  0.046787  0.045953  0.047118  0.108916  0.068425         0.485149\n",
       "  245  0.046392  0.045249  0.046639  0.140476  0.068167         0.487694\n",
       "  246  0.046233  0.045166  0.045760  0.167224  0.068242         0.476565\n",
       "  247  0.044931  0.043803  0.044728  0.182692  0.066864         0.484199,\n",
       "  0.04546604680178465),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  244  0.046787  0.045953  0.047118  0.108916  0.068425         0.485149\n",
       "  245  0.046392  0.045249  0.046639  0.140476  0.068167         0.487694\n",
       "  246  0.046233  0.045166  0.045760  0.167224  0.068242         0.476565\n",
       "  247  0.044931  0.043803  0.044728  0.182692  0.066864         0.484199\n",
       "  248  0.044322  0.044661  0.044975  0.160095  0.066482         0.497282,\n",
       "  0.04409518733315304),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  245  0.046392  0.045249  0.046639  0.140476  0.068167         0.487694\n",
       "  246  0.046233  0.045166  0.045760  0.167224  0.068242         0.476565\n",
       "  247  0.044931  0.043803  0.044728  0.182692  0.066864         0.484199\n",
       "  248  0.044322  0.044661  0.044975  0.160095  0.066482         0.497282\n",
       "  249  0.045872  0.045270  0.045491  0.198740  0.067809         0.476870,\n",
       "  0.044490627934579946),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  246  0.046233  0.045166  0.045760  0.167224  0.068242         0.476565\n",
       "  247  0.044931  0.043803  0.044728  0.182692  0.066864         0.484199\n",
       "  248  0.044322  0.044661  0.044975  0.160095  0.066482         0.497282\n",
       "  249  0.045872  0.045270  0.045491  0.198740  0.067809         0.476870\n",
       "  250  0.044688  0.044090  0.045387  0.172824  0.066470         0.490078,\n",
       "  0.044358811193316874),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  247  0.044931  0.043803  0.044728  0.182692  0.066864         0.484199\n",
       "  248  0.044322  0.044661  0.044975  0.160095  0.066482         0.497282\n",
       "  249  0.045872  0.045270  0.045491  0.198740  0.067809         0.476870\n",
       "  250  0.044688  0.044090  0.045387  0.172824  0.066470         0.490078\n",
       "  251  0.044840  0.043801  0.045546  0.094359  0.066857         0.486135,\n",
       "  0.043134144275237124),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  248  0.044322  0.044661  0.044975  0.160095  0.066482         0.497282\n",
       "  249  0.045872  0.045270  0.045491  0.198740  0.067809         0.476870\n",
       "  250  0.044688  0.044090  0.045387  0.172824  0.066470         0.490078\n",
       "  251  0.044840  0.043801  0.045546  0.094359  0.066857         0.486135\n",
       "  252  0.042529  0.042363  0.043366  0.288690  0.066728         0.477963,\n",
       "  0.043136540243451926),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  249  0.045872  0.045270  0.045491  0.198740  0.067809         0.476870\n",
       "  250  0.044688  0.044090  0.045387  0.172824  0.066470         0.490078\n",
       "  251  0.044840  0.043801  0.045546  0.094359  0.066857         0.486135\n",
       "  252  0.042529  0.042363  0.043366  0.288690  0.066728         0.477963\n",
       "  253  0.043752  0.042588  0.044486  0.134726  0.065532         0.487138,\n",
       "  0.04170097038829619),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  250  0.044688  0.044090  0.045387  0.172824  0.066470         0.490078\n",
       "  251  0.044840  0.043801  0.045546  0.094359  0.066857         0.486135\n",
       "  252  0.042529  0.042363  0.043366  0.288690  0.066728         0.477963\n",
       "  253  0.043752  0.042588  0.044486  0.134726  0.065532         0.487138\n",
       "  254  0.043812  0.042592  0.042967  0.165619  0.065534         0.476386,\n",
       "  0.043035880711343524),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  251  0.044840  0.043801  0.045546  0.094359  0.066857         0.486135\n",
       "  252  0.042529  0.042363  0.043366  0.288690  0.066728         0.477963\n",
       "  253  0.043752  0.042588  0.044486  0.134726  0.065532         0.487138\n",
       "  254  0.043812  0.042592  0.042967  0.165619  0.065534         0.476386\n",
       "  255  0.042590  0.042183  0.043153  0.141507  0.064132         0.497103,\n",
       "  0.043752467654814),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  252  0.042529  0.042363  0.043366  0.288690  0.066728         0.477963\n",
       "  253  0.043752  0.042588  0.044486  0.134726  0.065532         0.487138\n",
       "  254  0.043812  0.042592  0.042967  0.165619  0.065534         0.476386\n",
       "  255  0.042590  0.042183  0.043153  0.141507  0.064132         0.497103\n",
       "  256  0.042929  0.043370  0.043718  0.208917  0.065436         0.492479,\n",
       "  0.04375725959124359),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  253  0.043752  0.042588  0.044486  0.134726  0.065532         0.487138\n",
       "  254  0.043812  0.042592  0.042967  0.165619  0.065534         0.476386\n",
       "  255  0.042590  0.042183  0.043153  0.141507  0.064132         0.497103\n",
       "  256  0.042929  0.043370  0.043718  0.208917  0.065436         0.492479\n",
       "  257  0.044127  0.043192  0.044570  0.106358  0.066136         0.487156,\n",
       "  0.04399452780104465),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  254  0.043812  0.042592  0.042967  0.165619  0.065534         0.476386\n",
       "  255  0.042590  0.042183  0.043153  0.141507  0.064132         0.497103\n",
       "  256  0.042929  0.043370  0.043718  0.208917  0.065436         0.492479\n",
       "  257  0.044127  0.043192  0.044570  0.106358  0.066136         0.487156\n",
       "  258  0.044351  0.043434  0.045222  0.091246  0.066140         0.488895,\n",
       "  0.045161672237244654),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  255  0.042590  0.042183  0.043153  0.141507  0.064132         0.497103\n",
       "  256  0.042929  0.043370  0.043718  0.208917  0.065436         0.492479\n",
       "  257  0.044127  0.043192  0.044570  0.106358  0.066136         0.487156\n",
       "  258  0.044351  0.043434  0.045222  0.091246  0.066140         0.488895\n",
       "  259  0.044053  0.044405  0.045120  0.143485  0.066372         0.495848,\n",
       "  0.044878880631362444),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  256  0.042929  0.043370  0.043718  0.208917  0.065436         0.492479\n",
       "  257  0.044127  0.043192  0.044570  0.106358  0.066136         0.487156\n",
       "  258  0.044351  0.043434  0.045222  0.091246  0.066140         0.488895\n",
       "  259  0.044053  0.044405  0.045120  0.143485  0.066372         0.495848\n",
       "  260  0.045292  0.044498  0.046002  0.116462  0.067512         0.485006,\n",
       "  0.044574506066822464),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  257  0.044127  0.043192  0.044570  0.106358  0.066136         0.487156\n",
       "  258  0.044351  0.043434  0.045222  0.091246  0.066140         0.488895\n",
       "  259  0.044053  0.044405  0.045120  0.143485  0.066372         0.495848\n",
       "  260  0.045292  0.044498  0.046002  0.116462  0.067512         0.485006\n",
       "  261  0.045494  0.044258  0.045634  0.122348  0.067236         0.484845,\n",
       "  0.044634424516917),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  258  0.044351  0.043434  0.045222  0.091246  0.066140         0.488895\n",
       "  259  0.044053  0.044405  0.045120  0.143485  0.066372         0.495848\n",
       "  260  0.045292  0.044498  0.046002  0.116462  0.067512         0.485006\n",
       "  261  0.045494  0.044258  0.045634  0.122348  0.067236         0.484845\n",
       "  262  0.044936  0.044021  0.045789  0.066601  0.066939         0.487569,\n",
       "  0.045161672237244654),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  259  0.044053  0.044405  0.045120  0.143485  0.066372         0.495848\n",
       "  260  0.045292  0.044498  0.046002  0.116462  0.067512         0.485006\n",
       "  261  0.045494  0.044258  0.045634  0.122348  0.067236         0.484845\n",
       "  262  0.044936  0.044021  0.045789  0.066601  0.066939         0.487569\n",
       "  263  0.045080  0.044612  0.046137  0.083443  0.066997         0.491063,\n",
       "  0.044840525895201086),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  260  0.045292  0.044498  0.046002  0.116462  0.067512         0.485006\n",
       "  261  0.045494  0.044258  0.045634  0.122348  0.067236         0.484845\n",
       "  262  0.044936  0.044021  0.045789  0.066601  0.066939         0.487569\n",
       "  263  0.045080  0.044612  0.046137  0.083443  0.066997         0.491063\n",
       "  264  0.045680  0.044469  0.046225  0.102361  0.067512         0.484719,\n",
       "  0.04670029759272358),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  261  0.045494  0.044258  0.045634  0.122348  0.067236         0.484845\n",
       "  262  0.044936  0.044021  0.045789  0.066601  0.066939         0.487569\n",
       "  263  0.045080  0.044612  0.046137  0.083443  0.066997         0.491063\n",
       "  264  0.045680  0.044469  0.046225  0.102361  0.067512         0.484719\n",
       "  265  0.044931  0.045917  0.045856  0.182317  0.067198         0.501027,\n",
       "  0.04682492642934226),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  262  0.044936  0.044021  0.045789  0.066601  0.066939         0.487569\n",
       "  263  0.045080  0.044612  0.046137  0.083443  0.066997         0.491063\n",
       "  264  0.045680  0.044469  0.046225  0.102361  0.067512         0.484719\n",
       "  265  0.044931  0.045917  0.045856  0.182317  0.067198         0.501027\n",
       "  266  0.047083  0.046813  0.047814  0.160762  0.069015         0.488052,\n",
       "  0.04771166560551256),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  263  0.045080  0.044612  0.046137  0.083443  0.066997         0.491063\n",
       "  264  0.045680  0.044469  0.046225  0.102361  0.067512         0.484719\n",
       "  265  0.044931  0.045917  0.045856  0.182317  0.067198         0.501027\n",
       "  266  0.047083  0.046813  0.047814  0.160762  0.069015         0.488052\n",
       "  267  0.047672  0.046770  0.048589  0.106696  0.069136         0.493751,\n",
       "  0.05026405494199525),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  264  0.045680  0.044469  0.046225  0.102361  0.067512         0.484719\n",
       "  265  0.044931  0.045917  0.045856  0.182317  0.067198         0.501027\n",
       "  266  0.047083  0.046813  0.047814  0.160762  0.069015         0.488052\n",
       "  267  0.047672  0.046770  0.048589  0.106696  0.069136         0.493751\n",
       "  268  0.048748  0.049690  0.049664  0.252539  0.070002         0.506206,\n",
       "  0.05072899786637587),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  265  0.044931  0.045917  0.045856  0.182317  0.067198         0.501027\n",
       "  266  0.047083  0.046813  0.047814  0.160762  0.069015         0.488052\n",
       "  267  0.047672  0.046770  0.048589  0.106696  0.069136         0.493751\n",
       "  268  0.048748  0.049690  0.049664  0.252539  0.070002         0.506206\n",
       "  269  0.050493  0.050664  0.050916  0.250432  0.072495         0.490597,\n",
       "  0.04864873935891818),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  266  0.047083  0.046813  0.047814  0.160762  0.069015         0.488052\n",
       "  267  0.047672  0.046770  0.048589  0.106696  0.069136         0.493751\n",
       "  268  0.048748  0.049690  0.049664  0.252539  0.070002         0.506206\n",
       "  269  0.050493  0.050664  0.050916  0.250432  0.072495         0.490597\n",
       "  270  0.050428  0.050131  0.049495  0.274169  0.072949         0.471565,\n",
       "  0.048557673322031286),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  267  0.047672  0.046770  0.048589  0.106696  0.069136         0.493751\n",
       "  268  0.048748  0.049690  0.049664  0.252539  0.070002         0.506206\n",
       "  269  0.050493  0.050664  0.050916  0.250432  0.072495         0.490597\n",
       "  270  0.050428  0.050131  0.049495  0.274169  0.072949         0.471565\n",
       "  271  0.048211  0.048583  0.048916  0.376094  0.070917         0.486440,\n",
       "  0.04797768543389118),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  268  0.048748  0.049690  0.049664  0.252539  0.070002         0.506206\n",
       "  269  0.050493  0.050664  0.050916  0.250432  0.072495         0.490597\n",
       "  270  0.050428  0.050131  0.049495  0.274169  0.072949         0.471565\n",
       "  271  0.048211  0.048583  0.048916  0.376094  0.070917         0.486440\n",
       "  272  0.049294  0.048050  0.049390  0.184270  0.070828         0.482784,\n",
       "  0.048838068959698694),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  269  0.050493  0.050664  0.050916  0.250432  0.072495         0.490597\n",
       "  270  0.050428  0.050131  0.049495  0.274169  0.072949         0.471565\n",
       "  271  0.048211  0.048583  0.048916  0.376094  0.070917         0.486440\n",
       "  272  0.049294  0.048050  0.049390  0.184270  0.070828         0.482784\n",
       "  273  0.048568  0.048406  0.049594  0.140053  0.070262         0.493554,\n",
       "  0.04982547729033969),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  270  0.050428  0.050131  0.049495  0.274169  0.072949         0.471565\n",
       "  271  0.048211  0.048583  0.048916  0.376094  0.070917         0.486440\n",
       "  272  0.049294  0.048050  0.049390  0.184270  0.070828         0.482784\n",
       "  273  0.048568  0.048406  0.049594  0.140053  0.070262         0.493554\n",
       "  274  0.049807  0.049662  0.050689  0.227352  0.071102         0.494504,\n",
       "  0.050781718789463706),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  271  0.048211  0.048583  0.048916  0.376094  0.070917         0.486440\n",
       "  272  0.049294  0.048050  0.049390  0.184270  0.070828         0.482784\n",
       "  273  0.048568  0.048406  0.049594  0.140053  0.070262         0.493554\n",
       "  274  0.049807  0.049662  0.050689  0.227352  0.071102         0.494504\n",
       "  275  0.050351  0.050245  0.050865  0.246049  0.072067         0.494271,\n",
       "  0.050654703606992535),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  272  0.049294  0.048050  0.049390  0.184270  0.070828         0.482784\n",
       "  273  0.048568  0.048406  0.049594  0.140053  0.070262         0.493554\n",
       "  274  0.049807  0.049662  0.050689  0.227352  0.071102         0.494504\n",
       "  275  0.050351  0.050245  0.050865  0.246049  0.072067         0.494271\n",
       "  276  0.051095  0.051539  0.051885  0.316263  0.073000         0.486171,\n",
       "  0.051610945106116565),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  273  0.048568  0.048406  0.049594  0.140053  0.070262         0.493554\n",
       "  274  0.049807  0.049662  0.050689  0.227352  0.071102         0.494504\n",
       "  275  0.050351  0.050245  0.050865  0.246049  0.072067         0.494271\n",
       "  276  0.051095  0.051539  0.051885  0.316263  0.073000         0.486171\n",
       "  277  0.051456  0.051062  0.052471  0.206258  0.072876         0.494271,\n",
       "  0.05135211318238232),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  274  0.049807  0.049662  0.050689  0.227352  0.071102         0.494504\n",
       "  275  0.050351  0.050245  0.050865  0.246049  0.072067         0.494271\n",
       "  276  0.051095  0.051539  0.051885  0.316263  0.073000         0.486171\n",
       "  277  0.051456  0.051062  0.052471  0.206258  0.072876         0.494271\n",
       "  278  0.053020  0.051948  0.052464  0.240569  0.073810         0.485185,\n",
       "  0.05127063101835462),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  275  0.050351  0.050245  0.050865  0.246049  0.072067         0.494271\n",
       "  276  0.051095  0.051539  0.051885  0.316263  0.073000         0.486171\n",
       "  277  0.051456  0.051062  0.052471  0.206258  0.072876         0.494271\n",
       "  278  0.053020  0.051948  0.052464  0.240569  0.073810         0.485185\n",
       "  279  0.051925  0.051347  0.052663  0.167081  0.073558         0.486511,\n",
       "  0.04936772227060346),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  276  0.051095  0.051539  0.051885  0.316263  0.073000         0.486171\n",
       "  277  0.051456  0.051062  0.052471  0.206258  0.072876         0.494271\n",
       "  278  0.053020  0.051948  0.052464  0.240569  0.073810         0.485185\n",
       "  279  0.051925  0.051347  0.052663  0.167081  0.073558         0.486511\n",
       "  280  0.051749  0.050562  0.050778  0.194428  0.073478         0.472891,\n",
       "  0.05023529370105537),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  277  0.051456  0.051062  0.052471  0.206258  0.072876         0.494271\n",
       "  278  0.053020  0.051948  0.052464  0.240569  0.073810         0.485185\n",
       "  279  0.051925  0.051347  0.052663  0.167081  0.073558         0.486511\n",
       "  280  0.051749  0.050562  0.050778  0.194428  0.073478         0.472891\n",
       "  281  0.049935  0.049524  0.050895  0.136450  0.071620         0.493608,\n",
       "  0.051874568966280385),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  278  0.053020  0.051948  0.052464  0.240569  0.073810         0.485185\n",
       "  279  0.051925  0.051347  0.052663  0.167081  0.073558         0.486511\n",
       "  280  0.051749  0.050562  0.050778  0.194428  0.073478         0.472891\n",
       "  281  0.049935  0.049524  0.050895  0.136450  0.071620         0.493608\n",
       "  282  0.051749  0.051131  0.052399  0.222494  0.072467         0.499379,\n",
       "  0.05240661824539995),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  279  0.051925  0.051347  0.052663  0.167081  0.073558         0.486511\n",
       "  280  0.051749  0.050562  0.050778  0.194428  0.073478         0.472891\n",
       "  281  0.049935  0.049524  0.050895  0.136450  0.071620         0.493608\n",
       "  282  0.051749  0.051131  0.052399  0.222494  0.072467         0.499379\n",
       "  283  0.051607  0.051749  0.052520  0.222713  0.074068         0.491099,\n",
       "  0.05056602391595813),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  280  0.051749  0.050562  0.050778  0.194428  0.073478         0.472891\n",
       "  281  0.049935  0.049524  0.050895  0.136450  0.071620         0.493608\n",
       "  282  0.051749  0.051131  0.052399  0.222494  0.072467         0.499379\n",
       "  283  0.051607  0.051749  0.052520  0.222713  0.074068         0.491099\n",
       "  284  0.053128  0.052027  0.052023  0.222324  0.074587         0.473357,\n",
       "  0.05049652159300441),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  281  0.049935  0.049524  0.050895  0.136450  0.071620         0.493608\n",
       "  282  0.051749  0.051131  0.052399  0.222494  0.072467         0.499379\n",
       "  283  0.051607  0.051749  0.052520  0.222713  0.074068         0.491099\n",
       "  284  0.053128  0.052027  0.052023  0.222324  0.074587         0.473357\n",
       "  285  0.051381  0.050515  0.051360  0.202479  0.072790         0.486601,\n",
       "  0.0509854338218953),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  282  0.051749  0.051131  0.052399  0.222494  0.072467         0.499379\n",
       "  283  0.051607  0.051749  0.052520  0.222713  0.074068         0.491099\n",
       "  284  0.053128  0.052027  0.052023  0.222324  0.074587         0.473357\n",
       "  285  0.051381  0.050515  0.051360  0.202479  0.072790         0.486601\n",
       "  286  0.051730  0.050415  0.051866  0.194340  0.072722         0.490776,\n",
       "  0.05104294668141275),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  283  0.051607  0.051749  0.052520  0.222713  0.074068         0.491099\n",
       "  284  0.053128  0.052027  0.052023  0.222324  0.074587         0.473357\n",
       "  285  0.051381  0.050515  0.051360  0.202479  0.072790         0.486601\n",
       "  286  0.051730  0.050415  0.051866  0.194340  0.072722         0.490776\n",
       "  287  0.051886  0.050934  0.051975  0.164580  0.073199         0.487551,\n",
       "  0.050482145783715626),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  284  0.053128  0.052027  0.052023  0.222324  0.074587         0.473357\n",
       "  285  0.051381  0.050515  0.051360  0.202479  0.072790         0.486601\n",
       "  286  0.051730  0.050415  0.051866  0.194340  0.072722         0.490776\n",
       "  287  0.051886  0.050934  0.051975  0.164580  0.073199         0.487551\n",
       "  288  0.051530  0.050496  0.051638  0.135439  0.073256         0.482927,\n",
       "  0.049391691575113736),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  285  0.051381  0.050515  0.051360  0.202479  0.072790         0.486601\n",
       "  286  0.051730  0.050415  0.051866  0.194340  0.072722         0.490776\n",
       "  287  0.051886  0.050934  0.051975  0.164580  0.073199         0.487551\n",
       "  288  0.051530  0.050496  0.051638  0.135439  0.073256         0.482927\n",
       "  289  0.050880  0.049982  0.050599  0.207689  0.072708         0.478967,\n",
       "  0.048162232720604395),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  286  0.051730  0.050415  0.051866  0.194340  0.072722         0.490776\n",
       "  287  0.051886  0.050934  0.051975  0.164580  0.073199         0.487551\n",
       "  288  0.051530  0.050496  0.051638  0.135439  0.073256         0.482927\n",
       "  289  0.050880  0.049982  0.050599  0.207689  0.072708         0.478967\n",
       "  290  0.049578  0.048688  0.049180  0.226262  0.071643         0.477927,\n",
       "  0.04731622500408565),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  287  0.051886  0.050934  0.051975  0.164580  0.073199         0.487551\n",
       "  288  0.051530  0.050496  0.051638  0.135439  0.073256         0.482927\n",
       "  289  0.050880  0.049982  0.050599  0.207689  0.072708         0.478967\n",
       "  290  0.049578  0.048688  0.049180  0.226262  0.071643         0.477927\n",
       "  291  0.048808  0.047458  0.047164  0.256946  0.070442         0.480794,\n",
       "  0.047004662534901265),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  288  0.051530  0.050496  0.051638  0.135439  0.073256         0.482927\n",
       "  289  0.050880  0.049982  0.050599  0.207689  0.072708         0.478967\n",
       "  290  0.049578  0.048688  0.049180  0.226262  0.071643         0.477927\n",
       "  291  0.048808  0.047458  0.047164  0.256946  0.070442         0.480794\n",
       "  292  0.048303  0.047038  0.047615  0.207117  0.069616         0.484791,\n",
       "  0.04912805809258759),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  289  0.050880  0.049982  0.050599  0.207689  0.072708         0.478967\n",
       "  290  0.049578  0.048688  0.049180  0.226262  0.071643         0.477927\n",
       "  291  0.048808  0.047458  0.047164  0.256946  0.070442         0.480794\n",
       "  292  0.048303  0.047038  0.047615  0.207117  0.069616         0.484791\n",
       "  293  0.047725  0.048216  0.048104  0.183952  0.069312         0.502999,\n",
       "  0.04871584571365713),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  290  0.049578  0.048688  0.049180  0.226262  0.071643         0.477927\n",
       "  291  0.048808  0.047458  0.047164  0.256946  0.070442         0.480794\n",
       "  292  0.048303  0.047038  0.047615  0.207117  0.069616         0.484791\n",
       "  293  0.047725  0.048216  0.048104  0.183952  0.069312         0.502999\n",
       "  294  0.049521  0.048870  0.050086  0.172060  0.071386         0.484038,\n",
       "  0.0500148068911202),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  291  0.048808  0.047458  0.047164  0.256946  0.070442         0.480794\n",
       "  292  0.048303  0.047038  0.047615  0.207117  0.069616         0.484791\n",
       "  293  0.047725  0.048216  0.048104  0.183952  0.069312         0.502999\n",
       "  294  0.049521  0.048870  0.050086  0.172060  0.071386         0.484038\n",
       "  295  0.049290  0.049323  0.050381  0.170428  0.070983         0.496834,\n",
       "  0.04869666834557644),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  292  0.048303  0.047038  0.047615  0.207117  0.069616         0.484791\n",
       "  293  0.047725  0.048216  0.048104  0.183952  0.069312         0.502999\n",
       "  294  0.049521  0.048870  0.050086  0.172060  0.071386         0.484038\n",
       "  295  0.049290  0.049323  0.050381  0.170428  0.070983         0.496834\n",
       "  296  0.050731  0.049621  0.049715  0.338283  0.072252         0.477264,\n",
       "  0.05749221955028466),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  293  0.047725  0.048216  0.048104  0.183952  0.069312         0.502999\n",
       "  294  0.049521  0.048870  0.050086  0.172060  0.071386         0.484038\n",
       "  295  0.049290  0.049323  0.050381  0.170428  0.070983         0.496834\n",
       "  296  0.050731  0.049621  0.049715  0.338283  0.072252         0.477264\n",
       "  297  0.059349  0.057987  0.057415  0.556724  0.070964         0.552891,\n",
       "  0.0595892498352459),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  294  0.049521  0.048870  0.050086  0.172060  0.071386         0.484038\n",
       "  295  0.049290  0.049323  0.050381  0.170428  0.070983         0.496834\n",
       "  296  0.050731  0.049621  0.049715  0.338283  0.072252         0.477264\n",
       "  297  0.059349  0.057987  0.057415  0.556724  0.070964         0.552891\n",
       "  298  0.058764  0.058667  0.059641  0.229315  0.079554         0.502801,\n",
       "  0.05917224551988584),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  295  0.049290  0.049323  0.050381  0.170428  0.070983         0.496834\n",
       "  296  0.050731  0.049621  0.049715  0.338283  0.072252         0.477264\n",
       "  297  0.059349  0.057987  0.057415  0.556724  0.070964         0.552891\n",
       "  298  0.058764  0.058667  0.059641  0.229315  0.079554         0.502801\n",
       "  299  0.059344  0.058217  0.059803  0.167207  0.081602         0.484002,\n",
       "  0.061216545259396946),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  296  0.050731  0.049621  0.049715  0.338283  0.072252         0.477264\n",
       "  297  0.059349  0.057987  0.057415  0.556724  0.070964         0.552891\n",
       "  298  0.058764  0.058667  0.059641  0.229315  0.079554         0.502801\n",
       "  299  0.059344  0.058217  0.059803  0.167207  0.081602         0.484002\n",
       "  300  0.059465  0.060255  0.060610  0.216564  0.081195         0.502407,\n",
       "  0.06064616048884064),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  297  0.059349  0.057987  0.057415  0.556724  0.070964         0.552891\n",
       "  298  0.058764  0.058667  0.059641  0.229315  0.079554         0.502801\n",
       "  299  0.059344  0.058217  0.059803  0.167207  0.081602         0.484002\n",
       "  300  0.059465  0.060255  0.060610  0.216564  0.081195         0.502407\n",
       "  301  0.061948  0.060513  0.061940  0.124790  0.083191         0.482855,\n",
       "  0.06187082740692039),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  298  0.058764  0.058667  0.059641  0.229315  0.079554         0.502801\n",
       "  299  0.059344  0.058217  0.059803  0.167207  0.081602         0.484002\n",
       "  300  0.059465  0.060255  0.060610  0.216564  0.081195         0.502407\n",
       "  301  0.061948  0.060513  0.061940  0.124790  0.083191         0.482855\n",
       "  302  0.061628  0.060954  0.062749  0.143497  0.082634         0.496278,\n",
       "  0.06521888026032416),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  299  0.059344  0.058217  0.059803  0.167207  0.081602         0.484002\n",
       "  300  0.059465  0.060255  0.060610  0.216564  0.081195         0.502407\n",
       "  301  0.061948  0.060513  0.061940  0.124790  0.083191         0.482855\n",
       "  302  0.061628  0.060954  0.062749  0.143497  0.082634         0.496278\n",
       "  303  0.062824  0.064698  0.063836  0.348771  0.083830         0.512156,\n",
       "  0.0669540327435903),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  300  0.059465  0.060255  0.060610  0.216564  0.081195         0.502407\n",
       "  301  0.061948  0.060513  0.061940  0.124790  0.083191         0.482855\n",
       "  302  0.061628  0.060954  0.062749  0.143497  0.082634         0.496278\n",
       "  303  0.062824  0.064698  0.063836  0.348771  0.083830         0.512156\n",
       "  304  0.065621  0.066866  0.066137  0.397739  0.087100         0.500095,\n",
       "  0.06702593103475882),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  301  0.061948  0.060513  0.061940  0.124790  0.083191         0.482855\n",
       "  302  0.061628  0.060954  0.062749  0.143497  0.082634         0.496278\n",
       "  303  0.062824  0.064698  0.063836  0.348771  0.083830         0.512156\n",
       "  304  0.065621  0.066866  0.066137  0.397739  0.087100         0.500095\n",
       "  305  0.067989  0.067127  0.068114  0.256890  0.088794         0.487658,\n",
       "  0.06852859762229119),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  302  0.061628  0.060954  0.062749  0.143497  0.082634         0.496278\n",
       "  303  0.062824  0.064698  0.063836  0.348771  0.083830         0.512156\n",
       "  304  0.065621  0.066866  0.066137  0.397739  0.087100         0.500095\n",
       "  305  0.067989  0.067127  0.068114  0.256890  0.088794         0.487658\n",
       "  306  0.068160  0.067502  0.069133  0.181074  0.088864         0.498357,\n",
       "  0.06960228005338949),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  303  0.062824  0.064698  0.063836  0.348771  0.083830         0.512156\n",
       "  304  0.065621  0.066866  0.066137  0.397739  0.087100         0.500095\n",
       "  305  0.067989  0.067127  0.068114  0.256890  0.088794         0.487658\n",
       "  306  0.068160  0.067502  0.069133  0.181074  0.088864         0.498357\n",
       "  307  0.069982  0.068736  0.070119  0.214555  0.090332         0.495149,\n",
       "  0.07070471410306535),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  304  0.065621  0.066866  0.066137  0.397739  0.087100         0.500095\n",
       "  305  0.067989  0.067127  0.068114  0.256890  0.088794         0.487658\n",
       "  306  0.068160  0.067502  0.069133  0.181074  0.088864         0.498357\n",
       "  307  0.069982  0.068736  0.070119  0.214555  0.090332         0.495149\n",
       "  308  0.071214  0.070082  0.071742  0.233201  0.091380         0.495364,\n",
       "  0.06947525524855601),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  305  0.067989  0.067127  0.068114  0.256890  0.088794         0.487658\n",
       "  306  0.068160  0.067502  0.069133  0.181074  0.088864         0.498357\n",
       "  307  0.069982  0.068736  0.070119  0.214555  0.090332         0.495149\n",
       "  308  0.071214  0.070082  0.071742  0.233201  0.091380         0.495364\n",
       "  309  0.071009  0.069637  0.070877  0.191998  0.092457         0.477927,\n",
       "  0.06689890622992534),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  306  0.068160  0.067502  0.069133  0.181074  0.088864         0.498357\n",
       "  307  0.069982  0.068736  0.070119  0.214555  0.090332         0.495149\n",
       "  308  0.071214  0.070082  0.071742  0.233201  0.091380         0.495364\n",
       "  309  0.071009  0.069637  0.070877  0.191998  0.092457         0.477927\n",
       "  310  0.069185  0.067907  0.068322  0.254494  0.091256         0.467855,\n",
       "  0.06976284841323012),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  307  0.069982  0.068736  0.070119  0.214555  0.090332         0.495149\n",
       "  308  0.071214  0.070082  0.071742  0.233201  0.091380         0.495364\n",
       "  309  0.071009  0.069637  0.070877  0.191998  0.092457         0.477927\n",
       "  310  0.069185  0.067907  0.068322  0.254494  0.091256         0.467855\n",
       "  311  0.067185  0.068632  0.068177  0.221945  0.088740         0.508536,\n",
       "  0.0695950921487451),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  308  0.071214  0.070082  0.071742  0.233201  0.091380         0.495364\n",
       "  309  0.071009  0.069637  0.070877  0.191998  0.092457         0.477927\n",
       "  310  0.069185  0.067907  0.068322  0.254494  0.091256         0.467855\n",
       "  311  0.067185  0.068632  0.068177  0.221945  0.088740         0.508536\n",
       "  312  0.071219  0.069947  0.070943  0.171722  0.091537         0.485866,\n",
       "  0.07116965702744599),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  309  0.071009  0.069637  0.070877  0.191998  0.092457         0.477927\n",
       "  310  0.069185  0.067907  0.068322  0.254494  0.091256         0.467855\n",
       "  311  0.067185  0.068632  0.068177  0.221945  0.088740         0.508536\n",
       "  312  0.071219  0.069947  0.070943  0.171722  0.091537         0.485866\n",
       "  313  0.070497  0.070260  0.071592  0.189818  0.091373         0.498895,\n",
       "  0.0701702784780933),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  310  0.069185  0.067907  0.068322  0.254494  0.091256         0.467855\n",
       "  311  0.067185  0.068632  0.068177  0.221945  0.088740         0.508536\n",
       "  312  0.071219  0.069947  0.070943  0.171722  0.091537         0.485866\n",
       "  313  0.070497  0.070260  0.071592  0.189818  0.091373         0.498895\n",
       "  314  0.071040  0.069971  0.071451  0.209691  0.092911         0.479648,\n",
       "  0.07145245825569049),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  311  0.067185  0.068632  0.068177  0.221945  0.088740         0.508536\n",
       "  312  0.071219  0.069947  0.070943  0.171722  0.091537         0.485866\n",
       "  313  0.070497  0.070260  0.071592  0.189818  0.091373         0.498895\n",
       "  314  0.071040  0.069971  0.071451  0.209691  0.092911         0.479648\n",
       "  315  0.071478  0.070409  0.072217  0.211400  0.091935         0.496708,\n",
       "  0.07272265819221368),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  312  0.071219  0.069947  0.070943  0.171722  0.091537         0.485866\n",
       "  313  0.070497  0.070260  0.071592  0.189818  0.091373         0.498895\n",
       "  314  0.071040  0.069971  0.071451  0.209691  0.092911         0.479648\n",
       "  315  0.071478  0.070409  0.072217  0.211400  0.091935         0.496708\n",
       "  316  0.072826  0.071585  0.073530  0.223976  0.093187         0.496619,\n",
       "  0.071946162421011),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  313  0.070497  0.070260  0.071592  0.189818  0.091373         0.498895\n",
       "  314  0.071040  0.069971  0.071451  0.209691  0.092911         0.479648\n",
       "  315  0.071478  0.070409  0.072217  0.211400  0.091935         0.496708\n",
       "  316  0.072826  0.071585  0.073530  0.223976  0.093187         0.496619\n",
       "  317  0.073233  0.071748  0.073607  0.170790  0.094428         0.481314,\n",
       "  0.07413906030165104),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  314  0.071040  0.069971  0.071451  0.209691  0.092911         0.479648\n",
       "  315  0.071478  0.070409  0.072217  0.211400  0.091935         0.496708\n",
       "  316  0.072826  0.071585  0.073530  0.223976  0.093187         0.496619\n",
       "  317  0.073233  0.071748  0.073607  0.170790  0.094428         0.481314\n",
       "  318  0.072193  0.073047  0.072067  0.251288  0.093670         0.503518,\n",
       "  0.07584304595340022),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  315  0.071478  0.070409  0.072217  0.211400  0.091935         0.496708\n",
       "  316  0.072826  0.071585  0.073530  0.223976  0.093187         0.496619\n",
       "  317  0.073233  0.071748  0.073607  0.170790  0.094428         0.481314\n",
       "  318  0.072193  0.073047  0.072067  0.251288  0.093670         0.503518\n",
       "  319  0.074475  0.074784  0.075153  0.233311  0.095811         0.499862,\n",
       "  0.07737447378187244),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  316  0.072826  0.071585  0.073530  0.223976  0.093187         0.496619\n",
       "  317  0.073233  0.071748  0.073607  0.170790  0.094428         0.481314\n",
       "  318  0.072193  0.073047  0.072067  0.251288  0.093670         0.503518\n",
       "  319  0.074475  0.074784  0.075153  0.233311  0.095811         0.499862\n",
       "  320  0.076494  0.076540  0.077466  0.245205  0.097475         0.498572,\n",
       "  0.07875492674572553),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  317  0.073233  0.071748  0.073607  0.170790  0.094428         0.481314\n",
       "  318  0.072193  0.073047  0.072067  0.251288  0.093670         0.503518\n",
       "  319  0.074475  0.074784  0.075153  0.233311  0.095811         0.499862\n",
       "  320  0.076494  0.076540  0.077466  0.245205  0.097475         0.498572\n",
       "  321  0.078597  0.077495  0.079772  0.117631  0.098971         0.497443,\n",
       "  0.0775230719230014),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  318  0.072193  0.073047  0.072067  0.251288  0.093670         0.503518\n",
       "  319  0.074475  0.074784  0.075153  0.233311  0.095811         0.499862\n",
       "  320  0.076494  0.076540  0.077466  0.245205  0.097475         0.498572\n",
       "  321  0.078597  0.077495  0.079772  0.117631  0.098971         0.497443\n",
       "  322  0.079569  0.078085  0.079079  0.267665  0.100319         0.477909,\n",
       "  0.07274423152850917),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  319  0.074475  0.074784  0.075153  0.233311  0.095811         0.499862\n",
       "  320  0.076494  0.076540  0.077466  0.245205  0.097475         0.498572\n",
       "  321  0.078597  0.077495  0.079772  0.117631  0.098971         0.497443\n",
       "  322  0.079569  0.078085  0.079079  0.267665  0.100319         0.477909\n",
       "  323  0.078280  0.076938  0.074162  0.522754  0.099116         0.451386,\n",
       "  0.07307256577519713),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  320  0.076494  0.076540  0.077466  0.245205  0.097475         0.498572\n",
       "  321  0.078597  0.077495  0.079772  0.117631  0.098971         0.497443\n",
       "  322  0.079569  0.078085  0.079079  0.267665  0.100319         0.477909\n",
       "  323  0.078280  0.076938  0.074162  0.522754  0.099116         0.451386\n",
       "  324  0.073418  0.072684  0.072568  0.379266  0.094449         0.489576,\n",
       "  0.07527265156048159),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  321  0.078597  0.077495  0.079772  0.117631  0.098971         0.497443\n",
       "  322  0.079569  0.078085  0.079079  0.267665  0.100319         0.477909\n",
       "  323  0.078280  0.076938  0.074162  0.522754  0.099116         0.451386\n",
       "  324  0.073418  0.072684  0.072568  0.379266  0.094449         0.489576\n",
       "  325  0.074607  0.074341  0.075652  0.236933  0.094770         0.503572,\n",
       "  0.0761378270227187),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  322  0.079569  0.078085  0.079079  0.267665  0.100319         0.477909\n",
       "  323  0.078280  0.076938  0.074162  0.522754  0.099116         0.451386\n",
       "  324  0.073418  0.072684  0.072568  0.379266  0.094449         0.489576\n",
       "  325  0.074607  0.074341  0.075652  0.236933  0.094770         0.503572\n",
       "  326  0.076470  0.075310  0.077001  0.183387  0.096918         0.493590,\n",
       "  0.0732978445215619),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  323  0.078280  0.076938  0.074162  0.522754  0.099116         0.451386\n",
       "  324  0.073418  0.072684  0.072568  0.379266  0.094449         0.489576\n",
       "  325  0.074607  0.074341  0.075652  0.236933  0.094770         0.503572\n",
       "  326  0.076470  0.075310  0.077001  0.183387  0.096918         0.493590\n",
       "  327  0.076484  0.074829  0.074680  0.250177  0.097763         0.465884,\n",
       "  0.07298389570652504),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  324  0.073418  0.072684  0.072568  0.379266  0.094449         0.489576\n",
       "  325  0.074607  0.074341  0.075652  0.236933  0.094770         0.503572\n",
       "  326  0.076470  0.075310  0.077001  0.183387  0.096918         0.493590\n",
       "  327  0.076484  0.074829  0.074680  0.250177  0.097763         0.465884\n",
       "  328  0.074487  0.074571  0.074055  0.367802  0.094990         0.484773,\n",
       "  0.07290720547892691),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  325  0.074607  0.074341  0.075652  0.236933  0.094770         0.503572\n",
       "  326  0.076470  0.075310  0.077001  0.183387  0.096918         0.493590\n",
       "  327  0.076484  0.074829  0.074680  0.250177  0.097763         0.465884\n",
       "  328  0.074487  0.074571  0.074055  0.367802  0.094990         0.484773\n",
       "  329  0.073875  0.072270  0.073404  0.283658  0.094683         0.486547,\n",
       "  0.07444821718025832),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  326  0.076470  0.075310  0.077001  0.183387  0.096918         0.493590\n",
       "  327  0.076484  0.074829  0.074680  0.250177  0.097763         0.465884\n",
       "  328  0.074487  0.074571  0.074055  0.367802  0.094990         0.484773\n",
       "  329  0.073875  0.072270  0.073404  0.283658  0.094683         0.486547\n",
       "  330  0.073668  0.073206  0.074283  0.216625  0.094608         0.498644,\n",
       "  0.07410071518785198),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  327  0.076484  0.074829  0.074680  0.250177  0.097763         0.465884\n",
       "  328  0.074487  0.074571  0.074055  0.367802  0.094990         0.484773\n",
       "  329  0.073875  0.072270  0.073404  0.283658  0.094683         0.486547\n",
       "  330  0.073668  0.073206  0.074283  0.216625  0.094608         0.498644\n",
       "  331  0.076003  0.074421  0.075717  0.185817  0.096113         0.484522,\n",
       "  0.07491795204106856),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  328  0.074487  0.074571  0.074055  0.367802  0.094990         0.484773\n",
       "  329  0.073875  0.072270  0.073404  0.283658  0.094683         0.486547\n",
       "  330  0.073668  0.073206  0.074283  0.216625  0.094608         0.498644\n",
       "  331  0.076003  0.074421  0.075717  0.185817  0.096113         0.484522\n",
       "  332  0.075914  0.074286  0.076054  0.168954  0.095774         0.493232,\n",
       "  0.07608750206784566),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  329  0.073875  0.072270  0.073404  0.283658  0.094683         0.486547\n",
       "  330  0.073668  0.073206  0.074283  0.216625  0.094608         0.498644\n",
       "  331  0.076003  0.074421  0.075717  0.185817  0.096113         0.484522\n",
       "  332  0.075914  0.074286  0.076054  0.168954  0.095774         0.493232\n",
       "  333  0.075401  0.074947  0.076463  0.198475  0.096572         0.495866,\n",
       "  0.07643979599668159),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  330  0.073668  0.073206  0.074283  0.216625  0.094608         0.498644\n",
       "  331  0.076003  0.074421  0.075717  0.185817  0.096113         0.484522\n",
       "  332  0.075914  0.074286  0.076054  0.168954  0.095774         0.493232\n",
       "  333  0.075401  0.074947  0.076463  0.198475  0.096572         0.495866\n",
       "  334  0.076494  0.075357  0.077395  0.161178  0.097714         0.489755,\n",
       "  0.07730017952248912),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  331  0.076003  0.074421  0.075717  0.185817  0.096113         0.484522\n",
       "  332  0.075914  0.074286  0.076054  0.168954  0.095774         0.493232\n",
       "  333  0.075401  0.074947  0.076463  0.198475  0.096572         0.495866\n",
       "  334  0.076494  0.075357  0.077395  0.161178  0.097714         0.489755\n",
       "  335  0.076992  0.076166  0.077640  0.146958  0.098058         0.493554,\n",
       "  0.07912160610621255),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  332  0.075914  0.074286  0.076054  0.168954  0.095774         0.493232\n",
       "  333  0.075401  0.074947  0.076463  0.198475  0.096572         0.495866\n",
       "  334  0.076494  0.075357  0.077395  0.161178  0.097714         0.489755\n",
       "  335  0.076992  0.076166  0.077640  0.146958  0.098058         0.493554\n",
       "  336  0.078491  0.078384  0.079154  0.397117  0.098898         0.500741,\n",
       "  0.0777914877195948),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  333  0.075401  0.074947  0.076463  0.198475  0.096572         0.495866\n",
       "  334  0.076494  0.075357  0.077395  0.161178  0.097714         0.489755\n",
       "  335  0.076992  0.076166  0.077640  0.146958  0.098058         0.493554\n",
       "  336  0.078491  0.078384  0.079154  0.397117  0.098898         0.500741\n",
       "  337  0.080142  0.081633  0.078515  0.533481  0.100677         0.477174,\n",
       "  0.07902334254231896),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  334  0.076494  0.075357  0.077395  0.161178  0.097714         0.489755\n",
       "  335  0.076992  0.076166  0.077640  0.146958  0.098058         0.493554\n",
       "  336  0.078491  0.078384  0.079154  0.397117  0.098898         0.500741\n",
       "  337  0.080142  0.081633  0.078515  0.533481  0.100677         0.477174\n",
       "  338  0.079098  0.078313  0.079619  0.245198  0.099378         0.496332,\n",
       "  0.07820609606674009),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  335  0.076992  0.076166  0.077640  0.146958  0.098058         0.493554\n",
       "  336  0.078491  0.078384  0.079154  0.397117  0.098898         0.500741\n",
       "  337  0.080142  0.081633  0.078515  0.533481  0.100677         0.477174\n",
       "  338  0.079098  0.078313  0.079619  0.245198  0.099378         0.496332\n",
       "  339  0.080467  0.079417  0.078660  0.272858  0.100581         0.481009,\n",
       "  0.0795745595671569),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  336  0.078491  0.078384  0.079154  0.397117  0.098898         0.500741\n",
       "  337  0.080142  0.081633  0.078515  0.533481  0.100677         0.477174\n",
       "  338  0.079098  0.078313  0.079619  0.245198  0.099378         0.496332\n",
       "  339  0.080467  0.079417  0.078660  0.272858  0.100581         0.481009\n",
       "  340  0.080039  0.078469  0.079924  0.183463  0.099783         0.497353,\n",
       "  0.07930853973877826),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  337  0.080142  0.081633  0.078515  0.533481  0.100677         0.477174\n",
       "  338  0.079098  0.078313  0.079619  0.245198  0.099378         0.496332\n",
       "  339  0.080467  0.079417  0.078660  0.272858  0.100581         0.481009\n",
       "  340  0.080039  0.078469  0.079924  0.183463  0.099783         0.497353\n",
       "  341  0.080130  0.078384  0.080615  0.111672  0.101119         0.485131,\n",
       "  0.077801071592454),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  338  0.079098  0.078313  0.079619  0.245198  0.099378         0.496332\n",
       "  339  0.080467  0.079417  0.078660  0.272858  0.100581         0.481009\n",
       "  340  0.080039  0.078469  0.079924  0.183463  0.099783         0.497353\n",
       "  341  0.080130  0.078384  0.080615  0.111672  0.101119         0.485131\n",
       "  342  0.080060  0.078232  0.079156  0.162939  0.100860         0.475848,\n",
       "  0.0782923797895597),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  339  0.080467  0.079417  0.078660  0.272858  0.100581         0.481009\n",
       "  340  0.080039  0.078469  0.079924  0.183463  0.099783         0.497353\n",
       "  341  0.080130  0.078384  0.080615  0.111672  0.101119         0.485131\n",
       "  342  0.080060  0.078232  0.079156  0.162939  0.100860         0.475848\n",
       "  343  0.078251  0.077265  0.078793  0.173035  0.099387         0.490794,\n",
       "  0.0767249931931409),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  340  0.080039  0.078469  0.079924  0.183463  0.099783         0.497353\n",
       "  341  0.080130  0.078384  0.080615  0.111672  0.101119         0.485131\n",
       "  342  0.080060  0.078232  0.079156  0.162939  0.100860         0.475848\n",
       "  343  0.078251  0.077265  0.078793  0.173035  0.099387         0.490794\n",
       "  344  0.079127  0.077490  0.078287  0.168849  0.099867         0.475400,\n",
       "  0.0754571892248325),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  341  0.080130  0.078384  0.080615  0.111672  0.101119         0.485131\n",
       "  342  0.080060  0.078232  0.079156  0.162939  0.100860         0.475848\n",
       "  343  0.078251  0.077265  0.078793  0.173035  0.099387         0.490794\n",
       "  344  0.079127  0.077490  0.078287  0.168849  0.099867         0.475400\n",
       "  345  0.076549  0.074997  0.076972  0.184448  0.098336         0.477640,\n",
       "  0.08033908511964791),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  342  0.080060  0.078232  0.079156  0.162939  0.100860         0.475848\n",
       "  343  0.078251  0.077265  0.078793  0.173035  0.099387         0.490794\n",
       "  344  0.079127  0.077490  0.078287  0.168849  0.099867         0.475400\n",
       "  345  0.076549  0.074997  0.076972  0.184448  0.098336         0.477640\n",
       "  346  0.077813  0.079135  0.078057  0.319056  0.097098         0.523626,\n",
       "  0.08273808437857207),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  343  0.078251  0.077265  0.078793  0.173035  0.099387         0.490794\n",
       "  344  0.079127  0.077490  0.078287  0.168849  0.099867         0.475400\n",
       "  345  0.076549  0.074997  0.076972  0.184448  0.098336         0.477640\n",
       "  346  0.077813  0.079135  0.078057  0.319056  0.097098         0.523626\n",
       "  347  0.082958  0.082284  0.083272  0.371721  0.101866         0.505059,\n",
       "  0.08417605020194259),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  344  0.079127  0.077490  0.078287  0.168849  0.099867         0.475400\n",
       "  345  0.076549  0.074997  0.076972  0.184448  0.098336         0.477640\n",
       "  346  0.077813  0.079135  0.078057  0.319056  0.097098         0.523626\n",
       "  347  0.082958  0.082284  0.083272  0.371721  0.101866         0.505059\n",
       "  348  0.083463  0.082898  0.083696  0.262798  0.104209         0.497873,\n",
       "  0.08763196011446146),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  345  0.076549  0.074997  0.076972  0.184448  0.098336         0.477640\n",
       "  346  0.077813  0.079135  0.078057  0.319056  0.097098         0.523626\n",
       "  347  0.082958  0.082284  0.083272  0.371721  0.101866         0.505059\n",
       "  348  0.083463  0.082898  0.083696  0.262798  0.104209         0.497873\n",
       "  349  0.086079  0.087389  0.086539  0.431811  0.105613         0.512963,\n",
       "  0.08792913715199475),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  346  0.077813  0.079135  0.078057  0.319056  0.097098         0.523626\n",
       "  347  0.082958  0.082284  0.083272  0.371721  0.101866         0.505059\n",
       "  348  0.083463  0.082898  0.083696  0.262798  0.104209         0.497873\n",
       "  349  0.086079  0.087389  0.086539  0.431811  0.105613         0.512963\n",
       "  350  0.088375  0.088076  0.088404  0.311046  0.108988         0.489343,\n",
       "  0.08861456688631054),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  347  0.082958  0.082284  0.083272  0.371721  0.101866         0.505059\n",
       "  348  0.083463  0.082898  0.083696  0.262798  0.104209         0.497873\n",
       "  349  0.086079  0.087389  0.086539  0.431811  0.105613         0.512963\n",
       "  350  0.088375  0.088076  0.088404  0.311046  0.108988         0.489343\n",
       "  351  0.087896  0.087330  0.088671  0.221176  0.109278         0.492246,\n",
       "  0.0890627284108253),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  348  0.083463  0.082898  0.083696  0.262798  0.104209         0.497873\n",
       "  349  0.086079  0.087389  0.086539  0.431811  0.105613         0.512963\n",
       "  350  0.088375  0.088076  0.088404  0.311046  0.108988         0.489343\n",
       "  351  0.087896  0.087330  0.088671  0.221176  0.109278         0.492246\n",
       "  352  0.089545  0.088481  0.090400  0.218994  0.109948         0.490472,\n",
       "  0.08714544385378535),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  349  0.086079  0.087389  0.086539  0.431811  0.105613         0.512963\n",
       "  350  0.088375  0.088076  0.088404  0.311046  0.108988         0.489343\n",
       "  351  0.087896  0.087330  0.088671  0.221176  0.109278         0.492246\n",
       "  352  0.089545  0.088481  0.090400  0.218994  0.109948         0.490472\n",
       "  353  0.090134  0.088453  0.088540  0.246185  0.110385         0.472784,\n",
       "  0.08777336072858373),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  350  0.088375  0.088076  0.088404  0.311046  0.108988         0.489343\n",
       "  351  0.087896  0.087330  0.088671  0.221176  0.109278         0.492246\n",
       "  352  0.089545  0.088481  0.090400  0.218994  0.109948         0.490472\n",
       "  353  0.090134  0.088453  0.088540  0.246185  0.110385         0.472784\n",
       "  354  0.087870  0.086533  0.088567  0.186138  0.108513         0.491816,\n",
       "  0.08797946210686781),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  351  0.087896  0.087330  0.088671  0.221176  0.109278         0.492246\n",
       "  352  0.089545  0.088481  0.090400  0.218994  0.109948         0.490472\n",
       "  353  0.090134  0.088453  0.088540  0.246185  0.110385         0.472784\n",
       "  354  0.087870  0.086533  0.088567  0.186138  0.108513         0.491816\n",
       "  355  0.087569  0.087247  0.088789  0.201065  0.109126         0.488662,\n",
       "  0.08265899818275915),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  352  0.089545  0.088481  0.090400  0.218994  0.109948         0.490472\n",
       "  353  0.090134  0.088453  0.088540  0.246185  0.110385         0.472784\n",
       "  354  0.087870  0.086533  0.088567  0.186138  0.108513         0.491816\n",
       "  355  0.087569  0.087247  0.088789  0.201065  0.109126         0.488662\n",
       "  356  0.083776  0.084275  0.084119  0.498185  0.109328         0.447336,\n",
       "  0.08063147022075161),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  353  0.090134  0.088453  0.088540  0.246185  0.110385         0.472784\n",
       "  354  0.087870  0.086533  0.088567  0.186138  0.108513         0.491816\n",
       "  355  0.087569  0.087247  0.088789  0.201065  0.109126         0.488662\n",
       "  356  0.083776  0.084275  0.084119  0.498185  0.109328         0.447336\n",
       "  357  0.084708  0.083358  0.081637  0.353500  0.104132         0.471959,\n",
       "  0.07176641669308968),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  354  0.087870  0.086533  0.088567  0.186138  0.108513         0.491816\n",
       "  355  0.087569  0.087247  0.088789  0.201065  0.109126         0.488662\n",
       "  356  0.083776  0.084275  0.084119  0.498185  0.109328         0.447336\n",
       "  357  0.084708  0.083358  0.081637  0.353500  0.104132         0.471959\n",
       "  358  0.081707  0.080168  0.072369  1.000000  0.102152         0.420831,\n",
       "  0.07848650132676979),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  355  0.087569  0.087247  0.088789  0.201065  0.109126         0.488662\n",
       "  356  0.083776  0.084275  0.084119  0.498185  0.109328         0.447336\n",
       "  357  0.084708  0.083358  0.081637  0.353500  0.104132         0.471959\n",
       "  358  0.081707  0.080168  0.072369  1.000000  0.102152         0.420831\n",
       "  359  0.074169  0.077410  0.075031  0.553051  0.093494         0.537371,\n",
       "  0.0822084310676673),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  356  0.083776  0.084275  0.084119  0.498185  0.109328         0.447336\n",
       "  357  0.084708  0.083358  0.081637  0.353500  0.104132         0.471959\n",
       "  358  0.081707  0.080168  0.072369  1.000000  0.102152         0.420831\n",
       "  359  0.074169  0.077410  0.075031  0.553051  0.093494         0.537371\n",
       "  360  0.081064  0.081334  0.081995  0.376055  0.100057         0.514952,\n",
       "  0.07980463987231357),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  357  0.084708  0.083358  0.081637  0.353500  0.104132         0.471959\n",
       "  358  0.081707  0.080168  0.072369  1.000000  0.102152         0.420831\n",
       "  359  0.074169  0.077410  0.075031  0.553051  0.093494         0.537371\n",
       "  360  0.081064  0.081334  0.081995  0.376055  0.100057         0.514952\n",
       "  361  0.084729  0.083533  0.080721  0.455719  0.103692         0.469146,\n",
       "  0.08010900481449124),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  358  0.081707  0.080168  0.072369  1.000000  0.102152         0.420831\n",
       "  359  0.074169  0.077410  0.075031  0.553051  0.093494         0.537371\n",
       "  360  0.081064  0.081334  0.081995  0.376055  0.100057         0.514952\n",
       "  361  0.084729  0.083533  0.080721  0.455719  0.103692         0.469146\n",
       "  362  0.081909  0.080159  0.079350  0.314310  0.101344         0.489396,\n",
       "  0.07992207118192553),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  359  0.074169  0.077410  0.075031  0.553051  0.093494         0.537371\n",
       "  360  0.081064  0.081334  0.081995  0.376055  0.100057         0.514952\n",
       "  361  0.084729  0.083533  0.080721  0.455719  0.103692         0.469146\n",
       "  362  0.081909  0.080159  0.079350  0.314310  0.101344         0.489396\n",
       "  363  0.080816  0.079739  0.080663  0.205490  0.101641         0.485723,\n",
       "  0.07832353699871437),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  360  0.081064  0.081334  0.081995  0.376055  0.100057         0.514952\n",
       "  361  0.084729  0.083533  0.080721  0.455719  0.103692         0.469146\n",
       "  362  0.081909  0.080159  0.079350  0.314310  0.101344         0.489396\n",
       "  363  0.080816  0.079739  0.080663  0.205490  0.101641         0.485723\n",
       "  364  0.079427  0.078568  0.079699  0.208815  0.101459         0.475167,\n",
       "  0.07972314808592354),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  361  0.084729  0.083533  0.080721  0.455719  0.103692         0.469146\n",
       "  362  0.081909  0.080159  0.079350  0.314310  0.101344         0.489396\n",
       "  363  0.080816  0.079739  0.080663  0.205490  0.101641         0.485723\n",
       "  364  0.079427  0.078568  0.079699  0.208815  0.101459         0.475167\n",
       "  365  0.079750  0.080066  0.079449  0.536623  0.099898         0.497586,\n",
       "  0.07232242565435719),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  362  0.081909  0.080159  0.079350  0.314310  0.101344         0.489396\n",
       "  363  0.080816  0.079739  0.080663  0.205490  0.101641         0.485723\n",
       "  364  0.079427  0.078568  0.079699  0.208815  0.101459         0.475167\n",
       "  365  0.079750  0.080066  0.079449  0.536623  0.099898         0.497586\n",
       "  366  0.069753  0.071156  0.070621  0.659614  0.101264         0.431780,\n",
       "  0.070946774249296),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  363  0.080816  0.079739  0.080663  0.205490  0.101641         0.485723\n",
       "  364  0.079427  0.078568  0.079699  0.208815  0.101459         0.475167\n",
       "  365  0.079750  0.080066  0.079449  0.536623  0.099898         0.497586\n",
       "  366  0.069753  0.071156  0.070621  0.659614  0.101264         0.431780\n",
       "  367  0.073312  0.072222  0.072670  0.287107  0.094037         0.476834,\n",
       "  0.06747409255927356),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  364  0.079427  0.078568  0.079699  0.208815  0.101459         0.475167\n",
       "  365  0.079750  0.080066  0.079449  0.536623  0.099898         0.497586\n",
       "  366  0.069753  0.071156  0.070621  0.659614  0.101264         0.431780\n",
       "  367  0.073312  0.072222  0.072670  0.287107  0.094037         0.476834\n",
       "  368  0.070858  0.069236  0.067000  0.444579  0.092694         0.461153,\n",
       "  0.06832489221222189),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  365  0.079750  0.080066  0.079449  0.536623  0.099898         0.497586\n",
       "  366  0.069753  0.071156  0.070621  0.659614  0.101264         0.431780\n",
       "  367  0.073312  0.072222  0.072670  0.287107  0.094037         0.476834\n",
       "  368  0.070858  0.069236  0.067000  0.444579  0.092694         0.461153\n",
       "  369  0.068858  0.068288  0.068693  0.217365  0.089302         0.493483,\n",
       "  0.0642075122475352),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  366  0.069753  0.071156  0.070621  0.659614  0.101264         0.431780\n",
       "  367  0.073312  0.072222  0.072670  0.287107  0.094037         0.476834\n",
       "  368  0.070858  0.069236  0.067000  0.444579  0.092694         0.461153\n",
       "  369  0.068858  0.068288  0.068693  0.217365  0.089302         0.493483\n",
       "  370  0.068299  0.066819  0.064766  0.404406  0.090133         0.456332,\n",
       "  0.06448552153935011),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  367  0.073312  0.072222  0.072670  0.287107  0.094037         0.476834\n",
       "  368  0.070858  0.069236  0.067000  0.444579  0.092694         0.461153\n",
       "  369  0.068858  0.068288  0.068693  0.217365  0.089302         0.493483\n",
       "  370  0.068299  0.066819  0.064766  0.404406  0.090133         0.456332\n",
       "  371  0.064807  0.063736  0.062664  0.505929  0.086112         0.489199,\n",
       "  0.062014623989257445),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  368  0.070858  0.069236  0.067000  0.444579  0.092694         0.461153\n",
       "  369  0.068858  0.068288  0.068693  0.217365  0.089302         0.493483\n",
       "  370  0.068299  0.066819  0.064766  0.404406  0.090133         0.456332\n",
       "  371  0.064807  0.063736  0.062664  0.505929  0.086112         0.489199\n",
       "  372  0.065462  0.064615  0.063013  0.289654  0.086384         0.468644,\n",
       "  0.06293491634279719),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  369  0.068858  0.068288  0.068693  0.217365  0.089302         0.493483\n",
       "  370  0.068299  0.066819  0.064766  0.404406  0.090133         0.456332\n",
       "  371  0.064807  0.063736  0.062664  0.505929  0.086112         0.489199\n",
       "  372  0.065462  0.064615  0.063013  0.289654  0.086384         0.468644\n",
       "  373  0.063236  0.062269  0.062293  0.370191  0.083970         0.494002,\n",
       "  0.05888225638699694),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  370  0.068299  0.066819  0.064766  0.404406  0.090133         0.456332\n",
       "  371  0.064807  0.063736  0.062664  0.505929  0.086112         0.489199\n",
       "  372  0.065462  0.064615  0.063013  0.289654  0.086384         0.468644\n",
       "  373  0.063236  0.062269  0.062293  0.370191  0.083970         0.494002\n",
       "  374  0.059551  0.058975  0.059568  0.479507  0.084869         0.456816,\n",
       "  0.05831186199407831),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  371  0.064807  0.063736  0.062664  0.505929  0.086112         0.489199\n",
       "  372  0.065462  0.064615  0.063013  0.289654  0.086384         0.468644\n",
       "  373  0.063236  0.062269  0.062293  0.370191  0.083970         0.494002\n",
       "  374  0.059551  0.058975  0.059568  0.479507  0.084869         0.456816\n",
       "  375  0.059236  0.059233  0.059692  0.356338  0.080911         0.482855,\n",
       "  0.058086583247713555),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  372  0.065462  0.064615  0.063013  0.289654  0.086384         0.468644\n",
       "  373  0.063236  0.062269  0.062293  0.370191  0.083970         0.494002\n",
       "  374  0.059551  0.058975  0.059568  0.479507  0.084869         0.456816\n",
       "  375  0.059236  0.059233  0.059692  0.356338  0.080911         0.482855\n",
       "  376  0.058259  0.057885  0.058585  0.314818  0.080354         0.485436,\n",
       "  0.06385761428691406),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  373  0.063236  0.062269  0.062293  0.370191  0.083970         0.494002\n",
       "  374  0.059551  0.058975  0.059568  0.479507  0.084869         0.456816\n",
       "  375  0.059236  0.059233  0.059692  0.356338  0.080911         0.482855\n",
       "  376  0.058259  0.057885  0.058585  0.314818  0.080354         0.485436\n",
       "  377  0.059318  0.062861  0.060198  0.518368  0.080134         0.530274,\n",
       "  0.06440643534353718),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  374  0.059551  0.058975  0.059568  0.479507  0.084869         0.456816\n",
       "  375  0.059236  0.059233  0.059692  0.356338  0.080911         0.482855\n",
       "  376  0.058259  0.057885  0.058585  0.314818  0.080354         0.485436\n",
       "  377  0.059318  0.062861  0.060198  0.518368  0.080134         0.530274\n",
       "  378  0.065139  0.064080  0.064832  0.348219  0.085770         0.491224,\n",
       "  0.06388877149606874),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  375  0.059236  0.059233  0.059692  0.356338  0.080911         0.482855\n",
       "  376  0.058259  0.057885  0.058585  0.314818  0.080354         0.485436\n",
       "  377  0.059318  0.062861  0.060198  0.518368  0.080134         0.530274\n",
       "  378  0.065139  0.064080  0.064832  0.348219  0.085770         0.491224\n",
       "  379  0.064316  0.064411  0.065193  0.211177  0.086306         0.483250,\n",
       "  0.06362513801354258),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  376  0.058259  0.057885  0.058585  0.314818  0.080354         0.485436\n",
       "  377  0.059318  0.062861  0.060198  0.518368  0.080134         0.530274\n",
       "  378  0.065139  0.064080  0.064832  0.348219  0.085770         0.491224\n",
       "  379  0.064316  0.064411  0.065193  0.211177  0.086306         0.483250\n",
       "  380  0.064487  0.063324  0.064890  0.157427  0.085801         0.485149,\n",
       "  0.06664007430619111),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  377  0.059318  0.062861  0.060198  0.518368  0.080134         0.530274\n",
       "  378  0.065139  0.064080  0.064832  0.348219  0.085770         0.491224\n",
       "  379  0.064316  0.064411  0.065193  0.211177  0.086306         0.483250\n",
       "  380  0.064487  0.063324  0.064890  0.157427  0.085801         0.485149\n",
       "  381  0.064117  0.066236  0.065260  0.305201  0.085543         0.509665,\n",
       "  0.06647949632398818),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  378  0.065139  0.064080  0.064832  0.348219  0.085770         0.491224\n",
       "  379  0.064316  0.064411  0.065193  0.211177  0.086306         0.483250\n",
       "  380  0.064487  0.063324  0.064890  0.157427  0.085801         0.485149\n",
       "  381  0.064117  0.066236  0.065260  0.305201  0.085543         0.509665\n",
       "  382  0.066930  0.065959  0.067222  0.157624  0.088488         0.485920,\n",
       "  0.06959030021231549),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  379  0.064316  0.064411  0.065193  0.211177  0.086306         0.483250\n",
       "  380  0.064487  0.063324  0.064890  0.157427  0.085801         0.485149\n",
       "  381  0.064117  0.066236  0.065260  0.305201  0.085543         0.509665\n",
       "  382  0.066930  0.065959  0.067222  0.157624  0.088488         0.485920\n",
       "  383  0.067885  0.068715  0.068867  0.248299  0.088331         0.510382,\n",
       "  0.06293731231101199),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  380  0.064487  0.063324  0.064890  0.157427  0.085801         0.485149\n",
       "  381  0.064117  0.066236  0.065260  0.305201  0.085543         0.509665\n",
       "  382  0.066930  0.065959  0.067222  0.157624  0.088488         0.485920\n",
       "  383  0.067885  0.068715  0.068867  0.248299  0.088331         0.510382\n",
       "  384  0.070756  0.070099  0.058750  0.959159  0.091369         0.437372,\n",
       "  0.06345977771727236),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  381  0.064117  0.066236  0.065260  0.305201  0.085543         0.509665\n",
       "  382  0.066930  0.065959  0.067222  0.157624  0.088488         0.485920\n",
       "  383  0.067885  0.068715  0.068867  0.248299  0.088331         0.510382\n",
       "  384  0.070756  0.070099  0.058750  0.959159  0.091369         0.437372\n",
       "  385  0.064827  0.063442  0.064270  0.293273  0.084872         0.491027,\n",
       "  0.06625182160940861),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  382  0.066930  0.065959  0.067222  0.157624  0.088488         0.485920\n",
       "  383  0.067885  0.068715  0.068867  0.248299  0.088331         0.510382\n",
       "  384  0.070756  0.070099  0.058750  0.959159  0.091369         0.437372\n",
       "  385  0.064827  0.063442  0.064270  0.293273  0.084872         0.491027\n",
       "  386  0.063797  0.066203  0.064488  0.445758  0.085382         0.507999,\n",
       "  0.06666643957891619),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  383  0.067885  0.068715  0.068867  0.248299  0.088331         0.510382\n",
       "  384  0.070756  0.070099  0.058750  0.959159  0.091369         0.437372\n",
       "  385  0.064827  0.063442  0.064270  0.293273  0.084872         0.491027\n",
       "  386  0.063797  0.066203  0.064488  0.445758  0.085382         0.507999\n",
       "  387  0.068615  0.067658  0.067772  0.290860  0.088108         0.490221,\n",
       "  0.06425065892012616),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  384  0.070756  0.070099  0.058750  0.959159  0.091369         0.437372\n",
       "  385  0.064827  0.063442  0.064270  0.293273  0.084872         0.491027\n",
       "  386  0.063797  0.066203  0.064488  0.445758  0.085382         0.507999\n",
       "  387  0.068615  0.067658  0.067772  0.290860  0.088108         0.490221\n",
       "  388  0.067799  0.066748  0.065684  0.217347  0.088513         0.469056,\n",
       "  0.06337588996266753),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  385  0.064827  0.063442  0.064270  0.293273  0.084872         0.491027\n",
       "  386  0.063797  0.066203  0.064488  0.445758  0.085382         0.507999\n",
       "  387  0.068615  0.067658  0.067772  0.290860  0.088108         0.490221\n",
       "  388  0.067799  0.066748  0.065684  0.217347  0.088513         0.469056\n",
       "  389  0.063975  0.063205  0.063507  0.252351  0.086154         0.480579,\n",
       "  0.06084267799426553),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  386  0.063797  0.066203  0.064488  0.445758  0.085382         0.507999\n",
       "  387  0.068615  0.067658  0.067772  0.290860  0.088108         0.490221\n",
       "  388  0.067799  0.066748  0.065684  0.217347  0.088513         0.469056\n",
       "  389  0.063975  0.063205  0.063507  0.252351  0.086154         0.480579\n",
       "  390  0.061306  0.061210  0.061664  0.285596  0.085300         0.468178,\n",
       "  0.0582351717664802),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  387  0.068615  0.067658  0.067772  0.290860  0.088108         0.490221\n",
       "  388  0.067799  0.066748  0.065684  0.217347  0.088513         0.469056\n",
       "  389  0.063975  0.063205  0.063507  0.252351  0.086154         0.480579\n",
       "  390  0.061306  0.061210  0.061664  0.285596  0.085300         0.468178\n",
       "  391  0.061657  0.060854  0.059474  0.338252  0.082826         0.467622,\n",
       "  0.05691703322093644),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  388  0.067799  0.066748  0.065684  0.217347  0.088513         0.469056\n",
       "  389  0.063975  0.063205  0.063507  0.252351  0.086154         0.480579\n",
       "  390  0.061306  0.061210  0.061664  0.285596  0.085300         0.468178\n",
       "  391  0.061657  0.060854  0.059474  0.338252  0.082826         0.467622\n",
       "  392  0.058796  0.057529  0.057064  0.469967  0.080279         0.477264,\n",
       "  0.05681158175239845),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  389  0.063975  0.063205  0.063507  0.252351  0.086154         0.480579\n",
       "  390  0.061306  0.061210  0.061664  0.285596  0.085300         0.468178\n",
       "  391  0.061657  0.060854  0.059474  0.338252  0.082826         0.467622\n",
       "  392  0.058796  0.057529  0.057064  0.469967  0.080279         0.477264\n",
       "  393  0.058177  0.057909  0.058008  0.331733  0.078992         0.486332,\n",
       "  0.0601908014373192),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  390  0.061306  0.061210  0.061664  0.285596  0.085300         0.468178\n",
       "  391  0.061657  0.060854  0.059474  0.338252  0.082826         0.467622\n",
       "  392  0.058796  0.057529  0.057064  0.469967  0.080279         0.477264\n",
       "  393  0.058177  0.057909  0.058008  0.331733  0.078992         0.486332\n",
       "  394  0.057265  0.059394  0.057323  0.448687  0.078889         0.512389,\n",
       "  0.05859466322232283),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  391  0.061657  0.060854  0.059474  0.338252  0.082826         0.467622\n",
       "  392  0.058796  0.057529  0.057064  0.469967  0.080279         0.477264\n",
       "  393  0.058177  0.057909  0.058008  0.331733  0.078992         0.486332\n",
       "  394  0.057265  0.059394  0.057323  0.448687  0.078889         0.512389\n",
       "  395  0.060545  0.059378  0.059239  0.310484  0.082189         0.475185,\n",
       "  0.057221407785476436),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  392  0.058796  0.057529  0.057064  0.469967  0.080279         0.477264\n",
       "  393  0.058177  0.057909  0.058008  0.331733  0.078992         0.486332\n",
       "  394  0.057265  0.059394  0.057323  0.448687  0.078889         0.512389\n",
       "  395  0.060545  0.059378  0.059239  0.310484  0.082189         0.475185\n",
       "  396  0.060025  0.058537  0.058602  0.243544  0.080631         0.476852,\n",
       "  0.05746585427755957),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  393  0.058177  0.057909  0.058008  0.331733  0.078992         0.486332\n",
       "  394  0.057265  0.059394  0.057323  0.448687  0.078889         0.512389\n",
       "  395  0.060545  0.059378  0.059239  0.310484  0.082189         0.475185\n",
       "  396  0.060025  0.058537  0.058602  0.243544  0.080631         0.476852\n",
       "  397  0.057665  0.056885  0.057883  0.207910  0.079289         0.488948,\n",
       "  0.059479006430278314),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  394  0.057265  0.059394  0.057323  0.448687  0.078889         0.512389\n",
       "  395  0.060545  0.059378  0.059239  0.310484  0.082189         0.475185\n",
       "  396  0.060025  0.058537  0.058602  0.243544  0.080631         0.476852\n",
       "  397  0.057665  0.056885  0.057883  0.207910  0.079289         0.488948\n",
       "  398  0.058517  0.058854  0.059484  0.253052  0.079528         0.502174,\n",
       "  0.05749701148671424),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  395  0.060545  0.059378  0.059239  0.310484  0.082189         0.475185\n",
       "  396  0.060025  0.058537  0.058602  0.243544  0.080631         0.476852\n",
       "  397  0.057665  0.056885  0.057883  0.207910  0.079289         0.488948\n",
       "  398  0.058517  0.058854  0.059484  0.253052  0.079528         0.502174\n",
       "  399  0.060362  0.059226  0.058888  0.239029  0.081494         0.472300,\n",
       "  0.05756890977788279),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  396  0.060025  0.058537  0.058602  0.243544  0.080631         0.476852\n",
       "  397  0.057665  0.056885  0.057883  0.207910  0.079289         0.488948\n",
       "  398  0.058517  0.058854  0.059484  0.253052  0.079528         0.502174\n",
       "  399  0.060362  0.059226  0.058888  0.239029  0.081494         0.472300\n",
       "  400  0.057893  0.057435  0.058505  0.184664  0.079559         0.487658,\n",
       "  0.05796914231573927),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  397  0.057665  0.056885  0.057883  0.207910  0.079289         0.488948\n",
       "  398  0.058517  0.058854  0.059484  0.253052  0.079528         0.502174\n",
       "  399  0.060362  0.059226  0.058888  0.239029  0.081494         0.472300\n",
       "  400  0.057893  0.057435  0.058505  0.184664  0.079559         0.487658\n",
       "  401  0.058519  0.057826  0.059159  0.180741  0.079629         0.490113,\n",
       "  0.0636994322729259),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  398  0.058517  0.058854  0.059484  0.253052  0.079528         0.502174\n",
       "  399  0.060362  0.059226  0.058888  0.239029  0.081494         0.472300\n",
       "  400  0.057893  0.057435  0.058505  0.184664  0.079559         0.487658\n",
       "  401  0.058519  0.057826  0.059159  0.180741  0.079629         0.490113\n",
       "  402  0.064841  0.063594  0.064563  0.369147  0.080020         0.529970,\n",
       "  0.06463171408990195),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  399  0.060362  0.059226  0.058888  0.239029  0.081494         0.472300\n",
       "  400  0.057893  0.057435  0.058505  0.184664  0.079559         0.487658\n",
       "  401  0.058519  0.057826  0.059159  0.180741  0.079629         0.490113\n",
       "  402  0.064841  0.063594  0.064563  0.369147  0.080020         0.529970\n",
       "  403  0.064473  0.063978  0.065166  0.170744  0.085616         0.494092,\n",
       "  0.06643156733732992),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  400  0.057893  0.057435  0.058505  0.184664  0.079559         0.487658\n",
       "  401  0.058519  0.057826  0.059159  0.180741  0.079629         0.490113\n",
       "  402  0.064841  0.063594  0.064563  0.369147  0.080020         0.529970\n",
       "  403  0.064473  0.063978  0.065166  0.170744  0.085616         0.494092\n",
       "  404  0.065585  0.065435  0.066660  0.217496  0.086526         0.500579,\n",
       "  0.07069273426199137),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  401  0.058519  0.057826  0.059159  0.180741  0.079629         0.490113\n",
       "  402  0.064841  0.063594  0.064563  0.369147  0.080020         0.529970\n",
       "  403  0.064473  0.063978  0.065166  0.170744  0.085616         0.494092\n",
       "  404  0.065585  0.065435  0.066660  0.217496  0.086526         0.500579\n",
       "  405  0.067565  0.070445  0.068681  0.462717  0.088284         0.518984,\n",
       "  0.06912535728793488),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  402  0.064841  0.063594  0.064563  0.369147  0.080020         0.529970\n",
       "  403  0.064473  0.063978  0.065166  0.170744  0.085616         0.494092\n",
       "  404  0.065585  0.065435  0.066660  0.217496  0.086526         0.500579\n",
       "  405  0.067565  0.070445  0.068681  0.462717  0.088284         0.518984\n",
       "  406  0.069791  0.069141  0.069674  0.357742  0.092445         0.475400,\n",
       "  0.06949922455306629),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  403  0.064473  0.063978  0.065166  0.170744  0.085616         0.494092\n",
       "  404  0.065585  0.065435  0.066660  0.217496  0.086526         0.500579\n",
       "  405  0.067565  0.070445  0.068681  0.462717  0.088284         0.518984\n",
       "  406  0.069791  0.069141  0.069674  0.357742  0.092445         0.475400\n",
       "  407  0.069683  0.068755  0.069775  0.888227  0.090915         0.489916,\n",
       "  0.06942732626189775),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  404  0.065585  0.065435  0.066660  0.217496  0.086526         0.500579\n",
       "  405  0.067565  0.070445  0.068681  0.462717  0.088284         0.518984\n",
       "  406  0.069791  0.069141  0.069674  0.357742  0.092445         0.475400\n",
       "  407  0.069683  0.068755  0.069775  0.888227  0.090915         0.489916\n",
       "  408  0.069873  0.068897  0.070720  0.197475  0.091280         0.486583,\n",
       "  0.07293595709750449),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  405  0.067565  0.070445  0.068681  0.462717  0.088284         0.518984\n",
       "  406  0.069791  0.069141  0.069674  0.357742  0.092445         0.475400\n",
       "  407  0.069683  0.068755  0.069775  0.888227  0.090915         0.489916\n",
       "  408  0.069873  0.068897  0.070720  0.197475  0.091280         0.486583\n",
       "  409  0.069962  0.071843  0.070771  0.382163  0.091210         0.513357,\n",
       "  0.07381072605496308),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  406  0.069791  0.069141  0.069674  0.357742  0.092445         0.475400\n",
       "  407  0.069683  0.068755  0.069775  0.888227  0.090915         0.489916\n",
       "  408  0.069873  0.068897  0.070720  0.197475  0.091280         0.486583\n",
       "  409  0.069962  0.071843  0.070771  0.382163  0.091210         0.513357\n",
       "  410  0.074366  0.074135  0.074327  0.326092  0.094636         0.493662,\n",
       "  0.0745752419850918),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  407  0.069683  0.068755  0.069775  0.888227  0.090915         0.489916\n",
       "  408  0.069873  0.068897  0.070720  0.197475  0.091280         0.486583\n",
       "  409  0.069962  0.071843  0.070771  0.382163  0.091210         0.513357\n",
       "  410  0.074366  0.074135  0.074327  0.326092  0.094636         0.493662\n",
       "  411  0.073955  0.073973  0.074959  0.209048  0.095490         0.492837,\n",
       "  0.07337213878094523),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  408  0.069873  0.068897  0.070720  0.197475  0.091280         0.486583\n",
       "  409  0.069962  0.071843  0.070771  0.382163  0.091210         0.513357\n",
       "  410  0.074366  0.074135  0.074327  0.326092  0.094636         0.493662\n",
       "  411  0.073955  0.073973  0.074959  0.209048  0.095490         0.492837\n",
       "  412  0.075380  0.073727  0.074617  0.170797  0.096237         0.478124,\n",
       "  0.07582147261710473),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  409  0.069962  0.071843  0.070771  0.382163  0.091210         0.513357\n",
       "  410  0.074366  0.074135  0.074327  0.326092  0.094636         0.493662\n",
       "  411  0.073955  0.073973  0.074959  0.209048  0.095490         0.492837\n",
       "  412  0.075380  0.073727  0.074617  0.170797  0.096237         0.478124\n",
       "  413  0.074097  0.074751  0.074915  0.226600  0.095062         0.505436,\n",
       "  0.07421095859281958),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  410  0.074366  0.074135  0.074327  0.326092  0.094636         0.493662\n",
       "  411  0.073955  0.073973  0.074959  0.209048  0.095490         0.492837\n",
       "  412  0.075380  0.073727  0.074617  0.170797  0.096237         0.478124\n",
       "  413  0.074097  0.074751  0.074915  0.226600  0.095062         0.505436\n",
       "  414  0.076345  0.075208  0.075150  0.270068  0.097454         0.475078,\n",
       "  0.07404079673775746),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  411  0.073955  0.073973  0.074959  0.209048  0.095490         0.492837\n",
       "  412  0.075380  0.073727  0.074617  0.170797  0.096237         0.478124\n",
       "  413  0.074097  0.074751  0.074915  0.226600  0.095062         0.505436\n",
       "  414  0.076345  0.075208  0.075150  0.270068  0.097454         0.475078\n",
       "  415  0.074559  0.073367  0.074901  0.146258  0.095881         0.485848,\n",
       "  0.07241350131360641),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  412  0.075380  0.073727  0.074617  0.170797  0.096237         0.478124\n",
       "  413  0.074097  0.074751  0.074915  0.226600  0.095062         0.505436\n",
       "  414  0.076345  0.075208  0.075150  0.270068  0.097454         0.475078\n",
       "  415  0.074559  0.073367  0.074901  0.146258  0.095881         0.485848\n",
       "  416  0.074470  0.072995  0.073765  0.159293  0.095715         0.474952,\n",
       "  0.07353032079493338),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  413  0.074097  0.074751  0.074915  0.226600  0.095062         0.505436\n",
       "  414  0.076345  0.075208  0.075150  0.270068  0.097454         0.475078\n",
       "  415  0.074559  0.073367  0.074901  0.146258  0.095881         0.485848\n",
       "  416  0.074470  0.072995  0.073765  0.159293  0.095715         0.474952\n",
       "  417  0.073223  0.073644  0.073847  0.200803  0.094126         0.495472,\n",
       "  0.0729119974153565),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  414  0.076345  0.075208  0.075150  0.270068  0.097454         0.475078\n",
       "  415  0.074559  0.073367  0.074901  0.146258  0.095881         0.485848\n",
       "  416  0.074470  0.072995  0.073765  0.159293  0.095715         0.474952\n",
       "  417  0.073223  0.073644  0.073847  0.200803  0.094126         0.495472\n",
       "  418  0.074301  0.073011  0.074007  0.197838  0.095217         0.482497,\n",
       "  0.07441226803467406),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  415  0.074559  0.073367  0.074901  0.146258  0.095881         0.485848\n",
       "  416  0.074470  0.072995  0.073765  0.159293  0.095715         0.474952\n",
       "  417  0.073223  0.073644  0.073847  0.200803  0.094126         0.495472\n",
       "  418  0.074301  0.073011  0.074007  0.197838  0.095217         0.482497\n",
       "  419  0.075418  0.073914  0.075274  0.164858  0.094613         0.498339,\n",
       "  0.07549074240220197),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  416  0.074470  0.072995  0.073765  0.159293  0.095715         0.474952\n",
       "  417  0.073223  0.073644  0.073847  0.200803  0.094126         0.495472\n",
       "  418  0.074301  0.073011  0.074007  0.197838  0.095217         0.482497\n",
       "  419  0.075418  0.073914  0.075274  0.164858  0.094613         0.498339\n",
       "  420  0.075043  0.074473  0.075630  0.298388  0.096078         0.495185,\n",
       "  0.08078724664416266),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  417  0.073223  0.073644  0.073847  0.200803  0.094126         0.495472\n",
       "  418  0.074301  0.073011  0.074007  0.197838  0.095217         0.482497\n",
       "  419  0.075418  0.073914  0.075274  0.164858  0.094613         0.498339\n",
       "  420  0.075043  0.074473  0.075630  0.298388  0.096078         0.495185\n",
       "  421  0.084161  0.082701  0.082405  0.548427  0.097131         0.526726,\n",
       "  0.08160209715152673),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  418  0.074301  0.073011  0.074007  0.197838  0.095217         0.482497\n",
       "  419  0.075418  0.073914  0.075274  0.164858  0.094613         0.498339\n",
       "  420  0.075043  0.074473  0.075630  0.298388  0.096078         0.495185\n",
       "  421  0.084161  0.082701  0.082405  0.548427  0.097131         0.526726\n",
       "  422  0.081875  0.081275  0.082489  0.214835  0.102304         0.493214,\n",
       "  0.07840501916274209),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  419  0.075418  0.073914  0.075274  0.164858  0.094613         0.498339\n",
       "  420  0.075043  0.074473  0.075630  0.298388  0.096078         0.495185\n",
       "  421  0.084161  0.082701  0.082405  0.548427  0.097131         0.526726\n",
       "  422  0.081875  0.081275  0.082489  0.214835  0.102304         0.493214\n",
       "  423  0.081930  0.080407  0.079941  0.232654  0.103099         0.463214,\n",
       "  0.07811982196628278),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  420  0.075043  0.074473  0.075630  0.298388  0.096078         0.495185\n",
       "  421  0.084161  0.082701  0.082405  0.548427  0.097131         0.526726\n",
       "  422  0.081875  0.081275  0.082489  0.214835  0.102304         0.493214\n",
       "  423  0.081930  0.080407  0.079941  0.232654  0.103099         0.463214\n",
       "  424  0.079069  0.077801  0.079343  0.176925  0.099977         0.484988,\n",
       "  0.07669624157456333),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  421  0.084161  0.082701  0.082405  0.548427  0.097131         0.526726\n",
       "  422  0.081875  0.081275  0.082489  0.214835  0.102304         0.493214\n",
       "  423  0.081930  0.080407  0.079941  0.232654  0.103099         0.463214\n",
       "  424  0.079069  0.077801  0.079343  0.176925  0.099977         0.484988\n",
       "  425  0.077907  0.077059  0.078340  0.202710  0.099699         0.476475,\n",
       "  0.07619534950459844),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  422  0.081875  0.081275  0.082489  0.214835  0.102304         0.493214\n",
       "  423  0.081930  0.080407  0.079941  0.232654  0.103099         0.463214\n",
       "  424  0.079069  0.077801  0.079343  0.176925  0.099977         0.484988\n",
       "  425  0.077907  0.077059  0.078340  0.202710  0.099699         0.476475\n",
       "  426  0.076876  0.076779  0.077592  0.180415  0.098308         0.483375,\n",
       "  0.07163220398361181),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  423  0.081930  0.080407  0.079941  0.232654  0.103099         0.463214\n",
       "  424  0.079069  0.077801  0.079343  0.176925  0.099977         0.484988\n",
       "  425  0.077907  0.077059  0.078340  0.202710  0.099699         0.476475\n",
       "  426  0.076876  0.076779  0.077592  0.180415  0.098308         0.483375\n",
       "  427  0.076838  0.075289  0.073292  0.251923  0.097819         0.452999,\n",
       "  0.07064959721176271),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  424  0.079069  0.077801  0.079343  0.176925  0.099977         0.484988\n",
       "  425  0.077907  0.077059  0.078340  0.202710  0.099699         0.476475\n",
       "  426  0.076876  0.076779  0.077592  0.180415  0.098308         0.483375\n",
       "  427  0.076838  0.075289  0.073292  0.251923  0.097819         0.452999\n",
       "  428  0.072653  0.071272  0.070834  0.318645  0.093363         0.479773,\n",
       "  0.07049861272478126),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  425  0.077907  0.077059  0.078340  0.202710  0.099699         0.476475\n",
       "  426  0.076876  0.076779  0.077592  0.180415  0.098308         0.483375\n",
       "  427  0.076838  0.075289  0.073292  0.251923  0.097819         0.452999\n",
       "  428  0.072653  0.071272  0.070834  0.318645  0.093363         0.479773\n",
       "  429  0.071515  0.071037  0.071435  0.196247  0.092403         0.485992,\n",
       "  0.07063761737068873),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  426  0.076876  0.076779  0.077592  0.180415  0.098308         0.483375\n",
       "  427  0.076838  0.075289  0.073292  0.251923  0.097819         0.452999\n",
       "  428  0.072653  0.071272  0.070834  0.318645  0.093363         0.479773\n",
       "  429  0.071515  0.071037  0.071435  0.196247  0.092403         0.485992\n",
       "  430  0.071197  0.070414  0.071757  0.113056  0.092256         0.488160,\n",
       "  0.07052976993393595),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  427  0.076838  0.075289  0.073292  0.251923  0.097819         0.452999\n",
       "  428  0.072653  0.071272  0.070834  0.318645  0.093363         0.479773\n",
       "  429  0.071515  0.071037  0.071435  0.196247  0.092403         0.485992\n",
       "  430  0.071197  0.070414  0.071757  0.113056  0.092256         0.488160\n",
       "  431  0.071815  0.070843  0.071653  0.147425  0.092392         0.486314,\n",
       "  0.07064480527533311),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  428  0.072653  0.071272  0.070834  0.318645  0.093363         0.479773\n",
       "  429  0.071515  0.071037  0.071435  0.196247  0.092403         0.485992\n",
       "  430  0.071197  0.070414  0.071757  0.113056  0.092256         0.488160\n",
       "  431  0.071815  0.070843  0.071653  0.147425  0.092392         0.486314\n",
       "  432  0.071216  0.069999  0.071233  0.124406  0.092286         0.487981,\n",
       "  0.07400484759217318),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  429  0.071515  0.071037  0.071435  0.196247  0.092403         0.485992\n",
       "  430  0.071197  0.070414  0.071757  0.113056  0.092256         0.488160\n",
       "  431  0.071815  0.070843  0.071653  0.147425  0.092392         0.486314\n",
       "  432  0.071216  0.069999  0.071233  0.124406  0.092286         0.487981\n",
       "  433  0.071358  0.072815  0.072130  0.222234  0.092399         0.512246,\n",
       "  0.07261001881903131),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  430  0.071197  0.070414  0.071757  0.113056  0.092256         0.488160\n",
       "  431  0.071815  0.070843  0.071653  0.147425  0.092392         0.486314\n",
       "  432  0.071216  0.069999  0.071233  0.124406  0.092286         0.487981\n",
       "  433  0.071358  0.072815  0.072130  0.222234  0.092399         0.512246\n",
       "  434  0.074393  0.073457  0.074094  0.150397  0.095680         0.476690,\n",
       "  0.06878024164138102),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  431  0.071815  0.070843  0.071653  0.147425  0.092392         0.486314\n",
       "  432  0.071216  0.069999  0.071233  0.124406  0.092286         0.487981\n",
       "  433  0.071358  0.072815  0.072130  0.222234  0.092399         0.512246\n",
       "  434  0.074393  0.073457  0.074094  0.150397  0.095680         0.476690\n",
       "  435  0.073216  0.071798  0.070005  0.216207  0.094318         0.458483,\n",
       "  0.06569581264814107),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  432  0.071216  0.069999  0.071233  0.124406  0.092286         0.487981\n",
       "  433  0.071358  0.072815  0.072130  0.222234  0.092399         0.512246\n",
       "  434  0.074393  0.073457  0.074094  0.150397  0.095680         0.476690\n",
       "  435  0.073216  0.071798  0.070005  0.216207  0.094318         0.458483\n",
       "  436  0.068432  0.067096  0.067237  0.245268  0.090578         0.464056,\n",
       "  0.06619190315931406),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  433  0.071358  0.072815  0.072130  0.222234  0.092399         0.512246\n",
       "  434  0.074393  0.073457  0.074094  0.150397  0.095680         0.476690\n",
       "  435  0.073216  0.071798  0.070005  0.216207  0.094318         0.458483\n",
       "  436  0.068432  0.067096  0.067237  0.245268  0.090578         0.464056\n",
       "  437  0.066600  0.065978  0.065953  0.208844  0.087565         0.490830,\n",
       "  0.06501757081846968),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  434  0.074393  0.073457  0.074094  0.150397  0.095680         0.476690\n",
       "  435  0.073216  0.071798  0.070005  0.216207  0.094318         0.458483\n",
       "  436  0.068432  0.067096  0.067237  0.245268  0.090578         0.464056\n",
       "  437  0.066600  0.065978  0.065953  0.208844  0.087565         0.490830\n",
       "  438  0.066615  0.065198  0.066350  0.157789  0.088050         0.478339,\n",
       "  0.06578448271681318),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  435  0.073216  0.071798  0.070005  0.216207  0.094318         0.458483\n",
       "  436  0.068432  0.067096  0.067237  0.245268  0.090578         0.464056\n",
       "  437  0.066600  0.065978  0.065953  0.208844  0.087565         0.490830\n",
       "  438  0.066615  0.065198  0.066350  0.157789  0.088050         0.478339\n",
       "  439  0.065320  0.065904  0.066430  0.258797  0.086903         0.492855,\n",
       "  0.06494327655908635),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  436  0.068432  0.067096  0.067237  0.245268  0.090578         0.464056\n",
       "  437  0.066600  0.065978  0.065953  0.208844  0.087565         0.490830\n",
       "  438  0.066615  0.065198  0.066350  0.157789  0.088050         0.478339\n",
       "  439  0.065320  0.065904  0.066430  0.258797  0.086903         0.492855\n",
       "  440  0.067286  0.066371  0.066549  0.141831  0.087652         0.480830,\n",
       "  0.06471080028571488),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  437  0.066600  0.065978  0.065953  0.208844  0.087565         0.490830\n",
       "  438  0.066615  0.065198  0.066350  0.157789  0.088050         0.478339\n",
       "  439  0.065320  0.065904  0.066430  0.258797  0.086903         0.492855\n",
       "  440  0.067286  0.066371  0.066549  0.141831  0.087652         0.480830\n",
       "  441  0.065963  0.064639  0.064093  0.277146  0.086831         0.485382,\n",
       "  0.06493369268622716),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  438  0.066615  0.065198  0.066350  0.157789  0.088050         0.478339\n",
       "  439  0.065320  0.065904  0.066430  0.258797  0.086903         0.492855\n",
       "  440  0.067286  0.066371  0.066549  0.141831  0.087652         0.480830\n",
       "  441  0.065963  0.064639  0.064093  0.277146  0.086831         0.485382\n",
       "  442  0.064656  0.064290  0.065227  0.209192  0.086604         0.488787,\n",
       "  0.06604331464054743),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  439  0.065320  0.065904  0.066430  0.258797  0.086903         0.492855\n",
       "  440  0.067286  0.066371  0.066549  0.141831  0.087652         0.480830\n",
       "  441  0.065963  0.064639  0.064093  0.277146  0.086831         0.485382\n",
       "  442  0.064656  0.064290  0.065227  0.209192  0.086604         0.488787\n",
       "  443  0.066186  0.066862  0.067239  0.218401  0.086821         0.495418,\n",
       "  0.06740219426810504),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  440  0.067286  0.066371  0.066549  0.141831  0.087652         0.480830\n",
       "  441  0.065963  0.064639  0.064093  0.277146  0.086831         0.485382\n",
       "  442  0.064656  0.064290  0.065227  0.209192  0.086604         0.488787\n",
       "  443  0.066186  0.066862  0.067239  0.218401  0.086821         0.495418\n",
       "  444  0.066942  0.066757  0.066767  0.232284  0.087905         0.497282,\n",
       "  0.0678192082058274),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  441  0.065963  0.064639  0.064093  0.277146  0.086831         0.485382\n",
       "  442  0.064656  0.064290  0.065227  0.209192  0.086604         0.488787\n",
       "  443  0.066186  0.066862  0.067239  0.218401  0.086821         0.495418\n",
       "  444  0.066942  0.066757  0.066767  0.232284  0.087905         0.497282\n",
       "  445  0.067358  0.066653  0.066937  0.199224  0.089232         0.490239,\n",
       "  0.06742616357261531),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  442  0.064656  0.064290  0.065227  0.209192  0.086604         0.488787\n",
       "  443  0.066186  0.066862  0.067239  0.218401  0.086821         0.495418\n",
       "  444  0.066942  0.066757  0.066767  0.232284  0.087905         0.497282\n",
       "  445  0.067358  0.066653  0.066937  0.199224  0.089232         0.490239\n",
       "  446  0.068675  0.067336  0.068806  0.089098  0.089639         0.484181,\n",
       "  0.0651709512736659),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  443  0.066186  0.066862  0.067239  0.218401  0.086821         0.495418\n",
       "  444  0.066942  0.066757  0.066767  0.232284  0.087905         0.497282\n",
       "  445  0.067358  0.066653  0.066937  0.199224  0.089232         0.490239\n",
       "  446  0.068675  0.067336  0.068806  0.089098  0.089639         0.484181\n",
       "  447  0.067166  0.066179  0.066728  0.104884  0.089255         0.470257,\n",
       "  0.06514219003272603),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  444  0.066942  0.066757  0.066767  0.232284  0.087905         0.497282\n",
       "  445  0.067358  0.066653  0.066937  0.199224  0.089232         0.490239\n",
       "  446  0.068675  0.067336  0.068806  0.089098  0.089639         0.484181\n",
       "  447  0.067166  0.066179  0.066728  0.104884  0.089255         0.470257\n",
       "  448  0.066088  0.065527  0.065597  0.194036  0.087053         0.486905,\n",
       "  0.067730528514793),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  445  0.067358  0.066653  0.066937  0.199224  0.089232         0.490239\n",
       "  446  0.068675  0.067336  0.068806  0.089098  0.089639         0.484181\n",
       "  447  0.067166  0.066179  0.066728  0.104884  0.089255         0.470257\n",
       "  448  0.066088  0.065527  0.065597  0.194036  0.087053         0.486905\n",
       "  449  0.066025  0.066651  0.066765  0.152606  0.087025         0.506475,\n",
       "  0.06696840855287907),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  446  0.068675  0.067336  0.068806  0.089098  0.089639         0.484181\n",
       "  447  0.067166  0.066179  0.066728  0.104884  0.089255         0.470257\n",
       "  448  0.066088  0.065527  0.065597  0.194036  0.087053         0.486905\n",
       "  449  0.066025  0.066651  0.066765  0.152606  0.087025         0.506475\n",
       "  450  0.069226  0.067620  0.068182  0.155243  0.089553         0.481422,\n",
       "  0.0657701069075244),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  447  0.067166  0.066179  0.066728  0.104884  0.089255         0.470257\n",
       "  448  0.066088  0.065527  0.065597  0.194036  0.087053         0.486905\n",
       "  449  0.066025  0.066651  0.066765  0.152606  0.087025         0.506475\n",
       "  450  0.069226  0.067620  0.068182  0.155243  0.089553         0.481422\n",
       "  451  0.066660  0.066286  0.067382  0.135105  0.088808         0.478160,\n",
       "  0.0694968285848515),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  448  0.066088  0.065527  0.065597  0.194036  0.087053         0.486905\n",
       "  449  0.066025  0.066651  0.066765  0.152606  0.087025         0.506475\n",
       "  450  0.069226  0.067620  0.068182  0.155243  0.089553         0.481422\n",
       "  451  0.066660  0.066286  0.067382  0.135105  0.088808         0.478160\n",
       "  452  0.066757  0.068312  0.067903  0.249457  0.087638         0.514988,\n",
       "  0.06866281033176903),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  449  0.066025  0.066651  0.066765  0.152606  0.087025         0.506475\n",
       "  450  0.069226  0.067620  0.068182  0.155243  0.089553         0.481422\n",
       "  451  0.066660  0.066286  0.067382  0.135105  0.088808         0.478160\n",
       "  452  0.066757  0.068312  0.067903  0.249457  0.087638         0.514988\n",
       "  453  0.070766  0.069464  0.070374  0.216657  0.091278         0.480884,\n",
       "  0.07029010575592008),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  450  0.069226  0.067620  0.068182  0.155243  0.089553         0.481422\n",
       "  451  0.066660  0.066286  0.067382  0.135105  0.088808         0.478160\n",
       "  452  0.066757  0.068312  0.067903  0.249457  0.087638         0.514988\n",
       "  453  0.070766  0.069464  0.070374  0.216657  0.091278         0.480884\n",
       "  454  0.069445  0.069324  0.068458  0.251882  0.090463         0.499289,\n",
       "  0.06867718614105783),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  451  0.066660  0.066286  0.067382  0.135105  0.088808         0.478160\n",
       "  452  0.066757  0.068312  0.067903  0.249457  0.087638         0.514988\n",
       "  453  0.070766  0.069464  0.070374  0.216657  0.091278         0.480884\n",
       "  454  0.069445  0.069324  0.068458  0.251882  0.090463         0.499289\n",
       "  455  0.070513  0.069599  0.070095  0.149675  0.092052         0.475060,\n",
       "  0.06747169659105877),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  452  0.066757  0.068312  0.067903  0.249457  0.087638         0.514988\n",
       "  453  0.070766  0.069464  0.070374  0.216657  0.091278         0.480884\n",
       "  454  0.069445  0.069324  0.068458  0.251882  0.090463         0.499289\n",
       "  455  0.070513  0.069599  0.070095  0.149675  0.092052         0.475060\n",
       "  456  0.069587  0.068428  0.069041  0.121985  0.090477         0.478106,\n",
       "  0.06866520629998384),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  453  0.070766  0.069464  0.070374  0.216657  0.091278         0.480884\n",
       "  454  0.069445  0.069324  0.068458  0.251882  0.090463         0.499289\n",
       "  455  0.070513  0.069599  0.070095  0.149675  0.092052         0.475060\n",
       "  456  0.069587  0.068428  0.069041  0.121985  0.090477         0.478106\n",
       "  457  0.067796  0.067601  0.068344  0.186199  0.089300         0.496045,\n",
       "  0.06815473035715976),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  454  0.069445  0.069324  0.068458  0.251882  0.090463         0.499289\n",
       "  455  0.070513  0.069599  0.070095  0.149675  0.092052         0.475060\n",
       "  456  0.069587  0.068428  0.069041  0.121985  0.090477         0.478106\n",
       "  457  0.067796  0.067601  0.068344  0.186199  0.089300         0.496045\n",
       "  458  0.069945  0.068573  0.068441  0.188945  0.090465         0.483303,\n",
       "  0.06973888873108214),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  455  0.070513  0.069599  0.070095  0.149675  0.092052         0.475060\n",
       "  456  0.069587  0.068428  0.069041  0.121985  0.090477         0.478106\n",
       "  457  0.067796  0.067601  0.068344  0.186199  0.089300         0.496045\n",
       "  458  0.069945  0.068573  0.068441  0.188945  0.090465         0.483303\n",
       "  459  0.069173  0.068845  0.069526  0.164955  0.089967         0.498966,\n",
       "  0.06966699043991362),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  456  0.069587  0.068428  0.069041  0.121985  0.090477         0.478106\n",
       "  457  0.067796  0.067601  0.068344  0.186199  0.089300         0.496045\n",
       "  458  0.069945  0.068573  0.068441  0.188945  0.090465         0.483303\n",
       "  459  0.069173  0.068845  0.069526  0.164955  0.089967         0.498966\n",
       "  460  0.069770  0.068532  0.070737  0.128930  0.091514         0.486583,\n",
       "  0.06905345899676635),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  457  0.067796  0.067601  0.068344  0.186199  0.089300         0.496045\n",
       "  458  0.069945  0.068573  0.068441  0.188945  0.090465         0.483303\n",
       "  459  0.069173  0.068845  0.069526  0.164955  0.089967         0.498966\n",
       "  460  0.069770  0.068532  0.070737  0.128930  0.091514         0.486583\n",
       "  461  0.070191  0.069449  0.070490  0.185567  0.091444         0.482533,\n",
       "  0.06882098272339489),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  458  0.069945  0.068573  0.068441  0.188945  0.090465         0.483303\n",
       "  459  0.069173  0.068845  0.069526  0.164955  0.089967         0.498966\n",
       "  460  0.069770  0.068532  0.070737  0.128930  0.091514         0.486583\n",
       "  461  0.070191  0.069449  0.070490  0.185567  0.091444         0.482533\n",
       "  462  0.069512  0.068755  0.070376  0.098168  0.090845         0.485382,\n",
       "  0.07240631340896202),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  459  0.069173  0.068845  0.069526  0.164955  0.089967         0.498966\n",
       "  460  0.069770  0.068532  0.070737  0.128930  0.091514         0.486583\n",
       "  461  0.070191  0.069449  0.070490  0.185567  0.091444         0.482533\n",
       "  462  0.069512  0.068755  0.070376  0.098168  0.090845         0.485382\n",
       "  463  0.070241  0.071630  0.071161  0.212597  0.090617         0.513930,\n",
       "  0.07188384800270164),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  460  0.069770  0.068532  0.070737  0.128930  0.091514         0.486583\n",
       "  461  0.070191  0.069449  0.070490  0.185567  0.091444         0.482533\n",
       "  462  0.069512  0.068755  0.070376  0.098168  0.090845         0.485382\n",
       "  463  0.070241  0.071630  0.071161  0.212597  0.090617         0.513930\n",
       "  464  0.072773  0.072104  0.070737  0.143701  0.094119         0.483214,\n",
       "  0.07306058593412314),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  461  0.070191  0.069449  0.070490  0.185567  0.091444         0.482533\n",
       "  462  0.069512  0.068755  0.070376  0.098168  0.090845         0.485382\n",
       "  463  0.070241  0.071630  0.071161  0.212597  0.090617         0.513930\n",
       "  464  0.072773  0.072104  0.070737  0.143701  0.094119         0.483214\n",
       "  465  0.073057  0.072992  0.073823  0.129178  0.093609         0.495920,\n",
       "  0.07290720547892691),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  462  0.069512  0.068755  0.070376  0.098168  0.090845         0.485382\n",
       "  463  0.070241  0.071630  0.071161  0.212597  0.090617         0.513930\n",
       "  464  0.072773  0.072104  0.070737  0.143701  0.094119         0.483214\n",
       "  465  0.073057  0.072992  0.073823  0.129178  0.093609         0.495920\n",
       "  466  0.074017  0.072649  0.074431  0.086281  0.094758         0.485974,\n",
       "  0.07245184642740547),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  463  0.070241  0.071630  0.071161  0.212597  0.090617         0.513930\n",
       "  464  0.072773  0.072104  0.070737  0.143701  0.094119         0.483214\n",
       "  465  0.073057  0.072992  0.073823  0.129178  0.093609         0.495920\n",
       "  466  0.074017  0.072649  0.074431  0.086281  0.094758         0.485974\n",
       "  467  0.073767  0.072578  0.073888  0.099802  0.094608         0.483716,\n",
       "  0.07336015893987125),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  464  0.072773  0.072104  0.070737  0.143701  0.094119         0.483214\n",
       "  465  0.073057  0.072992  0.073823  0.129178  0.093609         0.495920\n",
       "  466  0.074017  0.072649  0.074431  0.086281  0.094758         0.485974\n",
       "  467  0.073767  0.072578  0.073888  0.099802  0.094608         0.483716\n",
       "  468  0.073358  0.072217  0.073917  0.090156  0.094163         0.493913,\n",
       "  0.07616419229544379),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  465  0.073057  0.072992  0.073823  0.129178  0.093609         0.495920\n",
       "  466  0.074017  0.072649  0.074431  0.086281  0.094758         0.485974\n",
       "  467  0.073767  0.072578  0.073888  0.099802  0.094608         0.483716\n",
       "  468  0.073358  0.072217  0.073917  0.090156  0.094163         0.493913\n",
       "  469  0.074316  0.074995  0.075053  0.161854  0.095050         0.508088,\n",
       "  0.07652847568771601),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  466  0.074017  0.072649  0.074431  0.086281  0.094758         0.485974\n",
       "  467  0.073767  0.072578  0.073888  0.099802  0.094608         0.483716\n",
       "  468  0.073358  0.072217  0.073917  0.090156  0.094163         0.493913\n",
       "  469  0.074316  0.074995  0.075053  0.161854  0.095050         0.508088\n",
       "  470  0.076147  0.075263  0.076630  0.152120  0.097789         0.489845,\n",
       "  0.07745596556826247),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  467  0.073767  0.072578  0.073888  0.099802  0.094608         0.483716\n",
       "  468  0.073358  0.072217  0.073917  0.090156  0.094163         0.493913\n",
       "  469  0.074316  0.074995  0.075053  0.161854  0.095050         0.508088\n",
       "  470  0.076147  0.075263  0.076630  0.152120  0.097789         0.489845\n",
       "  471  0.077216  0.076315  0.078355  0.052362  0.098145         0.494056,\n",
       "  0.07703895163054009),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  468  0.073358  0.072217  0.073917  0.090156  0.094163         0.493913\n",
       "  469  0.074316  0.074995  0.075053  0.161854  0.095050         0.508088\n",
       "  470  0.076147  0.075263  0.076630  0.152120  0.097789         0.489845\n",
       "  471  0.077216  0.076315  0.078355  0.052362  0.098145         0.494056\n",
       "  472  0.077572  0.076085  0.077432  0.121171  0.099050         0.484002,\n",
       "  0.07745356960004766),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  469  0.074316  0.074995  0.075053  0.161854  0.095050         0.508088\n",
       "  470  0.076147  0.075263  0.076630  0.152120  0.097789         0.489845\n",
       "  471  0.077216  0.076315  0.078355  0.052362  0.098145         0.494056\n",
       "  472  0.077572  0.076085  0.077432  0.121171  0.099050         0.484002\n",
       "  473  0.078020  0.076696  0.078967  0.089587  0.098643         0.490221,\n",
       "  0.07679689148430942),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  470  0.076147  0.075263  0.076630  0.152120  0.097789         0.489845\n",
       "  471  0.077216  0.076315  0.078355  0.052362  0.098145         0.494056\n",
       "  472  0.077572  0.076085  0.077432  0.121171  0.099050         0.484002\n",
       "  473  0.078020  0.076696  0.078967  0.089587  0.098643         0.490221\n",
       "  474  0.078662  0.077291  0.077490  0.146863  0.099048         0.482210,\n",
       "  0.07625765430054547),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  471  0.077216  0.076315  0.078355  0.052362  0.098145         0.494056\n",
       "  472  0.077572  0.076085  0.077432  0.121171  0.099050         0.484002\n",
       "  473  0.078020  0.076696  0.078967  0.089587  0.098643         0.490221\n",
       "  474  0.078662  0.077291  0.077490  0.146863  0.099048         0.482210\n",
       "  475  0.078034  0.076661  0.077611  0.107812  0.098407         0.483088,\n",
       "  0.07778190384673563),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  472  0.077572  0.076085  0.077432  0.121171  0.099050         0.484002\n",
       "  473  0.078020  0.076696  0.078967  0.089587  0.098643         0.490221\n",
       "  474  0.078662  0.077291  0.077490  0.146863  0.099048         0.482210\n",
       "  475  0.078034  0.076661  0.077611  0.107812  0.098407         0.483088\n",
       "  476  0.076848  0.076618  0.076800  0.145146  0.097880         0.498518,\n",
       "  0.07602997958596591),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  473  0.078020  0.076696  0.078967  0.089587  0.098643         0.490221\n",
       "  474  0.078662  0.077291  0.077490  0.146863  0.099048         0.482210\n",
       "  475  0.078034  0.076661  0.077611  0.107812  0.098407         0.483088\n",
       "  476  0.076848  0.076618  0.076800  0.145146  0.097880         0.498518\n",
       "  477  0.077709  0.076071  0.077696  0.119251  0.099369         0.474020,\n",
       "  0.07401203549681756),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  474  0.078662  0.077291  0.077490  0.146863  0.099048         0.482210\n",
       "  475  0.078034  0.076661  0.077611  0.107812  0.098407         0.483088\n",
       "  476  0.076848  0.076618  0.076800  0.145146  0.097880         0.498518\n",
       "  477  0.077709  0.076071  0.077696  0.119251  0.099369         0.474020\n",
       "  478  0.075762  0.075028  0.075298  0.168331  0.097658         0.472031,\n",
       "  0.07273464765564998),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  475  0.078034  0.076661  0.077611  0.107812  0.098407         0.483088\n",
       "  476  0.076848  0.076618  0.076800  0.145146  0.097880         0.498518\n",
       "  477  0.077709  0.076071  0.077696  0.119251  0.099369         0.474020\n",
       "  478  0.075762  0.075028  0.075298  0.168331  0.097658         0.472031\n",
       "  479  0.074879  0.073405  0.073307  0.183596  0.095687         0.477569,\n",
       "  0.07378675675045282),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  476  0.076848  0.076618  0.076800  0.145146  0.097880         0.498518\n",
       "  477  0.077709  0.076071  0.077696  0.119251  0.099369         0.474020\n",
       "  478  0.075762  0.075028  0.075298  0.168331  0.097658         0.472031\n",
       "  479  0.074879  0.073405  0.073307  0.183596  0.095687         0.477569\n",
       "  480  0.073507  0.073289  0.074545  0.141269  0.094440         0.494988,\n",
       "  0.07262680021889718),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  477  0.077709  0.076071  0.077696  0.119251  0.099369         0.474020\n",
       "  478  0.075762  0.075028  0.075298  0.168331  0.097658         0.472031\n",
       "  479  0.074879  0.073405  0.073307  0.183596  0.095687         0.477569\n",
       "  480  0.073507  0.073289  0.074545  0.141269  0.094440         0.494988\n",
       "  481  0.074739  0.073187  0.073096  0.207548  0.095467         0.478447,\n",
       "  0.07165617328812209),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  478  0.075762  0.075028  0.075298  0.168331  0.097658         0.472031\n",
       "  479  0.074879  0.073405  0.073307  0.183596  0.095687         0.477569\n",
       "  480  0.073507  0.073289  0.074545  0.141269  0.094440         0.494988\n",
       "  481  0.074739  0.073187  0.073096  0.207548  0.095467         0.478447\n",
       "  482  0.071192  0.070952  0.072350  0.207102  0.094334         0.479863,\n",
       "  0.06880900288232089),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  479  0.074879  0.073405  0.073307  0.183596  0.095687         0.477569\n",
       "  480  0.073507  0.073289  0.074545  0.141269  0.094440         0.494988\n",
       "  481  0.074739  0.073187  0.073096  0.207548  0.095467         0.478447\n",
       "  482  0.071192  0.070952  0.072350  0.207102  0.094334         0.479863\n",
       "  483  0.073423  0.071734  0.070170  0.304887  0.093386         0.465830,\n",
       "  0.06952558982579136),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  480  0.073507  0.073289  0.074545  0.141269  0.094440         0.494988\n",
       "  481  0.074739  0.073187  0.073096  0.207548  0.095467         0.478447\n",
       "  482  0.071192  0.070952  0.072350  0.207102  0.094334         0.479863\n",
       "  483  0.073423  0.071734  0.070170  0.304887  0.093386         0.465830\n",
       "  484  0.069057  0.068727  0.068240  0.285815  0.090606         0.492479,\n",
       "  0.06971491942657188),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  481  0.074739  0.073187  0.073096  0.207548  0.095467         0.478447\n",
       "  482  0.071192  0.070952  0.072350  0.207102  0.094334         0.479863\n",
       "  483  0.073423  0.071734  0.070170  0.304887  0.093386         0.465830\n",
       "  484  0.069057  0.068727  0.068240  0.285815  0.090606         0.492479\n",
       "  485  0.070665  0.069234  0.069092  0.196585  0.091306         0.488536,\n",
       "  0.06934584409787005),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  482  0.071192  0.070952  0.072350  0.207102  0.094334         0.479863\n",
       "  483  0.073423  0.071734  0.070170  0.304887  0.093386         0.465830\n",
       "  484  0.069057  0.068727  0.068240  0.285815  0.090606         0.492479\n",
       "  485  0.070665  0.069234  0.069092  0.196585  0.091306         0.488536\n",
       "  486  0.068788  0.068622  0.069720  0.140033  0.091490         0.484361,\n",
       "  0.06842075018553839),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  483  0.073423  0.071734  0.070170  0.304887  0.093386         0.465830\n",
       "  484  0.069057  0.068727  0.068240  0.285815  0.090606         0.492479\n",
       "  485  0.070665  0.069234  0.069092  0.196585  0.091306         0.488536\n",
       "  486  0.068788  0.068622  0.069720  0.140033  0.091490         0.484361\n",
       "  487  0.069553  0.068765  0.069526  0.134458  0.091130         0.480203,\n",
       "  0.06767780759170515),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  484  0.069057  0.068727  0.068240  0.285815  0.090606         0.492479\n",
       "  485  0.070665  0.069234  0.069092  0.196585  0.091306         0.488536\n",
       "  486  0.068788  0.068622  0.069720  0.140033  0.091490         0.484361\n",
       "  487  0.069553  0.068765  0.069526  0.134458  0.091130         0.480203\n",
       "  488  0.069353  0.067717  0.068303  0.137114  0.090227         0.481565,\n",
       "  0.06904866706033676),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  485  0.070665  0.069234  0.069092  0.196585  0.091306         0.488536\n",
       "  486  0.068788  0.068622  0.069720  0.140033  0.091490         0.484361\n",
       "  487  0.069553  0.068765  0.069526  0.134458  0.091130         0.480203\n",
       "  488  0.069353  0.067717  0.068303  0.137114  0.090227         0.481565\n",
       "  489  0.068061  0.068208  0.069218  0.099231  0.089501         0.497371,\n",
       "  0.06868437404570221),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  486  0.068788  0.068622  0.069720  0.140033  0.091490         0.484361\n",
       "  487  0.069553  0.068765  0.069526  0.134458  0.091130         0.480203\n",
       "  488  0.069353  0.067717  0.068303  0.137114  0.090227         0.481565\n",
       "  489  0.068061  0.068208  0.069218  0.099231  0.089501         0.497371\n",
       "  490  0.069505  0.068115  0.069601  0.111677  0.090840         0.484396,\n",
       "  0.06602654286304384),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  487  0.069553  0.068765  0.069526  0.134458  0.091130         0.480203\n",
       "  488  0.069353  0.067717  0.068303  0.137114  0.090227         0.481565\n",
       "  489  0.068061  0.068208  0.069218  0.099231  0.089501         0.497371\n",
       "  490  0.069505  0.068115  0.069601  0.111677  0.090840         0.484396\n",
       "  491  0.068810  0.067310  0.067712  0.132787  0.090484         0.467246,\n",
       "  0.06404215195126496),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  488  0.069353  0.067717  0.068303  0.137114  0.090227         0.481565\n",
       "  489  0.068061  0.068208  0.069218  0.099231  0.089501         0.497371\n",
       "  490  0.069505  0.068115  0.069601  0.111677  0.090840         0.484396\n",
       "  491  0.068810  0.067310  0.067712  0.132787  0.090484         0.467246\n",
       "  492  0.066521  0.065272  0.065214  0.172546  0.087888         0.472282,\n",
       "  0.06599778162210397),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  489  0.068061  0.068208  0.069218  0.099231  0.089501         0.497371\n",
       "  490  0.069505  0.068115  0.069601  0.111677  0.090840         0.484396\n",
       "  491  0.068810  0.067310  0.067712  0.132787  0.090484         0.467246\n",
       "  492  0.066521  0.065272  0.065214  0.172546  0.087888         0.472282\n",
       "  493  0.063965  0.065447  0.064769  0.153798  0.085951         0.501744,\n",
       "  0.06562870629340214),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  490  0.069505  0.068115  0.069601  0.111677  0.090840         0.484396\n",
       "  491  0.068810  0.067310  0.067712  0.132787  0.090484         0.467246\n",
       "  492  0.066521  0.065272  0.065214  0.172546  0.087888         0.472282\n",
       "  493  0.063965  0.065447  0.064769  0.153798  0.085951         0.501744\n",
       "  494  0.067474  0.066103  0.066801  0.123836  0.087860         0.484361,\n",
       "  0.06661131306525124),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  491  0.068810  0.067310  0.067712  0.132787  0.090484         0.467246\n",
       "  492  0.066521  0.065272  0.065214  0.172546  0.087888         0.472282\n",
       "  493  0.063965  0.065447  0.064769  0.153798  0.085951         0.501744\n",
       "  494  0.067474  0.066103  0.066801  0.123836  0.087860         0.484361\n",
       "  495  0.065534  0.065878  0.066656  0.095906  0.087500         0.494468,\n",
       "  0.06734227581801049),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  492  0.066521  0.065272  0.065214  0.172546  0.087888         0.472282\n",
       "  493  0.063965  0.065447  0.064769  0.153798  0.085951         0.501744\n",
       "  494  0.067474  0.066103  0.066801  0.123836  0.087860         0.484361\n",
       "  495  0.065534  0.065878  0.066656  0.095906  0.087500         0.494468\n",
       "  496  0.068258  0.067009  0.068579  0.139598  0.088460         0.492586,\n",
       "  0.06636925291902059),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  493  0.063965  0.065447  0.064769  0.153798  0.085951         0.501744\n",
       "  494  0.067474  0.066103  0.066801  0.123836  0.087860         0.484361\n",
       "  495  0.065534  0.065878  0.066656  0.095906  0.087500         0.494468\n",
       "  496  0.068258  0.067009  0.068579  0.139598  0.088460         0.492586\n",
       "  497  0.068254  0.067051  0.067918  0.104762  0.089173         0.479845,\n",
       "  0.06571258442564466),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  494  0.067474  0.066103  0.066801  0.123836  0.087860         0.484361\n",
       "  495  0.065534  0.065878  0.066656  0.095906  0.087500         0.494468\n",
       "  496  0.068258  0.067009  0.068579  0.139598  0.088460         0.492586\n",
       "  497  0.068254  0.067051  0.067918  0.104762  0.089173         0.479845\n",
       "  498  0.066107  0.065390  0.066973  0.103568  0.088223         0.482210,\n",
       "  0.06434651689344266),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  495  0.065534  0.065878  0.066656  0.095906  0.087500         0.494468\n",
       "  496  0.068258  0.067009  0.068579  0.139598  0.088460         0.492586\n",
       "  497  0.068254  0.067051  0.067918  0.104762  0.089173         0.479845\n",
       "  498  0.066107  0.065390  0.066973  0.103568  0.088223         0.482210\n",
       "  499  0.066232  0.064824  0.065893  0.091518  0.087582         0.476906,\n",
       "  0.06456939967159263),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  496  0.068258  0.067009  0.068579  0.139598  0.088460         0.492586\n",
       "  497  0.068254  0.067051  0.067918  0.104762  0.089173         0.479845\n",
       "  498  0.066107  0.065390  0.066973  0.103568  0.088223         0.482210\n",
       "  499  0.066232  0.064824  0.065893  0.091518  0.087582         0.476906\n",
       "  500  0.065534  0.064788  0.066055  0.120750  0.086248         0.488787,\n",
       "  0.06733508791336609),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  497  0.068254  0.067051  0.067918  0.104762  0.089173         0.479845\n",
       "  498  0.066107  0.065390  0.066973  0.103568  0.088223         0.482210\n",
       "  499  0.066232  0.064824  0.065893  0.091518  0.087582         0.476906\n",
       "  500  0.065534  0.064788  0.066055  0.120750  0.086248         0.488787\n",
       "  501  0.066162  0.066338  0.067007  0.162827  0.086465         0.507801,\n",
       "  0.06895279946465795),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  498  0.066107  0.065390  0.066973  0.103568  0.088223         0.482210\n",
       "  499  0.066232  0.064824  0.065893  0.091518  0.087582         0.476906\n",
       "  500  0.065534  0.064788  0.066055  0.120750  0.086248         0.488787\n",
       "  501  0.066162  0.066338  0.067007  0.162827  0.086465         0.507801\n",
       "  502  0.068417  0.067945  0.069313  0.142273  0.089166         0.499217,\n",
       "  0.06849264847670691),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  499  0.066232  0.064824  0.065893  0.091518  0.087582         0.476906\n",
       "  500  0.065534  0.064788  0.066055  0.120750  0.086248         0.488787\n",
       "  501  0.066162  0.066338  0.067007  0.162827  0.086465         0.507801\n",
       "  502  0.068417  0.067945  0.069313  0.142273  0.089166         0.499217\n",
       "  503  0.069118  0.068312  0.069749  0.123472  0.090746         0.483680,\n",
       "  0.06790787827449951),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  500  0.065534  0.064788  0.066055  0.120750  0.086248         0.488787\n",
       "  501  0.066162  0.066338  0.067007  0.162827  0.086465         0.507801\n",
       "  502  0.068417  0.067945  0.069313  0.142273  0.089166         0.499217\n",
       "  503  0.069118  0.068312  0.069749  0.123472  0.090746         0.483680\n",
       "  504  0.069098  0.067622  0.068981  0.120358  0.090297         0.482748,\n",
       "  0.0664555366418402),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  501  0.066162  0.066338  0.067007  0.162827  0.086465         0.507801\n",
       "  502  0.068417  0.067945  0.069313  0.142273  0.089166         0.499217\n",
       "  503  0.069118  0.068312  0.069749  0.123472  0.090746         0.483680\n",
       "  504  0.069098  0.067622  0.068981  0.120358  0.090297         0.482748\n",
       "  505  0.067123  0.065705  0.067641  0.097764  0.089726         0.476260,\n",
       "  0.06669279522927896),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  502  0.068417  0.067945  0.069313  0.142273  0.089166         0.499217\n",
       "  503  0.069118  0.068312  0.069749  0.123472  0.090746         0.483680\n",
       "  504  0.069098  0.067622  0.068981  0.120358  0.090297         0.482748\n",
       "  505  0.067123  0.065705  0.067641  0.097764  0.089726         0.476260\n",
       "  506  0.067033  0.065767  0.067549  0.101189  0.088307         0.488895,\n",
       "  0.06552804676129376),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  503  0.069118  0.068312  0.069749  0.123472  0.090746         0.483680\n",
       "  504  0.069098  0.067622  0.068981  0.120358  0.090297         0.482748\n",
       "  505  0.067123  0.065705  0.067641  0.097764  0.089726         0.476260\n",
       "  506  0.067033  0.065767  0.067549  0.101189  0.088307         0.488895\n",
       "  507  0.067012  0.065532  0.067021  0.088497  0.088539         0.478411,\n",
       "  0.06559994505246228),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  504  0.069098  0.067622  0.068981  0.120358  0.090297         0.482748\n",
       "  505  0.067123  0.065705  0.067641  0.097764  0.089726         0.476260\n",
       "  506  0.067033  0.065767  0.067549  0.101189  0.088307         0.488895\n",
       "  507  0.067012  0.065532  0.067021  0.088497  0.088539         0.478411\n",
       "  508  0.066242  0.065089  0.066924  0.084610  0.087402         0.487658,\n",
       "  0.06548730567927989),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  505  0.067123  0.065705  0.067641  0.097764  0.089726         0.476260\n",
       "  506  0.067033  0.065767  0.067549  0.101189  0.088307         0.488895\n",
       "  507  0.067012  0.065532  0.067021  0.088497  0.088539         0.478411\n",
       "  508  0.066242  0.065089  0.066924  0.084610  0.087402         0.487658\n",
       "  509  0.065914  0.064833  0.066983  0.059902  0.087472         0.486278,\n",
       "  0.06733029597693652),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  506  0.067033  0.065767  0.067549  0.101189  0.088307         0.488895\n",
       "  507  0.067012  0.065532  0.067021  0.088497  0.088539         0.478411\n",
       "  508  0.066242  0.065089  0.066924  0.084610  0.087402         0.487658\n",
       "  509  0.065914  0.064833  0.066983  0.059902  0.087472         0.486278\n",
       "  510  0.066523  0.066167  0.067588  0.101186  0.087362         0.500902,\n",
       "  0.0668509772432671),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  507  0.067012  0.065532  0.067021  0.088497  0.088539         0.478411\n",
       "  508  0.066242  0.065089  0.066924  0.084610  0.087402         0.487658\n",
       "  509  0.065914  0.064833  0.066983  0.059902  0.087472         0.486278\n",
       "  510  0.066523  0.066167  0.067588  0.101186  0.087362         0.500902\n",
       "  511  0.067767  0.066492  0.068121  0.108386  0.089162         0.483536,\n",
       "  0.06728236699027826),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  508  0.066242  0.065089  0.066924  0.084610  0.087402         0.487658\n",
       "  509  0.065914  0.064833  0.066983  0.059902  0.087472         0.486278\n",
       "  510  0.066523  0.066167  0.067588  0.101186  0.087362         0.500902\n",
       "  511  0.067767  0.066492  0.068121  0.108386  0.089162         0.483536\n",
       "  512  0.067390  0.067044  0.068438  0.098277  0.088694         0.490346,\n",
       "  0.06675031771115869),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  509  0.065914  0.064833  0.066983  0.059902  0.087472         0.486278\n",
       "  510  0.066523  0.066167  0.067588  0.101186  0.087362         0.500902\n",
       "  511  0.067767  0.066492  0.068121  0.108386  0.089162         0.483536\n",
       "  512  0.067390  0.067044  0.068438  0.098277  0.088694         0.490346\n",
       "  513  0.067938  0.066452  0.068363  0.071899  0.089115         0.483142,\n",
       "  0.06676708948866228),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  510  0.066523  0.066167  0.067588  0.101186  0.087362         0.500902\n",
       "  511  0.067767  0.066492  0.068121  0.108386  0.089162         0.483536\n",
       "  512  0.067390  0.067044  0.068438  0.098277  0.088694         0.490346\n",
       "  513  0.067938  0.066452  0.068363  0.071899  0.089115         0.483142\n",
       "  514  0.067818  0.066246  0.068119  0.064918  0.088595         0.487246,\n",
       "  0.06814754245251536),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  511  0.067767  0.066492  0.068121  0.108386  0.089162         0.483536\n",
       "  512  0.067390  0.067044  0.068438  0.098277  0.088694         0.490346\n",
       "  513  0.067938  0.066452  0.068363  0.071899  0.089115         0.483142\n",
       "  514  0.067818  0.066246  0.068119  0.064918  0.088595         0.487246\n",
       "  515  0.067548  0.067222  0.068179  0.099005  0.088612         0.497443,\n",
       "  0.0671361744397264),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  512  0.067390  0.067044  0.068438  0.098277  0.088694         0.490346\n",
       "  513  0.067938  0.066452  0.068363  0.071899  0.089115         0.483142\n",
       "  514  0.067818  0.066246  0.068119  0.064918  0.088595         0.487246\n",
       "  515  0.067548  0.067222  0.068179  0.099005  0.088612         0.497443\n",
       "  516  0.068085  0.066698  0.068717  0.090487  0.089960         0.479558,\n",
       "  0.06674073383829951),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  513  0.067938  0.066452  0.068363  0.071899  0.089115         0.483142\n",
       "  514  0.067818  0.066246  0.068119  0.064918  0.088595         0.487246\n",
       "  515  0.067548  0.067222  0.068179  0.099005  0.088612         0.497443\n",
       "  516  0.068085  0.066698  0.068717  0.090487  0.089960         0.479558\n",
       "  517  0.067433  0.066357  0.068169  0.093354  0.088972         0.484164,\n",
       "  0.06658734376074096),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  514  0.067818  0.066246  0.068119  0.064918  0.088595         0.487246\n",
       "  515  0.067548  0.067222  0.068179  0.099005  0.088612         0.497443\n",
       "  516  0.068085  0.066698  0.068717  0.090487  0.089960         0.479558\n",
       "  517  0.067433  0.066357  0.068169  0.093354  0.088972         0.484164\n",
       "  518  0.066790  0.066129  0.068005  0.074903  0.088586         0.485974,\n",
       "  0.06808283206599124),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  515  0.067548  0.067222  0.068179  0.099005  0.088612         0.497443\n",
       "  516  0.068085  0.066698  0.068717  0.090487  0.089960         0.479558\n",
       "  517  0.067433  0.066357  0.068169  0.093354  0.088972         0.484164\n",
       "  518  0.066790  0.066129  0.068005  0.074903  0.088586         0.485974\n",
       "  519  0.067161  0.067054  0.068259  0.110069  0.088436         0.498303,\n",
       "  0.06995458360458773),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  516  0.068085  0.066698  0.068717  0.090487  0.089960         0.479558\n",
       "  517  0.067433  0.066357  0.068169  0.093354  0.088972         0.484164\n",
       "  518  0.066790  0.066129  0.068005  0.074903  0.088586         0.485974\n",
       "  519  0.067161  0.067054  0.068259  0.110069  0.088436         0.498303\n",
       "  520  0.068783  0.068959  0.069981  0.132277  0.089897         0.501117,\n",
       "  0.07347040234483883),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  517  0.067433  0.066357  0.068169  0.093354  0.088972         0.484164\n",
       "  518  0.066790  0.066129  0.068005  0.074903  0.088586         0.485974\n",
       "  519  0.067161  0.067054  0.068259  0.110069  0.088436         0.498303\n",
       "  520  0.068783  0.068959  0.069981  0.132277  0.089897         0.501117\n",
       "  521  0.071226  0.072284  0.072457  0.237499  0.091725         0.513411,\n",
       "  0.07284968299704717),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  518  0.066790  0.066129  0.068005  0.074903  0.088586         0.485974\n",
       "  519  0.067161  0.067054  0.068259  0.110069  0.088436         0.498303\n",
       "  520  0.068783  0.068959  0.069981  0.132277  0.089897         0.501117\n",
       "  521  0.071226  0.072284  0.072457  0.237499  0.091725         0.513411\n",
       "  522  0.073428  0.072170  0.074111  0.130426  0.095158         0.482479,\n",
       "  0.07426367951590743),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  519  0.067161  0.067054  0.068259  0.110069  0.088436         0.498303\n",
       "  520  0.068783  0.068959  0.069981  0.132277  0.089897         0.501117\n",
       "  521  0.071226  0.072284  0.072457  0.237499  0.091725         0.513411\n",
       "  522  0.073428  0.072170  0.074111  0.130426  0.095158         0.482479\n",
       "  523  0.074193  0.073063  0.075276  0.190555  0.094552         0.497694,\n",
       "  0.07541644814281864),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  520  0.068783  0.068959  0.069981  0.132277  0.089897         0.501117\n",
       "  521  0.071226  0.072284  0.072457  0.237499  0.091725         0.513411\n",
       "  522  0.073428  0.072170  0.074111  0.130426  0.095158         0.482479\n",
       "  523  0.074193  0.073063  0.075276  0.190555  0.094552         0.497694\n",
       "  524  0.074800  0.075111  0.075937  0.172763  0.095933         0.495741,\n",
       "  0.0728089419150333),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  521  0.071226  0.072284  0.072457  0.237499  0.091725         0.513411\n",
       "  522  0.073428  0.072170  0.074111  0.130426  0.095158         0.482479\n",
       "  523  0.074193  0.073063  0.075276  0.190555  0.094552         0.497694\n",
       "  524  0.074800  0.075111  0.075937  0.172763  0.095933         0.495741\n",
       "  525  0.076114  0.074353  0.071885  0.347555  0.097059         0.467622,\n",
       "  0.07117685455445268),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  522  0.073428  0.072170  0.074111  0.130426  0.095158         0.482479\n",
       "  523  0.074193  0.073063  0.075276  0.190555  0.094552         0.497694\n",
       "  524  0.074800  0.075111  0.075937  0.172763  0.095933         0.495741\n",
       "  525  0.076114  0.074353  0.071885  0.347555  0.097059         0.467622\n",
       "  526  0.074102  0.072433  0.072340  0.222409  0.094512         0.474916,\n",
       "  0.07353750869957776),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  523  0.074193  0.073063  0.075276  0.190555  0.094552         0.497694\n",
       "  524  0.074800  0.075111  0.075937  0.172763  0.095933         0.495741\n",
       "  525  0.076114  0.074353  0.071885  0.347555  0.097059         0.467622\n",
       "  526  0.074102  0.072433  0.072340  0.222409  0.094512         0.474916\n",
       "  527  0.072465  0.072684  0.073610  0.259972  0.092918         0.504773,\n",
       "  0.07280175401038892),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  524  0.074800  0.075111  0.075937  0.172763  0.095933         0.495741\n",
       "  525  0.076114  0.074353  0.071885  0.347555  0.097059         0.467622\n",
       "  526  0.074102  0.072433  0.072340  0.222409  0.094512         0.474916\n",
       "  527  0.072465  0.072684  0.073610  0.259972  0.092918         0.504773\n",
       "  528  0.073591  0.072447  0.073973  0.112998  0.095224         0.481619,\n",
       "  0.07284968299704717),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  525  0.076114  0.074353  0.071885  0.347555  0.097059         0.467622\n",
       "  526  0.074102  0.072433  0.072340  0.222409  0.094512         0.474916\n",
       "  527  0.072465  0.072684  0.073610  0.259972  0.092918         0.504773\n",
       "  528  0.073591  0.072447  0.073973  0.112998  0.095224         0.481619\n",
       "  529  0.073736  0.072921  0.074249  0.139454  0.094505         0.487479,\n",
       "  0.07354230063600735),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  526  0.074102  0.072433  0.072340  0.222409  0.094512         0.474916\n",
       "  527  0.072465  0.072684  0.073610  0.259972  0.092918         0.504773\n",
       "  528  0.073591  0.072447  0.073973  0.112998  0.095224         0.481619\n",
       "  529  0.073736  0.072921  0.074249  0.139454  0.094505         0.487479\n",
       "  530  0.073717  0.072502  0.074072  0.128560  0.094552         0.492300,\n",
       "  0.07259564300974253),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  527  0.072465  0.072684  0.073610  0.259972  0.092918         0.504773\n",
       "  528  0.073591  0.072447  0.073973  0.112998  0.095224         0.481619\n",
       "  529  0.073736  0.072921  0.074249  0.139454  0.094505         0.487479\n",
       "  530  0.073717  0.072502  0.074072  0.128560  0.094552         0.492300\n",
       "  531  0.073909  0.073341  0.074218  0.142845  0.095228         0.480042,\n",
       "  0.07275381540136837),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  528  0.073591  0.072447  0.073973  0.112998  0.095224         0.481619\n",
       "  529  0.073736  0.072921  0.074249  0.139454  0.094505         0.487479\n",
       "  530  0.073717  0.072502  0.074072  0.128560  0.094552         0.492300\n",
       "  531  0.073909  0.073341  0.074218  0.142845  0.095228         0.480042\n",
       "  532  0.073375  0.072336  0.073774  0.124133  0.094304         0.488303,\n",
       "  0.07235118689529708),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  529  0.073736  0.072921  0.074249  0.139454  0.094505         0.487479\n",
       "  530  0.073717  0.072502  0.074072  0.128560  0.094552         0.492300\n",
       "  531  0.073909  0.073341  0.074218  0.142845  0.095228         0.480042\n",
       "  532  0.073375  0.072336  0.073774  0.124133  0.094304         0.488303\n",
       "  533  0.073628  0.072014  0.073985  0.080436  0.094458         0.484110,\n",
       "  0.07224094349032949),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  530  0.073717  0.072502  0.074072  0.128560  0.094552         0.492300\n",
       "  531  0.073909  0.073341  0.074218  0.142845  0.095228         0.480042\n",
       "  532  0.073375  0.072336  0.073774  0.124133  0.094304         0.488303\n",
       "  533  0.073628  0.072014  0.073985  0.080436  0.094458         0.484110\n",
       "  534  0.072850  0.072104  0.073837  0.088730  0.094065         0.486296,\n",
       "  0.07286405880633595),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  531  0.073909  0.073341  0.074218  0.142845  0.095228         0.480042\n",
       "  532  0.073375  0.072336  0.073774  0.124133  0.094304         0.488303\n",
       "  533  0.073628  0.072014  0.073985  0.080436  0.094458         0.484110\n",
       "  534  0.072850  0.072104  0.073837  0.088730  0.094065         0.486296\n",
       "  535  0.072703  0.072213  0.073205  0.132824  0.093957         0.491780,\n",
       "  0.0756585082890493),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  532  0.073375  0.072336  0.073774  0.124133  0.094304         0.488303\n",
       "  533  0.073628  0.072014  0.073985  0.080436  0.094458         0.484110\n",
       "  534  0.072850  0.072104  0.073837  0.088730  0.094065         0.486296\n",
       "  535  0.072703  0.072213  0.073205  0.132824  0.093957         0.491780\n",
       "  536  0.073589  0.074419  0.074382  0.161941  0.094566         0.508017,\n",
       "  0.07472622647207325),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  533  0.073628  0.072014  0.073985  0.080436  0.094458         0.484110\n",
       "  534  0.072850  0.072104  0.073837  0.088730  0.094065         0.486296\n",
       "  535  0.072703  0.072213  0.073205  0.132824  0.093957         0.491780\n",
       "  536  0.073589  0.074419  0.074382  0.161941  0.094566         0.508017\n",
       "  537  0.075931  0.075004  0.076274  0.140731  0.097295         0.480149,\n",
       "  0.07680888094774571),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  534  0.072850  0.072104  0.073837  0.088730  0.094065         0.486296\n",
       "  535  0.072703  0.072213  0.073205  0.132824  0.093957         0.491780\n",
       "  536  0.073589  0.074419  0.074382  0.161941  0.094566         0.508017\n",
       "  537  0.075931  0.075004  0.076274  0.140731  0.097295         0.480149\n",
       "  538  0.074853  0.075874  0.076005  0.178352  0.096385         0.502694,\n",
       "  0.07884599278261244),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  535  0.072703  0.072213  0.073205  0.132824  0.093957         0.491780\n",
       "  536  0.073589  0.074419  0.074382  0.161941  0.094566         0.508017\n",
       "  537  0.075931  0.075004  0.076274  0.140731  0.097295         0.480149\n",
       "  538  0.074853  0.075874  0.076005  0.178352  0.096385         0.502694\n",
       "  539  0.078361  0.078253  0.079316  0.184355  0.098418         0.502353,\n",
       "  0.07828279591670051),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  536  0.073589  0.074419  0.074382  0.161941  0.094566         0.508017\n",
       "  537  0.075931  0.075004  0.076274  0.140731  0.097295         0.480149\n",
       "  538  0.074853  0.075874  0.076005  0.178352  0.096385         0.502694\n",
       "  539  0.078361  0.078253  0.079316  0.184355  0.098418         0.502353\n",
       "  540  0.079988  0.079651  0.079067  0.237996  0.100408         0.482909,\n",
       "  0.07824684677111625),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  537  0.075931  0.075004  0.076274  0.140731  0.097295         0.480149\n",
       "  538  0.074853  0.075874  0.076005  0.178352  0.096385         0.502694\n",
       "  539  0.078361  0.078253  0.079316  0.184355  0.098418         0.502353\n",
       "  540  0.079988  0.079651  0.079067  0.237996  0.100408         0.482909\n",
       "  541  0.078414  0.077770  0.079399  0.136976  0.099858         0.486852,\n",
       "  0.07847212551748102),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  538  0.074853  0.075874  0.076005  0.178352  0.096385         0.502694\n",
       "  539  0.078361  0.078253  0.079316  0.184355  0.098418         0.502353\n",
       "  540  0.079988  0.079651  0.079067  0.237996  0.100408         0.482909\n",
       "  541  0.078414  0.077770  0.079399  0.136976  0.099858         0.486852\n",
       "  542  0.079266  0.077715  0.079457  0.117726  0.099823         0.488805,\n",
       "  0.07843857234011156),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  539  0.078361  0.078253  0.079316  0.184355  0.098418         0.502353\n",
       "  540  0.079988  0.079651  0.079067  0.237996  0.100408         0.482909\n",
       "  541  0.078414  0.077770  0.079399  0.136976  0.099858         0.486852\n",
       "  542  0.079266  0.077715  0.079457  0.117726  0.099823         0.488805\n",
       "  543  0.079074  0.077673  0.079835  0.088015  0.100043         0.486870,\n",
       "  0.0771084539534938),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  540  0.079988  0.079651  0.079067  0.237996  0.100408         0.482909\n",
       "  541  0.078414  0.077770  0.079399  0.136976  0.099858         0.486852\n",
       "  542  0.079266  0.077715  0.079457  0.117726  0.099823         0.488805\n",
       "  543  0.079074  0.077673  0.079835  0.088015  0.100043         0.486870\n",
       "  544  0.079059  0.077602  0.078812  0.104966  0.100010         0.477174,\n",
       "  0.07684243412511518),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  541  0.078414  0.077770  0.079399  0.136976  0.099858         0.486852\n",
       "  542  0.079266  0.077715  0.079457  0.117726  0.099823         0.488805\n",
       "  543  0.079074  0.077673  0.079835  0.088015  0.100043         0.486870\n",
       "  544  0.079059  0.077602  0.078812  0.104966  0.100010         0.477174\n",
       "  545  0.077278  0.076310  0.077689  0.126653  0.098711         0.485131,\n",
       "  0.07652368375128642),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  542  0.079266  0.077715  0.079457  0.117726  0.099823         0.488805\n",
       "  543  0.079074  0.077673  0.079835  0.088015  0.100043         0.486870\n",
       "  544  0.079059  0.077602  0.078812  0.104966  0.100010         0.477174\n",
       "  545  0.077278  0.076310  0.077689  0.126653  0.098711         0.485131\n",
       "  546  0.077926  0.076547  0.077882  0.146265  0.098451         0.484737,\n",
       "  0.07814139530257826),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  543  0.079074  0.077673  0.079835  0.088015  0.100043         0.486870\n",
       "  544  0.079059  0.077602  0.078812  0.104966  0.100010         0.477174\n",
       "  545  0.077278  0.076310  0.077689  0.126653  0.098711         0.485131\n",
       "  546  0.077926  0.076547  0.077882  0.146265  0.098451         0.484737\n",
       "  547  0.077262  0.077673  0.078384  0.279708  0.098140         0.499217,\n",
       "  0.08619639025930573),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  544  0.079059  0.077602  0.078812  0.104966  0.100010         0.477174\n",
       "  545  0.077278  0.076310  0.077689  0.126653  0.098711         0.485131\n",
       "  546  0.077926  0.076547  0.077882  0.146265  0.098451         0.484737\n",
       "  547  0.077262  0.077673  0.078384  0.279708  0.098140         0.499217\n",
       "  548  0.086590  0.084924  0.086624  0.283220  0.099720         0.547353,\n",
       "  0.09125562629146534),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  545  0.077278  0.076310  0.077689  0.126653  0.098711         0.485131\n",
       "  546  0.077926  0.076547  0.077882  0.146265  0.098451         0.484737\n",
       "  547  0.077262  0.077673  0.078384  0.279708  0.098140         0.499217\n",
       "  548  0.086590  0.084924  0.086624  0.283220  0.099720         0.547353\n",
       "  549  0.087381  0.090740  0.088266  0.367296  0.107586         0.524952,\n",
       "  0.08946536653925888),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  546  0.077926  0.076547  0.077882  0.146265  0.098451         0.484737\n",
       "  547  0.077262  0.077673  0.078384  0.279708  0.098140         0.499217\n",
       "  548  0.086590  0.084924  0.086624  0.283220  0.099720         0.547353\n",
       "  549  0.087381  0.090740  0.088266  0.367296  0.107586         0.524952\n",
       "  550  0.090751  0.089133  0.090940  0.210549  0.112527         0.473734,\n",
       "  0.09265045506460724),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  547  0.077262  0.077673  0.078384  0.279708  0.098140         0.499217\n",
       "  548  0.086590  0.084924  0.086624  0.283220  0.099720         0.547353\n",
       "  549  0.087381  0.090740  0.088266  0.367296  0.107586         0.524952\n",
       "  550  0.090751  0.089133  0.090940  0.210549  0.112527         0.473734\n",
       "  551  0.091042  0.091655  0.091836  0.223375  0.110779         0.510938,\n",
       "  0.0922909636087646),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  548  0.086590  0.084924  0.086624  0.283220  0.099720         0.547353\n",
       "  549  0.087381  0.090740  0.088266  0.367296  0.107586         0.524952\n",
       "  550  0.090751  0.089133  0.090940  0.210549  0.112527         0.473734\n",
       "  551  0.091042  0.091655  0.091836  0.223375  0.110779         0.510938\n",
       "  552  0.093494  0.092474  0.094038  0.170939  0.113889         0.484432,\n",
       "  0.08991832000020322),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  549  0.087381  0.090740  0.088266  0.367296  0.107586         0.524952\n",
       "  550  0.090751  0.089133  0.090940  0.210549  0.112527         0.473734\n",
       "  551  0.091042  0.091655  0.091836  0.223375  0.110779         0.510938\n",
       "  552  0.093494  0.092474  0.094038  0.170939  0.113889         0.484432\n",
       "  553  0.092584  0.090882  0.091202  0.160535  0.113538         0.469379,\n",
       "  0.09024665424689118),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  550  0.090751  0.089133  0.090940  0.210549  0.112527         0.473734\n",
       "  551  0.091042  0.091655  0.091836  0.223375  0.110779         0.510938\n",
       "  552  0.093494  0.092474  0.094038  0.170939  0.113889         0.484432\n",
       "  553  0.092584  0.090882  0.091202  0.160535  0.113538         0.469379\n",
       "  554  0.090399  0.089749  0.090601  0.159541  0.111221         0.489576,\n",
       "  0.09020351719666253),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  551  0.091042  0.091655  0.091836  0.223375  0.110779         0.510938\n",
       "  552  0.093494  0.092474  0.094038  0.170939  0.113889         0.484432\n",
       "  553  0.092584  0.090882  0.091202  0.160535  0.113538         0.469379\n",
       "  554  0.090399  0.089749  0.090601  0.159541  0.111221         0.489576\n",
       "  555  0.091179  0.090152  0.091529  0.152786  0.111542         0.486798,\n",
       "  0.08807293373433181),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  552  0.093494  0.092474  0.094038  0.170939  0.113889         0.484432\n",
       "  553  0.092584  0.090882  0.091202  0.160535  0.113538         0.469379\n",
       "  554  0.090399  0.089749  0.090601  0.159541  0.111221         0.489576\n",
       "  555  0.091179  0.090152  0.091529  0.152786  0.111542         0.486798\n",
       "  556  0.091347  0.089434  0.089451  0.131890  0.111500         0.471189,\n",
       "  0.08865051603189483),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  553  0.092584  0.090882  0.091202  0.160535  0.113538         0.469379\n",
       "  554  0.090399  0.089749  0.090601  0.159541  0.111221         0.489576\n",
       "  555  0.091179  0.090152  0.091529  0.152786  0.111542         0.486798\n",
       "  556  0.091347  0.089434  0.089451  0.131890  0.111500         0.471189\n",
       "  557  0.088638  0.088154  0.089722  0.127278  0.109419         0.491440,\n",
       "  0.08910347911520146),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  554  0.090399  0.089749  0.090601  0.159541  0.111221         0.489576\n",
       "  555  0.091179  0.090152  0.091529  0.152786  0.111542         0.486798\n",
       "  556  0.091347  0.089434  0.089451  0.131890  0.111500         0.471189\n",
       "  557  0.088638  0.088154  0.089722  0.127278  0.109419         0.491440\n",
       "  558  0.089776  0.088159  0.089402  0.119276  0.109983         0.490508,\n",
       "  0.0903377299061404),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  555  0.091179  0.090152  0.091529  0.152786  0.111542         0.486798\n",
       "  556  0.091347  0.089434  0.089451  0.131890  0.111500         0.471189\n",
       "  557  0.088638  0.088154  0.089722  0.127278  0.109419         0.491440\n",
       "  558  0.089776  0.088159  0.089402  0.119276  0.109983         0.490508\n",
       "  559  0.090146  0.089560  0.091163  0.121314  0.110425         0.496350,\n",
       "  0.08928801677955236),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  556  0.091347  0.089434  0.089451  0.131890  0.111500         0.471189\n",
       "  557  0.088638  0.088154  0.089722  0.127278  0.109419         0.491440\n",
       "  558  0.089776  0.088159  0.089402  0.119276  0.109983         0.490508\n",
       "  559  0.090146  0.089560  0.091163  0.121314  0.110425         0.496350\n",
       "  560  0.090912  0.089408  0.091027  0.119018  0.111631         0.479271,\n",
       "  0.08987039101354496),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  557  0.088638  0.088154  0.089722  0.127278  0.109419         0.491440\n",
       "  558  0.089776  0.088159  0.089402  0.119276  0.109983         0.490508\n",
       "  559  0.090146  0.089560  0.091163  0.121314  0.110425         0.496350\n",
       "  560  0.090912  0.089408  0.091027  0.119018  0.111631         0.479271\n",
       "  561  0.089323  0.090053  0.090260  0.193640  0.110605         0.491475,\n",
       "  0.08930239258884116),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  558  0.089776  0.088159  0.089402  0.119276  0.109983         0.490508\n",
       "  559  0.090146  0.089560  0.091163  0.121314  0.110425         0.496350\n",
       "  560  0.090912  0.089408  0.091027  0.119018  0.111631         0.479271\n",
       "  561  0.089323  0.090053  0.090260  0.193640  0.110605         0.491475\n",
       "  562  0.090803  0.089519  0.091049  0.118583  0.111174         0.482873,\n",
       "  0.08952287939877633),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  559  0.090146  0.089560  0.091163  0.121314  0.110425         0.496350\n",
       "  560  0.090912  0.089408  0.091027  0.119018  0.111631         0.479271\n",
       "  561  0.089323  0.090053  0.090260  0.193640  0.110605         0.491475\n",
       "  562  0.090803  0.089519  0.091049  0.118583  0.111174         0.482873\n",
       "  563  0.090149  0.088389  0.090669  0.067929  0.110619         0.488769,\n",
       "  0.0913155447415599),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  560  0.090912  0.089408  0.091027  0.119018  0.111631         0.479271\n",
       "  561  0.089323  0.090053  0.090260  0.193640  0.110605         0.491475\n",
       "  562  0.090803  0.089519  0.091049  0.118583  0.111174         0.482873\n",
       "  563  0.090149  0.088389  0.090669  0.067929  0.110619         0.488769\n",
       "  564  0.090418  0.089917  0.091601  0.105514  0.110835         0.500526,\n",
       "  0.09329753968512397),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  561  0.089323  0.090053  0.090260  0.193640  0.110605         0.491475\n",
       "  562  0.090803  0.089519  0.091049  0.118583  0.111174         0.482873\n",
       "  563  0.090149  0.088389  0.090669  0.067929  0.110619         0.488769\n",
       "  564  0.090418  0.089917  0.091601  0.105514  0.110835         0.500526\n",
       "  565  0.091814  0.091998  0.093142  0.175601  0.112586         0.501941,\n",
       "  0.09392784290577483),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  562  0.090803  0.089519  0.091049  0.118583  0.111174         0.482873\n",
       "  563  0.090149  0.088389  0.090669  0.067929  0.110619         0.488769\n",
       "  564  0.090418  0.089917  0.091601  0.105514  0.110835         0.500526\n",
       "  565  0.091814  0.091998  0.093142  0.175601  0.112586         0.501941\n",
       "  566  0.094878  0.094344  0.095911  0.203508  0.114521         0.491834,\n",
       "  0.09487930209083156),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  563  0.090149  0.088389  0.090669  0.067929  0.110619         0.488769\n",
       "  564  0.090418  0.089917  0.091601  0.105514  0.110835         0.500526\n",
       "  565  0.091814  0.091998  0.093142  0.175601  0.112586         0.501941\n",
       "  566  0.094878  0.094344  0.095911  0.203508  0.114521         0.491834\n",
       "  567  0.095256  0.093830  0.096063  0.123773  0.115137         0.494235,\n",
       "  0.09557191972979175),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  564  0.090418  0.089917  0.091601  0.105514  0.110835         0.500526\n",
       "  565  0.091814  0.091998  0.093142  0.175601  0.112586         0.501941\n",
       "  566  0.094878  0.094344  0.095911  0.203508  0.114521         0.491834\n",
       "  567  0.095256  0.093830  0.096063  0.123773  0.115137         0.494235\n",
       "  568  0.094835  0.094299  0.096170  0.133950  0.116066         0.492300,\n",
       "  0.09467558705839996),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  565  0.091814  0.091998  0.093142  0.175601  0.112586         0.501941\n",
       "  566  0.094878  0.094344  0.095911  0.203508  0.114521         0.491834\n",
       "  567  0.095256  0.093830  0.096063  0.123773  0.115137         0.494235\n",
       "  568  0.094835  0.094299  0.096170  0.133950  0.116066         0.492300\n",
       "  569  0.096074  0.094001  0.096085  0.124549  0.116742         0.480418,\n",
       "  0.09817463402114748),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  566  0.094878  0.094344  0.095911  0.203508  0.114521         0.491834\n",
       "  567  0.095256  0.093830  0.096063  0.123773  0.115137         0.494235\n",
       "  568  0.094835  0.094299  0.096170  0.133950  0.116066         0.492300\n",
       "  569  0.096074  0.094001  0.096085  0.124549  0.116742         0.480418\n",
       "  570  0.095626  0.096750  0.096855  0.204889  0.115867         0.513285,\n",
       "  0.0977815893879354),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  567  0.095256  0.093830  0.096063  0.123773  0.115137         0.494235\n",
       "  568  0.094835  0.094299  0.096170  0.133950  0.116066         0.492300\n",
       "  569  0.096074  0.094001  0.096085  0.124549  0.116742         0.480418\n",
       "  570  0.095626  0.096750  0.096855  0.204889  0.115867         0.513285\n",
       "  571  0.099099  0.097461  0.099265  0.109376  0.119284         0.484181,\n",
       "  0.09706021050803532),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  568  0.094835  0.094299  0.096170  0.133950  0.116066         0.492300\n",
       "  569  0.096074  0.094001  0.096085  0.124549  0.116742         0.480418\n",
       "  570  0.095626  0.096750  0.096855  0.204889  0.115867         0.513285\n",
       "  571  0.099099  0.097461  0.099265  0.109376  0.119284         0.484181\n",
       "  572  0.097542  0.096157  0.098834  0.042049  0.118900         0.481726,\n",
       "  0.0922070854765221),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  569  0.096074  0.094001  0.096085  0.124549  0.116742         0.480418\n",
       "  570  0.095626  0.096750  0.096855  0.204889  0.115867         0.513285\n",
       "  571  0.099099  0.097461  0.099265  0.109376  0.119284         0.484181\n",
       "  572  0.097542  0.096157  0.098834  0.042049  0.118900         0.481726\n",
       "  573  0.096789  0.094802  0.094171  0.178065  0.118196         0.450831,\n",
       "  0.09334546867178221),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  570  0.095626  0.096750  0.096855  0.204889  0.115867         0.513285\n",
       "  571  0.099099  0.097461  0.099265  0.109376  0.119284         0.484181\n",
       "  572  0.097542  0.096157  0.098834  0.042049  0.118900         0.481726\n",
       "  573  0.096789  0.094802  0.094171  0.178065  0.118196         0.450831\n",
       "  574  0.091918  0.091977  0.092301  0.189568  0.113456         0.495633,\n",
       "  0.0921831161720118),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  571  0.099099  0.097461  0.099265  0.109376  0.119284         0.484181\n",
       "  572  0.097542  0.096157  0.098834  0.042049  0.118900         0.481726\n",
       "  573  0.096789  0.094802  0.094171  0.178065  0.118196         0.450831\n",
       "  574  0.091918  0.091977  0.092301  0.189568  0.113456         0.495633\n",
       "  575  0.095073  0.093133  0.093566  0.153513  0.114568         0.478429,\n",
       "  0.09222146128581087),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  572  0.097542  0.096157  0.098834  0.042049  0.118900         0.481726\n",
       "  573  0.096789  0.094802  0.094171  0.178065  0.118196         0.450831\n",
       "  574  0.091918  0.091977  0.092301  0.189568  0.113456         0.495633\n",
       "  575  0.095073  0.093133  0.093566  0.153513  0.114568         0.478429\n",
       "  576  0.092654  0.092105  0.093396  0.135568  0.113433         0.487407,\n",
       "  0.09126041822789495),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  573  0.096789  0.094802  0.094171  0.178065  0.118196         0.450831\n",
       "  574  0.091918  0.091977  0.092301  0.189568  0.113456         0.495633\n",
       "  575  0.095073  0.093133  0.093566  0.153513  0.114568         0.478429\n",
       "  576  0.092654  0.092105  0.093396  0.135568  0.113433         0.487407\n",
       "  577  0.093085  0.091628  0.092658  0.136879  0.113470         0.479934,\n",
       "  0.09223104515867006),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  574  0.091918  0.091977  0.092301  0.189568  0.113456         0.495633\n",
       "  575  0.095073  0.093133  0.093566  0.153513  0.114568         0.478429\n",
       "  576  0.092654  0.092105  0.093396  0.135568  0.113433         0.487407\n",
       "  577  0.093085  0.091628  0.092658  0.136879  0.113470         0.479934\n",
       "  578  0.092368  0.091401  0.092863  0.119086  0.112532         0.494379,\n",
       "  0.09274632266028604),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  575  0.095073  0.093133  0.093566  0.153513  0.114568         0.478429\n",
       "  576  0.092654  0.092105  0.093396  0.135568  0.113433         0.487407\n",
       "  577  0.093085  0.091628  0.092658  0.136879  0.113470         0.479934\n",
       "  578  0.092368  0.091401  0.092863  0.119086  0.112532         0.494379\n",
       "  579  0.093427  0.091877  0.094205  0.099703  0.113480         0.490974,\n",
       "  0.09315613907100172),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  576  0.092654  0.092105  0.093396  0.135568  0.113433         0.487407\n",
       "  577  0.093085  0.091628  0.092658  0.136879  0.113470         0.479934\n",
       "  578  0.092368  0.091401  0.092863  0.119086  0.112532         0.494379\n",
       "  579  0.093427  0.091877  0.094205  0.099703  0.113480         0.490974\n",
       "  580  0.093321  0.092579  0.094116  0.108150  0.113983         0.490185,\n",
       "  0.09170140147012762),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  577  0.093085  0.091628  0.092658  0.136879  0.113470         0.479934\n",
       "  578  0.092368  0.091401  0.092863  0.119086  0.112532         0.494379\n",
       "  579  0.093427  0.091877  0.094205  0.099703  0.113480         0.490974\n",
       "  580  0.093321  0.092579  0.094116  0.108150  0.113983         0.490185\n",
       "  581  0.094108  0.092437  0.093607  0.113355  0.114383         0.476243,\n",
       "  0.09205369539896355),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  578  0.092368  0.091401  0.092863  0.119086  0.112532         0.494379\n",
       "  579  0.093427  0.091877  0.094205  0.099703  0.113480         0.490974\n",
       "  580  0.093321  0.092579  0.094116  0.108150  0.113983         0.490185\n",
       "  581  0.094108  0.092437  0.093607  0.113355  0.114383         0.476243\n",
       "  582  0.092113  0.091678  0.093021  0.096587  0.112962         0.489755,\n",
       "  0.0920105583487349),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  579  0.093427  0.091877  0.094205  0.099703  0.113480         0.490974\n",
       "  580  0.093321  0.092579  0.094116  0.108150  0.113983         0.490185\n",
       "  581  0.094108  0.092437  0.093607  0.113355  0.114383         0.476243\n",
       "  582  0.092113  0.091678  0.093021  0.096587  0.112962         0.489755\n",
       "  583  0.092830  0.091749  0.093941  0.079281  0.113306         0.486798,\n",
       "  0.09149529046948121),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  580  0.093321  0.092579  0.094116  0.108150  0.113983         0.490185\n",
       "  581  0.094108  0.092437  0.093607  0.113355  0.114383         0.476243\n",
       "  582  0.092113  0.091678  0.093021  0.096587  0.112962         0.489755\n",
       "  583  0.092830  0.091749  0.093941  0.079281  0.113306         0.486798\n",
       "  584  0.092572  0.091147  0.093089  0.101600  0.113264         0.483268,\n",
       "  0.090826632512669),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  581  0.094108  0.092437  0.093607  0.113355  0.114383         0.476243\n",
       "  582  0.092113  0.091678  0.093021  0.096587  0.112962         0.489755\n",
       "  583  0.092830  0.091749  0.093941  0.079281  0.113306         0.486798\n",
       "  584  0.092572  0.091147  0.093089  0.101600  0.113264         0.483268\n",
       "  585  0.092774  0.090944  0.092299  0.113302  0.112761         0.482121,\n",
       "  0.09157677263350894),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  582  0.092113  0.091678  0.093021  0.096587  0.112962         0.489755\n",
       "  583  0.092830  0.091749  0.093941  0.079281  0.113306         0.486798\n",
       "  584  0.092572  0.091147  0.093089  0.101600  0.113264         0.483268\n",
       "  585  0.092774  0.090944  0.092299  0.113302  0.112761         0.482121\n",
       "  586  0.091706  0.090529  0.092357  0.115413  0.112108         0.492730,\n",
       "  0.09114059095006818),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  583  0.092830  0.091749  0.093941  0.079281  0.113306         0.486798\n",
       "  584  0.092572  0.091147  0.093089  0.101600  0.113264         0.483268\n",
       "  585  0.092774  0.090944  0.092299  0.113302  0.112761         0.482121\n",
       "  586  0.091706  0.090529  0.092357  0.115413  0.112108         0.492730\n",
       "  587  0.092281  0.090614  0.092982  0.126162  0.112841         0.483859,\n",
       "  0.08694653038014566),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  584  0.092572  0.091147  0.093089  0.101600  0.113264         0.483268\n",
       "  585  0.092774  0.090944  0.092299  0.113302  0.112761         0.482121\n",
       "  586  0.091706  0.090529  0.092357  0.115413  0.112108         0.492730\n",
       "  587  0.092281  0.090614  0.092982  0.126162  0.112841         0.483859\n",
       "  588  0.092250  0.090290  0.088351  0.194895  0.112415         0.455759,\n",
       "  0.08834374549914004),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  585  0.092774  0.090944  0.092299  0.113302  0.112761         0.482121\n",
       "  586  0.091706  0.090529  0.092357  0.115413  0.112108         0.492730\n",
       "  587  0.092281  0.090614  0.092982  0.126162  0.112841         0.483859\n",
       "  588  0.092250  0.090290  0.088351  0.194895  0.112415         0.455759\n",
       "  589  0.087247  0.087107  0.087813  0.160122  0.108319         0.497568,\n",
       "  0.08697049006229363),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  586  0.091706  0.090529  0.092357  0.115413  0.112108         0.492730\n",
       "  587  0.092281  0.090614  0.092982  0.126162  0.112841         0.483859\n",
       "  588  0.092250  0.090290  0.088351  0.194895  0.112415         0.455759\n",
       "  589  0.087247  0.087107  0.087813  0.160122  0.108319         0.497568\n",
       "  590  0.089239  0.087626  0.088741  0.106144  0.109683         0.476852,\n",
       "  0.08536476797443808),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  587  0.092281  0.090614  0.092982  0.126162  0.112841         0.483859\n",
       "  588  0.092250  0.090290  0.088351  0.194895  0.112415         0.455759\n",
       "  589  0.087247  0.087107  0.087813  0.160122  0.108319         0.497568\n",
       "  590  0.089239  0.087626  0.088741  0.106144  0.109683         0.476852\n",
       "  591  0.087839  0.086145  0.086321  0.169032  0.108342         0.475114,\n",
       "  0.08521617945567143),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  588  0.092250  0.090290  0.088351  0.194895  0.112415         0.455759\n",
       "  589  0.087247  0.087107  0.087813  0.160122  0.108319         0.497568\n",
       "  590  0.089239  0.087626  0.088741  0.106144  0.109683         0.476852\n",
       "  591  0.087839  0.086145  0.086321  0.169032  0.108342         0.475114\n",
       "  592  0.086231  0.084578  0.086416  0.096913  0.106774         0.486009,\n",
       "  0.08568351834826685),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  589  0.087247  0.087107  0.087813  0.160122  0.108319         0.497568\n",
       "  590  0.089239  0.087626  0.088741  0.106144  0.109683         0.476852\n",
       "  591  0.087839  0.086145  0.086321  0.169032  0.108342         0.475114\n",
       "  592  0.086231  0.084578  0.086416  0.096913  0.106774         0.486009\n",
       "  593  0.085995  0.084775  0.086866  0.050358  0.106629         0.490615,\n",
       "  0.08819994891680298),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  590  0.089239  0.087626  0.088741  0.106144  0.109683         0.476852\n",
       "  591  0.087839  0.086145  0.086321  0.169032  0.108342         0.475114\n",
       "  592  0.086231  0.084578  0.086416  0.096913  0.106774         0.486009\n",
       "  593  0.085995  0.084775  0.086866  0.050358  0.106629         0.490615\n",
       "  594  0.086833  0.086815  0.087959  0.102828  0.107085         0.505938,\n",
       "  0.08688900789826591),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  591  0.087839  0.086145  0.086321  0.169032  0.108342         0.475114\n",
       "  592  0.086231  0.084578  0.086416  0.096913  0.106774         0.486009\n",
       "  593  0.085995  0.084775  0.086866  0.050358  0.106629         0.490615\n",
       "  594  0.086833  0.086815  0.087959  0.102828  0.107085         0.505938\n",
       "  595  0.088546  0.086934  0.088722  0.075700  0.109543         0.477318,\n",
       "  0.08639051179651583),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  592  0.086231  0.084578  0.086416  0.096913  0.106774         0.486009\n",
       "  593  0.085995  0.084775  0.086866  0.050358  0.106629         0.490615\n",
       "  594  0.086833  0.086815  0.087959  0.102828  0.107085         0.505938\n",
       "  595  0.088546  0.086934  0.088722  0.075700  0.109543         0.477318\n",
       "  596  0.087338  0.085960  0.088143  0.062152  0.108263         0.483393,\n",
       "  0.0880945070706273),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  593  0.085995  0.084775  0.086866  0.050358  0.106629         0.490615\n",
       "  594  0.086833  0.086815  0.087959  0.102828  0.107085         0.505938\n",
       "  595  0.088546  0.086934  0.088722  0.075700  0.109543         0.477318\n",
       "  596  0.087338  0.085960  0.088143  0.062152  0.108263         0.483393\n",
       "  597  0.088277  0.088849  0.088445  0.187352  0.107776         0.499862,\n",
       "  0.09184998998889425),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  594  0.086833  0.086815  0.087959  0.102828  0.107085         0.505938\n",
       "  595  0.088546  0.086934  0.088722  0.075700  0.109543         0.477318\n",
       "  596  0.087338  0.085960  0.088143  0.062152  0.108263         0.483393\n",
       "  597  0.088277  0.088849  0.088445  0.187352  0.107776         0.499862\n",
       "  598  0.088999  0.090576  0.090199  0.191757  0.109440         0.515203,\n",
       "  0.0927918556787295),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  595  0.088546  0.086934  0.088722  0.075700  0.109543         0.477318\n",
       "  596  0.087338  0.085960  0.088143  0.062152  0.108263         0.483393\n",
       "  597  0.088277  0.088849  0.088445  0.187352  0.107776         0.499862\n",
       "  598  0.088999  0.090576  0.090199  0.191757  0.109440         0.515203\n",
       "  599  0.092247  0.091418  0.092563  0.167057  0.113107         0.494164,\n",
       "  0.09192668021649236),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  596  0.087338  0.085960  0.088143  0.062152  0.108263         0.483393\n",
       "  597  0.088277  0.088849  0.088445  0.187352  0.107776         0.499862\n",
       "  598  0.088999  0.090576  0.090199  0.191757  0.109440         0.515203\n",
       "  599  0.092247  0.091418  0.092563  0.167057  0.113107         0.494164\n",
       "  600  0.093496  0.091979  0.093554  0.115547  0.114027         0.480651,\n",
       "  0.09238683120444341),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  597  0.088277  0.088849  0.088445  0.187352  0.107776         0.499862\n",
       "  598  0.088999  0.090576  0.090199  0.191757  0.109440         0.515203\n",
       "  599  0.092247  0.091418  0.092563  0.167057  0.113107         0.494164\n",
       "  600  0.093496  0.091979  0.093554  0.115547  0.114027         0.480651\n",
       "  601  0.092957  0.091595  0.093312  0.130798  0.113182         0.490561,\n",
       "  0.09333588479892303),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  598  0.088999  0.090576  0.090199  0.191757  0.109440         0.515203\n",
       "  599  0.092247  0.091418  0.092563  0.167057  0.113107         0.494164\n",
       "  600  0.093496  0.091979  0.093554  0.115547  0.114027         0.480651\n",
       "  601  0.092957  0.091595  0.093312  0.130798  0.113182         0.490561\n",
       "  602  0.092712  0.092851  0.093515  0.145078  0.113632         0.494217,\n",
       "  0.09579480250794173),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  599  0.092247  0.091418  0.092563  0.167057  0.113107         0.494164\n",
       "  600  0.093496  0.091979  0.093554  0.115547  0.114027         0.480651\n",
       "  601  0.092957  0.091595  0.093312  0.130798  0.113182         0.490561\n",
       "  602  0.092712  0.092851  0.093515  0.145078  0.113632         0.494217\n",
       "  603  0.095785  0.094795  0.096940  0.175232  0.114559         0.505508,\n",
       "  0.09706021050803532),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  600  0.093496  0.091979  0.093554  0.115547  0.114027         0.480651\n",
       "  601  0.092957  0.091595  0.093312  0.130798  0.113182         0.490561\n",
       "  602  0.092712  0.092851  0.093515  0.145078  0.113632         0.494217\n",
       "  603  0.095785  0.094795  0.096940  0.175232  0.114559         0.505508\n",
       "  604  0.096938  0.095565  0.097865  0.108719  0.116960         0.496583,\n",
       "  0.09688765268475841),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  601  0.092957  0.091595  0.093312  0.130798  0.113182         0.490561\n",
       "  602  0.092712  0.092851  0.093515  0.145078  0.113632         0.494217\n",
       "  603  0.095785  0.094795  0.096940  0.175232  0.114559         0.505508\n",
       "  604  0.096938  0.095565  0.097865  0.108719  0.116960         0.496583\n",
       "  605  0.098276  0.097461  0.098665  0.184005  0.118196         0.485830,\n",
       "  0.09521003230573433),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  602  0.092712  0.092851  0.093515  0.145078  0.113632         0.494217\n",
       "  603  0.095785  0.094795  0.096940  0.175232  0.114559         0.505508\n",
       "  604  0.096938  0.095565  0.097865  0.108719  0.116960         0.496583\n",
       "  605  0.098276  0.097461  0.098665  0.184005  0.118196         0.485830\n",
       "  606  0.097275  0.096221  0.096504  0.162849  0.118027         0.474576,\n",
       "  0.09294523613392572),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  603  0.095785  0.094795  0.096940  0.175232  0.114559         0.505508\n",
       "  604  0.096938  0.095565  0.097865  0.108719  0.116960         0.496583\n",
       "  605  0.098276  0.097461  0.098665  0.184005  0.118196         0.485830\n",
       "  606  0.097275  0.096221  0.096504  0.162849  0.118027         0.474576\n",
       "  607  0.095140  0.093546  0.094821  0.144190  0.116389         0.470185,\n",
       "  0.09340538712187677),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  604  0.096938  0.095565  0.097865  0.108719  0.116960         0.496583\n",
       "  605  0.098276  0.097461  0.098665  0.184005  0.118196         0.485830\n",
       "  606  0.097275  0.096221  0.096504  0.162849  0.118027         0.474576\n",
       "  607  0.095140  0.093546  0.094821  0.144190  0.116389         0.470185\n",
       "  608  0.093566  0.092190  0.094653  0.120993  0.114177         0.490561,\n",
       "  0.09127001172311644),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  605  0.098276  0.097461  0.098665  0.184005  0.118196         0.485830\n",
       "  606  0.097275  0.096221  0.096504  0.162849  0.118027         0.474576\n",
       "  607  0.095140  0.093546  0.094821  0.144190  0.116389         0.470185\n",
       "  608  0.093566  0.092190  0.094653  0.120993  0.114177         0.490561\n",
       "  609  0.094654  0.092638  0.092658  0.131355  0.114626         0.471153,\n",
       "  0.09084101794432009),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  606  0.097275  0.096221  0.096504  0.162849  0.118027         0.474576\n",
       "  607  0.095140  0.093546  0.094821  0.144190  0.116389         0.470185\n",
       "  608  0.093566  0.092190  0.094653  0.120993  0.114177         0.490561\n",
       "  609  0.094654  0.092638  0.092658  0.131355  0.114626         0.471153\n",
       "  610  0.091836  0.090766  0.092364  0.113385  0.112541         0.483913,\n",
       "  0.09563902608453068),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  607  0.095140  0.093546  0.094821  0.144190  0.116389         0.470185\n",
       "  608  0.093566  0.092190  0.094653  0.120993  0.114177         0.490561\n",
       "  609  0.094654  0.092638  0.092658  0.131355  0.114626         0.471153\n",
       "  610  0.091836  0.090766  0.092364  0.113385  0.112541         0.483913\n",
       "  611  0.092717  0.094271  0.093818  0.147287  0.112122         0.522998,\n",
       "  0.09300755055223507),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  608  0.093566  0.092190  0.094653  0.120993  0.114177         0.490561\n",
       "  609  0.094654  0.092638  0.092658  0.131355  0.114626         0.471153\n",
       "  610  0.091836  0.090766  0.092364  0.113385  0.112541         0.483913\n",
       "  611  0.092717  0.094271  0.093818  0.147287  0.112122         0.522998\n",
       "  612  0.096700  0.095446  0.094392  0.154807  0.116808         0.467443,\n",
       "  0.09486491665918047),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  609  0.094654  0.092638  0.092658  0.131355  0.114626         0.471153\n",
       "  610  0.091836  0.090766  0.092364  0.113385  0.112541         0.483913\n",
       "  611  0.092717  0.094271  0.093818  0.147287  0.112122         0.522998\n",
       "  612  0.096700  0.095446  0.094392  0.154807  0.116808         0.467443\n",
       "  613  0.094276  0.093882  0.094724  0.133597  0.114238         0.501009,\n",
       "  0.0940572636788231),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  610  0.091836  0.090766  0.092364  0.113385  0.112541         0.483913\n",
       "  611  0.092717  0.094271  0.093818  0.147287  0.112122         0.522998\n",
       "  612  0.096700  0.095446  0.094392  0.154807  0.116808         0.467443\n",
       "  613  0.094276  0.093882  0.094724  0.133597  0.114238         0.501009\n",
       "  614  0.095978  0.094084  0.095564  0.116065  0.116052         0.481081,\n",
       "  0.09450063326690825),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  611  0.092717  0.094271  0.093818  0.147287  0.112122         0.522998\n",
       "  612  0.096700  0.095446  0.094392  0.154807  0.116808         0.467443\n",
       "  613  0.094276  0.093882  0.094724  0.133597  0.114238         0.501009\n",
       "  614  0.095978  0.094084  0.095564  0.116065  0.116052         0.481081\n",
       "  615  0.095015  0.093906  0.095739  0.101588  0.115263         0.490436,\n",
       "  0.09622140031852328),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  612  0.096700  0.095446  0.094392  0.154807  0.116808         0.467443\n",
       "  613  0.094276  0.093882  0.094724  0.133597  0.114238         0.501009\n",
       "  614  0.095978  0.094084  0.095564  0.116065  0.116052         0.481081\n",
       "  615  0.095015  0.093906  0.095739  0.101588  0.115263         0.490436\n",
       "  616  0.095614  0.095565  0.096778  0.296797  0.115696         0.499988,\n",
       "  0.09148810256483682),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  613  0.094276  0.093882  0.094724  0.133597  0.114238         0.501009\n",
       "  614  0.095978  0.094084  0.095564  0.116065  0.116052         0.481081\n",
       "  615  0.095015  0.093906  0.095739  0.101588  0.115263         0.490436\n",
       "  616  0.095614  0.095565  0.096778  0.296797  0.115696         0.499988\n",
       "  617  0.097903  0.095804  0.093391  0.380776  0.117377         0.451727,\n",
       "  0.09142818411474228),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  614  0.095978  0.094084  0.095564  0.116065  0.116052         0.481081\n",
       "  615  0.095015  0.093906  0.095739  0.101588  0.115263         0.490436\n",
       "  616  0.095614  0.095565  0.096778  0.296797  0.115696         0.499988\n",
       "  617  0.097903  0.095804  0.093391  0.380776  0.117377         0.451727\n",
       "  618  0.092274  0.090825  0.092490  0.152813  0.112754         0.486673,\n",
       "  0.08799145157030411),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  615  0.095015  0.093906  0.095739  0.101588  0.115263         0.490436\n",
       "  616  0.095614  0.095565  0.096778  0.296797  0.115696         0.499988\n",
       "  617  0.097903  0.095804  0.093391  0.380776  0.117377         0.451727\n",
       "  618  0.092274  0.090825  0.092490  0.152813  0.112754         0.486673\n",
       "  619  0.091044  0.089166  0.089676  0.175200  0.112696         0.461422,\n",
       "  0.08902677926524104),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  616  0.095614  0.095565  0.096778  0.296797  0.115696         0.499988\n",
       "  617  0.097903  0.095804  0.093391  0.380776  0.117377         0.451727\n",
       "  618  0.092274  0.090825  0.092490  0.152813  0.112754         0.486673\n",
       "  619  0.091044  0.089166  0.089676  0.175200  0.112696         0.461422\n",
       "  620  0.088782  0.088052  0.089935  0.129350  0.109339         0.494862,\n",
       "  0.0886744853364051),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  617  0.097903  0.095804  0.093391  0.380776  0.117377         0.451727\n",
       "  618  0.092274  0.090825  0.092490  0.152813  0.112754         0.486673\n",
       "  619  0.091044  0.089166  0.089676  0.175200  0.112696         0.461422\n",
       "  620  0.088782  0.088052  0.089935  0.129350  0.109339         0.494862\n",
       "  621  0.090159  0.088301  0.090303  0.100111  0.110350         0.484486,\n",
       "  0.08891893182848824),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  618  0.092274  0.090825  0.092490  0.152813  0.112754         0.486673\n",
       "  619  0.091044  0.089166  0.089676  0.175200  0.112696         0.461422\n",
       "  620  0.088782  0.088052  0.089935  0.129350  0.109339         0.494862\n",
       "  621  0.090159  0.088301  0.090303  0.100111  0.110350         0.484486\n",
       "  622  0.088770  0.088159  0.089424  0.099063  0.110006         0.488948,\n",
       "  0.08672843953842528),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  619  0.091044  0.089166  0.089676  0.175200  0.112696         0.461422\n",
       "  620  0.088782  0.088052  0.089935  0.129350  0.109339         0.494862\n",
       "  621  0.090159  0.088301  0.090303  0.100111  0.110350         0.484486\n",
       "  622  0.088770  0.088159  0.089424  0.099063  0.110006         0.488948\n",
       "  623  0.089636  0.087906  0.088540  0.118081  0.110245         0.470741,\n",
       "  0.08586566004440296),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  620  0.088782  0.088052  0.089935  0.129350  0.109339         0.494862\n",
       "  621  0.090159  0.088301  0.090303  0.100111  0.110350         0.484486\n",
       "  622  0.088770  0.088159  0.089424  0.099063  0.110006         0.488948\n",
       "  623  0.089636  0.087906  0.088540  0.118081  0.110245         0.470741\n",
       "  624  0.086996  0.085467  0.086970  0.139873  0.108106         0.480669,\n",
       "  0.08605978158161307),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  621  0.090159  0.088301  0.090303  0.100111  0.110350         0.484486\n",
       "  622  0.088770  0.088159  0.089424  0.099063  0.110006         0.488948\n",
       "  623  0.089636  0.087906  0.088540  0.118081  0.110245         0.470741\n",
       "  624  0.086996  0.085467  0.086970  0.139873  0.108106         0.480669\n",
       "  625  0.086628  0.085559  0.087513  0.098720  0.107263         0.488572,\n",
       "  0.0876966705009856),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  622  0.088770  0.088159  0.089424  0.099063  0.110006         0.488948\n",
       "  623  0.089636  0.087906  0.088540  0.118081  0.110245         0.470741\n",
       "  624  0.086996  0.085467  0.086970  0.139873  0.108106         0.480669\n",
       "  625  0.086628  0.085559  0.087513  0.098720  0.107263         0.488572\n",
       "  626  0.086833  0.087064  0.087988  0.138518  0.107453         0.499361,\n",
       "  0.08662778000631689),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  623  0.089636  0.087906  0.088540  0.118081  0.110245         0.470741\n",
       "  624  0.086996  0.085467  0.086970  0.139873  0.108106         0.480669\n",
       "  625  0.086628  0.085559  0.087513  0.098720  0.107263         0.488572\n",
       "  626  0.086833  0.087064  0.087988  0.138518  0.107453         0.499361\n",
       "  627  0.088277  0.086353  0.088351  0.098229  0.109051         0.479128,\n",
       "  0.08865770393653921),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  624  0.086996  0.085467  0.086970  0.139873  0.108106         0.480669\n",
       "  625  0.086628  0.085559  0.087513  0.098720  0.107263         0.488572\n",
       "  626  0.086833  0.087064  0.087988  0.138518  0.107453         0.499361\n",
       "  627  0.088277  0.086353  0.088351  0.098229  0.109051         0.479128\n",
       "  628  0.087506  0.087306  0.088676  0.150149  0.108007         0.502300,\n",
       "  0.08917537740636998),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  625  0.086628  0.085559  0.087513  0.098720  0.107263         0.488572\n",
       "  626  0.086833  0.087064  0.087988  0.138518  0.107453         0.499361\n",
       "  627  0.088277  0.086353  0.088351  0.098229  0.109051         0.479128\n",
       "  628  0.087506  0.087306  0.088676  0.150149  0.108007         0.502300\n",
       "  629  0.088871  0.087922  0.089317  0.098842  0.109990         0.490992,\n",
       "  0.09007650201419137),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  626  0.086833  0.087064  0.087988  0.138518  0.107453         0.499361\n",
       "  627  0.088277  0.086353  0.088351  0.098229  0.109051         0.479128\n",
       "  628  0.087506  0.087306  0.088676  0.150149  0.108007         0.502300\n",
       "  629  0.088871  0.087922  0.089317  0.098842  0.109990         0.490992\n",
       "  630  0.089201  0.089327  0.090051  0.137102  0.110495         0.493859,\n",
       "  0.09007409642361427),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  627  0.088277  0.086353  0.088351  0.098229  0.109051         0.479128\n",
       "  628  0.087506  0.087306  0.088676  0.150149  0.108007         0.502300\n",
       "  629  0.088871  0.087922  0.089317  0.098842  0.109990         0.490992\n",
       "  630  0.089201  0.089327  0.090051  0.137102  0.110495         0.493859\n",
       "  631  0.091331  0.090645  0.091662  0.139615  0.111375         0.487103,\n",
       "  0.08882067788695695),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  628  0.087506  0.087306  0.088676  0.150149  0.108007         0.502300\n",
       "  629  0.088871  0.087922  0.089317  0.098842  0.109990         0.490992\n",
       "  630  0.089201  0.089327  0.090051  0.137102  0.110495         0.493859\n",
       "  631  0.091331  0.090645  0.091662  0.139615  0.111375         0.487103\n",
       "  632  0.090623  0.088917  0.090066  0.094342  0.111373         0.477748,\n",
       "  0.08745700632296974),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  629  0.088871  0.087922  0.089317  0.098842  0.109990         0.490992\n",
       "  630  0.089201  0.089327  0.090051  0.137102  0.110495         0.493859\n",
       "  631  0.091331  0.090645  0.091662  0.139615  0.111375         0.487103\n",
       "  632  0.090623  0.088917  0.090066  0.094342  0.111373         0.477748\n",
       "  633  0.089920  0.088515  0.089085  0.096465  0.110149         0.476923,\n",
       "  0.08358409209509081),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  630  0.089201  0.089327  0.090051  0.137102  0.110495         0.493859\n",
       "  631  0.091331  0.090645  0.091662  0.139615  0.111375         0.487103\n",
       "  632  0.090623  0.088917  0.090066  0.094342  0.111373         0.477748\n",
       "  633  0.089920  0.088515  0.089085  0.096465  0.110149         0.476923\n",
       "  634  0.085629  0.084853  0.085069  0.226423  0.108817         0.458160,\n",
       "  0.08374705642314623),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  631  0.091331  0.090645  0.091662  0.139615  0.111375         0.487103\n",
       "  632  0.090623  0.088917  0.090066  0.094342  0.111373         0.477748\n",
       "  633  0.089920  0.088515  0.089085  0.096465  0.110149         0.476923\n",
       "  634  0.085629  0.084853  0.085069  0.226423  0.108817         0.458160\n",
       "  635  0.084525  0.083401  0.084069  0.195276  0.105035         0.488339,\n",
       "  0.08345467132204253),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  632  0.090623  0.088917  0.090066  0.094342  0.111373         0.477748\n",
       "  633  0.089920  0.088515  0.089085  0.096465  0.110149         0.476923\n",
       "  634  0.085629  0.084853  0.085069  0.226423  0.108817         0.458160\n",
       "  635  0.084525  0.083401  0.084069  0.195276  0.105035         0.488339\n",
       "  636  0.082662  0.083104  0.083332  0.211157  0.105194         0.484934,\n",
       "  0.08116590584572365),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  633  0.089920  0.088515  0.089085  0.096465  0.110149         0.476923\n",
       "  634  0.085629  0.084853  0.085069  0.226423  0.108817         0.458160\n",
       "  635  0.084525  0.083401  0.084069  0.195276  0.105035         0.488339\n",
       "  636  0.082662  0.083104  0.083332  0.211157  0.105194         0.484934\n",
       "  637  0.083249  0.082223  0.083012  0.160039  0.104909         0.470006,\n",
       "  0.08170994458827952),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  634  0.085629  0.084853  0.085069  0.226423  0.108817         0.458160\n",
       "  635  0.084525  0.083401  0.084069  0.195276  0.105035         0.488339\n",
       "  636  0.082662  0.083104  0.083332  0.211157  0.105194         0.484934\n",
       "  637  0.083249  0.082223  0.083012  0.160039  0.104909         0.470006\n",
       "  638  0.081302  0.081464  0.082606  0.154481  0.102673         0.491189,\n",
       "  0.08568831028469644),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  635  0.084525  0.083401  0.084069  0.195276  0.105035         0.488339\n",
       "  636  0.082662  0.083104  0.083332  0.211157  0.105194         0.484934\n",
       "  637  0.083249  0.082223  0.083012  0.160039  0.104909         0.470006\n",
       "  638  0.081302  0.081464  0.082606  0.154481  0.102673         0.491189\n",
       "  639  0.083817  0.084664  0.085120  0.183149  0.103205         0.516869,\n",
       "  0.08523055526496022),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  636  0.082662  0.083104  0.083332  0.211157  0.105194         0.484934\n",
       "  637  0.083249  0.082223  0.083012  0.160039  0.104909         0.470006\n",
       "  638  0.081302  0.081464  0.082606  0.154481  0.102673         0.491189\n",
       "  639  0.083817  0.084664  0.085120  0.183149  0.103205         0.516869\n",
       "  640  0.087480  0.085645  0.086793  0.158835  0.107090         0.483698,\n",
       "  0.08501006845502504),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  637  0.083249  0.082223  0.083012  0.160039  0.104909         0.470006\n",
       "  638  0.081302  0.081464  0.082606  0.154481  0.102673         0.491189\n",
       "  639  0.083817  0.084664  0.085120  0.183149  0.103205         0.516869\n",
       "  640  0.087480  0.085645  0.086793  0.158835  0.107090         0.483698\n",
       "  641  0.086645  0.086204  0.086747  0.130345  0.106643         0.485472,\n",
       "  0.08458826258087307),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  638  0.081302  0.081464  0.082606  0.154481  0.102673         0.491189\n",
       "  639  0.083817  0.084664  0.085120  0.183149  0.103205         0.516869\n",
       "  640  0.087480  0.085645  0.086793  0.158835  0.107090         0.483698\n",
       "  641  0.086645  0.086204  0.086747  0.130345  0.106643         0.485472\n",
       "  642  0.086351  0.084521  0.085779  0.120978  0.106428         0.483966,\n",
       "  0.08501725635966942),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  639  0.083817  0.084664  0.085120  0.183149  0.103205         0.516869\n",
       "  640  0.087480  0.085645  0.086793  0.158835  0.107090         0.483698\n",
       "  641  0.086645  0.086204  0.086747  0.130345  0.106643         0.485472\n",
       "  642  0.086351  0.084521  0.085779  0.120978  0.106428         0.483966\n",
       "  643  0.085044  0.083775  0.086023  0.084194  0.106016         0.490328,\n",
       "  0.0822084310676673),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  640  0.087480  0.085645  0.086793  0.158835  0.107090         0.483698\n",
       "  641  0.086645  0.086204  0.086747  0.130345  0.106643         0.485472\n",
       "  642  0.086351  0.084521  0.085779  0.120978  0.106428         0.483966\n",
       "  643  0.085044  0.083775  0.086023  0.084194  0.106016         0.490328\n",
       "  644  0.084452  0.082988  0.084018  0.155021  0.106435         0.466117,\n",
       "  0.08339954480837758),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  641  0.086645  0.086204  0.086747  0.130345  0.106643         0.485472\n",
       "  642  0.086351  0.084521  0.085779  0.120978  0.106428         0.483966\n",
       "  643  0.085044  0.083775  0.086023  0.084194  0.106016         0.490328\n",
       "  644  0.084452  0.082988  0.084018  0.155021  0.106435         0.466117\n",
       "  645  0.082797  0.082213  0.083090  0.194902  0.103692         0.496027,\n",
       "  0.08296575909315163),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  642  0.086351  0.084521  0.085779  0.120978  0.106428         0.483966\n",
       "  643  0.085044  0.083775  0.086023  0.084194  0.106016         0.490328\n",
       "  644  0.084452  0.082988  0.084018  0.155021  0.106435         0.466117\n",
       "  645  0.082797  0.082213  0.083090  0.194902  0.103692         0.496027\n",
       "  646  0.083911  0.082488  0.084287  0.095840  0.104855         0.483877,\n",
       "  0.08168597528376922),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  643  0.085044  0.083775  0.086023  0.084194  0.106016         0.490328\n",
       "  644  0.084452  0.082988  0.084018  0.155021  0.106435         0.466117\n",
       "  645  0.082797  0.082213  0.083090  0.194902  0.103692         0.496027\n",
       "  646  0.083911  0.082488  0.084287  0.095840  0.104855         0.483877\n",
       "  647  0.083379  0.081749  0.083306  0.137552  0.104431         0.477551,\n",
       "  0.08321500714402667),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  644  0.084452  0.082988  0.084018  0.155021  0.106435         0.466117\n",
       "  645  0.082797  0.082213  0.083090  0.194902  0.103692         0.496027\n",
       "  646  0.083911  0.082488  0.084287  0.095840  0.104855         0.483877\n",
       "  647  0.083379  0.081749  0.083306  0.137552  0.104431         0.477551\n",
       "  648  0.082802  0.082175  0.083485  0.126330  0.103181         0.498554,\n",
       "  0.08274766825143125),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  645  0.082797  0.082213  0.083090  0.194902  0.103692         0.496027\n",
       "  646  0.083911  0.082488  0.084287  0.095840  0.104855         0.483877\n",
       "  647  0.083379  0.081749  0.083306  0.137552  0.104431         0.477551\n",
       "  648  0.082802  0.082175  0.083485  0.126330  0.103181         0.498554\n",
       "  649  0.083413  0.081962  0.084204  0.083126  0.104675         0.483626,\n",
       "  0.0854486461066806),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  646  0.083911  0.082488  0.084287  0.095840  0.104855         0.483877\n",
       "  647  0.083379  0.081749  0.083306  0.137552  0.104431         0.477551\n",
       "  648  0.082802  0.082175  0.083485  0.126330  0.103181         0.498554\n",
       "  649  0.083413  0.081962  0.084204  0.083126  0.104675         0.483626\n",
       "  650  0.083295  0.084088  0.084594  0.140938  0.104218         0.507318,\n",
       "  0.08676438868400956),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  647  0.083379  0.081749  0.083306  0.137552  0.104431         0.477551\n",
       "  648  0.082802  0.082175  0.083485  0.126330  0.103181         0.498554\n",
       "  649  0.083413  0.081962  0.084204  0.083126  0.104675         0.483626\n",
       "  650  0.083295  0.084088  0.084594  0.140938  0.104218         0.507318\n",
       "  651  0.085776  0.085415  0.086367  0.138078  0.106856         0.496959,\n",
       "  0.0867140541067742),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  648  0.082802  0.082175  0.083485  0.126330  0.103181         0.498554\n",
       "  649  0.083413  0.081962  0.084204  0.083126  0.104675         0.483626\n",
       "  650  0.083295  0.084088  0.084594  0.140938  0.104218         0.507318\n",
       "  651  0.085776  0.085415  0.086367  0.138078  0.106856         0.496959\n",
       "  652  0.087182  0.085704  0.087469  0.099905  0.108141         0.486744,\n",
       "  0.08747377810047331),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  649  0.083413  0.081962  0.084204  0.083126  0.104675         0.483626\n",
       "  650  0.083295  0.084088  0.084594  0.140938  0.104218         0.507318\n",
       "  651  0.085776  0.085415  0.086367  0.138078  0.106856         0.496959\n",
       "  652  0.087182  0.085704  0.087469  0.099905  0.108141         0.486744\n",
       "  653  0.086965  0.086086  0.087116  0.114474  0.108092         0.492802,\n",
       "  0.08714304788557056),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  650  0.083295  0.084088  0.084594  0.140938  0.104218         0.507318\n",
       "  651  0.085776  0.085415  0.086367  0.138078  0.106856         0.496959\n",
       "  652  0.087182  0.085704  0.087469  0.099905  0.108141         0.486744\n",
       "  653  0.086965  0.086086  0.087116  0.114474  0.108092         0.492802\n",
       "  654  0.087687  0.086140  0.088259  0.090903  0.108834         0.484647,\n",
       "  0.08672604357021049),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  651  0.085776  0.085415  0.086367  0.138078  0.106856         0.496959\n",
       "  652  0.087182  0.085704  0.087469  0.099905  0.108141         0.486744\n",
       "  653  0.086965  0.086086  0.087116  0.114474  0.108092         0.492802\n",
       "  654  0.087687  0.086140  0.088259  0.090903  0.108834         0.484647\n",
       "  655  0.087283  0.086190  0.088213  0.111587  0.108511         0.484002,\n",
       "  0.08649596326505382),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  652  0.087182  0.085704  0.087469  0.099905  0.108141         0.486744\n",
       "  653  0.086965  0.086086  0.087116  0.114474  0.108092         0.492802\n",
       "  654  0.087687  0.086140  0.088259  0.090903  0.108834         0.484647\n",
       "  655  0.087283  0.086190  0.088213  0.111587  0.108511         0.484002\n",
       "  656  0.087928  0.086322  0.086965  0.096918  0.108103         0.485400,\n",
       "  0.0858345028352483),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  653  0.086965  0.086086  0.087116  0.114474  0.108092         0.492802\n",
       "  654  0.087687  0.086140  0.088259  0.090903  0.108834         0.484647\n",
       "  655  0.087283  0.086190  0.088213  0.111587  0.108511         0.484002\n",
       "  656  0.087928  0.086322  0.086965  0.096918  0.108103         0.485400\n",
       "  657  0.087338  0.085706  0.087242  0.082122  0.107879         0.482174,\n",
       "  0.0859231729039204),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  654  0.087687  0.086140  0.088259  0.090903  0.108834         0.484647\n",
       "  655  0.087283  0.086190  0.088213  0.111587  0.108511         0.484002\n",
       "  656  0.087928  0.086322  0.086965  0.096918  0.108103         0.485400\n",
       "  657  0.087338  0.085706  0.087242  0.082122  0.107879         0.482174\n",
       "  658  0.086294  0.084552  0.086268  0.083783  0.107233         0.487784,\n",
       "  0.08929520468419676),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  655  0.087283  0.086190  0.088213  0.111587  0.108511         0.484002\n",
       "  656  0.087928  0.086322  0.086965  0.096918  0.108103         0.485400\n",
       "  657  0.087338  0.085706  0.087242  0.082122  0.107879         0.482174\n",
       "  658  0.086294  0.084552  0.086268  0.083783  0.107233         0.487784\n",
       "  659  0.087807  0.088278  0.089145  0.158050  0.107319         0.512335,\n",
       "  0.08891653586027344),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  656  0.087928  0.086322  0.086965  0.096918  0.108103         0.485400\n",
       "  657  0.087338  0.085706  0.087242  0.082122  0.107879         0.482174\n",
       "  658  0.086294  0.084552  0.086268  0.083783  0.107233         0.487784\n",
       "  659  0.087807  0.088278  0.089145  0.158050  0.107319         0.512335\n",
       "  660  0.089754  0.087981  0.090497  0.091817  0.110612         0.484289,\n",
       "  0.08903397679224773),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  657  0.087338  0.085706  0.087242  0.082122  0.107879         0.482174\n",
       "  658  0.086294  0.084552  0.086268  0.083783  0.107233         0.487784\n",
       "  659  0.087807  0.088278  0.089145  0.158050  0.107319         0.512335\n",
       "  660  0.089754  0.087981  0.090497  0.091817  0.110612         0.484289\n",
       "  661  0.089552  0.087766  0.090507  0.065961  0.110243         0.487999,\n",
       "  0.08824309558939394),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  658  0.086294  0.084552  0.086268  0.083783  0.107233         0.487784\n",
       "  659  0.087807  0.088278  0.089145  0.158050  0.107319         0.512335\n",
       "  660  0.089754  0.087981  0.090497  0.091817  0.110612         0.484289\n",
       "  661  0.089552  0.087766  0.090507  0.065961  0.110243         0.487999\n",
       "  662  0.089956  0.087981  0.089531  0.074385  0.110357         0.481207,\n",
       "  0.08783326955631596),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  659  0.087807  0.088278  0.089145  0.158050  0.107319         0.512335\n",
       "  660  0.089754  0.087981  0.090497  0.091817  0.110612         0.484289\n",
       "  661  0.089552  0.087766  0.090507  0.065961  0.110243         0.487999\n",
       "  662  0.089956  0.087981  0.089531  0.074385  0.110357         0.481207\n",
       "  663  0.088539  0.087458  0.089426  0.072371  0.109585         0.484056,\n",
       "  0.08736113872729094),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  660  0.089754  0.087981  0.090497  0.091817  0.110612         0.484289\n",
       "  661  0.089552  0.087766  0.090507  0.065961  0.110243         0.487999\n",
       "  662  0.089956  0.087981  0.089531  0.074385  0.110357         0.481207\n",
       "  663  0.088539  0.087458  0.089426  0.072371  0.109585         0.484056\n",
       "  664  0.088291  0.087128  0.088809  0.092625  0.109185         0.483590,\n",
       "  0.08804656846160673),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  661  0.089552  0.087766  0.090507  0.065961  0.110243         0.487999\n",
       "  662  0.089956  0.087981  0.089531  0.074385  0.110357         0.481207\n",
       "  663  0.088539  0.087458  0.089426  0.072371  0.109585         0.484056\n",
       "  664  0.088291  0.087128  0.088809  0.092625  0.109185         0.483590\n",
       "  665  0.087795  0.086796  0.088719  0.065769  0.108724         0.492246,\n",
       "  0.08778294460144291),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  662  0.089956  0.087981  0.089531  0.074385  0.110357         0.481207\n",
       "  663  0.088539  0.087458  0.089426  0.072371  0.109585         0.484056\n",
       "  664  0.088291  0.087128  0.088809  0.092625  0.109185         0.483590\n",
       "  665  0.087795  0.086796  0.088719  0.065769  0.108724         0.492246\n",
       "  666  0.088866  0.086979  0.088990  0.067883  0.109393         0.485149,\n",
       "  0.08969543722205325),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  663  0.088539  0.087458  0.089426  0.072371  0.109585         0.484056\n",
       "  664  0.088291  0.087128  0.088809  0.092625  0.109185         0.483590\n",
       "  665  0.087795  0.086796  0.088719  0.065769  0.108724         0.492246\n",
       "  666  0.088866  0.086979  0.088990  0.067883  0.109393         0.485149\n",
       "  667  0.088869  0.088927  0.090151  0.123416  0.109136         0.501422,\n",
       "  0.08934313367085502),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  664  0.088291  0.087128  0.088809  0.092625  0.109185         0.483590\n",
       "  665  0.087795  0.086796  0.088719  0.065769  0.108724         0.492246\n",
       "  666  0.088866  0.086979  0.088990  0.067883  0.109393         0.485149\n",
       "  667  0.088869  0.088927  0.090151  0.123416  0.109136         0.501422\n",
       "  668  0.090154  0.088785  0.090986  0.077984  0.111003         0.484486,\n",
       "  0.09011245115977563),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  665  0.087795  0.086796  0.088719  0.065769  0.108724         0.492246\n",
       "  666  0.088866  0.086979  0.088990  0.067883  0.109393         0.485149\n",
       "  667  0.088869  0.088927  0.090151  0.123416  0.109136         0.501422\n",
       "  668  0.090154  0.088785  0.090986  0.077984  0.111003         0.484486\n",
       "  669  0.089542  0.089617  0.090485  0.137830  0.110659         0.492873,\n",
       "  0.0890675299696172),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  666  0.088866  0.086979  0.088990  0.067883  0.109393         0.485149\n",
       "  667  0.088869  0.088927  0.090151  0.123416  0.109136         0.501422\n",
       "  668  0.090154  0.088785  0.090986  0.077984  0.111003         0.484486\n",
       "  669  0.089542  0.089617  0.090485  0.137830  0.110659         0.492873\n",
       "  670  0.090322  0.090102  0.090499  0.267694  0.111411         0.479307,\n",
       "  0.09166304673396623),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  667  0.088869  0.088927  0.090151  0.123416  0.109136         0.501422\n",
       "  668  0.090154  0.088785  0.090986  0.077984  0.111003         0.484486\n",
       "  669  0.089542  0.089617  0.090485  0.137830  0.110659         0.492873\n",
       "  670  0.090322  0.090102  0.090499  0.267694  0.111411         0.479307\n",
       "  671  0.094178  0.092602  0.093510  0.295712  0.110390         0.506529,\n",
       "  0.09084820584896448),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  668  0.090154  0.088785  0.090986  0.077984  0.111003         0.484486\n",
       "  669  0.089542  0.089617  0.090485  0.137830  0.110659         0.492873\n",
       "  670  0.090322  0.090102  0.090499  0.267694  0.111411         0.479307\n",
       "  671  0.094178  0.092602  0.093510  0.295712  0.110390         0.506529\n",
       "  672  0.091670  0.090825  0.092599  0.137929  0.112925         0.481027,\n",
       "  0.09047673455204785),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  669  0.089542  0.089617  0.090485  0.137830  0.110659         0.492873\n",
       "  670  0.090322  0.090102  0.090499  0.267694  0.111411         0.479307\n",
       "  671  0.094178  0.092602  0.093510  0.295712  0.110390         0.506529\n",
       "  672  0.091670  0.090825  0.092599  0.137929  0.112925         0.481027\n",
       "  673  0.091311  0.089635  0.091943  0.089772  0.112129         0.484343,\n",
       "  0.09058697795701545),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  670  0.090322  0.090102  0.090499  0.267694  0.111411         0.479307\n",
       "  671  0.094178  0.092602  0.093510  0.295712  0.110390         0.506529\n",
       "  672  0.091670  0.090825  0.092599  0.137929  0.112925         0.481027\n",
       "  673  0.091311  0.089635  0.091943  0.089772  0.112129         0.484343\n",
       "  674  0.091646  0.089965  0.092079  0.096390  0.111766         0.487945,\n",
       "  0.09135149388714416),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  671  0.094178  0.092602  0.093510  0.295712  0.110390         0.506529\n",
       "  672  0.091670  0.090825  0.092599  0.137929  0.112925         0.481027\n",
       "  673  0.091311  0.089635  0.091943  0.089772  0.112129         0.484343\n",
       "  674  0.091646  0.089965  0.092079  0.096390  0.111766         0.487945\n",
       "  675  0.091189  0.090695  0.092321  0.100235  0.111874         0.492837,\n",
       "  0.09083142444909859),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  672  0.091670  0.090825  0.092599  0.137929  0.112925         0.481027\n",
       "  673  0.091311  0.089635  0.091943  0.089772  0.112129         0.484343\n",
       "  674  0.091646  0.089965  0.092079  0.096390  0.111766         0.487945\n",
       "  675  0.091189  0.090695  0.092321  0.100235  0.111874         0.492837\n",
       "  676  0.091663  0.090209  0.092616  0.071067  0.112621         0.483232,\n",
       "  0.08900281958309306),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  673  0.091311  0.089635  0.091943  0.089772  0.112129         0.484343\n",
       "  674  0.091646  0.089965  0.092079  0.096390  0.111766         0.487945\n",
       "  675  0.091189  0.090695  0.092321  0.100235  0.111874         0.492837\n",
       "  676  0.091663  0.090209  0.092616  0.071067  0.112621         0.483232\n",
       "  677  0.091441  0.089960  0.090933  0.088470  0.112113         0.473447,\n",
       "  0.08843242519017444),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  674  0.091646  0.089965  0.092079  0.096390  0.111766         0.487945\n",
       "  675  0.091189  0.090695  0.092321  0.100235  0.111874         0.492837\n",
       "  676  0.091663  0.090209  0.092616  0.071067  0.112621         0.483232\n",
       "  677  0.091441  0.089960  0.090933  0.088470  0.112113         0.473447\n",
       "  678  0.089766  0.087936  0.089187  0.088884  0.110327         0.482855,\n",
       "  0.08766072135540134),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  675  0.091189  0.090695  0.092321  0.100235  0.111874         0.492837\n",
       "  676  0.091663  0.090209  0.092616  0.071067  0.112621         0.483232\n",
       "  677  0.091441  0.089960  0.090933  0.088470  0.112113         0.473447\n",
       "  678  0.089766  0.087936  0.089187  0.088884  0.110327         0.482855\n",
       "  679  0.088914  0.087526  0.089564  0.074399  0.109770         0.481350,\n",
       "  0.0894461891711782),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  676  0.091663  0.090209  0.092616  0.071067  0.112621         0.483232\n",
       "  677  0.091441  0.089960  0.090933  0.088470  0.112113         0.473447\n",
       "  678  0.089766  0.087936  0.089187  0.088884  0.110327         0.482855\n",
       "  679  0.088914  0.087526  0.089564  0.074399  0.109770         0.481350\n",
       "  680  0.088329  0.088235  0.089458  0.087327  0.109016         0.500472,\n",
       "  0.08894050516478373),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  677  0.091441  0.089960  0.090933  0.088470  0.112113         0.473447\n",
       "  678  0.089766  0.087936  0.089187  0.088884  0.110327         0.482855\n",
       "  679  0.088914  0.087526  0.089564  0.074399  0.109770         0.481350\n",
       "  680  0.088329  0.088235  0.089458  0.087327  0.109016         0.500472\n",
       "  681  0.089268  0.088417  0.089601  0.095981  0.110760         0.483339,\n",
       "  0.08801780722066688),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  678  0.089766  0.087936  0.089187  0.088884  0.110327         0.482855\n",
       "  679  0.088914  0.087526  0.089564  0.074399  0.109770         0.481350\n",
       "  680  0.088329  0.088235  0.089458  0.087327  0.109016         0.500472\n",
       "  681  0.089268  0.088417  0.089601  0.095981  0.110760         0.483339\n",
       "  682  0.089754  0.087939  0.089744  0.073363  0.110266         0.480221,\n",
       "  0.08790756381569928),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  679  0.088914  0.087526  0.089564  0.074399  0.109770         0.481350\n",
       "  680  0.088329  0.088235  0.089458  0.087327  0.109016         0.500472\n",
       "  681  0.089268  0.088417  0.089601  0.095981  0.110760         0.483339\n",
       "  682  0.089754  0.087939  0.089744  0.073363  0.110266         0.480221\n",
       "  683  0.088308  0.086837  0.089322  0.070566  0.109365         0.486296,\n",
       "  0.08849234364026899),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  680  0.088329  0.088235  0.089458  0.087327  0.109016         0.500472\n",
       "  681  0.089268  0.088417  0.089601  0.095981  0.110760         0.483339\n",
       "  682  0.089754  0.087939  0.089744  0.073363  0.110266         0.480221\n",
       "  683  0.088308  0.086837  0.089322  0.070566  0.109365         0.486296\n",
       "  684  0.088313  0.087680  0.089116  0.094519  0.109257         0.491493,\n",
       "  0.08662778000631689),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  681  0.089268  0.088417  0.089601  0.095981  0.110760         0.483339\n",
       "  682  0.089754  0.087939  0.089744  0.073363  0.110266         0.480221\n",
       "  683  0.088308  0.086837  0.089322  0.070566  0.109365         0.486296\n",
       "  684  0.088313  0.087680  0.089116  0.094519  0.109257         0.491493\n",
       "  685  0.088527  0.087150  0.088424  0.089567  0.109828         0.473178,\n",
       "  0.08789079203819569),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  682  0.089754  0.087939  0.089744  0.073363  0.110266         0.480221\n",
       "  683  0.088308  0.086837  0.089322  0.070566  0.109365         0.486296\n",
       "  684  0.088313  0.087680  0.089116  0.094519  0.109257         0.491493\n",
       "  685  0.088527  0.087150  0.088424  0.089567  0.109828         0.473178\n",
       "  686  0.087273  0.086559  0.088419  0.071514  0.108007         0.496565,\n",
       "  0.08670207426570022),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  683  0.088308  0.086837  0.089322  0.070566  0.109365         0.486296\n",
       "  684  0.088313  0.087680  0.089116  0.094519  0.109257         0.491493\n",
       "  685  0.088527  0.087150  0.088424  0.089567  0.109828         0.473178\n",
       "  686  0.087273  0.086559  0.088419  0.071514  0.108007         0.496565\n",
       "  687  0.088151  0.086680  0.088177  0.094101  0.109241         0.478232,\n",
       "  0.08579615772144923),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  684  0.088313  0.087680  0.089116  0.094519  0.109257         0.491493\n",
       "  685  0.088527  0.087150  0.088424  0.089567  0.109828         0.473178\n",
       "  686  0.087273  0.086559  0.088419  0.071514  0.108007         0.496565\n",
       "  687  0.088151  0.086680  0.088177  0.094101  0.109241         0.478232\n",
       "  688  0.087304  0.085503  0.087549  0.100092  0.108080         0.480346,\n",
       "  0.08929760065241156),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  685  0.088527  0.087150  0.088424  0.089567  0.109828         0.473178\n",
       "  686  0.087273  0.086559  0.088419  0.071514  0.108007         0.496565\n",
       "  687  0.088151  0.086680  0.088177  0.094101  0.109241         0.478232\n",
       "  688  0.087304  0.085503  0.087549  0.100092  0.108080         0.480346\n",
       "  689  0.087314  0.088015  0.087939  0.159324  0.107195         0.513303,\n",
       "  0.08890216005098467),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  686  0.087273  0.086559  0.088419  0.071514  0.108007         0.496565\n",
       "  687  0.088151  0.086680  0.088177  0.094101  0.109241         0.478232\n",
       "  688  0.087304  0.085503  0.087549  0.100092  0.108080         0.480346\n",
       "  689  0.087314  0.088015  0.087939  0.159324  0.107195         0.513303\n",
       "  690  0.089831  0.088507  0.090795  0.113265  0.110615         0.484164,\n",
       "  0.08874877959578842),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  687  0.088151  0.086680  0.088177  0.094101  0.109241         0.478232\n",
       "  688  0.087304  0.085503  0.087549  0.100092  0.108080         0.480346\n",
       "  689  0.087314  0.088015  0.087939  0.159324  0.107195         0.513303\n",
       "  690  0.089831  0.088507  0.090795  0.113265  0.110615         0.484164\n",
       "  691  0.089728  0.087910  0.090538  0.089801  0.110229         0.485974,\n",
       "  0.08881588595052735),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  688  0.087304  0.085503  0.087549  0.100092  0.108080         0.480346\n",
       "  689  0.087314  0.088015  0.087939  0.159324  0.107195         0.513303\n",
       "  690  0.089831  0.088507  0.090795  0.113265  0.110615         0.484164\n",
       "  691  0.089728  0.087910  0.090538  0.089801  0.110229         0.485974\n",
       "  692  0.089126  0.089451  0.089804  0.149719  0.110079         0.487622,\n",
       "  0.09007650201419137),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  689  0.087314  0.088015  0.087939  0.159324  0.107195         0.513303\n",
       "  690  0.089831  0.088507  0.090795  0.113265  0.110615         0.484164\n",
       "  691  0.089728  0.087910  0.090538  0.089801  0.110229         0.485974\n",
       "  692  0.089126  0.089451  0.089804  0.149719  0.110079         0.487622\n",
       "  693  0.089961  0.089406  0.091325  0.093281  0.110144         0.496547,\n",
       "  0.08962353893088473),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  690  0.089831  0.088507  0.090795  0.113265  0.110615         0.484164\n",
       "  691  0.089728  0.087910  0.090538  0.089801  0.110229         0.485974\n",
       "  692  0.089126  0.089451  0.089804  0.149719  0.110079         0.487622\n",
       "  693  0.089961  0.089406  0.091325  0.093281  0.110144         0.496547\n",
       "  694  0.091843  0.090448  0.091507  0.122985  0.111375         0.483733,\n",
       "  0.08970981303134203),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  691  0.089728  0.087910  0.090538  0.089801  0.110229         0.485974\n",
       "  692  0.089126  0.089451  0.089804  0.149719  0.110079         0.487622\n",
       "  693  0.089961  0.089406  0.091325  0.093281  0.110144         0.496547\n",
       "  694  0.091843  0.090448  0.091507  0.122985  0.111375         0.483733\n",
       "  695  0.090479  0.089451  0.090962  0.101420  0.110933         0.487766,\n",
       "  0.09191229478484128),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  692  0.089126  0.089451  0.089804  0.149719  0.110079         0.487622\n",
       "  693  0.089961  0.089406  0.091325  0.093281  0.110144         0.496547\n",
       "  694  0.091843  0.090448  0.091507  0.122985  0.111375         0.483733\n",
       "  695  0.090479  0.089451  0.090962  0.101420  0.110933         0.487766\n",
       "  696  0.091574  0.090813  0.092360  0.129997  0.111017         0.503590,\n",
       "  0.09277267831064881),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  693  0.089961  0.089406  0.091325  0.093281  0.110144         0.496547\n",
       "  694  0.091843  0.090448  0.091507  0.122985  0.111375         0.483733\n",
       "  695  0.090479  0.089451  0.090962  0.101420  0.110933         0.487766\n",
       "  696  0.091574  0.090813  0.092360  0.129997  0.111017         0.503590\n",
       "  697  0.092849  0.092437  0.093990  0.126823  0.113168         0.493554,\n",
       "  0.09552638671134829),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  694  0.091843  0.090448  0.091507  0.122985  0.111375         0.483733\n",
       "  695  0.090479  0.089451  0.090962  0.101420  0.110933         0.487766\n",
       "  696  0.091574  0.090813  0.092360  0.129997  0.111017         0.503590\n",
       "  697  0.092849  0.092437  0.093990  0.126823  0.113168         0.493554\n",
       "  698  0.092729  0.094105  0.093748  0.176146  0.114009         0.507712,\n",
       "  0.09536101679271577),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  695  0.090479  0.089451  0.090962  0.101420  0.110933         0.487766\n",
       "  696  0.091574  0.090813  0.092360  0.129997  0.111017         0.503590\n",
       "  697  0.092849  0.092437  0.093990  0.126823  0.113168         0.493554\n",
       "  698  0.092729  0.094105  0.093748  0.176146  0.114009         0.507712\n",
       "  699  0.096594  0.096461  0.097153  0.216973  0.116698         0.485884,\n",
       "  0.09595777645835947),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  696  0.091574  0.090813  0.092360  0.129997  0.111017         0.503590\n",
       "  697  0.092849  0.092437  0.093990  0.126823  0.113168         0.493554\n",
       "  698  0.092729  0.094105  0.093748  0.176146  0.114009         0.507712\n",
       "  699  0.096594  0.096461  0.097153  0.216973  0.116698         0.485884\n",
       "  700  0.096700  0.095569  0.097364  0.116632  0.116536         0.491583,\n",
       "  0.09755631064157062),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  697  0.092849  0.092437  0.093990  0.126823  0.113168         0.493554\n",
       "  698  0.092729  0.094105  0.093748  0.176146  0.114009         0.507712\n",
       "  699  0.096594  0.096461  0.097153  0.216973  0.116698         0.485884\n",
       "  700  0.096700  0.095569  0.097364  0.116632  0.116536         0.491583\n",
       "  701  0.095956  0.096870  0.097281  0.172624  0.117119         0.499074,\n",
       "  0.10037711577464674),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  698  0.092729  0.094105  0.093748  0.176146  0.114009         0.507712\n",
       "  699  0.096594  0.096461  0.097153  0.216973  0.116698         0.485884\n",
       "  700  0.096700  0.095569  0.097364  0.116632  0.116536         0.491583\n",
       "  701  0.095956  0.096870  0.097281  0.172624  0.117119         0.499074\n",
       "  702  0.098805  0.098882  0.099466  0.254013  0.118680         0.508214,\n",
       "  0.10023571516052449),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  699  0.096594  0.096461  0.097153  0.216973  0.116698         0.485884\n",
       "  700  0.096700  0.095569  0.097364  0.116632  0.116536         0.491583\n",
       "  701  0.095956  0.096870  0.097281  0.172624  0.117119         0.499074\n",
       "  702  0.098805  0.098882  0.099466  0.254013  0.118680         0.508214\n",
       "  703  0.100490  0.099304  0.101561  0.191735  0.121435         0.486063,\n",
       "  0.09947119923039577),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  700  0.096700  0.095569  0.097364  0.116632  0.116536         0.491583\n",
       "  701  0.095956  0.096870  0.097281  0.172624  0.117119         0.499074\n",
       "  702  0.098805  0.098882  0.099466  0.254013  0.118680         0.508214\n",
       "  703  0.100490  0.099304  0.101561  0.191735  0.121435         0.486063\n",
       "  704  0.101212  0.100660  0.100895  0.258473  0.121297         0.481404,\n",
       "  0.09957425473071896),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  701  0.095956  0.096870  0.097281  0.172624  0.117119         0.499074\n",
       "  702  0.098805  0.098882  0.099466  0.254013  0.118680         0.508214\n",
       "  703  0.100490  0.099304  0.101561  0.191735  0.121435         0.486063\n",
       "  704  0.101212  0.100660  0.100895  0.258473  0.121297         0.481404\n",
       "  705  0.100358  0.099034  0.100157  0.154557  0.120550         0.487891,\n",
       "  0.09857965849543357),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  702  0.098805  0.098882  0.099466  0.254013  0.118680         0.508214\n",
       "  703  0.100490  0.099304  0.101561  0.191735  0.121435         0.486063\n",
       "  704  0.101212  0.100660  0.100895  0.258473  0.121297         0.481404\n",
       "  705  0.100358  0.099034  0.100157  0.154557  0.120550         0.487891\n",
       "  706  0.099833  0.098705  0.100287  0.112952  0.120651         0.479683,\n",
       "  0.0970050936167327),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  703  0.100490  0.099304  0.101561  0.191735  0.121435         0.486063\n",
       "  704  0.101212  0.100660  0.100895  0.258473  0.121297         0.481404\n",
       "  705  0.100358  0.099034  0.100157  0.154557  0.120550         0.487891\n",
       "  706  0.099833  0.098705  0.100287  0.112952  0.120651         0.479683\n",
       "  707  0.098555  0.097145  0.098633  0.156048  0.119680         0.475346,\n",
       "  0.09711773298991508),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  704  0.101212  0.100660  0.100895  0.258473  0.121297         0.481404\n",
       "  705  0.100358  0.099034  0.100157  0.154557  0.120550         0.487891\n",
       "  706  0.099833  0.098705  0.100287  0.112952  0.120651         0.479683\n",
       "  707  0.098555  0.097145  0.098633  0.156048  0.119680         0.475346\n",
       "  708  0.098165  0.096404  0.097548  0.170994  0.118142         0.487963,\n",
       "  0.09654254666056687),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  705  0.100358  0.099034  0.100157  0.154557  0.120550         0.487891\n",
       "  706  0.099833  0.098705  0.100287  0.112952  0.120651         0.479683\n",
       "  707  0.098555  0.097145  0.098633  0.156048  0.119680         0.475346\n",
       "  708  0.098165  0.096404  0.097548  0.170994  0.118142         0.487963\n",
       "  709  0.097754  0.095773  0.098277  0.112307  0.118252         0.482820,\n",
       "  0.0972735094133261),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  706  0.099833  0.098705  0.100287  0.112952  0.120651         0.479683\n",
       "  707  0.098555  0.097145  0.098633  0.156048  0.119680         0.475346\n",
       "  708  0.098165  0.096404  0.097548  0.170994  0.118142         0.487963\n",
       "  709  0.097754  0.095773  0.098277  0.112307  0.118252         0.482820\n",
       "  710  0.098430  0.096513  0.098163  0.150079  0.117690         0.492586,\n",
       "  0.09950475240776524),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  707  0.098555  0.097145  0.098633  0.156048  0.119680         0.475346\n",
       "  708  0.098165  0.096404  0.097548  0.170994  0.118142         0.487963\n",
       "  709  0.097754  0.095773  0.098277  0.112307  0.118252         0.482820\n",
       "  710  0.098430  0.096513  0.098163  0.150079  0.117690         0.492586\n",
       "  711  0.097946  0.098172  0.098892  0.117529  0.118404         0.503805,\n",
       "  0.09929145350247444),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  708  0.098165  0.096404  0.097548  0.170994  0.118142         0.487963\n",
       "  709  0.097754  0.095773  0.098277  0.112307  0.118252         0.482820\n",
       "  710  0.098430  0.096513  0.098163  0.150079  0.117690         0.492586\n",
       "  711  0.097946  0.098172  0.098892  0.117529  0.118404         0.503805\n",
       "  712  0.099831  0.098468  0.100660  0.105840  0.120583         0.485526,\n",
       "  0.09825132424874561),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  709  0.097754  0.095773  0.098277  0.112307  0.118252         0.482820\n",
       "  710  0.098430  0.096513  0.098163  0.150079  0.117690         0.492586\n",
       "  711  0.097946  0.098172  0.098892  0.117529  0.118404         0.503805\n",
       "  712  0.099831  0.098468  0.100660  0.105840  0.120583         0.485526\n",
       "  713  0.100541  0.098823  0.100060  0.105424  0.120375         0.479343,\n",
       "  0.0992435245158162),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  710  0.098430  0.096513  0.098163  0.150079  0.117690         0.492586\n",
       "  711  0.097946  0.098172  0.098892  0.117529  0.118404         0.503805\n",
       "  712  0.099831  0.098468  0.100660  0.105840  0.120583         0.485526\n",
       "  713  0.100541  0.098823  0.100060  0.105424  0.120375         0.479343\n",
       "  714  0.099101  0.098003  0.099386  0.107163  0.119359         0.494540,\n",
       "  0.10184863477538673),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  711  0.097946  0.098172  0.098892  0.117529  0.118404         0.503805\n",
       "  712  0.099831  0.098468  0.100660  0.105840  0.120583         0.485526\n",
       "  713  0.100541  0.098823  0.100060  0.105424  0.120375         0.479343\n",
       "  714  0.099101  0.098003  0.099386  0.107163  0.119359         0.494540\n",
       "  715  0.100411  0.100302  0.101765  0.175122  0.120328         0.506601,\n",
       "  0.10243340497759414),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  712  0.099831  0.098468  0.100660  0.105840  0.120583         0.485526\n",
       "  713  0.100541  0.098823  0.100060  0.105424  0.120375         0.479343\n",
       "  714  0.099101  0.098003  0.099386  0.107163  0.119359         0.494540\n",
       "  715  0.100411  0.100302  0.101765  0.175122  0.120328         0.506601\n",
       "  716  0.103433  0.102672  0.103528  0.192667  0.122872         0.491493,\n",
       "  0.10311404277548035),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  713  0.100541  0.098823  0.100060  0.105424  0.120375         0.479343\n",
       "  714  0.099101  0.098003  0.099386  0.107163  0.119359         0.494540\n",
       "  715  0.100411  0.100302  0.101765  0.175122  0.120328         0.506601\n",
       "  716  0.103433  0.102672  0.103528  0.192667  0.122872         0.491493\n",
       "  717  0.104336  0.102248  0.104221  0.138292  0.123443         0.492210,\n",
       "  0.10216259321278591),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  714  0.099101  0.098003  0.099386  0.107163  0.119359         0.494540\n",
       "  715  0.100411  0.100302  0.101765  0.175122  0.120328         0.506601\n",
       "  716  0.103433  0.102672  0.103528  0.192667  0.122872         0.491493\n",
       "  717  0.104336  0.102248  0.104221  0.138292  0.123443         0.492210\n",
       "  718  0.102475  0.101013  0.102603  0.148838  0.124108         0.480006,\n",
       "  0.10185582268003114),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  715  0.100411  0.100302  0.101765  0.175122  0.120328         0.506601\n",
       "  716  0.103433  0.102672  0.103528  0.192667  0.122872         0.491493\n",
       "  717  0.104336  0.102248  0.104221  0.138292  0.123443         0.492210\n",
       "  718  0.102475  0.101013  0.102603  0.148838  0.124108         0.480006\n",
       "  719  0.102447  0.101371  0.103630  0.101293  0.123179         0.484827,\n",
       "  0.10130220968697841),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  716  0.103433  0.102672  0.103528  0.192667  0.122872         0.491493\n",
       "  717  0.104336  0.102248  0.104221  0.138292  0.123443         0.492210\n",
       "  718  0.102475  0.101013  0.102603  0.148838  0.124108         0.480006\n",
       "  719  0.102447  0.101371  0.103630  0.101293  0.123179         0.484827\n",
       "  720  0.102721  0.100873  0.102579  0.094293  0.122879         0.482981,\n",
       "  0.10314280401642022),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  717  0.104336  0.102248  0.104221  0.138292  0.123443         0.492210\n",
       "  718  0.102475  0.101013  0.102603  0.148838  0.124108         0.480006\n",
       "  719  0.102447  0.101371  0.103630  0.101293  0.123179         0.484827\n",
       "  720  0.102721  0.100873  0.102579  0.094293  0.122879         0.482981\n",
       "  721  0.102593  0.101928  0.103727  0.084734  0.122338         0.500884,\n",
       "  0.10409185761089985),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  718  0.102475  0.101013  0.102603  0.148838  0.124108         0.480006\n",
       "  719  0.102447  0.101371  0.103630  0.101293  0.123179         0.484827\n",
       "  720  0.102721  0.100873  0.102579  0.094293  0.122879         0.482981\n",
       "  721  0.102593  0.101928  0.103727  0.084734  0.122338         0.500884\n",
       "  722  0.103934  0.102532  0.104526  0.045432  0.124136         0.494217,\n",
       "  0.10583898031287765),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  719  0.102447  0.101371  0.103630  0.101293  0.123179         0.484827\n",
       "  720  0.102721  0.100873  0.102579  0.094293  0.122879         0.482981\n",
       "  721  0.102593  0.101928  0.103727  0.084734  0.122338         0.500884\n",
       "  722  0.103934  0.102532  0.104526  0.045432  0.124136         0.494217\n",
       "  723  0.105017  0.104902  0.105652  0.120132  0.125063         0.500185,\n",
       "  0.10530454468790561),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  720  0.102721  0.100873  0.102579  0.094293  0.122879         0.482981\n",
       "  721  0.102593  0.101928  0.103727  0.084734  0.122338         0.500884\n",
       "  722  0.103934  0.102532  0.104526  0.045432  0.124136         0.494217\n",
       "  723  0.105017  0.104902  0.105652  0.120132  0.125063         0.500185\n",
       "  724  0.106386  0.104776  0.107246  0.066720  0.126769         0.483124,\n",
       "  0.10606187271338992),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  721  0.102593  0.101928  0.103727  0.084734  0.122338         0.500884\n",
       "  722  0.103934  0.102532  0.104526  0.045432  0.124136         0.494217\n",
       "  723  0.105017  0.104902  0.105652  0.120132  0.125063         0.500185\n",
       "  724  0.106386  0.104776  0.107246  0.066720  0.126769         0.483124\n",
       "  725  0.106807  0.105930  0.107617  0.090625  0.126247         0.492784,\n",
       "  0.10624880634595564),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  722  0.103934  0.102532  0.104526  0.045432  0.124136         0.494217\n",
       "  723  0.105017  0.104902  0.105652  0.120132  0.125063         0.500185\n",
       "  724  0.106386  0.104776  0.107246  0.066720  0.126769         0.483124\n",
       "  725  0.106807  0.105930  0.107617  0.090625  0.126247         0.492784\n",
       "  726  0.106998  0.105518  0.107954  0.093695  0.126987         0.488518,\n",
       "  0.10651961811076387),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  723  0.105017  0.104902  0.105652  0.120132  0.125063         0.500185\n",
       "  724  0.106386  0.104776  0.107246  0.066720  0.126769         0.483124\n",
       "  725  0.106807  0.105930  0.107617  0.090625  0.126247         0.492784\n",
       "  726  0.106998  0.105518  0.107954  0.093695  0.126987         0.488518\n",
       "  727  0.106954  0.105399  0.107556  0.080397  0.127169         0.489146,\n",
       "  0.10672572911141025),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  724  0.106386  0.104776  0.107246  0.066720  0.126769         0.483124\n",
       "  725  0.106807  0.105930  0.107617  0.090625  0.126247         0.492784\n",
       "  726  0.106998  0.105518  0.107954  0.093695  0.126987         0.488518\n",
       "  727  0.106954  0.105399  0.107556  0.080397  0.127169         0.489146\n",
       "  728  0.107496  0.105594  0.107607  0.083615  0.127434         0.488662,\n",
       "  0.10836260840842048),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  725  0.106807  0.105930  0.107617  0.090625  0.126247         0.492784\n",
       "  726  0.106998  0.105518  0.107954  0.093695  0.126987         0.488518\n",
       "  727  0.106954  0.105399  0.107556  0.080397  0.127169         0.489146\n",
       "  728  0.107496  0.105594  0.107607  0.083615  0.127434         0.488662\n",
       "  729  0.107989  0.106862  0.109310  0.127288  0.127635         0.499361,\n",
       "  0.10856153150442248),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  726  0.106998  0.105518  0.107954  0.093695  0.126987         0.488518\n",
       "  727  0.106954  0.105399  0.107556  0.080397  0.127169         0.489146\n",
       "  728  0.107496  0.105594  0.107607  0.083615  0.127434         0.488662\n",
       "  729  0.107989  0.106862  0.109310  0.127288  0.127635         0.499361\n",
       "  730  0.108564  0.108262  0.109809  0.160426  0.129234         0.488608,\n",
       "  0.10904324620630669),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  727  0.106954  0.105399  0.107556  0.080397  0.127169         0.489146\n",
       "  728  0.107496  0.105594  0.107607  0.083615  0.127434         0.488662\n",
       "  729  0.107989  0.106862  0.109310  0.127288  0.127635         0.499361\n",
       "  730  0.108564  0.108262  0.109809  0.160426  0.129234         0.488608\n",
       "  731  0.109705  0.108061  0.110553  0.105190  0.129428         0.490723,\n",
       "  0.10772511728312524),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  728  0.107496  0.105594  0.107607  0.083615  0.127434         0.488662\n",
       "  729  0.107989  0.106862  0.109310  0.127288  0.127635         0.499361\n",
       "  730  0.108564  0.108262  0.109809  0.160426  0.129234         0.488608\n",
       "  731  0.109705  0.108061  0.110553  0.105190  0.129428         0.490723\n",
       "  732  0.109428  0.107295  0.108477  0.147790  0.129898         0.477264,\n",
       "  0.10750702644140485),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  729  0.107989  0.106862  0.109310  0.127288  0.127635         0.499361\n",
       "  730  0.108564  0.108262  0.109809  0.160426  0.129234         0.488608\n",
       "  731  0.109705  0.108061  0.110553  0.105190  0.129428         0.490723\n",
       "  732  0.109428  0.107295  0.108477  0.147790  0.129898         0.477264\n",
       "  733  0.109325  0.107063  0.108096  0.270555  0.128611         0.485490,\n",
       "  0.10068388630740153),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  730  0.108564  0.108262  0.109809  0.160426  0.129234         0.488608\n",
       "  731  0.109705  0.108061  0.110553  0.105190  0.129428         0.490723\n",
       "  732  0.109428  0.107295  0.108477  0.147790  0.129898         0.477264\n",
       "  733  0.109325  0.107063  0.108096  0.270555  0.128611         0.485490\n",
       "  734  0.099323  0.099873  0.100166  0.432186  0.128398         0.436099,\n",
       "  0.09886006375546329),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  731  0.109705  0.108061  0.110553  0.105190  0.129428         0.490723\n",
       "  732  0.109428  0.107295  0.108477  0.147790  0.129898         0.477264\n",
       "  733  0.109325  0.107063  0.108096  0.270555  0.128611         0.485490\n",
       "  734  0.099323  0.099873  0.100166  0.432186  0.128398         0.436099\n",
       "  735  0.101034  0.099119  0.100808  0.154520  0.121735         0.473483,\n",
       "  0.09921715924309113),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  732  0.109428  0.107295  0.108477  0.147790  0.129898         0.477264\n",
       "  733  0.109325  0.107063  0.108096  0.270555  0.128611         0.485490\n",
       "  734  0.099323  0.099873  0.100166  0.432186  0.128398         0.436099\n",
       "  735  0.101034  0.099119  0.100808  0.154520  0.121735         0.473483\n",
       "  736  0.098697  0.098809  0.099587  0.135436  0.119953         0.489791,\n",
       "  0.09820099929387256),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  733  0.109325  0.107063  0.108096  0.270555  0.128611         0.485490\n",
       "  734  0.099323  0.099873  0.100166  0.432186  0.128398         0.436099\n",
       "  735  0.101034  0.099119  0.100808  0.154520  0.121735         0.473483\n",
       "  736  0.098697  0.098809  0.099587  0.135436  0.119953         0.489791\n",
       "  737  0.100545  0.098413  0.099212  0.134777  0.120302         0.479522,\n",
       "  0.09777919341972059),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  734  0.099323  0.099873  0.100166  0.432186  0.128398         0.436099\n",
       "  735  0.101034  0.099119  0.100808  0.154520  0.121735         0.473483\n",
       "  736  0.098697  0.098809  0.099587  0.135436  0.119953         0.489791\n",
       "  737  0.100545  0.098413  0.099212  0.134777  0.120302         0.479522\n",
       "  738  0.098562  0.097363  0.097594  0.167331  0.119310         0.483966,\n",
       "  0.09871147523669664),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  735  0.101034  0.099119  0.100808  0.154520  0.121735         0.473483\n",
       "  736  0.098697  0.098809  0.099587  0.135436  0.119953         0.489791\n",
       "  737  0.100545  0.098413  0.099212  0.134777  0.120302         0.479522\n",
       "  738  0.098562  0.097363  0.097594  0.167331  0.119310         0.483966\n",
       "  739  0.098512  0.098318  0.099197  0.133904  0.118898         0.494092,\n",
       "  0.09972284324948562),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  736  0.098697  0.098809  0.099587  0.135436  0.119953         0.489791\n",
       "  737  0.100545  0.098413  0.099212  0.134777  0.120302         0.479522\n",
       "  738  0.098562  0.097363  0.097594  0.167331  0.119310         0.483966\n",
       "  739  0.098512  0.098318  0.099197  0.133904  0.118898         0.494092\n",
       "  740  0.099568  0.098963  0.100408  0.096305  0.119808         0.494683,\n",
       "  0.09825851215338999),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  737  0.100545  0.098413  0.099212  0.134777  0.120302         0.479522\n",
       "  738  0.098562  0.097363  0.097594  0.167331  0.119310         0.483966\n",
       "  739  0.098512  0.098318  0.099197  0.133904  0.118898         0.494092\n",
       "  740  0.099568  0.098963  0.100408  0.096305  0.119808         0.494683\n",
       "  741  0.101328  0.099224  0.100287  0.103777  0.120796         0.476171,\n",
       "  0.09896311925578648),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  738  0.098562  0.097363  0.097594  0.167331  0.119310         0.483966\n",
       "  739  0.098512  0.098318  0.099197  0.133904  0.118898         0.494092\n",
       "  740  0.099568  0.098963  0.100408  0.096305  0.119808         0.494683\n",
       "  741  0.101328  0.099224  0.100287  0.103777  0.120796         0.476171\n",
       "  742  0.098986  0.098292  0.099837  0.107613  0.119366         0.492389,\n",
       "  0.09850057229962066),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  739  0.098512  0.098318  0.099197  0.133904  0.118898         0.494092\n",
       "  740  0.099568  0.098963  0.100408  0.096305  0.119808         0.494683\n",
       "  741  0.101328  0.099224  0.100287  0.103777  0.120796         0.476171\n",
       "  742  0.098986  0.098292  0.099837  0.107613  0.119366         0.492389\n",
       "  743  0.099761  0.097932  0.100045  0.076642  0.120054         0.483662,\n",
       "  0.0965808917743659),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  740  0.099568  0.098963  0.100408  0.096305  0.119808         0.494683\n",
       "  741  0.101328  0.099224  0.100287  0.103777  0.120796         0.476171\n",
       "  742  0.098986  0.098292  0.099837  0.107613  0.119366         0.492389\n",
       "  743  0.099761  0.097932  0.100045  0.076642  0.120054         0.483662\n",
       "  744  0.098878  0.097508  0.098592  0.077160  0.119602         0.472766,\n",
       "  0.09825851215338999),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  741  0.101328  0.099224  0.100287  0.103777  0.120796         0.476171\n",
       "  742  0.098986  0.098292  0.099837  0.107613  0.119366         0.492389\n",
       "  743  0.099761  0.097932  0.100045  0.076642  0.120054         0.483662\n",
       "  744  0.098878  0.097508  0.098592  0.077160  0.119602         0.472766\n",
       "  745  0.097181  0.096785  0.098471  0.088721  0.117728         0.499665,\n",
       "  0.09970126991319013),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  742  0.098986  0.098292  0.099837  0.107613  0.119366         0.492389\n",
       "  743  0.099761  0.097932  0.100045  0.076642  0.120054         0.483662\n",
       "  744  0.098878  0.097508  0.098592  0.077160  0.119602         0.472766\n",
       "  745  0.097181  0.096785  0.098471  0.088721  0.117728         0.499665\n",
       "  746  0.098805  0.099091  0.100055  0.103626  0.119366         0.497909,\n",
       "  0.10204036034438203),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  743  0.099761  0.097932  0.100045  0.076642  0.120054         0.483662\n",
       "  744  0.098878  0.097508  0.098592  0.077160  0.119602         0.472766\n",
       "  745  0.097181  0.096785  0.098471  0.088721  0.117728         0.499665\n",
       "  746  0.098805  0.099091  0.100055  0.103626  0.119366         0.497909\n",
       "  747  0.101113  0.100489  0.102005  0.098871  0.120775         0.504611,\n",
       "  0.09939211303458285),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  744  0.098878  0.097508  0.098592  0.077160  0.119602         0.472766\n",
       "  745  0.097181  0.096785  0.098471  0.088721  0.117728         0.499665\n",
       "  746  0.098805  0.099091  0.100055  0.103626  0.119366         0.497909\n",
       "  747  0.101113  0.100489  0.102005  0.098871  0.120775         0.504611\n",
       "  748  0.101465  0.100736  0.101409  0.117787  0.123059         0.467318,\n",
       "  0.0996365691490283),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  745  0.097181  0.096785  0.098471  0.088721  0.117728         0.499665\n",
       "  746  0.098805  0.099091  0.100055  0.103626  0.119366         0.497909\n",
       "  747  0.101113  0.100489  0.102005  0.098871  0.120775         0.504611\n",
       "  748  0.101465  0.100736  0.101409  0.117787  0.123059         0.467318\n",
       "  749  0.098909  0.098816  0.099229  0.142809  0.120473         0.488949,\n",
       "  0.09957665069893376),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  746  0.098805  0.099091  0.100055  0.103626  0.119366         0.497909\n",
       "  747  0.101113  0.100489  0.102005  0.098871  0.120775         0.504611\n",
       "  748  0.101465  0.100736  0.101409  0.117787  0.123059         0.467318\n",
       "  749  0.098909  0.098816  0.099229  0.142809  0.120473         0.488949\n",
       "  750  0.101161  0.099060  0.101021  0.077235  0.120712         0.486673,\n",
       "  0.09792299000205765),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  747  0.101113  0.100489  0.102005  0.098871  0.120775         0.504611\n",
       "  748  0.101465  0.100736  0.101409  0.117787  0.123059         0.467318\n",
       "  749  0.098909  0.098816  0.099229  0.142809  0.120473         0.488949\n",
       "  750  0.101161  0.099060  0.101021  0.077235  0.120712         0.486673\n",
       "  751  0.100242  0.098503  0.099803  0.088288  0.120653         0.474755,\n",
       "  0.09527474269225845),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  748  0.101465  0.100736  0.101409  0.117787  0.123059         0.467318\n",
       "  749  0.098909  0.098816  0.099229  0.142809  0.120473         0.488949\n",
       "  750  0.101161  0.099060  0.101021  0.077235  0.120712         0.486673\n",
       "  751  0.100242  0.098503  0.099803  0.088288  0.120653         0.474755\n",
       "  752  0.098625  0.097150  0.097068  0.131481  0.119038         0.467318,\n",
       "  0.09382958896424354),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  749  0.098909  0.098816  0.099229  0.142809  0.120473         0.488949\n",
       "  750  0.101161  0.099060  0.101021  0.077235  0.120712         0.486673\n",
       "  751  0.100242  0.098503  0.099803  0.088288  0.120653         0.474755\n",
       "  752  0.098625  0.097150  0.097068  0.131481  0.119038         0.467318\n",
       "  753  0.094539  0.093534  0.093132  0.210194  0.116452         0.476314,\n",
       "  0.09587149273553985),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  750  0.101161  0.099060  0.101021  0.077235  0.120712         0.486673\n",
       "  751  0.100242  0.098503  0.099803  0.088288  0.120653         0.474755\n",
       "  752  0.098625  0.097150  0.097068  0.131481  0.119038         0.467318\n",
       "  753  0.094539  0.093534  0.093132  0.210194  0.116452         0.476314\n",
       "  754  0.095843  0.094617  0.095843  0.133181  0.115041         0.502389,\n",
       "  0.09536341276093056),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  751  0.100242  0.098503  0.099803  0.088288  0.120653         0.474755\n",
       "  752  0.098625  0.097150  0.097068  0.131481  0.119038         0.467318\n",
       "  753  0.094539  0.093534  0.093132  0.210194  0.116452         0.476314\n",
       "  754  0.095843  0.094617  0.095843  0.133181  0.115041         0.502389\n",
       "  755  0.097051  0.094986  0.096896  0.065511  0.117035         0.483321,\n",
       "  0.09744606723660304),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  752  0.098625  0.097150  0.097068  0.131481  0.119038         0.467318\n",
       "  753  0.094539  0.093534  0.093132  0.210194  0.116452         0.476314\n",
       "  754  0.095843  0.094617  0.095843  0.133181  0.115041         0.502389\n",
       "  755  0.097051  0.094986  0.096896  0.065511  0.117035         0.483321\n",
       "  756  0.096204  0.096314  0.097325  0.087726  0.116539         0.502694,\n",
       "  0.09891758623734304),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  753  0.094539  0.093534  0.093132  0.210194  0.116452         0.476314\n",
       "  754  0.095843  0.094617  0.095843  0.133181  0.115041         0.502389\n",
       "  755  0.097051  0.094986  0.096896  0.065511  0.117035         0.483321\n",
       "  756  0.096204  0.096314  0.097325  0.087726  0.116539         0.502694\n",
       "  757  0.098856  0.098231  0.100106  0.079021  0.118573         0.498124,\n",
       "  0.09878337352786516),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  754  0.095843  0.094617  0.095843  0.133181  0.115041         0.502389\n",
       "  755  0.097051  0.094986  0.096896  0.065511  0.117035         0.483321\n",
       "  756  0.096204  0.096314  0.097325  0.087726  0.116539         0.502694\n",
       "  757  0.098856  0.098231  0.100106  0.079021  0.118573         0.498124\n",
       "  758  0.100310  0.098202  0.099561  0.074740  0.120010         0.486117,\n",
       "  0.09945682342110698),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  755  0.097051  0.094986  0.096896  0.065511  0.117035         0.483321\n",
       "  756  0.096204  0.096314  0.097325  0.087726  0.116539         0.502694\n",
       "  757  0.098856  0.098231  0.100106  0.079021  0.118573         0.498124\n",
       "  758  0.100310  0.098202  0.099561  0.074740  0.120010         0.486117\n",
       "  759  0.099494  0.098065  0.099924  0.060048  0.119879         0.492156,\n",
       "  0.0990398094833846),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  756  0.096204  0.096314  0.097325  0.087726  0.116539         0.502694\n",
       "  757  0.098856  0.098231  0.100106  0.079021  0.118573         0.498124\n",
       "  758  0.100310  0.098202  0.099561  0.074740  0.120010         0.486117\n",
       "  759  0.099494  0.098065  0.099924  0.060048  0.119879         0.492156\n",
       "  760  0.099932  0.098515  0.100626  0.056458  0.120536         0.484002,\n",
       "  0.09739573265936768),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  757  0.098856  0.098231  0.100106  0.079021  0.118573         0.498124\n",
       "  758  0.100310  0.098202  0.099561  0.074740  0.120010         0.486117\n",
       "  759  0.099494  0.098065  0.099924  0.060048  0.119879         0.492156\n",
       "  760  0.099932  0.098515  0.100626  0.056458  0.120536         0.484002\n",
       "  761  0.099234  0.097930  0.099268  0.079536  0.120129         0.474827,\n",
       "  0.0989487338241354),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  758  0.100310  0.098202  0.099561  0.074740  0.120010         0.486117\n",
       "  759  0.099494  0.098065  0.099924  0.060048  0.119879         0.492156\n",
       "  760  0.099932  0.098515  0.100626  0.056458  0.120536         0.484002\n",
       "  761  0.099234  0.097930  0.099268  0.079536  0.120129         0.474827\n",
       "  762  0.098346  0.097532  0.099616  0.061912  0.118523         0.498733,\n",
       "  0.09859404392708467),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  759  0.099494  0.098065  0.099924  0.060048  0.119879         0.492156\n",
       "  760  0.099932  0.098515  0.100626  0.056458  0.120536         0.484002\n",
       "  761  0.099234  0.097930  0.099268  0.079536  0.120129         0.474827\n",
       "  762  0.098346  0.097532  0.099616  0.061912  0.118523         0.498733\n",
       "  763  0.099434  0.098029  0.100428  0.064402  0.120040         0.484468,\n",
       "  0.09951673224883921),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  760  0.099932  0.098515  0.100626  0.056458  0.120536         0.484002\n",
       "  761  0.099234  0.097930  0.099268  0.079536  0.120129         0.474827\n",
       "  762  0.098346  0.097532  0.099616  0.061912  0.118523         0.498733\n",
       "  763  0.099434  0.098029  0.100428  0.064402  0.120040         0.484468\n",
       "  764  0.099612  0.098290  0.100643  0.072329  0.119694         0.494020,\n",
       "  0.10188937585740059),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  761  0.099234  0.097930  0.099268  0.079536  0.120129         0.474827\n",
       "  762  0.098346  0.097532  0.099616  0.061912  0.118523         0.498733\n",
       "  763  0.099434  0.098029  0.100428  0.064402  0.120040         0.484468\n",
       "  764  0.099612  0.098290  0.100643  0.072329  0.119694         0.494020\n",
       "  765  0.100074  0.101015  0.101406  0.089747  0.120595         0.504862,\n",
       "  0.10252448063684334),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  762  0.098346  0.097532  0.099616  0.061912  0.118523         0.498733\n",
       "  763  0.099434  0.098029  0.100428  0.064402  0.120040         0.484468\n",
       "  764  0.099612  0.098290  0.100643  0.072329  0.119694         0.494020\n",
       "  765  0.100074  0.101015  0.101406  0.089747  0.120595         0.504862\n",
       "  766  0.102090  0.101366  0.103255  0.080485  0.122912         0.491870,\n",
       "  0.10149872719240328),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  763  0.099434  0.098029  0.100428  0.064402  0.120040         0.484468\n",
       "  764  0.099612  0.098290  0.100643  0.072329  0.119694         0.494020\n",
       "  765  0.100074  0.101015  0.101406  0.089747  0.120595         0.504862\n",
       "  766  0.102090  0.101366  0.103255  0.080485  0.122912         0.491870\n",
       "  767  0.103525  0.101449  0.102283  0.088098  0.123532         0.479450,\n",
       "  0.1004993390206883),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  764  0.099612  0.098290  0.100643  0.072329  0.119694         0.494020\n",
       "  765  0.100074  0.101015  0.101406  0.089747  0.120595         0.504862\n",
       "  766  0.102090  0.101366  0.103255  0.080485  0.122912         0.491870\n",
       "  767  0.103525  0.101449  0.102283  0.088098  0.123532         0.479450\n",
       "  768  0.100757  0.099413  0.101934  0.089015  0.122530         0.479647,\n",
       "  0.0993321945844883),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  765  0.100074  0.101015  0.101406  0.089747  0.120595         0.504862\n",
       "  766  0.102090  0.101366  0.103255  0.080485  0.122912         0.491870\n",
       "  767  0.103525  0.101449  0.102283  0.088098  0.123532         0.479450\n",
       "  768  0.100757  0.099413  0.101934  0.089015  0.122530         0.479647\n",
       "  769  0.101580  0.099610  0.100501  0.078366  0.121554         0.478393,\n",
       "  0.10097387544029043),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  766  0.102090  0.101366  0.103255  0.080485  0.122912         0.491870\n",
       "  767  0.103525  0.101449  0.102283  0.088098  0.123532         0.479450\n",
       "  768  0.100757  0.099413  0.101934  0.089015  0.122530         0.479647\n",
       "  769  0.101580  0.099610  0.100501  0.078366  0.121554         0.478393\n",
       "  770  0.100548  0.099510  0.101675  0.065665  0.120415         0.499396,\n",
       "  0.1012902202235421),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  767  0.103525  0.101449  0.102283  0.088098  0.123532         0.479450\n",
       "  768  0.100757  0.099413  0.101934  0.089015  0.122530         0.479647\n",
       "  769  0.101580  0.099610  0.100501  0.078366  0.121554         0.478393\n",
       "  770  0.100548  0.099510  0.101675  0.065665  0.120415         0.499396\n",
       "  771  0.101400  0.101011  0.102467  0.072534  0.122018         0.489486,\n",
       "  0.10180070578872848),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  768  0.100757  0.099413  0.101934  0.089015  0.122530         0.479647\n",
       "  769  0.101580  0.099610  0.100501  0.078366  0.121554         0.478393\n",
       "  770  0.100548  0.099510  0.101675  0.065665  0.120415         0.499396\n",
       "  771  0.101400  0.101011  0.102467  0.072534  0.122018         0.489486\n",
       "  772  0.102249  0.100828  0.103489  0.045902  0.122327         0.490938,\n",
       "  0.10275215535142292),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  769  0.101580  0.099610  0.100501  0.078366  0.121554         0.478393\n",
       "  770  0.100548  0.099510  0.101675  0.065665  0.120415         0.499396\n",
       "  771  0.101400  0.101011  0.102467  0.072534  0.122018         0.489486\n",
       "  772  0.102249  0.100828  0.103489  0.045902  0.122327         0.490938\n",
       "  773  0.102042  0.101553  0.103247  0.067156  0.122825         0.494235,\n",
       "  0.10192532500298486),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  770  0.100548  0.099510  0.101675  0.065665  0.120415         0.499396\n",
       "  771  0.101400  0.101011  0.102467  0.072534  0.122018         0.489486\n",
       "  772  0.102249  0.100828  0.103489  0.045902  0.122327         0.490938\n",
       "  773  0.102042  0.101553  0.103247  0.067156  0.122825         0.494235\n",
       "  774  0.102844  0.101319  0.103695  0.053355  0.123754         0.480938,\n",
       "  0.1043147403890498),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  771  0.101400  0.101011  0.102467  0.072534  0.122018         0.489486\n",
       "  772  0.102249  0.100828  0.103489  0.045902  0.122327         0.490938\n",
       "  773  0.102042  0.101553  0.103247  0.067156  0.122825         0.494235\n",
       "  774  0.102844  0.101319  0.103695  0.053355  0.123754         0.480938\n",
       "  775  0.102841  0.103207  0.103744  0.102444  0.122947         0.504988,\n",
       "  0.10709001250368248),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  772  0.102249  0.100828  0.103489  0.045902  0.122327         0.490938\n",
       "  773  0.102042  0.101553  0.103247  0.067156  0.122825         0.494235\n",
       "  774  0.102844  0.101319  0.103695  0.053355  0.123754         0.480938\n",
       "  775  0.102841  0.103207  0.103744  0.102444  0.122947         0.504988\n",
       "  776  0.105910  0.106099  0.107278  0.134337  0.125280         0.507873,\n",
       "  0.10852318639062343),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  773  0.102042  0.101553  0.103247  0.067156  0.122825         0.494235\n",
       "  774  0.102844  0.101319  0.103695  0.053355  0.123754         0.480938\n",
       "  775  0.102841  0.103207  0.103744  0.102444  0.122947         0.504988\n",
       "  776  0.105910  0.106099  0.107278  0.134337  0.125280         0.507873\n",
       "  777  0.107770  0.107840  0.109014  0.134291  0.127991         0.497837,\n",
       "  0.11026551715617162),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  774  0.102844  0.101319  0.103695  0.053355  0.123754         0.480938\n",
       "  775  0.102841  0.103207  0.103744  0.102444  0.122947         0.504988\n",
       "  776  0.105910  0.106099  0.107278  0.134337  0.125280         0.507873\n",
       "  777  0.107770  0.107840  0.109014  0.134291  0.127991         0.497837\n",
       "  778  0.110018  0.108786  0.110662  0.194775  0.129390         0.500149,\n",
       "  0.1121636339674932),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  775  0.102841  0.103207  0.103744  0.102444  0.122947         0.504988\n",
       "  776  0.105910  0.106099  0.107278  0.134337  0.125280         0.507873\n",
       "  777  0.107770  0.107840  0.109014  0.134291  0.127991         0.497837\n",
       "  778  0.110018  0.108786  0.110662  0.194775  0.129390         0.500149\n",
       "  779  0.111139  0.111314  0.112398  0.128745  0.131092         0.501314,\n",
       "  0.11239849658671715),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  776  0.105910  0.106099  0.107278  0.134337  0.125280         0.507873\n",
       "  777  0.107770  0.107840  0.109014  0.134291  0.127991         0.497837\n",
       "  778  0.110018  0.108786  0.110662  0.194775  0.129390         0.500149\n",
       "  779  0.111139  0.111314  0.112398  0.128745  0.131092         0.501314\n",
       "  780  0.111861  0.110864  0.113091  0.066321  0.132946         0.488877,\n",
       "  0.11219718714486267),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  777  0.107770  0.107840  0.109014  0.134291  0.127991         0.497837\n",
       "  778  0.110018  0.108786  0.110662  0.194775  0.129390         0.500149\n",
       "  779  0.111139  0.111314  0.112398  0.128745  0.131092         0.501314\n",
       "  780  0.111861  0.110864  0.113091  0.066321  0.132946         0.488877\n",
       "  781  0.113401  0.111393  0.113321  0.081292  0.133175         0.485615,\n",
       "  0.11203900513087452),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  778  0.110018  0.108786  0.110662  0.194775  0.129390         0.500149\n",
       "  779  0.111139  0.111314  0.112398  0.128745  0.131092         0.501314\n",
       "  780  0.111861  0.110864  0.113091  0.066321  0.132946         0.488877\n",
       "  781  0.113401  0.111393  0.113321  0.081292  0.133175         0.485615\n",
       "  782  0.113722  0.111381  0.113798  0.049920  0.132978         0.485938,\n",
       "  0.11198388823957188),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  779  0.111139  0.111314  0.112398  0.128745  0.131092         0.501314\n",
       "  780  0.111861  0.110864  0.113091  0.066321  0.132946         0.488877\n",
       "  781  0.113401  0.111393  0.113321  0.081292  0.133175         0.485615\n",
       "  782  0.113722  0.111381  0.113798  0.049920  0.132978         0.485938\n",
       "  783  0.112583  0.110862  0.113396  0.064106  0.132824         0.486708,\n",
       "  0.11564589953037484),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  780  0.111861  0.110864  0.113091  0.066321  0.132946         0.488877\n",
       "  781  0.113401  0.111393  0.113321  0.081292  0.133175         0.485615\n",
       "  782  0.113722  0.111381  0.113798  0.049920  0.132978         0.485938\n",
       "  783  0.112583  0.110862  0.113396  0.064106  0.132824         0.486708\n",
       "  784  0.113298  0.114369  0.114726  0.114499  0.132770         0.514504,\n",
       "  0.1160868827726075),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  781  0.113401  0.111393  0.113321  0.081292  0.133175         0.485615\n",
       "  782  0.113722  0.111381  0.113798  0.049920  0.132978         0.485938\n",
       "  783  0.112583  0.110862  0.113396  0.064106  0.132824         0.486708\n",
       "  784  0.113298  0.114369  0.114726  0.114499  0.132770         0.514504\n",
       "  785  0.116525  0.117322  0.117245  0.171809  0.136346         0.490418,\n",
       "  0.11599820308157306),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  782  0.113722  0.111381  0.113798  0.049920  0.132978         0.485938\n",
       "  783  0.112583  0.110862  0.113396  0.064106  0.132824         0.486708\n",
       "  784  0.113298  0.114369  0.114726  0.114499  0.132770         0.514504\n",
       "  785  0.116525  0.117322  0.117245  0.171809  0.136346         0.490418\n",
       "  786  0.117216  0.115469  0.117330  0.094266  0.136777         0.486457,\n",
       "  0.11480229740443319),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  783  0.112583  0.110862  0.113396  0.064106  0.132824         0.486708\n",
       "  784  0.113298  0.114369  0.114726  0.114499  0.132770         0.514504\n",
       "  785  0.116525  0.117322  0.117245  0.171809  0.136346         0.490418\n",
       "  786  0.117216  0.115469  0.117330  0.094266  0.136777         0.486457\n",
       "  787  0.116937  0.114544  0.116530  0.072551  0.136690         0.478178,\n",
       "  0.11840200389928913),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  784  0.113298  0.114369  0.114726  0.114499  0.132770         0.514504\n",
       "  785  0.116525  0.117322  0.117245  0.171809  0.136346         0.490418\n",
       "  786  0.117216  0.115469  0.117330  0.094266  0.136777         0.486457\n",
       "  787  0.116937  0.114544  0.116530  0.072551  0.136690         0.478178\n",
       "  788  0.117423  0.117130  0.118940  0.123192  0.135522         0.514038,\n",
       "  0.12213351751304581),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  785  0.116525  0.117322  0.117245  0.171809  0.136346         0.490418\n",
       "  786  0.117216  0.115469  0.117330  0.094266  0.136777         0.486457\n",
       "  787  0.116937  0.114544  0.116530  0.072551  0.136690         0.478178\n",
       "  788  0.117423  0.117130  0.118940  0.123192  0.135522         0.514038\n",
       "  789  0.119322  0.120509  0.120621  0.122202  0.139038         0.515024,\n",
       "  0.12346603186787834),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  786  0.117216  0.115469  0.117330  0.094266  0.136777         0.486457\n",
       "  787  0.116937  0.114544  0.116530  0.072551  0.136690         0.478178\n",
       "  788  0.117423  0.117130  0.118940  0.123192  0.135522         0.514038\n",
       "  789  0.119322  0.120509  0.120621  0.122202  0.139038         0.515024\n",
       "  790  0.124161  0.123713  0.124119  0.213166  0.142682         0.497085,\n",
       "  0.12591296973582305),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  787  0.116937  0.114544  0.116530  0.072551  0.136690         0.478178\n",
       "  788  0.117423  0.117130  0.118940  0.123192  0.135522         0.514038\n",
       "  789  0.119322  0.120509  0.120621  0.122202  0.139038         0.515024\n",
       "  790  0.124161  0.123713  0.124119  0.213166  0.142682         0.497085\n",
       "  791  0.125666  0.124164  0.126156  0.130900  0.143983         0.505418,\n",
       "  0.12510051519667378),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  788  0.117423  0.117130  0.118940  0.123192  0.135522         0.514038\n",
       "  789  0.119322  0.120509  0.120621  0.122202  0.139038         0.515024\n",
       "  790  0.124161  0.123713  0.124119  0.213166  0.142682         0.497085\n",
       "  791  0.125666  0.124164  0.126156  0.130900  0.143983         0.505418\n",
       "  792  0.128621  0.127891  0.124267  0.286895  0.146373         0.481045,\n",
       "  0.12878890138256413),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  789  0.119322  0.120509  0.120621  0.122202  0.139038         0.515024\n",
       "  790  0.124161  0.123713  0.124119  0.213166  0.142682         0.497085\n",
       "  791  0.125666  0.124164  0.126156  0.130900  0.143983         0.505418\n",
       "  792  0.128621  0.127891  0.124267  0.286895  0.146373         0.481045\n",
       "  793  0.126296  0.127178  0.126505  0.165879  0.145580         0.514701,\n",
       "  0.12464756173572943),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  790  0.124161  0.123713  0.124119  0.213166  0.142682         0.497085\n",
       "  791  0.125666  0.124164  0.126156  0.130900  0.143983         0.505418\n",
       "  792  0.128621  0.127891  0.124267  0.286895  0.146373         0.481045\n",
       "  793  0.126296  0.127178  0.126505  0.165879  0.145580         0.514701\n",
       "  794  0.129783  0.127524  0.125853  0.168801  0.149182         0.456153,\n",
       "  0.12366255899566554),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  791  0.125666  0.124164  0.126156  0.130900  0.143983         0.505418\n",
       "  792  0.128621  0.127891  0.124267  0.286895  0.146373         0.481045\n",
       "  793  0.126296  0.127178  0.126505  0.165879  0.145580         0.514701\n",
       "  794  0.129783  0.127524  0.125853  0.168801  0.149182         0.456153\n",
       "  795  0.124975  0.124220  0.124991  0.146460  0.145137         0.479755,\n",
       "  0.12785182762915848),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  792  0.128621  0.127891  0.124267  0.286895  0.146373         0.481045\n",
       "  793  0.126296  0.127178  0.126505  0.165879  0.145580         0.514701\n",
       "  794  0.129783  0.127524  0.125853  0.168801  0.149182         0.456153\n",
       "  795  0.124975  0.124220  0.124991  0.146460  0.145137         0.479755\n",
       "  796  0.127853  0.126135  0.127316  0.146586  0.144175         0.518447,\n",
       "  0.12932333700753618),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  793  0.126296  0.127178  0.126505  0.165879  0.145580         0.514701\n",
       "  794  0.129783  0.127524  0.125853  0.168801  0.149182         0.456153\n",
       "  795  0.124975  0.124220  0.124991  0.146460  0.145137         0.479755\n",
       "  796  0.127853  0.126135  0.127316  0.146586  0.144175         0.518447\n",
       "  797  0.129047  0.127882  0.128990  0.298819  0.148267         0.498124,\n",
       "  0.13054321198918634),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  794  0.129783  0.127524  0.125853  0.168801  0.149182         0.456153\n",
       "  795  0.124975  0.124220  0.124991  0.146460  0.145137         0.479755\n",
       "  796  0.127853  0.126135  0.127316  0.146586  0.144175         0.518447\n",
       "  797  0.129047  0.127882  0.128990  0.298819  0.148267         0.498124\n",
       "  798  0.133655  0.131939  0.132558  0.383945  0.149704         0.496242,\n",
       "  0.13199076168541604),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  795  0.124975  0.124220  0.124991  0.146460  0.145137         0.479755\n",
       "  796  0.127853  0.126135  0.127316  0.146586  0.144175         0.518447\n",
       "  797  0.129047  0.127882  0.128990  0.298819  0.148267         0.498124\n",
       "  798  0.133655  0.131939  0.132558  0.383945  0.149704         0.496242\n",
       "  799  0.129831  0.131112  0.130874  0.162005  0.150895         0.497945,\n",
       "  0.13798708109334362),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  796  0.127853  0.126135  0.127316  0.146586  0.144175         0.518447\n",
       "  797  0.129047  0.127882  0.128990  0.298819  0.148267         0.498124\n",
       "  798  0.133655  0.131939  0.132558  0.383945  0.149704         0.496242\n",
       "  799  0.129831  0.131112  0.130874  0.162005  0.150895         0.497945\n",
       "  800  0.135265  0.136468  0.136620  0.165150  0.152309         0.531959,\n",
       "  0.1379990705567799),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  797  0.129047  0.127882  0.128990  0.298819  0.148267         0.498124\n",
       "  798  0.133655  0.131939  0.132558  0.383945  0.149704         0.496242\n",
       "  799  0.129831  0.131112  0.130874  0.162005  0.150895         0.497945\n",
       "  800  0.135265  0.136468  0.136620  0.165150  0.152309         0.531959\n",
       "  801  0.138023  0.136437  0.136513  0.179999  0.158165         0.487210,\n",
       "  0.13624712705128558),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  798  0.133655  0.131939  0.132558  0.383945  0.149704         0.496242\n",
       "  799  0.129831  0.131112  0.130874  0.162005  0.150895         0.497945\n",
       "  800  0.135265  0.136468  0.136620  0.165150  0.152309         0.531959\n",
       "  801  0.138023  0.136437  0.136513  0.179999  0.158165         0.487210\n",
       "  802  0.139460  0.136792  0.137480  0.140870  0.158176         0.474020,\n",
       "  0.13770668545567621),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  799  0.129831  0.131112  0.130874  0.162005  0.150895         0.497945\n",
       "  800  0.135265  0.136468  0.136620  0.165150  0.152309         0.531959\n",
       "  801  0.138023  0.136437  0.136513  0.179999  0.158165         0.487210\n",
       "  802  0.139460  0.136792  0.137480  0.140870  0.158176         0.474020\n",
       "  803  0.138341  0.136216  0.138572  0.081482  0.156465         0.498035,\n",
       "  0.13881629778763416),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  800  0.135265  0.136468  0.136620  0.165150  0.152309         0.531959\n",
       "  801  0.138023  0.136437  0.136513  0.179999  0.158165         0.487210\n",
       "  802  0.139460  0.136792  0.137480  0.140870  0.158176         0.474020\n",
       "  803  0.138341  0.136216  0.138572  0.081482  0.156465         0.498035\n",
       "  804  0.139241  0.137036  0.139548  0.074487  0.157891         0.495418,\n",
       "  0.1425406427414711),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  801  0.138023  0.136437  0.136513  0.179999  0.158165         0.487210\n",
       "  802  0.139460  0.136792  0.137480  0.140870  0.158176         0.474020\n",
       "  803  0.138341  0.136216  0.138572  0.081482  0.156465         0.498035\n",
       "  804  0.139241  0.137036  0.139548  0.074487  0.157891         0.495418\n",
       "  805  0.139179  0.141755  0.140861  0.167750  0.158974         0.514970,\n",
       "  0.14547168165715246),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  802  0.139460  0.136792  0.137480  0.140870  0.158176         0.474020\n",
       "  803  0.138341  0.136216  0.138572  0.081482  0.156465         0.498035\n",
       "  804  0.139241  0.137036  0.139548  0.074487  0.157891         0.495418\n",
       "  805  0.139179  0.141755  0.140861  0.167750  0.158974         0.514970\n",
       "  806  0.144757  0.143435  0.145349  0.167171  0.162612         0.509038,\n",
       "  0.1445633691446867),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  803  0.138341  0.136216  0.138572  0.081482  0.156465         0.498035\n",
       "  804  0.139241  0.137036  0.139548  0.074487  0.157891         0.495418\n",
       "  805  0.139179  0.141755  0.140861  0.167750  0.158974         0.514970\n",
       "  806  0.144757  0.143435  0.145349  0.167171  0.162612         0.509038\n",
       "  807  0.145262  0.145027  0.146740  0.158667  0.165474         0.480328,\n",
       "  0.14649022795222352),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  804  0.139241  0.137036  0.139548  0.074487  0.157891         0.495418\n",
       "  805  0.139179  0.141755  0.140861  0.167750  0.158974         0.514970\n",
       "  806  0.144757  0.143435  0.145349  0.167171  0.162612         0.509038\n",
       "  807  0.145262  0.145027  0.146740  0.158667  0.165474         0.480328\n",
       "  808  0.147120  0.144994  0.145664  0.141989  0.164587         0.501529,\n",
       "  0.14994134592831282),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  805  0.139179  0.141755  0.140861  0.167750  0.158974         0.514970\n",
       "  806  0.144757  0.143435  0.145349  0.167171  0.162612         0.509038\n",
       "  807  0.145262  0.145027  0.146740  0.158667  0.165474         0.480328\n",
       "  808  0.147120  0.144994  0.145664  0.141989  0.164587         0.501529\n",
       "  809  0.146273  0.148940  0.147779  0.215985  0.166469         0.512927,\n",
       "  0.15380946821976216),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  806  0.144757  0.143435  0.145349  0.167171  0.162612         0.509038\n",
       "  807  0.145262  0.145027  0.146740  0.158667  0.165474         0.480328\n",
       "  808  0.147120  0.144994  0.145664  0.141989  0.164587         0.501529\n",
       "  809  0.146273  0.148940  0.147779  0.215985  0.166469         0.512927\n",
       "  810  0.153630  0.151679  0.152364  0.205103  0.169839         0.516045,\n",
       "  0.15168847825265294),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  807  0.145262  0.145027  0.146740  0.158667  0.165474         0.480328\n",
       "  808  0.147120  0.144994  0.145664  0.141989  0.164587         0.501529\n",
       "  809  0.146273  0.148940  0.147779  0.215985  0.166469         0.512927\n",
       "  810  0.153630  0.151679  0.152364  0.205103  0.169839         0.516045\n",
       "  811  0.154489  0.152971  0.151913  0.201143  0.173617         0.471261,\n",
       "  0.14231774071859649),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  808  0.147120  0.144994  0.145664  0.141989  0.164587         0.501529\n",
       "  809  0.146273  0.148940  0.147779  0.215985  0.166469         0.512927\n",
       "  810  0.153630  0.151679  0.152364  0.205103  0.169839         0.516045\n",
       "  811  0.154489  0.152971  0.151913  0.201143  0.173617         0.471261\n",
       "  812  0.152918  0.150044  0.140781  0.401536  0.171545         0.417049,\n",
       "  0.1351590784332608),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  809  0.146273  0.148940  0.147779  0.215985  0.166469         0.512927\n",
       "  810  0.153630  0.151679  0.152364  0.205103  0.169839         0.516045\n",
       "  811  0.154489  0.152971  0.151913  0.201143  0.173617         0.471261\n",
       "  812  0.152918  0.150044  0.140781  0.401536  0.171545         0.417049\n",
       "  813  0.138762  0.137482  0.136913  0.276901  0.162394         0.433590,\n",
       "  0.127513899887249),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  810  0.153630  0.151679  0.152364  0.205103  0.169839         0.516045\n",
       "  811  0.154489  0.152971  0.151913  0.201143  0.173617         0.471261\n",
       "  812  0.152918  0.150044  0.140781  0.401536  0.171545         0.417049\n",
       "  813  0.138762  0.137482  0.136913  0.276901  0.162394         0.433590\n",
       "  814  0.134421  0.134650  0.128435  0.248666  0.155403         0.429952,\n",
       "  0.13433944561182945),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  811  0.154489  0.152971  0.151913  0.201143  0.173617         0.471261\n",
       "  812  0.152918  0.150044  0.140781  0.401536  0.171545         0.417049\n",
       "  813  0.138762  0.137482  0.136913  0.276901  0.162394         0.433590\n",
       "  814  0.134421  0.134650  0.128435  0.248666  0.155403         0.429952\n",
       "  815  0.131352  0.132515  0.129855  0.205324  0.147937         0.538160,\n",
       "  0.12981704117285667),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  812  0.152918  0.150044  0.140781  0.401536  0.171545         0.417049\n",
       "  813  0.138762  0.137482  0.136913  0.276901  0.162394         0.433590\n",
       "  814  0.134421  0.134650  0.128435  0.248666  0.155403         0.429952\n",
       "  815  0.131352  0.132515  0.129855  0.205324  0.147937         0.538160\n",
       "  816  0.138161  0.135968  0.130872  0.196794  0.154602         0.453304,\n",
       "  0.1269339216214712),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  813  0.138762  0.137482  0.136913  0.276901  0.162394         0.433590\n",
       "  814  0.134421  0.134650  0.128435  0.248666  0.155403         0.429952\n",
       "  815  0.131352  0.132515  0.129855  0.205324  0.147937         0.538160\n",
       "  816  0.138161  0.135968  0.130872  0.196794  0.154602         0.453304\n",
       "  817  0.129807  0.129334  0.127900  0.169382  0.150186         0.465562,\n",
       "  0.12788777677474275),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  814  0.134421  0.134650  0.128435  0.248666  0.155403         0.429952\n",
       "  815  0.131352  0.132515  0.129855  0.205324  0.147937         0.538160\n",
       "  816  0.138161  0.135968  0.130872  0.196794  0.154602         0.453304\n",
       "  817  0.129807  0.129334  0.127900  0.169382  0.150186         0.465562\n",
       "  818  0.128693  0.126488  0.125967  0.219855  0.147370         0.494253,\n",
       "  0.12602321314079065),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  815  0.131352  0.132515  0.129855  0.205324  0.147937         0.538160\n",
       "  816  0.138161  0.135968  0.130872  0.196794  0.154602         0.453304\n",
       "  817  0.129807  0.129334  0.127900  0.169382  0.150186         0.465562\n",
       "  818  0.128693  0.126488  0.125967  0.219855  0.147370         0.494253\n",
       "  819  0.127646  0.126792  0.126568  0.134349  0.148302         0.473178,\n",
       "  0.1314611083745113),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  816  0.138161  0.135968  0.130872  0.196794  0.154602         0.453304\n",
       "  817  0.129807  0.129334  0.127900  0.169382  0.150186         0.465562\n",
       "  818  0.128693  0.126488  0.125967  0.219855  0.147370         0.494253\n",
       "  819  0.127646  0.126792  0.126568  0.134349  0.148302         0.473178\n",
       "  820  0.129304  0.132083  0.130049  0.239260  0.146481         0.527783,\n",
       "  0.13433224808482278),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  817  0.129807  0.129334  0.127900  0.169382  0.150186         0.465562\n",
       "  818  0.128693  0.126488  0.125967  0.219855  0.147370         0.494253\n",
       "  819  0.127646  0.126792  0.126568  0.134349  0.148302         0.473178\n",
       "  820  0.129304  0.132083  0.130049  0.239260  0.146481         0.527783\n",
       "  821  0.131059  0.134659  0.132279  0.170496  0.151791         0.508590,\n",
       "  0.1382099638714936),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  818  0.128693  0.126488  0.125967  0.219855  0.147370         0.494253\n",
       "  819  0.127646  0.126792  0.126568  0.134349  0.148302         0.473178\n",
       "  820  0.129304  0.132083  0.130049  0.239260  0.146481         0.527783\n",
       "  821  0.131059  0.134659  0.132279  0.170496  0.151791         0.508590\n",
       "  822  0.137371  0.136629  0.138584  0.066518  0.154595         0.516117,\n",
       "  0.1356456043162992),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  819  0.127646  0.126792  0.126568  0.134349  0.148302         0.473178\n",
       "  820  0.129304  0.132083  0.130049  0.239260  0.146481         0.527783\n",
       "  821  0.131059  0.134659  0.132279  0.170496  0.151791         0.508590\n",
       "  822  0.137371  0.136629  0.138584  0.066518  0.154595         0.516117\n",
       "  823  0.139826  0.140212  0.137831  0.165062  0.158382         0.467945,\n",
       "  0.13745981412829136),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  820  0.129304  0.132083  0.130049  0.239260  0.146481         0.527783\n",
       "  821  0.131059  0.134659  0.132279  0.170496  0.151791         0.508590\n",
       "  822  0.137371  0.136629  0.138584  0.066518  0.154595         0.516117\n",
       "  823  0.139826  0.140212  0.137831  0.165062  0.158382         0.467945\n",
       "  824  0.138526  0.136190  0.134261  0.216491  0.155878         0.500687,\n",
       "  0.14193908151703546),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  821  0.131059  0.134659  0.132279  0.170496  0.151791         0.508590\n",
       "  822  0.137371  0.136629  0.138584  0.066518  0.154595         0.516117\n",
       "  823  0.139826  0.140212  0.137831  0.165062  0.158382         0.467945\n",
       "  824  0.138526  0.136190  0.134261  0.216491  0.155878         0.500687\n",
       "  825  0.140286  0.140425  0.139560  0.192460  0.157650         0.520615,\n",
       "  0.14307508798880544),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  822  0.137371  0.136629  0.138584  0.066518  0.154595         0.516117\n",
       "  823  0.139826  0.140212  0.137831  0.165062  0.158382         0.467945\n",
       "  824  0.138526  0.136190  0.134261  0.216491  0.155878         0.500687\n",
       "  825  0.140286  0.140425  0.139560  0.192460  0.157650         0.520615\n",
       "  826  0.142365  0.142437  0.143346  0.150903  0.162024         0.495615,\n",
       "  0.1421164216543797),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  823  0.139826  0.140212  0.137831  0.165062  0.158382         0.467945\n",
       "  824  0.138526  0.136190  0.134261  0.216491  0.155878         0.500687\n",
       "  825  0.140286  0.140425  0.139560  0.192460  0.157650         0.520615\n",
       "  826  0.142365  0.142437  0.143346  0.150903  0.162024         0.495615\n",
       "  827  0.147238  0.144397  0.141975  0.191949  0.163133         0.479952,\n",
       "  0.1393675244348344),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  824  0.138526  0.136190  0.134261  0.216491  0.155878         0.500687\n",
       "  825  0.140286  0.140425  0.139560  0.192460  0.157650         0.520615\n",
       "  826  0.142365  0.142437  0.143346  0.150903  0.162024         0.495615\n",
       "  827  0.147238  0.144397  0.141975  0.191949  0.163133         0.479952\n",
       "  828  0.142427  0.140591  0.141740  0.105105  0.162197         0.466565,\n",
       "  0.13999782765548527),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  825  0.140286  0.140425  0.139560  0.192460  0.157650         0.520615\n",
       "  826  0.142365  0.142437  0.143346  0.150903  0.162024         0.495615\n",
       "  827  0.147238  0.144397  0.141975  0.191949  0.163133         0.479952\n",
       "  828  0.142427  0.140591  0.141740  0.105105  0.162197         0.466565\n",
       "  829  0.139371  0.139880  0.140767  0.102836  0.159513         0.491834,\n",
       "  0.14343695616813823),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  826  0.142365  0.142437  0.143346  0.150903  0.162024         0.495615\n",
       "  827  0.147238  0.144397  0.141975  0.191949  0.163133         0.479952\n",
       "  828  0.142427  0.140591  0.141740  0.105105  0.162197         0.466565\n",
       "  829  0.139371  0.139880  0.140767  0.102836  0.159513         0.491834\n",
       "  830  0.142841  0.141522  0.143281  0.102296  0.160128         0.512837,\n",
       "  0.14745128063250176),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  827  0.147238  0.144397  0.141975  0.191949  0.163133         0.479952\n",
       "  828  0.142427  0.140591  0.141740  0.105105  0.162197         0.466565\n",
       "  829  0.139371  0.139880  0.140767  0.102836  0.159513         0.491834\n",
       "  830  0.142841  0.141522  0.143281  0.102296  0.160128         0.512837\n",
       "  831  0.144062  0.145700  0.145584  0.119307  0.163487         0.517138,\n",
       "  0.14735780900503775),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  828  0.142427  0.140591  0.141740  0.105105  0.162197         0.466565\n",
       "  829  0.139371  0.139880  0.140767  0.102836  0.159513         0.491834\n",
       "  830  0.142841  0.141522  0.143281  0.102296  0.160128         0.512837\n",
       "  831  0.144062  0.145700  0.145584  0.119307  0.163487         0.517138\n",
       "  832  0.148199  0.146042  0.148852  0.093600  0.167407         0.486422,\n",
       "  0.14820860865798607),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  829  0.139371  0.139880  0.140767  0.102836  0.159513         0.491834\n",
       "  830  0.142841  0.141522  0.143281  0.102296  0.160128         0.512837\n",
       "  831  0.144062  0.145700  0.145584  0.119307  0.163487         0.517138\n",
       "  832  0.148199  0.146042  0.148852  0.093600  0.167407         0.486422\n",
       "  833  0.148439  0.147653  0.149700  0.093700  0.167316         0.493483,\n",
       "  0.14360231646440846),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  830  0.142841  0.141522  0.143281  0.102296  0.160128         0.512837\n",
       "  831  0.144062  0.145700  0.145584  0.119307  0.163487         0.517138\n",
       "  832  0.148199  0.146042  0.148852  0.093600  0.167407         0.486422\n",
       "  833  0.148439  0.147653  0.149700  0.093700  0.167316         0.493483\n",
       "  834  0.149390  0.146750  0.146013  0.149228  0.168147         0.452676,\n",
       "  0.14363827523235506),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  831  0.144062  0.145700  0.145584  0.119307  0.163487         0.517138\n",
       "  832  0.148199  0.146042  0.148852  0.093600  0.167407         0.486422\n",
       "  833  0.148439  0.147653  0.149700  0.093700  0.167316         0.493483\n",
       "  834  0.149390  0.146750  0.146013  0.149228  0.168147         0.452676\n",
       "  835  0.147960  0.145169  0.143523  0.149714  0.163648         0.487389,\n",
       "  0.14236806567346955),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  832  0.148199  0.146042  0.148852  0.093600  0.167407         0.486422\n",
       "  833  0.148439  0.147653  0.149700  0.093700  0.167316         0.493483\n",
       "  834  0.149390  0.146750  0.146013  0.149228  0.168147         0.452676\n",
       "  835  0.147960  0.145169  0.143523  0.149714  0.163648         0.487389\n",
       "  836  0.143703  0.141212  0.141757  0.122513  0.163683         0.477622,\n",
       "  0.14138785486983524),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  833  0.148439  0.147653  0.149700  0.093700  0.167316         0.493483\n",
       "  834  0.149390  0.146750  0.146013  0.149228  0.168147         0.452676\n",
       "  835  0.147960  0.145169  0.143523  0.149714  0.163648         0.487389\n",
       "  836  0.143703  0.141212  0.141757  0.122513  0.163683         0.477622\n",
       "  837  0.141585  0.141705  0.143223  0.089256  0.162443         0.479791,\n",
       "  0.13641970411928714),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  834  0.149390  0.146750  0.146013  0.149228  0.168147         0.452676\n",
       "  835  0.147960  0.145169  0.143523  0.149714  0.163648         0.487389\n",
       "  836  0.143703  0.141212  0.141757  0.122513  0.163683         0.477622\n",
       "  837  0.141585  0.141705  0.143223  0.089256  0.162443         0.479791\n",
       "  838  0.141703  0.140691  0.137509  0.133342  0.161486         0.449970,\n",
       "  0.13740710282756582),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  835  0.147960  0.145169  0.143523  0.149714  0.163648         0.487389\n",
       "  836  0.143703  0.141212  0.141757  0.122513  0.163683         0.477622\n",
       "  837  0.141585  0.141705  0.143223  0.089256  0.162443         0.479791\n",
       "  838  0.141703  0.140691  0.137509  0.133342  0.161486         0.449970\n",
       "  839  0.138372  0.136257  0.134803  0.174222  0.156634         0.494504,\n",
       "  0.13837054185369652),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  836  0.143703  0.141212  0.141757  0.122513  0.163683         0.477622\n",
       "  837  0.141585  0.141705  0.143223  0.089256  0.162443         0.479791\n",
       "  838  0.141703  0.140691  0.137509  0.133342  0.161486         0.449970\n",
       "  839  0.138372  0.136257  0.134803  0.174222  0.156634         0.494504\n",
       "  840  0.138384  0.136918  0.138800  0.107440  0.157598         0.494325,\n",
       "  0.14132314448331112),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  837  0.141585  0.141705  0.143223  0.089256  0.162443         0.479791\n",
       "  838  0.141703  0.140691  0.137509  0.133342  0.161486         0.449970\n",
       "  839  0.138372  0.136257  0.134803  0.174222  0.156634         0.494504\n",
       "  840  0.138384  0.136918  0.138800  0.107440  0.157598         0.494325\n",
       "  841  0.141181  0.139643  0.141612  0.107467  0.158539         0.509199,\n",
       "  0.14300077448469747),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  838  0.141703  0.140691  0.137509  0.133342  0.161486         0.449970\n",
       "  839  0.138372  0.136257  0.134803  0.174222  0.156634         0.494504\n",
       "  840  0.138384  0.136918  0.138800  0.107440  0.157598         0.494325\n",
       "  841  0.141181  0.139643  0.141612  0.107467  0.158539         0.509199\n",
       "  842  0.144081  0.141601  0.144671  0.130812  0.161423         0.499665,\n",
       "  0.1439689958248955),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  839  0.138372  0.136257  0.134803  0.174222  0.156634         0.494504\n",
       "  840  0.138384  0.136918  0.138800  0.107440  0.157598         0.494325\n",
       "  841  0.141181  0.139643  0.141612  0.107467  0.158539         0.509199\n",
       "  842  0.144081  0.141601  0.144671  0.130812  0.161423         0.499665\n",
       "  843  0.143385  0.141949  0.144628  0.039512  0.163061         0.494361,\n",
       "  0.14639198363305453),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  840  0.138384  0.136918  0.138800  0.107440  0.157598         0.494325\n",
       "  841  0.141181  0.139643  0.141612  0.107467  0.158539         0.509199\n",
       "  842  0.144081  0.141601  0.144671  0.130812  0.161423         0.499665\n",
       "  843  0.143385  0.141949  0.144628  0.039512  0.163061         0.494361\n",
       "  844  0.144348  0.144909  0.145875  0.061442  0.164006         0.505239,\n",
       "  0.1439714014154726),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  841  0.141181  0.139643  0.141612  0.107467  0.158539         0.509199\n",
       "  842  0.144081  0.141601  0.144671  0.130812  0.161423         0.499665\n",
       "  843  0.143385  0.141949  0.144628  0.039512  0.163061         0.494361\n",
       "  844  0.144348  0.144909  0.145875  0.061442  0.164006         0.505239\n",
       "  845  0.146292  0.145568  0.146488  0.071475  0.166373         0.469020,\n",
       "  0.14440038557190665),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  842  0.144081  0.141601  0.144671  0.130812  0.161423         0.499665\n",
       "  843  0.143385  0.141949  0.144628  0.039512  0.163061         0.494361\n",
       "  844  0.144348  0.144909  0.145875  0.061442  0.164006         0.505239\n",
       "  845  0.146292  0.145568  0.146488  0.071475  0.166373         0.469020\n",
       "  846  0.145777  0.143660  0.145470  0.062237  0.164009         0.490328,\n",
       "  0.1417521478844698),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  843  0.143385  0.141949  0.144628  0.039512  0.163061         0.494361\n",
       "  844  0.144348  0.144909  0.145875  0.061442  0.164006         0.505239\n",
       "  845  0.146292  0.145568  0.146488  0.071475  0.166373         0.469020\n",
       "  846  0.145777  0.143660  0.145470  0.062237  0.164009         0.490328\n",
       "  847  0.144247  0.142366  0.144027  0.057701  0.164428         0.467318,\n",
       "  0.1402446797381455),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  844  0.144348  0.144909  0.145875  0.061442  0.164006         0.505239\n",
       "  845  0.146292  0.145568  0.146488  0.071475  0.166373         0.469020\n",
       "  846  0.145777  0.143660  0.145470  0.062237  0.164009         0.490328\n",
       "  847  0.144247  0.142366  0.144027  0.057701  0.164428         0.467318\n",
       "  848  0.142875  0.141153  0.140914  0.104662  0.161842         0.475848,\n",
       "  0.14027822329315265),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  845  0.146292  0.145568  0.146488  0.071475  0.166373         0.469020\n",
       "  846  0.145777  0.143660  0.145470  0.062237  0.164009         0.490328\n",
       "  847  0.144247  0.142366  0.144027  0.057701  0.164428         0.467318\n",
       "  848  0.142875  0.141153  0.140914  0.104662  0.161842         0.475848\n",
       "  849  0.141044  0.138660  0.140621  0.079016  0.160369         0.487371,\n",
       "  0.13348864595888113),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  846  0.145777  0.143660  0.145470  0.062237  0.164009         0.490328\n",
       "  847  0.144247  0.142366  0.144027  0.057701  0.164428         0.467318\n",
       "  848  0.142875  0.141153  0.140914  0.104662  0.161842         0.475848\n",
       "  849  0.141044  0.138660  0.140621  0.079016  0.160369         0.487371\n",
       "  850  0.139703  0.137264  0.135409  0.130272  0.160402         0.436350,\n",
       "  0.1316312606072111),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  847  0.144247  0.142366  0.144027  0.057701  0.164428         0.467318\n",
       "  848  0.142875  0.141153  0.140914  0.104662  0.161842         0.475848\n",
       "  849  0.141044  0.138660  0.140621  0.079016  0.160369         0.487371\n",
       "  850  0.139703  0.137264  0.135409  0.130272  0.160402         0.436350\n",
       "  851  0.133506  0.132837  0.131134  0.155656  0.153771         0.473232,\n",
       "  0.12742043788214732),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  848  0.142875  0.141153  0.140914  0.104662  0.161842         0.475848\n",
       "  849  0.141044  0.138660  0.140621  0.079016  0.160369         0.487371\n",
       "  850  0.139703  0.137264  0.135409  0.130272  0.160402         0.436350\n",
       "  851  0.133506  0.132837  0.131134  0.155656  0.153771         0.473232\n",
       "  852  0.133280  0.132287  0.129596  0.129769  0.151958         0.455633,\n",
       "  0.13257793747820057),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  849  0.141044  0.138660  0.140621  0.079016  0.160369         0.487371\n",
       "  850  0.139703  0.137264  0.135409  0.130272  0.160402         0.436350\n",
       "  851  0.133506  0.132837  0.131134  0.155656  0.153771         0.473232\n",
       "  852  0.133280  0.132287  0.129596  0.129769  0.151958         0.455633\n",
       "  853  0.127754  0.130718  0.127539  0.163836  0.147845         0.525687,\n",
       "  0.13102733228164762),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  850  0.139703  0.137264  0.135409  0.130272  0.160402         0.436350\n",
       "  851  0.133506  0.132837  0.131134  0.155656  0.153771         0.473232\n",
       "  852  0.133280  0.132287  0.129596  0.129769  0.151958         0.455633\n",
       "  853  0.127754  0.130718  0.127539  0.163836  0.147845         0.525687\n",
       "  854  0.131357  0.131633  0.131802  0.153973  0.152882         0.475526,\n",
       "  0.12899500276084822),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  851  0.133506  0.132837  0.131134  0.155656  0.153771         0.473232\n",
       "  852  0.133280  0.132287  0.129596  0.129769  0.151958         0.455633\n",
       "  853  0.127754  0.130718  0.127539  0.163836  0.147845         0.525687\n",
       "  854  0.131357  0.131633  0.131802  0.153973  0.152882         0.475526\n",
       "  855  0.130801  0.129801  0.129380  0.120961  0.151368         0.471923,\n",
       "  0.13272652599696722),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  852  0.133280  0.132287  0.129596  0.129769  0.151958         0.455633\n",
       "  853  0.127754  0.130718  0.127539  0.163836  0.147845         0.525687\n",
       "  854  0.131357  0.131633  0.131802  0.153973  0.152882         0.475526\n",
       "  855  0.130801  0.129801  0.129380  0.120961  0.151368         0.471923\n",
       "  856  0.132832  0.131681  0.133047  0.108082  0.149383         0.515024,\n",
       "  0.12885120617851117),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  853  0.127754  0.130718  0.127539  0.163836  0.147845         0.525687\n",
       "  854  0.131357  0.131633  0.131802  0.153973  0.152882         0.475526\n",
       "  855  0.130801  0.129801  0.129380  0.120961  0.151368         0.471923\n",
       "  856  0.132832  0.131681  0.133047  0.108082  0.149383         0.515024\n",
       "  857  0.131571  0.129702  0.130656  0.135341  0.153027         0.458142,\n",
       "  0.12365056953222923),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  854  0.131357  0.131633  0.131802  0.153973  0.152882         0.475526\n",
       "  855  0.130801  0.129801  0.129380  0.120961  0.151368         0.471923\n",
       "  856  0.132832  0.131681  0.133047  0.108082  0.149383         0.515024\n",
       "  857  0.131571  0.129702  0.130656  0.135341  0.153027         0.458142\n",
       "  858  0.127497  0.127555  0.122554  0.256759  0.149243         0.448232,\n",
       "  0.12001731948236617),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  855  0.130801  0.129801  0.129380  0.120961  0.151368         0.471923\n",
       "  856  0.132832  0.131681  0.133047  0.108082  0.149383         0.515024\n",
       "  857  0.131571  0.129702  0.130656  0.135341  0.153027         0.458142\n",
       "  858  0.127497  0.127555  0.122554  0.256759  0.149243         0.448232\n",
       "  859  0.125521  0.124178  0.121605  0.199764  0.144164         0.459952,\n",
       "  0.11988789870931789),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  856  0.132832  0.131681  0.133047  0.108082  0.149383         0.515024\n",
       "  857  0.131571  0.129702  0.130656  0.135341  0.153027         0.458142\n",
       "  858  0.127497  0.127555  0.122554  0.256759  0.149243         0.448232\n",
       "  859  0.125521  0.124178  0.121605  0.199764  0.144164         0.459952\n",
       "  860  0.122537  0.120445  0.121711  0.207616  0.140615         0.486153,\n",
       "  0.11607728927738599),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  857  0.131571  0.129702  0.130656  0.135341  0.153027         0.458142\n",
       "  858  0.127497  0.127555  0.122554  0.256759  0.149243         0.448232\n",
       "  859  0.125521  0.124178  0.121605  0.199764  0.144164         0.459952\n",
       "  860  0.122537  0.120445  0.121711  0.207616  0.140615         0.486153\n",
       "  861  0.111387  0.117485  0.112689  0.231012  0.140489         0.458626,\n",
       "  0.10751421434604923),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  858  0.127497  0.127555  0.122554  0.256759  0.149243         0.448232\n",
       "  859  0.125521  0.124178  0.121605  0.199764  0.144164         0.459952\n",
       "  860  0.122537  0.120445  0.121711  0.207616  0.140615         0.486153\n",
       "  861  0.111387  0.117485  0.112689  0.231012  0.140489         0.458626\n",
       "  862  0.111069  0.110494  0.102467  0.412567  0.136768         0.423089,\n",
       "  0.11371423916404609),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  859  0.125521  0.124178  0.121605  0.199764  0.144164         0.459952\n",
       "  860  0.122537  0.120445  0.121711  0.207616  0.140615         0.486153\n",
       "  861  0.111387  0.117485  0.112689  0.231012  0.140489         0.458626\n",
       "  862  0.111069  0.110494  0.102467  0.412567  0.136768         0.423089\n",
       "  863  0.110610  0.113265  0.110979  0.228565  0.128405         0.533482,\n",
       "  0.1117753812707107),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  860  0.122537  0.120445  0.121711  0.207616  0.140615         0.486153\n",
       "  861  0.111387  0.117485  0.112689  0.231012  0.140489         0.458626\n",
       "  862  0.111069  0.110494  0.102467  0.412567  0.136768         0.423089\n",
       "  863  0.110610  0.113265  0.110979  0.228565  0.128405         0.533482\n",
       "  864  0.118554  0.116893  0.113895  0.169340  0.134460         0.472622,\n",
       "  0.10927811844789294),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  861  0.111387  0.117485  0.112689  0.231012  0.140489         0.458626\n",
       "  862  0.111069  0.110494  0.102467  0.412567  0.136768         0.423089\n",
       "  863  0.110610  0.113265  0.110979  0.228565  0.128405         0.533482\n",
       "  864  0.118554  0.116893  0.113895  0.169340  0.134460         0.472622\n",
       "  865  0.113539  0.111499  0.109637  0.141386  0.132566         0.468447,\n",
       "  0.10796957339757067),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  862  0.111069  0.110494  0.102467  0.412567  0.136768         0.423089\n",
       "  863  0.110610  0.113265  0.110979  0.228565  0.128405         0.533482\n",
       "  864  0.118554  0.116893  0.113895  0.169340  0.134460         0.472622\n",
       "  865  0.113539  0.111499  0.109637  0.141386  0.132566         0.468447\n",
       "  866  0.111012  0.108914  0.107716  0.152728  0.130128         0.477336,\n",
       "  0.10743033621380675),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  863  0.110610  0.113265  0.110979  0.228565  0.128405         0.533482\n",
       "  864  0.118554  0.116893  0.113895  0.169340  0.134460         0.472622\n",
       "  865  0.113539  0.111499  0.109637  0.141386  0.132566         0.468447\n",
       "  866  0.111012  0.108914  0.107716  0.152728  0.130128         0.477336\n",
       "  867  0.108297  0.108701  0.108404  0.193022  0.128850         0.483088,\n",
       "  0.111272093232531),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  864  0.118554  0.116893  0.113895  0.169340  0.134460         0.472622\n",
       "  865  0.113539  0.111499  0.109637  0.141386  0.132566         0.468447\n",
       "  866  0.111012  0.108914  0.107716  0.152728  0.130128         0.477336\n",
       "  867  0.108297  0.108701  0.108404  0.193022  0.128850         0.483088\n",
       "  868  0.105847  0.111679  0.106171  0.361624  0.128323         0.515848,\n",
       "  0.09967251829461256),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  865  0.113539  0.111499  0.109637  0.141386  0.132566         0.468447\n",
       "  866  0.111012  0.108914  0.107716  0.152728  0.130128         0.477336\n",
       "  867  0.108297  0.108701  0.108404  0.193022  0.128850         0.483088\n",
       "  868  0.105847  0.111679  0.106171  0.361624  0.128323         0.515848\n",
       "  869  0.103359  0.103070  0.100287  0.428007  0.132075         0.400383,\n",
       "  0.09476666271764918),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  866  0.111012  0.108914  0.107716  0.152728  0.130128         0.477336\n",
       "  867  0.108297  0.108701  0.108404  0.193022  0.128850         0.483088\n",
       "  868  0.105847  0.111679  0.106171  0.361624  0.128323         0.515848\n",
       "  869  0.103359  0.103070  0.100287  0.428007  0.132075         0.400383\n",
       "  870  0.098642  0.097408  0.096061  0.319929  0.120747         0.450436,\n",
       "  0.0974916002550465),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  867  0.108297  0.108701  0.108404  0.193022  0.128850         0.483088\n",
       "  868  0.105847  0.111679  0.106171  0.361624  0.128323         0.515848\n",
       "  869  0.103359  0.103070  0.100287  0.428007  0.132075         0.400383\n",
       "  870  0.098642  0.097408  0.096061  0.319929  0.120747         0.450436\n",
       "  871  0.093915  0.096513  0.095085  0.272406  0.115956         0.507497,\n",
       "  0.09627172527339634),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  868  0.105847  0.111679  0.106171  0.361624  0.128323         0.515848\n",
       "  869  0.103359  0.103070  0.100287  0.428007  0.132075         0.400383\n",
       "  870  0.098642  0.097408  0.096061  0.319929  0.120747         0.450436\n",
       "  871  0.093915  0.096513  0.095085  0.272406  0.115956         0.507497\n",
       "  872  0.099140  0.097027  0.097364  0.185647  0.118617         0.477999,\n",
       "  0.09704823066696135),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  869  0.103359  0.103070  0.100287  0.428007  0.132075         0.400383\n",
       "  870  0.098642  0.097408  0.096061  0.319929  0.120747         0.450436\n",
       "  871  0.093915  0.096513  0.095085  0.272406  0.115956         0.507497\n",
       "  872  0.099140  0.097027  0.097364  0.185647  0.118617         0.477999\n",
       "  873  0.095703  0.097743  0.096596  0.192761  0.117426         0.492927,\n",
       "  0.09986184789539307),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  870  0.098642  0.097408  0.096061  0.319929  0.120747         0.450436\n",
       "  871  0.093915  0.096513  0.095085  0.272406  0.115956         0.507497\n",
       "  872  0.099140  0.097027  0.097364  0.185647  0.118617         0.477999\n",
       "  873  0.095703  0.097743  0.096596  0.192761  0.117426         0.492927\n",
       "  874  0.098724  0.098581  0.099972  0.166006  0.118184         0.508160,\n",
       "  0.10093312473591427),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  871  0.093915  0.096513  0.095085  0.272406  0.115956         0.507497\n",
       "  872  0.099140  0.097027  0.097364  0.185647  0.118617         0.477999\n",
       "  873  0.095703  0.097743  0.096596  0.192761  0.117426         0.492927\n",
       "  874  0.098724  0.098581  0.099972  0.166006  0.118184         0.508160\n",
       "  875  0.101397  0.099999  0.101111  0.141612  0.120932         0.495131,\n",
       "  0.10019737004672542),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  872  0.099140  0.097027  0.097364  0.185647  0.118617         0.477999\n",
       "  873  0.095703  0.097743  0.096596  0.192761  0.117426         0.492927\n",
       "  874  0.098724  0.098581  0.099972  0.166006  0.118184         0.508160\n",
       "  875  0.101397  0.099999  0.101111  0.141612  0.120932         0.495131\n",
       "  876  0.102088  0.101631  0.101021  0.161953  0.121978         0.481619,\n",
       "  0.10415896396563877),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  873  0.095703  0.097743  0.096596  0.192761  0.117426         0.492927\n",
       "  874  0.098724  0.098581  0.099972  0.166006  0.118184         0.508160\n",
       "  875  0.101397  0.099999  0.101111  0.141612  0.120932         0.495131\n",
       "  876  0.102088  0.101631  0.101021  0.161953  0.121978         0.481619\n",
       "  877  0.101874  0.102672  0.102392  0.161039  0.121259         0.516744,\n",
       "  0.10359096554093496),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  874  0.098724  0.098581  0.099972  0.166006  0.118184         0.508160\n",
       "  875  0.101397  0.099999  0.101111  0.141612  0.120932         0.495131\n",
       "  876  0.102088  0.101631  0.101021  0.161953  0.121978         0.481619\n",
       "  877  0.101874  0.102672  0.102392  0.161039  0.121259         0.516744\n",
       "  878  0.105688  0.104106  0.105374  0.157439  0.125128         0.482873,\n",
       "  0.10296545425671368),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  875  0.101397  0.099999  0.101111  0.141612  0.120932         0.495131\n",
       "  876  0.102088  0.101631  0.101021  0.161953  0.121978         0.481619\n",
       "  877  0.101874  0.102672  0.102392  0.161039  0.121259         0.516744\n",
       "  878  0.105688  0.104106  0.105374  0.157439  0.125128         0.482873\n",
       "  879  0.103272  0.102120  0.103758  0.127361  0.124574         0.482443,\n",
       "  0.09800686813430015),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  876  0.102088  0.101631  0.101021  0.161953  0.121978         0.481619\n",
       "  877  0.101874  0.102672  0.102392  0.161039  0.121259         0.516744\n",
       "  878  0.105688  0.104106  0.105374  0.157439  0.125128         0.482873\n",
       "  879  0.103272  0.102120  0.103758  0.127361  0.124574         0.482443\n",
       "  880  0.104868  0.102688  0.099440  0.154367  0.123963         0.450042,\n",
       "  0.09801885759773644),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  877  0.101874  0.102672  0.102392  0.161039  0.121259         0.516744\n",
       "  878  0.105688  0.104106  0.105374  0.157439  0.125128         0.482873\n",
       "  879  0.103272  0.102120  0.103758  0.127361  0.124574         0.482443\n",
       "  880  0.104868  0.102688  0.099440  0.154367  0.123963         0.450042\n",
       "  881  0.097304  0.096989  0.097579  0.161951  0.119120         0.487210,\n",
       "  0.09654733859699645),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  878  0.105688  0.104106  0.105374  0.157439  0.125128         0.482873\n",
       "  879  0.103272  0.102120  0.103758  0.127361  0.124574         0.482443\n",
       "  880  0.104868  0.102688  0.099440  0.154367  0.123963         0.450042\n",
       "  881  0.097304  0.096989  0.097579  0.161951  0.119120         0.487210\n",
       "  882  0.099551  0.097510  0.097744  0.138007  0.119132         0.476117,\n",
       "  0.09773126443306235),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  879  0.103272  0.102120  0.103758  0.127361  0.124574         0.482443\n",
       "  880  0.104868  0.102688  0.099440  0.154367  0.123963         0.450042\n",
       "  881  0.097304  0.096989  0.097579  0.161951  0.119120         0.487210\n",
       "  882  0.099551  0.097510  0.097744  0.138007  0.119132         0.476117\n",
       "  883  0.096955  0.096513  0.097272  0.134069  0.117695         0.495974,\n",
       "  0.09261210995080818),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  880  0.104868  0.102688  0.099440  0.154367  0.123963         0.450042\n",
       "  881  0.097304  0.096989  0.097579  0.161951  0.119120         0.487210\n",
       "  882  0.099551  0.097510  0.097744  0.138007  0.119132         0.476117\n",
       "  883  0.096955  0.096513  0.097272  0.134069  0.117695         0.495974\n",
       "  884  0.097891  0.095920  0.094411  0.203559  0.118851         0.448841,\n",
       "  0.08727965656326322),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  881  0.097304  0.096989  0.097579  0.161951  0.119120         0.487210\n",
       "  882  0.099551  0.097510  0.097744  0.138007  0.119132         0.476117\n",
       "  883  0.096955  0.096513  0.097272  0.134069  0.117695         0.495974\n",
       "  884  0.097891  0.095920  0.094411  0.203559  0.118851         0.448841\n",
       "  885  0.087121  0.086434  0.084991  0.566329  0.113852         0.447246,\n",
       "  0.0893575191025061),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  882  0.099551  0.097510  0.097744  0.138007  0.119132         0.476117\n",
       "  883  0.096955  0.096513  0.097272  0.134069  0.117695         0.495974\n",
       "  884  0.097891  0.095920  0.094411  0.203559  0.118851         0.448841\n",
       "  885  0.087121  0.086434  0.084991  0.566329  0.113852         0.447246\n",
       "  886  0.086864  0.088571  0.088089  0.246103  0.108644         0.502658,\n",
       "  0.08996385301864668),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  883  0.096955  0.096513  0.097272  0.134069  0.117695         0.495974\n",
       "  884  0.097891  0.095920  0.094411  0.203559  0.118851         0.448841\n",
       "  885  0.087121  0.086434  0.084991  0.566329  0.113852         0.447246\n",
       "  886  0.086864  0.088571  0.088089  0.246103  0.108644         0.502658\n",
       "  887  0.089360  0.089425  0.089959  0.160120  0.110673         0.491655,\n",
       "  0.0889548809740725),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  884  0.097891  0.095920  0.094411  0.203559  0.118851         0.448841\n",
       "  885  0.087121  0.086434  0.084991  0.566329  0.113852         0.447246\n",
       "  886  0.086864  0.088571  0.088089  0.246103  0.108644         0.502658\n",
       "  887  0.089360  0.089425  0.089959  0.160120  0.110673         0.491655\n",
       "  888  0.089689  0.089579  0.089303  0.229161  0.111265         0.479576,\n",
       "  0.08556128547986298),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  885  0.087121  0.086434  0.084991  0.566329  0.113852         0.447246\n",
       "  886  0.086864  0.088571  0.088089  0.246103  0.108644         0.502658\n",
       "  887  0.089360  0.089425  0.089959  0.160120  0.110673         0.491655\n",
       "  888  0.089689  0.089579  0.089303  0.229161  0.111265         0.479576\n",
       "  889  0.089603  0.087915  0.085781  0.183655  0.110280         0.461744,\n",
       "  0.08258470392337582),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  886  0.086864  0.088571  0.088089  0.246103  0.108644         0.502658\n",
       "  887  0.089360  0.089425  0.089959  0.160120  0.110673         0.491655\n",
       "  888  0.089689  0.089579  0.089303  0.229161  0.111265         0.479576\n",
       "  889  0.089603  0.087915  0.085781  0.183655  0.110280         0.461744\n",
       "  890  0.084655  0.083327  0.082310  0.331229  0.106966         0.464863,\n",
       "  0.08332764651720904),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  887  0.089360  0.089425  0.089959  0.160120  0.110673         0.491655\n",
       "  888  0.089689  0.089579  0.089303  0.229161  0.111265         0.479576\n",
       "  889  0.089603  0.087915  0.085781  0.183655  0.110280         0.461744\n",
       "  890  0.084655  0.083327  0.082310  0.331229  0.106966         0.464863\n",
       "  891  0.083283  0.083519  0.084301  0.180787  0.104059         0.492676,\n",
       "  0.07973274158114504),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  888  0.089689  0.089579  0.089303  0.229161  0.111265         0.479576\n",
       "  889  0.089603  0.087915  0.085781  0.183655  0.110280         0.461744\n",
       "  890  0.084655  0.083327  0.082310  0.331229  0.106966         0.464863\n",
       "  891  0.083283  0.083519  0.084301  0.180787  0.104059         0.492676\n",
       "  892  0.083870  0.083313  0.081196  0.181602  0.104785         0.460239,\n",
       "  0.07988851800455607),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  889  0.089603  0.087915  0.085781  0.183655  0.110280         0.461744\n",
       "  890  0.084655  0.083327  0.082310  0.331229  0.106966         0.464863\n",
       "  891  0.083283  0.083519  0.084301  0.180787  0.104059         0.492676\n",
       "  892  0.083870  0.083313  0.081196  0.181602  0.104785         0.460239\n",
       "  893  0.079343  0.080161  0.079999  0.196237  0.101274         0.488285,\n",
       "  0.0751600121872992),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  890  0.084655  0.083327  0.082310  0.331229  0.106966         0.464863\n",
       "  891  0.083283  0.083519  0.084301  0.180787  0.104059         0.492676\n",
       "  892  0.083870  0.083313  0.081196  0.181602  0.104785         0.460239\n",
       "  893  0.079343  0.080161  0.079999  0.196237  0.101274         0.488285\n",
       "  894  0.079331  0.078028  0.076802  0.194189  0.101426         0.451762,\n",
       "  0.08144391513753858),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  891  0.083283  0.083519  0.084301  0.180787  0.104059         0.492676\n",
       "  892  0.083870  0.083313  0.081196  0.181602  0.104785         0.460239\n",
       "  893  0.079343  0.080161  0.079999  0.196237  0.101274         0.488285\n",
       "  894  0.079331  0.078028  0.076802  0.194189  0.101426         0.451762\n",
       "  895  0.078472  0.080197  0.079614  0.214604  0.096808         0.534109,\n",
       "  0.0815253973015663),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  892  0.083870  0.083313  0.081196  0.181602  0.104785         0.460239\n",
       "  893  0.079343  0.080161  0.079999  0.196237  0.101274         0.488285\n",
       "  894  0.079331  0.078028  0.076802  0.194189  0.101426         0.451762\n",
       "  895  0.078472  0.080197  0.079614  0.214604  0.096808         0.534109\n",
       "  896  0.082022  0.082028  0.082865  0.161701  0.102945         0.487730,\n",
       "  0.08220363913123771),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  893  0.079343  0.080161  0.079999  0.196237  0.101274         0.488285\n",
       "  894  0.079331  0.078028  0.076802  0.194189  0.101426         0.451762\n",
       "  895  0.078472  0.080197  0.079614  0.214604  0.096808         0.534109\n",
       "  896  0.082022  0.082028  0.082865  0.161701  0.102945         0.487730\n",
       "  897  0.080255  0.081815  0.080615  0.187841  0.103025         0.492192,\n",
       "  0.08098376414958755),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  894  0.079331  0.078028  0.076802  0.194189  0.101426         0.451762\n",
       "  895  0.078472  0.080197  0.079614  0.214604  0.096808         0.534109\n",
       "  896  0.082022  0.082028  0.082865  0.161701  0.102945         0.487730\n",
       "  897  0.080255  0.081815  0.080615  0.187841  0.103025         0.492192\n",
       "  898  0.082737  0.082374  0.081060  0.159815  0.103687         0.477999,\n",
       "  0.07665789646076429),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  895  0.078472  0.080197  0.079614  0.214604  0.096808         0.534109\n",
       "  896  0.082022  0.082028  0.082865  0.161701  0.102945         0.487730\n",
       "  897  0.080255  0.081815  0.080615  0.187841  0.103025         0.492192\n",
       "  898  0.082737  0.082374  0.081060  0.159815  0.103687         0.477999\n",
       "  899  0.079129  0.078670  0.076577  0.191771  0.102496         0.454773,\n",
       "  0.08128095080948314),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  896  0.082022  0.082028  0.082865  0.161701  0.102945         0.487730\n",
       "  897  0.080255  0.081815  0.080615  0.187841  0.103025         0.492192\n",
       "  898  0.082737  0.082374  0.081060  0.159815  0.103687         0.477999\n",
       "  899  0.079129  0.078670  0.076577  0.191771  0.102496         0.454773\n",
       "  900  0.079367  0.080360  0.079827  0.175937  0.098271         0.521690,\n",
       "  0.0795649756942977),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  897  0.080255  0.081815  0.080615  0.187841  0.103025         0.492192\n",
       "  898  0.082737  0.082374  0.081060  0.159815  0.103687         0.477999\n",
       "  899  0.079129  0.078670  0.076577  0.191771  0.102496         0.454773\n",
       "  900  0.079367  0.080360  0.079827  0.175937  0.098271         0.521690\n",
       "  901  0.082287  0.081938  0.081315  0.150198  0.102786         0.474289,\n",
       "  0.07993644699121433),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  898  0.082737  0.082374  0.081060  0.159815  0.103687         0.477999\n",
       "  899  0.079129  0.078670  0.076577  0.191771  0.102496         0.454773\n",
       "  900  0.079367  0.080360  0.079827  0.175937  0.098271         0.521690\n",
       "  901  0.082287  0.081938  0.081315  0.150198  0.102786         0.474289\n",
       "  902  0.078968  0.079142  0.077882  0.241031  0.101110         0.489898,\n",
       "  0.08640968916459651),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  899  0.079129  0.078670  0.076577  0.191771  0.102496         0.454773\n",
       "  900  0.079367  0.080360  0.079827  0.175937  0.098271         0.521690\n",
       "  901  0.082287  0.081938  0.081315  0.150198  0.102786         0.474289\n",
       "  902  0.078968  0.079142  0.077882  0.241031  0.101110         0.489898\n",
       "  903  0.081642  0.086270  0.082780  0.164415  0.101473         0.535525,\n",
       "  0.08406580679697502),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  900  0.079367  0.080360  0.079827  0.175937  0.098271         0.521690\n",
       "  901  0.082287  0.081938  0.081315  0.150198  0.102786         0.474289\n",
       "  902  0.078968  0.079142  0.077882  0.241031  0.101110         0.489898\n",
       "  903  0.081642  0.086270  0.082780  0.164415  0.101473         0.535525\n",
       "  904  0.086221  0.084301  0.084786  0.141748  0.107794         0.469594,\n",
       "  0.08584169073989269),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  901  0.082287  0.081938  0.081315  0.150198  0.102786         0.474289\n",
       "  902  0.078968  0.079142  0.077882  0.241031  0.101110         0.489898\n",
       "  903  0.081642  0.086270  0.082780  0.164415  0.101473         0.535525\n",
       "  904  0.086221  0.084301  0.084786  0.141748  0.107794         0.469594\n",
       "  905  0.085049  0.085581  0.085582  0.127011  0.105505         0.500400,\n",
       "  0.08246007508675714),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  902  0.078968  0.079142  0.077882  0.241031  0.101110         0.489898\n",
       "  903  0.081642  0.086270  0.082780  0.164415  0.101473         0.535525\n",
       "  904  0.086221  0.084301  0.084786  0.141748  0.107794         0.469594\n",
       "  905  0.085049  0.085581  0.085582  0.127011  0.105505         0.500400\n",
       "  906  0.083463  0.082201  0.083451  0.141763  0.107240         0.461834,\n",
       "  0.08102211888574892),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  903  0.081642  0.086270  0.082780  0.164415  0.101473         0.535525\n",
       "  904  0.086221  0.084301  0.084786  0.141748  0.107794         0.469594\n",
       "  905  0.085049  0.085581  0.085582  0.127011  0.105505         0.500400\n",
       "  906  0.083463  0.082201  0.083451  0.141763  0.107240         0.461834\n",
       "  907  0.083815  0.083377  0.081954  0.106348  0.103937         0.476368,\n",
       "  0.08159489962452003),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  904  0.086221  0.084301  0.084786  0.141748  0.107794         0.469594\n",
       "  905  0.085049  0.085581  0.085582  0.127011  0.105505         0.500400\n",
       "  906  0.083463  0.082201  0.083451  0.141763  0.107240         0.461834\n",
       "  907  0.083815  0.083377  0.081954  0.106348  0.103937         0.476368\n",
       "  908  0.080970  0.080798  0.081397  0.108055  0.102533         0.491404,\n",
       "  0.08764393995553545),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  905  0.085049  0.085581  0.085582  0.127011  0.105505         0.500400\n",
       "  906  0.083463  0.082201  0.083451  0.141763  0.107240         0.461834\n",
       "  907  0.083815  0.083377  0.081954  0.106348  0.103937         0.476368\n",
       "  908  0.080970  0.080798  0.081397  0.108055  0.102533         0.491404\n",
       "  909  0.083882  0.086441  0.084996  0.148109  0.103092         0.532353,\n",
       "  0.08764154398732064),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  906  0.083463  0.082201  0.083451  0.141763  0.107240         0.461834\n",
       "  907  0.083815  0.083377  0.081954  0.106348  0.103937         0.476368\n",
       "  908  0.080970  0.080798  0.081397  0.108055  0.102533         0.491404\n",
       "  909  0.083882  0.086441  0.084996  0.148109  0.103092         0.532353\n",
       "  910  0.089215  0.088631  0.088271  0.145820  0.109000         0.487103,\n",
       "  0.08510593605070384),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  907  0.083815  0.083377  0.081954  0.106348  0.103937         0.476368\n",
       "  908  0.080970  0.080798  0.081397  0.108055  0.102533         0.491404\n",
       "  909  0.083882  0.086441  0.084996  0.148109  0.103092         0.532353\n",
       "  910  0.089215  0.088631  0.088271  0.145820  0.109000         0.487103\n",
       "  911  0.087249  0.085680  0.085301  0.164777  0.108998         0.468160,\n",
       "  0.08893331726013934),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  908  0.080970  0.080798  0.081397  0.108055  0.102533         0.491404\n",
       "  909  0.083882  0.086441  0.084996  0.148109  0.103092         0.532353\n",
       "  910  0.089215  0.088631  0.088271  0.145820  0.109000         0.487103\n",
       "  911  0.087249  0.085680  0.085301  0.164777  0.108998         0.468160\n",
       "  912  0.086113  0.089126  0.087256  0.143344  0.106521         0.515740,\n",
       "  0.09030657269698572),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  909  0.083882  0.086441  0.084996  0.148109  0.103092         0.532353\n",
       "  910  0.089215  0.088631  0.088271  0.145820  0.109000         0.487103\n",
       "  911  0.087249  0.085680  0.085301  0.164777  0.108998         0.468160\n",
       "  912  0.086113  0.089126  0.087256  0.143344  0.106521         0.515740\n",
       "  913  0.090931  0.090929  0.091454  0.144470  0.110259         0.497389,\n",
       "  0.08814722799371513),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  910  0.089215  0.088631  0.088271  0.145820  0.109000         0.487103\n",
       "  911  0.087249  0.085680  0.085301  0.164777  0.108998         0.468160\n",
       "  912  0.086113  0.089126  0.087256  0.143344  0.106521         0.515740\n",
       "  913  0.090931  0.090929  0.091454  0.144470  0.110259         0.497389\n",
       "  914  0.089971  0.088252  0.088663  0.110502  0.111600         0.470974,\n",
       "  0.08727965656326322),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  911  0.087249  0.085680  0.085301  0.164777  0.108998         0.468160\n",
       "  912  0.086113  0.089126  0.087256  0.143344  0.106521         0.515740\n",
       "  913  0.090931  0.090929  0.091454  0.144470  0.110259         0.497389\n",
       "  914  0.089971  0.088252  0.088663  0.110502  0.111600         0.470974\n",
       "  915  0.089030  0.087744  0.087581  0.147014  0.109491         0.480633,\n",
       "  0.08845159293589283),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  912  0.086113  0.089126  0.087256  0.143344  0.106521         0.515740\n",
       "  913  0.090931  0.090929  0.091454  0.144470  0.110259         0.497389\n",
       "  914  0.089971  0.088252  0.088663  0.110502  0.111600         0.470974\n",
       "  915  0.089030  0.087744  0.087581  0.147014  0.109491         0.480633\n",
       "  916  0.088026  0.088185  0.088625  0.123282  0.108644         0.495884,\n",
       "  0.08566434098018617),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  913  0.090931  0.090929  0.091454  0.144470  0.110259         0.497389\n",
       "  914  0.089971  0.088252  0.088663  0.110502  0.111600         0.470974\n",
       "  915  0.089030  0.087744  0.087581  0.147014  0.109491         0.480633\n",
       "  916  0.088026  0.088185  0.088625  0.123282  0.108644         0.495884\n",
       "  917  0.087812  0.086621  0.086968  0.101325  0.109789         0.466278,\n",
       "  0.08427670973405099),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  914  0.089971  0.088252  0.088663  0.110502  0.111600         0.470974\n",
       "  915  0.089030  0.087744  0.087581  0.147014  0.109491         0.480633\n",
       "  916  0.088026  0.088185  0.088625  0.123282  0.108644         0.495884\n",
       "  917  0.087812  0.086621  0.086968  0.101325  0.109789         0.466278\n",
       "  918  0.086149  0.084296  0.085791  0.093369  0.107067         0.476744,\n",
       "  0.08312154513892496),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  915  0.089030  0.087744  0.087581  0.147014  0.109491         0.480633\n",
       "  916  0.088026  0.088185  0.088625  0.123282  0.108644         0.495884\n",
       "  917  0.087812  0.086621  0.086968  0.101325  0.109789         0.466278\n",
       "  918  0.086149  0.084296  0.085791  0.093369  0.107067         0.476744\n",
       "  919  0.086383  0.084834  0.084233  0.111247  0.105711         0.478483,\n",
       "  0.08508436271440836),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  916  0.088026  0.088185  0.088625  0.123282  0.108644         0.495884\n",
       "  917  0.087812  0.086621  0.086968  0.101325  0.109789         0.466278\n",
       "  918  0.086149  0.084296  0.085791  0.093369  0.107067         0.476744\n",
       "  919  0.086383  0.084834  0.084233  0.111247  0.105711         0.478483\n",
       "  920  0.083078  0.084493  0.083574  0.185508  0.104583         0.501798,\n",
       "  0.08376862975944172),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  917  0.087812  0.086621  0.086968  0.101325  0.109789         0.466278\n",
       "  918  0.086149  0.084296  0.085791  0.093369  0.107067         0.476744\n",
       "  919  0.086383  0.084834  0.084233  0.111247  0.105711         0.478483\n",
       "  920  0.083078  0.084493  0.083574  0.185508  0.104583         0.501798\n",
       "  921  0.085781  0.084751  0.084911  0.324694  0.106500         0.477282,\n",
       "  0.10530694065612041),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  918  0.086149  0.084296  0.085791  0.093369  0.107067         0.476744\n",
       "  919  0.086383  0.084834  0.084233  0.111247  0.105711         0.478483\n",
       "  920  0.083078  0.084493  0.083574  0.185508  0.104583         0.501798\n",
       "  921  0.085781  0.084751  0.084911  0.324694  0.106500         0.477282\n",
       "  922  0.104933  0.105684  0.103865  0.443455  0.105215         0.648177,\n",
       "  0.10491868795933788),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  919  0.086383  0.084834  0.084233  0.111247  0.105711         0.478483\n",
       "  920  0.083078  0.084493  0.083574  0.185508  0.104583         0.501798\n",
       "  921  0.085781  0.084751  0.084911  0.324694  0.106500         0.477282\n",
       "  922  0.104933  0.105684  0.103865  0.443455  0.105215         0.648177\n",
       "  923  0.105939  0.104473  0.105202  0.180862  0.126249         0.484217,\n",
       "  0.10904324620630669),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  920  0.083078  0.084493  0.083574  0.185508  0.104583         0.501798\n",
       "  921  0.085781  0.084751  0.084911  0.324694  0.106500         0.477282\n",
       "  922  0.104933  0.105684  0.103865  0.443455  0.105215         0.648177\n",
       "  923  0.105939  0.104473  0.105202  0.180862  0.126249         0.484217\n",
       "  924  0.105501  0.108795  0.106963  0.192995  0.125870         0.517963,\n",
       "  0.10700373840322518),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  921  0.085781  0.084751  0.084911  0.324694  0.106500         0.477282\n",
       "  922  0.104933  0.105684  0.103865  0.443455  0.105215         0.648177\n",
       "  923  0.105939  0.104473  0.105202  0.180862  0.126249         0.484217\n",
       "  924  0.105501  0.108795  0.106963  0.192995  0.125870         0.517963\n",
       "  925  0.110403  0.108435  0.107784  0.119611  0.129898         0.471870,\n",
       "  0.10617690805478712),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  922  0.104933  0.105684  0.103865  0.443455  0.105215         0.648177\n",
       "  923  0.105939  0.104473  0.105202  0.180862  0.126249         0.484217\n",
       "  924  0.105501  0.108795  0.106963  0.192995  0.125870         0.517963\n",
       "  925  0.110403  0.108435  0.107784  0.119611  0.129898         0.471870\n",
       "  926  0.108802  0.107293  0.107559  0.100484  0.127906         0.480938,\n",
       "  0.10642136416923255),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  923  0.105939  0.104473  0.105202  0.180862  0.126249         0.484217\n",
       "  924  0.105501  0.108795  0.106963  0.192995  0.125870         0.517963\n",
       "  925  0.110403  0.108435  0.107784  0.119611  0.129898         0.471870\n",
       "  926  0.108802  0.107293  0.107559  0.100484  0.127906         0.480938\n",
       "  927  0.108256  0.106940  0.108215  0.101196  0.127099         0.488949,\n",
       "  0.10835302453556127),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  924  0.105501  0.108795  0.106963  0.192995  0.125870         0.517963\n",
       "  925  0.110403  0.108435  0.107784  0.119611  0.129898         0.471870\n",
       "  926  0.108802  0.107293  0.107559  0.100484  0.127906         0.480938\n",
       "  927  0.108256  0.106940  0.108215  0.101196  0.127099         0.488949\n",
       "  928  0.107500  0.107843  0.107312  0.097407  0.127338         0.501565,\n",
       "  0.10987486849117432),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  925  0.110403  0.108435  0.107784  0.119611  0.129898         0.471870\n",
       "  926  0.108802  0.107293  0.107559  0.100484  0.127906         0.480938\n",
       "  927  0.108256  0.106940  0.108215  0.101196  0.127099         0.488949\n",
       "  928  0.107500  0.107843  0.107312  0.097407  0.127338         0.501565\n",
       "  929  0.108692  0.109404  0.109978  0.105613  0.129224         0.498500,\n",
       "  0.11366630055502554),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  926  0.108802  0.107293  0.107559  0.100484  0.127906         0.480938\n",
       "  927  0.108256  0.106940  0.108215  0.101196  0.127099         0.488949\n",
       "  928  0.107500  0.107843  0.107312  0.097407  0.127338         0.501565\n",
       "  929  0.108692  0.109404  0.109978  0.105613  0.129224         0.498500\n",
       "  930  0.111430  0.114490  0.112028  0.192131  0.130710         0.515472,\n",
       "  0.11816952762591766),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  927  0.108256  0.106940  0.108215  0.101196  0.127099         0.488949\n",
       "  928  0.107500  0.107843  0.107312  0.097407  0.127338         0.501565\n",
       "  929  0.108692  0.109404  0.109978  0.105613  0.129224         0.498500\n",
       "  930  0.111430  0.114490  0.112028  0.192131  0.130710         0.515472\n",
       "  931  0.115305  0.116876  0.116509  0.160506  0.134413         0.520794,\n",
       "  0.11534393055641194),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  928  0.107500  0.107843  0.107312  0.097407  0.127338         0.501565\n",
       "  929  0.108692  0.109404  0.109978  0.105613  0.129224         0.498500\n",
       "  930  0.111430  0.114490  0.112028  0.192131  0.130710         0.515472\n",
       "  931  0.115305  0.116876  0.116509  0.160506  0.134413         0.520794\n",
       "  932  0.120162  0.118658  0.117073  0.170138  0.138811         0.465992,\n",
       "  0.11860570930935842),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  929  0.108692  0.109404  0.109978  0.105613  0.129224         0.498500\n",
       "  930  0.111430  0.114490  0.112028  0.192131  0.130710         0.515472\n",
       "  931  0.115305  0.116876  0.116509  0.160506  0.134413         0.520794\n",
       "  932  0.120162  0.118658  0.117073  0.170138  0.138811         0.465992\n",
       "  933  0.120251  0.117841  0.118970  0.152679  0.136051         0.511511,\n",
       "  0.1165590136016325),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  930  0.111430  0.114490  0.112028  0.192131  0.130710         0.515472\n",
       "  931  0.115305  0.116876  0.116509  0.160506  0.134413         0.520794\n",
       "  932  0.120162  0.118658  0.117073  0.170138  0.138811         0.465992\n",
       "  933  0.120251  0.117841  0.118970  0.152679  0.136051         0.511511\n",
       "  934  0.118359  0.116182  0.117969  0.112504  0.139237         0.471816,\n",
       "  0.11479510949978881),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  931  0.115305  0.116876  0.116509  0.160506  0.134413         0.520794\n",
       "  932  0.120162  0.118658  0.117073  0.170138  0.138811         0.465992\n",
       "  933  0.120251  0.117841  0.118970  0.152679  0.136051         0.511511\n",
       "  934  0.118359  0.116182  0.117969  0.112504  0.139237         0.471816\n",
       "  935  0.118183  0.117957  0.116378  0.160754  0.137238         0.473931,\n",
       "  0.11575614293534243),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  932  0.120162  0.118658  0.117073  0.170138  0.138811         0.465992\n",
       "  933  0.120251  0.117841  0.118970  0.152679  0.136051         0.511511\n",
       "  934  0.118359  0.116182  0.117969  0.112504  0.139237         0.471816\n",
       "  935  0.118183  0.117957  0.116378  0.160754  0.137238         0.473931\n",
       "  936  0.117204  0.115542  0.116979  0.124510  0.135515         0.494307,\n",
       "  0.11340507266307652),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  933  0.120251  0.117841  0.118970  0.152679  0.136051         0.511511\n",
       "  934  0.118359  0.116182  0.117969  0.112504  0.139237         0.471816\n",
       "  935  0.118183  0.117957  0.116378  0.160754  0.137238         0.473931\n",
       "  936  0.117204  0.115542  0.116979  0.124510  0.135515         0.494307\n",
       "  937  0.115471  0.114523  0.115135  0.108992  0.136454         0.469540,\n",
       "  0.11621868989150826),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  934  0.118359  0.116182  0.117969  0.112504  0.139237         0.471816\n",
       "  935  0.118183  0.117957  0.116378  0.160754  0.137238         0.473931\n",
       "  936  0.117204  0.115542  0.116979  0.124510  0.135515         0.494307\n",
       "  937  0.115471  0.114523  0.115135  0.108992  0.136454         0.469540\n",
       "  938  0.114448  0.114938  0.114556  0.118210  0.134158         0.508160,\n",
       "  0.11575374696712763),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  935  0.118183  0.117957  0.116378  0.160754  0.137238         0.473931\n",
       "  936  0.117204  0.115542  0.116979  0.124510  0.135515         0.494307\n",
       "  937  0.115471  0.114523  0.115135  0.108992  0.136454         0.469540\n",
       "  938  0.114448  0.114938  0.114556  0.118210  0.134158         0.508160\n",
       "  939  0.117211  0.115222  0.116891  0.125498  0.136906         0.483644,\n",
       "  0.11414802487927207),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  936  0.117204  0.115542  0.116979  0.124510  0.135515         0.494307\n",
       "  937  0.115471  0.114523  0.115135  0.108992  0.136454         0.469540\n",
       "  938  0.114448  0.114938  0.114556  0.118210  0.134158         0.508160\n",
       "  939  0.117211  0.115222  0.116891  0.125498  0.136906         0.483644\n",
       "  940  0.117274  0.115990  0.116092  0.106338  0.136452         0.475114,\n",
       "  0.11527442823345822),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  937  0.115471  0.114523  0.115135  0.108992  0.136454         0.469540\n",
       "  938  0.114448  0.114938  0.114556  0.118210  0.134158         0.508160\n",
       "  939  0.117211  0.115222  0.116891  0.125498  0.136906         0.483644\n",
       "  940  0.117274  0.115990  0.116092  0.106338  0.136452         0.475114\n",
       "  941  0.115471  0.114037  0.116179  0.105533  0.134884         0.495543,\n",
       "  0.11505154545530825),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  938  0.114448  0.114938  0.114556  0.118210  0.134158         0.508160\n",
       "  939  0.117211  0.115222  0.116891  0.125498  0.136906         0.483644\n",
       "  940  0.117274  0.115990  0.116092  0.106338  0.136452         0.475114\n",
       "  941  0.115471  0.114037  0.116179  0.105533  0.134884         0.495543\n",
       "  942  0.116056  0.114447  0.116836  0.103864  0.135984         0.485454,\n",
       "  0.11444040998037579),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  939  0.117211  0.115222  0.116891  0.125498  0.136906         0.483644\n",
       "  940  0.117274  0.115990  0.116092  0.106338  0.136452         0.475114\n",
       "  941  0.115471  0.114037  0.116179  0.105533  0.134884         0.495543\n",
       "  942  0.116056  0.114447  0.116836  0.103864  0.135984         0.485454\n",
       "  943  0.115363  0.115443  0.115673  0.136224  0.135766         0.482551,\n",
       "  0.11469924190411),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  940  0.117274  0.115990  0.116092  0.106338  0.136452         0.475114\n",
       "  941  0.115471  0.114037  0.116179  0.105533  0.134884         0.495543\n",
       "  942  0.116056  0.114447  0.116836  0.103864  0.135984         0.485454\n",
       "  943  0.115363  0.115443  0.115673  0.136224  0.135766         0.482551\n",
       "  944  0.114419  0.113926  0.115525  0.080502  0.135169         0.489056,\n",
       "  0.10784254859273722),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  941  0.115471  0.114037  0.116179  0.105533  0.134884         0.495543\n",
       "  942  0.116056  0.114447  0.116836  0.103864  0.135984         0.485454\n",
       "  943  0.115363  0.115443  0.115673  0.136224  0.135766         0.482551\n",
       "  944  0.114419  0.113926  0.115525  0.080502  0.135169         0.489056\n",
       "  945  0.115356  0.113672  0.109465  0.157230  0.135422         0.435848,\n",
       "  0.10771552378790374),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  942  0.116056  0.114447  0.116836  0.103864  0.135984         0.485454\n",
       "  943  0.115363  0.115443  0.115673  0.136224  0.135766         0.482551\n",
       "  944  0.114419  0.113926  0.115525  0.080502  0.135169         0.489056\n",
       "  945  0.115356  0.113672  0.109465  0.157230  0.135422         0.435848\n",
       "  946  0.108961  0.107227  0.107614  0.123377  0.128726         0.486171,\n",
       "  0.10655556725634811),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  943  0.115363  0.115443  0.115673  0.136224  0.135766         0.482551\n",
       "  944  0.114419  0.113926  0.115525  0.080502  0.135169         0.489056\n",
       "  945  0.115356  0.113672  0.109465  0.157230  0.135422         0.435848\n",
       "  946  0.108961  0.107227  0.107614  0.123377  0.128726         0.486171\n",
       "  947  0.107760  0.106940  0.107023  0.107693  0.128602         0.478447,\n",
       "  0.11045724272516692),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  944  0.114419  0.113926  0.115525  0.080502  0.135169         0.489056\n",
       "  945  0.115356  0.113672  0.109465  0.157230  0.135422         0.435848\n",
       "  946  0.108961  0.107227  0.107614  0.123377  0.128726         0.486171\n",
       "  947  0.107760  0.106940  0.107023  0.107693  0.128602         0.478447\n",
       "  948  0.107279  0.109215  0.108486  0.093926  0.127469         0.516296,\n",
       "  0.11221635489058103),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  945  0.115356  0.113672  0.109465  0.157230  0.135422         0.435848\n",
       "  946  0.108961  0.107227  0.107614  0.123377  0.128726         0.486171\n",
       "  947  0.107760  0.106940  0.107023  0.107693  0.128602         0.478447\n",
       "  948  0.107279  0.109215  0.108486  0.093926  0.127469         0.516296\n",
       "  949  0.112809  0.111322  0.112665  0.098423  0.131279         0.500275,\n",
       "  0.11575374696712763),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  946  0.108961  0.107227  0.107614  0.123377  0.128726         0.486171\n",
       "  947  0.107760  0.106940  0.107023  0.107693  0.128602         0.478447\n",
       "  948  0.107279  0.109215  0.108486  0.093926  0.127469         0.516296\n",
       "  949  0.112809  0.111322  0.112665  0.098423  0.131279         0.500275\n",
       "  950  0.114458  0.114732  0.115595  0.117751  0.132997         0.513572,\n",
       "  0.11642480089215464),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  947  0.107760  0.106940  0.107023  0.107693  0.128602         0.478447\n",
       "  948  0.107279  0.109215  0.108486  0.093926  0.127469         0.516296\n",
       "  949  0.112809  0.111322  0.112665  0.098423  0.131279         0.500275\n",
       "  950  0.114458  0.114732  0.115595  0.117751  0.132997         0.513572\n",
       "  951  0.116547  0.115689  0.117557  0.078352  0.136452         0.492138,\n",
       "  0.11383646241008767),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  948  0.107279  0.109215  0.108486  0.093926  0.127469         0.516296\n",
       "  949  0.112809  0.111322  0.112665  0.098423  0.131279         0.500275\n",
       "  950  0.114458  0.114732  0.115595  0.117751  0.132997         0.513572\n",
       "  951  0.116547  0.115689  0.117557  0.078352  0.136452         0.492138\n",
       "  952  0.116313  0.114260  0.115128  0.089273  0.137107         0.467766,\n",
       "  0.11199107614421626),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  949  0.112809  0.111322  0.112665  0.098423  0.131279         0.500275\n",
       "  950  0.114458  0.114732  0.115595  0.117751  0.132997         0.513572\n",
       "  951  0.116547  0.115689  0.117557  0.078352  0.136452         0.492138\n",
       "  952  0.116313  0.114260  0.115128  0.089273  0.137107         0.467766\n",
       "  953  0.114869  0.113457  0.112546  0.104623  0.134579         0.473321,\n",
       "  0.11317021004385257),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  950  0.114458  0.114732  0.115595  0.117751  0.132997         0.513572\n",
       "  951  0.116547  0.115689  0.117557  0.078352  0.136452         0.492138\n",
       "  952  0.116313  0.114260  0.115128  0.089273  0.137107         0.467766\n",
       "  953  0.114869  0.113457  0.112546  0.104623  0.134579         0.473321\n",
       "  954  0.112181  0.112864  0.113500  0.081708  0.132777         0.495938,\n",
       "  0.11654462816998142),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  951  0.116547  0.115689  0.117557  0.078352  0.136452         0.492138\n",
       "  952  0.116313  0.114260  0.115128  0.089273  0.137107         0.467766\n",
       "  953  0.114869  0.113457  0.112546  0.104623  0.134579         0.473321\n",
       "  954  0.112181  0.112864  0.113500  0.081708  0.132777         0.495938\n",
       "  955  0.115009  0.115244  0.116325  0.095171  0.133929         0.512353,\n",
       "  0.11191917785304774),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  952  0.116313  0.114260  0.115128  0.089273  0.137107         0.467766\n",
       "  953  0.114869  0.113457  0.112546  0.104623  0.134579         0.473321\n",
       "  954  0.112181  0.112864  0.113500  0.081708  0.132777         0.495938\n",
       "  955  0.115009  0.115244  0.116325  0.095171  0.133929         0.512353\n",
       "  956  0.115652  0.113509  0.114094  0.115063  0.137224         0.452533,\n",
       "  0.10973107190883728),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  953  0.114869  0.113457  0.112546  0.104623  0.134579         0.473321\n",
       "  954  0.112181  0.112864  0.113500  0.081708  0.132777         0.495938\n",
       "  955  0.115009  0.115244  0.116325  0.095171  0.133929         0.512353\n",
       "  956  0.115652  0.113509  0.114094  0.115063  0.137224         0.452533\n",
       "  957  0.112838  0.110968  0.108862  0.128533  0.132707         0.470759,\n",
       "  0.10884433273266698),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  954  0.112181  0.112864  0.113500  0.081708  0.132777         0.495938\n",
       "  955  0.115009  0.115244  0.116325  0.095171  0.133929         0.512353\n",
       "  956  0.115652  0.113509  0.114094  0.115063  0.137224         0.452533\n",
       "  957  0.112838  0.110968  0.108862  0.128533  0.132707         0.470759\n",
       "  958  0.108386  0.108319  0.109196  0.088864  0.130570         0.480490,\n",
       "  0.10669457190225558),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  955  0.115009  0.115244  0.116325  0.095171  0.133929         0.512353\n",
       "  956  0.115652  0.113509  0.114094  0.115063  0.137224         0.452533\n",
       "  957  0.112838  0.110968  0.108862  0.128533  0.132707         0.470759\n",
       "  958  0.108386  0.108319  0.109196  0.088864  0.130570         0.480490\n",
       "  959  0.109994  0.107968  0.108634  0.092647  0.129704         0.471045,\n",
       "  0.1085519476315633),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  956  0.115652  0.113509  0.114094  0.115063  0.137224         0.452533\n",
       "  957  0.112838  0.110968  0.108862  0.128533  0.132707         0.470759\n",
       "  958  0.108386  0.108319  0.109196  0.088864  0.130570         0.480490\n",
       "  959  0.109994  0.107968  0.108634  0.092647  0.129704         0.471045\n",
       "  960  0.108194  0.108125  0.109220  0.133478  0.127605         0.501009,\n",
       "  0.11300004818879043),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  957  0.112838  0.110968  0.108862  0.128533  0.132707         0.470759\n",
       "  958  0.108386  0.108319  0.109196  0.088864  0.130570         0.480490\n",
       "  959  0.109994  0.107968  0.108634  0.092647  0.129704         0.471045\n",
       "  960  0.108194  0.108125  0.109220  0.133478  0.127605         0.501009\n",
       "  961  0.111257  0.112319  0.112723  0.150329  0.129418         0.520382,\n",
       "  0.11331161065797483),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  958  0.108386  0.108319  0.109196  0.088864  0.130570         0.480490\n",
       "  959  0.109994  0.107968  0.108634  0.092647  0.129704         0.471045\n",
       "  960  0.108194  0.108125  0.109220  0.133478  0.127605         0.501009\n",
       "  961  0.111257  0.112319  0.112723  0.150329  0.129418         0.520382\n",
       "  962  0.112463  0.113125  0.113973  0.086077  0.133762         0.489450,\n",
       "  0.11250873999168474),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  959  0.109994  0.107968  0.108634  0.092647  0.129704         0.471045\n",
       "  960  0.108194  0.108125  0.109220  0.133478  0.127605         0.501009\n",
       "  961  0.111257  0.112319  0.112723  0.150329  0.129418         0.520382\n",
       "  962  0.112463  0.113125  0.113973  0.086077  0.133762         0.489450\n",
       "  963  0.114833  0.112881  0.114428  0.084133  0.134067         0.481117,\n",
       "  0.11081194224457998),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  960  0.108194  0.108125  0.109220  0.133478  0.127605         0.501009\n",
       "  961  0.111257  0.112319  0.112723  0.150329  0.129418         0.520382\n",
       "  962  0.112463  0.113125  0.113973  0.086077  0.133762         0.489450\n",
       "  963  0.114833  0.112881  0.114428  0.084133  0.134067         0.481117\n",
       "  964  0.111984  0.110729  0.112195  0.082146  0.133283         0.474432,\n",
       "  0.11028948646068192),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  961  0.111257  0.112319  0.112723  0.150329  0.129418         0.520382\n",
       "  962  0.112463  0.113125  0.113973  0.086077  0.133762         0.489450\n",
       "  963  0.114833  0.112881  0.114428  0.084133  0.134067         0.481117\n",
       "  964  0.111984  0.110729  0.112195  0.082146  0.133283         0.474432\n",
       "  965  0.109780  0.109494  0.110172  0.138136  0.131626         0.483214,\n",
       "  0.1069893529715741),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  962  0.112463  0.113125  0.113973  0.086077  0.133762         0.489450\n",
       "  963  0.114833  0.112881  0.114428  0.084133  0.134067         0.481117\n",
       "  964  0.111984  0.110729  0.112195  0.082146  0.133283         0.474432\n",
       "  965  0.109780  0.109494  0.110172  0.138136  0.131626         0.483214\n",
       "  966  0.110172  0.107883  0.108646  0.145433  0.131115         0.462443,\n",
       "  0.10669696787047038),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  963  0.114833  0.112881  0.114428  0.084133  0.134067         0.481117\n",
       "  964  0.111984  0.110729  0.112195  0.082146  0.133283         0.474432\n",
       "  965  0.109780  0.109494  0.110172  0.138136  0.131626         0.483214\n",
       "  966  0.110172  0.107883  0.108646  0.145433  0.131115         0.462443\n",
       "  967  0.107375  0.106975  0.108043  0.088341  0.127892         0.484934,\n",
       "  0.1059995582950806),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  964  0.111984  0.110729  0.112195  0.082146  0.133283         0.474432\n",
       "  965  0.109780  0.109494  0.110172  0.138136  0.131626         0.483214\n",
       "  966  0.110172  0.107883  0.108646  0.145433  0.131115         0.462443\n",
       "  967  0.107375  0.106975  0.108043  0.088341  0.127892         0.484934\n",
       "  968  0.107322  0.106511  0.106367  0.113557  0.127607         0.481906,\n",
       "  0.10808460873896787),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  965  0.109780  0.109494  0.110172  0.138136  0.131626         0.483214\n",
       "  966  0.110172  0.107883  0.108646  0.145433  0.131115         0.462443\n",
       "  967  0.107375  0.106975  0.108043  0.088341  0.127892         0.484934\n",
       "  968  0.107322  0.106511  0.106367  0.113557  0.127607         0.481906\n",
       "  969  0.107281  0.108077  0.108443  0.100170  0.126926         0.502712,\n",
       "  0.1027689271289265),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  966  0.110172  0.107883  0.108646  0.145433  0.131115         0.462443\n",
       "  967  0.107375  0.106975  0.108043  0.088341  0.127892         0.484934\n",
       "  968  0.107322  0.106511  0.106367  0.113557  0.127607         0.481906\n",
       "  969  0.107281  0.108077  0.108443  0.100170  0.126926         0.502712\n",
       "  970  0.107072  0.105027  0.104710  0.137562  0.128962         0.447372,\n",
       "  0.10258918140100516),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  967  0.107375  0.106975  0.108043  0.088341  0.127892         0.484934\n",
       "  968  0.107322  0.106511  0.106367  0.113557  0.127607         0.481906\n",
       "  969  0.107281  0.108077  0.108443  0.100170  0.126926         0.502712\n",
       "  970  0.107072  0.105027  0.104710  0.137562  0.128962         0.447372\n",
       "  971  0.103120  0.101489  0.101520  0.132405  0.123771         0.485776,\n",
       "  0.10219374079957828),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  968  0.107322  0.106511  0.106367  0.113557  0.127607         0.481906\n",
       "  969  0.107281  0.108077  0.108443  0.100170  0.126926         0.502712\n",
       "  970  0.107072  0.105027  0.104710  0.137562  0.128962         0.447372\n",
       "  971  0.103120  0.101489  0.101520  0.132405  0.123771         0.485776\n",
       "  972  0.104273  0.103385  0.103451  0.091489  0.123595         0.484164,\n",
       "  0.10418532923836384),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  969  0.107281  0.108077  0.108443  0.100170  0.126926         0.502712\n",
       "  970  0.107072  0.105027  0.104710  0.137562  0.128962         0.447372\n",
       "  971  0.103120  0.101489  0.101520  0.132405  0.123771         0.485776\n",
       "  972  0.104273  0.103385  0.103451  0.091489  0.123595         0.484164\n",
       "  973  0.101171  0.103082  0.101983  0.120545  0.123209         0.502013,\n",
       "  0.10234233894070725),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  970  0.107072  0.105027  0.104710  0.137562  0.128962         0.447372\n",
       "  971  0.103120  0.101489  0.101520  0.132405  0.123771         0.485776\n",
       "  972  0.104273  0.103385  0.103451  0.091489  0.123595         0.484164\n",
       "  973  0.101171  0.103082  0.101983  0.120545  0.123209         0.502013\n",
       "  974  0.105245  0.103949  0.104177  0.102622  0.125154         0.473339,\n",
       "  0.10472935835855739),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  971  0.103120  0.101489  0.101520  0.132405  0.123771         0.485776\n",
       "  972  0.104273  0.103385  0.103451  0.091489  0.123595         0.484164\n",
       "  973  0.101171  0.103082  0.101983  0.120545  0.123209         0.502013\n",
       "  974  0.105245  0.103949  0.104177  0.102622  0.125154         0.473339\n",
       "  975  0.103891  0.103677  0.104526  0.058292  0.123354         0.504970,\n",
       "  0.1063854150236483),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  972  0.104273  0.103385  0.103451  0.091489  0.123595         0.484164\n",
       "  973  0.101171  0.103082  0.101983  0.120545  0.123209         0.502013\n",
       "  974  0.105245  0.103949  0.104177  0.102622  0.125154         0.473339\n",
       "  975  0.103891  0.103677  0.104526  0.058292  0.123354         0.504970\n",
       "  976  0.106639  0.105992  0.106488  0.103405  0.125685         0.499504,\n",
       "  0.10893060683312429),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  973  0.101171  0.103082  0.101983  0.120545  0.123209         0.502013\n",
       "  974  0.105245  0.103949  0.104177  0.102622  0.125154         0.473339\n",
       "  975  0.103891  0.103677  0.104526  0.058292  0.123354         0.504970\n",
       "  976  0.106639  0.105992  0.106488  0.103405  0.125685         0.499504\n",
       "  977  0.107527  0.107459  0.107554  0.119877  0.127303         0.506153,\n",
       "  0.10581981256715929),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  974  0.105245  0.103949  0.104177  0.102622  0.125154         0.473339\n",
       "  975  0.103891  0.103677  0.104526  0.058292  0.123354         0.504970\n",
       "  976  0.106639  0.105992  0.106488  0.103405  0.125685         0.499504\n",
       "  977  0.107527  0.107459  0.107554  0.119877  0.127303         0.506153\n",
       "  978  0.108675  0.107575  0.107731  0.100943  0.129788         0.463859,\n",
       "  0.10558494032557302),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  975  0.103891  0.103677  0.104526  0.058292  0.123354         0.504970\n",
       "  976  0.106639  0.105992  0.106488  0.103405  0.125685         0.499504\n",
       "  977  0.107527  0.107459  0.107554  0.119877  0.127303         0.506153\n",
       "  978  0.108675  0.107575  0.107731  0.100943  0.129788         0.463859\n",
       "  979  0.107289  0.106110  0.105306  0.105265  0.126750         0.485364,\n",
       "  0.10396243683785157),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  976  0.106639  0.105992  0.106488  0.103405  0.125685         0.499504\n",
       "  977  0.107527  0.107459  0.107554  0.119877  0.127303         0.506153\n",
       "  978  0.108675  0.107575  0.107731  0.100943  0.129788         0.463859\n",
       "  979  0.107289  0.106110  0.105306  0.105265  0.126750         0.485364\n",
       "  980  0.105243  0.103741  0.102571  0.121068  0.126521         0.474988,\n",
       "  0.10104337776324417),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  977  0.107527  0.107459  0.107554  0.119877  0.127303         0.506153\n",
       "  978  0.108675  0.107575  0.107731  0.100943  0.129788         0.463859\n",
       "  979  0.107289  0.106110  0.105306  0.105265  0.126750         0.485364\n",
       "  980  0.105243  0.103741  0.102571  0.121068  0.126521         0.474988\n",
       "  981  0.105845  0.103873  0.101607  0.107530  0.124936         0.465293,\n",
       "  0.09971805131305601),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  978  0.108675  0.107575  0.107731  0.100943  0.129788         0.463859\n",
       "  979  0.107289  0.106110  0.105306  0.105265  0.126750         0.485364\n",
       "  980  0.105243  0.103741  0.102571  0.121068  0.126521         0.474988\n",
       "  981  0.105845  0.103873  0.101607  0.107530  0.124936         0.465293\n",
       "  982  0.100377  0.100897  0.098132  0.147571  0.122086         0.477210,\n",
       "  0.10439382658486272),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  979  0.107289  0.106110  0.105306  0.105265  0.126750         0.485364\n",
       "  980  0.105243  0.103741  0.102571  0.121068  0.126521         0.474988\n",
       "  981  0.105845  0.103873  0.101607  0.107530  0.124936         0.465293\n",
       "  982  0.100377  0.100897  0.098132  0.147571  0.122086         0.477210\n",
       "  983  0.099838  0.103030  0.100433  0.115250  0.120791         0.522084,\n",
       "  0.10387616273739426),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  980  0.105243  0.103741  0.102571  0.121068  0.126521         0.474988\n",
       "  981  0.105845  0.103873  0.101607  0.107530  0.124936         0.465293\n",
       "  982  0.100377  0.100897  0.098132  0.147571  0.122086         0.477210\n",
       "  983  0.099838  0.103030  0.100433  0.115250  0.120791         0.522084\n",
       "  984  0.104680  0.103160  0.103800  0.213643  0.125358         0.483250,\n",
       "  0.09138504706451363),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  981  0.105845  0.103873  0.101607  0.107530  0.124936         0.465293\n",
       "  982  0.100377  0.100897  0.098132  0.147571  0.122086         0.477210\n",
       "  983  0.099838  0.103030  0.100433  0.115250  0.120791         0.522084\n",
       "  984  0.104680  0.103160  0.103800  0.213643  0.125358         0.483250\n",
       "  985  0.096062  0.094138  0.092582  0.274573  0.124852         0.393716,\n",
       "  0.0883844962035162),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  982  0.100377  0.100897  0.098132  0.147571  0.122086         0.477210\n",
       "  983  0.099838  0.103030  0.100433  0.115250  0.120791         0.522084\n",
       "  984  0.104680  0.103160  0.103800  0.213643  0.125358         0.483250\n",
       "  985  0.096062  0.094138  0.092582  0.274573  0.124852         0.393716\n",
       "  986  0.091858  0.090609  0.089557  0.143448  0.112653         0.464683,\n",
       "  0.09037607501993947),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  983  0.099838  0.103030  0.100433  0.115250  0.120791         0.522084\n",
       "  984  0.104680  0.103160  0.103800  0.213643  0.125358         0.483250\n",
       "  985  0.096062  0.094138  0.092582  0.274573  0.124852         0.393716\n",
       "  986  0.091858  0.090609  0.089557  0.143448  0.112653         0.464683\n",
       "  987  0.088450  0.089699  0.089533  0.114017  0.109723         0.502013,\n",
       "  0.09327836231704328),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  984  0.104680  0.103160  0.103800  0.213643  0.125358         0.483250\n",
       "  985  0.096062  0.094138  0.092582  0.274573  0.124852         0.393716\n",
       "  986  0.091858  0.090609  0.089557  0.143448  0.112653         0.464683\n",
       "  987  0.088450  0.089699  0.089533  0.114017  0.109723         0.502013\n",
       "  988  0.092033  0.093723  0.092561  0.118945  0.111668         0.508823,\n",
       "  0.09001897953231162),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  985  0.096062  0.094138  0.092582  0.274573  0.124852         0.393716\n",
       "  986  0.091858  0.090609  0.089557  0.143448  0.112653         0.464683\n",
       "  987  0.088450  0.089699  0.089533  0.114017  0.109723         0.502013\n",
       "  988  0.092033  0.093723  0.092561  0.118945  0.111668         0.508823\n",
       "  989  0.095665  0.093638  0.091960  0.086031  0.114502         0.462748,\n",
       "  0.09393983236921112),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  986  0.091858  0.090609  0.089557  0.143448  0.112653         0.464683\n",
       "  987  0.088450  0.089699  0.089533  0.114017  0.109723         0.502013\n",
       "  988  0.092033  0.093723  0.092561  0.118945  0.111668         0.508823\n",
       "  989  0.095665  0.093638  0.091960  0.086031  0.114502         0.462748\n",
       "  990  0.093208  0.092752  0.093384  0.077337  0.111319         0.516439,\n",
       "  0.09037847098815424),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  987  0.088450  0.089699  0.089533  0.114017  0.109723         0.502013\n",
       "  988  0.092033  0.093723  0.092561  0.118945  0.111668         0.508823\n",
       "  989  0.095665  0.093638  0.091960  0.086031  0.114502         0.462748\n",
       "  990  0.093208  0.092752  0.093384  0.077337  0.111319         0.516439\n",
       "  991  0.094555  0.092505  0.091841  0.076766  0.115148         0.460490,\n",
       "  0.09181404084330998),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  988  0.092033  0.093723  0.092561  0.118945  0.111668         0.508823\n",
       "  989  0.095665  0.093638  0.091960  0.086031  0.114502         0.462748\n",
       "  990  0.093208  0.092752  0.093384  0.077337  0.111319         0.516439\n",
       "  991  0.094555  0.092505  0.091841  0.076766  0.115148         0.460490\n",
       "  992  0.091477  0.091361  0.092536  0.068078  0.111670         0.497855,\n",
       "  0.09171577727941639),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  989  0.095665  0.093638  0.091960  0.086031  0.114502         0.462748\n",
       "  990  0.093208  0.092752  0.093384  0.077337  0.111319         0.516439\n",
       "  991  0.094555  0.092505  0.091841  0.076766  0.115148         0.460490\n",
       "  992  0.091477  0.091361  0.092536  0.068078  0.111670         0.497855\n",
       "  993  0.092969  0.091067  0.091279  0.084810  0.113072         0.486386,\n",
       "  0.08957080838543457),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  990  0.093208  0.092752  0.093384  0.077337  0.111319         0.516439\n",
       "  991  0.094555  0.092505  0.091841  0.076766  0.115148         0.460490\n",
       "  992  0.091477  0.091361  0.092536  0.068078  0.111670         0.497855\n",
       "  993  0.092969  0.091067  0.091279  0.084810  0.113072         0.486386\n",
       "  994  0.090337  0.089851  0.090948  0.069593  0.112976         0.471081,\n",
       "  0.08815920783478913),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  991  0.094555  0.092505  0.091841  0.076766  0.115148         0.460490\n",
       "  992  0.091477  0.091361  0.092536  0.068078  0.111670         0.497855\n",
       "  993  0.092969  0.091067  0.091279  0.084810  0.113072         0.486386\n",
       "  994  0.090337  0.089851  0.090948  0.069593  0.112976         0.471081\n",
       "  995  0.089843  0.088033  0.088782  0.073066  0.110882         0.476565,\n",
       "  0.08699445936680392),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  992  0.091477  0.091361  0.092536  0.068078  0.111670         0.497855\n",
       "  993  0.092969  0.091067  0.091279  0.084810  0.113072         0.486386\n",
       "  994  0.090337  0.089851  0.090948  0.069593  0.112976         0.471081\n",
       "  995  0.089843  0.088033  0.088782  0.073066  0.110882         0.476565\n",
       "  996  0.088787  0.087984  0.088637  0.060396  0.109503         0.478411,\n",
       "  0.09103274351331539),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  993  0.092969  0.091067  0.091279  0.084810  0.113072         0.486386\n",
       "  994  0.090337  0.089851  0.090948  0.069593  0.112976         0.471081\n",
       "  995  0.089843  0.088033  0.088782  0.073066  0.110882         0.476565\n",
       "  996  0.088787  0.087984  0.088637  0.060396  0.109503         0.478411\n",
       "  997  0.088659  0.089659  0.089710  0.087091  0.108366         0.517317,\n",
       "  0.09258814064629789),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  994  0.090337  0.089851  0.090948  0.069593  0.112976         0.471081\n",
       "  995  0.089843  0.088033  0.088782  0.073066  0.110882         0.476565\n",
       "  996  0.088787  0.087984  0.088637  0.060396  0.109503         0.478411\n",
       "  997  0.088659  0.089659  0.089710  0.087091  0.108366         0.517317\n",
       "  998  0.091254  0.091956  0.091207  0.082015  0.112309         0.498751,\n",
       "  0.09085778972182366),\n",
       " (         Open      High       Low    Volume  Previous  last_difference\n",
       "  995  0.089843  0.088033  0.088782  0.073066  0.110882         0.476565\n",
       "  996  0.088787  0.087984  0.088637  0.060396  0.109503         0.478411\n",
       "  997  0.088659  0.089659  0.089710  0.087091  0.108366         0.517317\n",
       "  998  0.091254  0.091956  0.091207  0.082015  0.112309         0.498751\n",
       "  999  0.092127  0.090588  0.092151  0.067372  0.113828         0.474182,\n",
       "  0.09466600318554078),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  996   0.088787  0.087984  0.088637  0.060396  0.109503         0.478411\n",
       "  997   0.088659  0.089659  0.089710  0.087091  0.108366         0.517317\n",
       "  998   0.091254  0.091956  0.091207  0.082015  0.112309         0.498751\n",
       "  999   0.092127  0.090588  0.092151  0.067372  0.113828         0.474182\n",
       "  1000  0.091682  0.093373  0.091977  0.090856  0.112138         0.515597,\n",
       "  0.09606321830453514),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  997   0.088659  0.089659  0.089710  0.087091  0.108366         0.517317\n",
       "  998   0.091254  0.091956  0.091207  0.082015  0.112309         0.498751\n",
       "  999   0.092127  0.090588  0.092151  0.067372  0.113828         0.474182\n",
       "  1000  0.091682  0.093373  0.091977  0.090856  0.112138         0.515597\n",
       "  1001  0.094647  0.096484  0.095874  0.103018  0.115858         0.497568,\n",
       "  0.09648742014690191),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  998   0.091254  0.091956  0.091207  0.082015  0.112309         0.498751\n",
       "  999   0.092127  0.090588  0.092151  0.067372  0.113828         0.474182\n",
       "  1000  0.091682  0.093373  0.091977  0.090856  0.112138         0.515597\n",
       "  1001  0.094647  0.096484  0.095874  0.103018  0.115858         0.497568\n",
       "  1002  0.096940  0.095833  0.097381  0.066932  0.117222         0.490293,\n",
       "  0.09586909676732505),\n",
       " (          Open      High       Low    Volume  Previous  last_difference\n",
       "  999   0.092127  0.090588  0.092151  0.067372  0.113828         0.474182\n",
       "  1000  0.091682  0.093373  0.091977  0.090856  0.112138         0.515597\n",
       "  1001  0.094647  0.096484  0.095874  0.103018  0.115858         0.497568\n",
       "  1002  0.096940  0.095833  0.097381  0.066932  0.117222         0.490293\n",
       "  1003  0.096844  0.095219  0.096383  0.088088  0.117636         0.482497,\n",
       "  0.0971776418176473),\n",
       " ...]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "429332a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = akbankPriceDataModule_normal(train_sequences, test_sequences, batch_size = BATCH_SIZE)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d328914d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9e892aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': tensor([[[1.9493e-04, 5.4979e-04, 0.0000e+00, 5.4358e-01, 0.0000e+00,\n",
      "          6.6694e-01],\n",
      "         [4.3800e-04, 1.7394e-03, 1.0997e-03, 2.7789e-01, 2.3485e-02,\n",
      "          5.0140e-01],\n",
      "         [2.7820e-03, 2.7822e-03, 3.1706e-03, 2.2215e-01, 2.5350e-02,\n",
      "          4.8907e-01],\n",
      "         [2.9000e-03, 2.3366e-03, 1.8433e-03, 1.8552e-01, 2.5605e-02,\n",
      "          4.7900e-01],\n",
      "         [1.3886e-03, 1.4835e-03, 1.9183e-03, 1.1176e-01, 2.4545e-02,\n",
      "          4.8915e-01]],\n",
      "\n",
      "        [[4.3800e-04, 1.7394e-03, 1.0997e-03, 2.7789e-01, 2.3485e-02,\n",
      "          5.0140e-01],\n",
      "         [2.7820e-03, 2.7822e-03, 3.1706e-03, 2.2215e-01, 2.5350e-02,\n",
      "          4.8907e-01],\n",
      "         [2.9000e-03, 2.3366e-03, 1.8433e-03, 1.8552e-01, 2.5605e-02,\n",
      "          4.7900e-01],\n",
      "         [1.3886e-03, 1.4835e-03, 1.9183e-03, 1.1176e-01, 2.4545e-02,\n",
      "          4.8915e-01],\n",
      "         [1.3862e-03, 1.4716e-03, 2.1073e-03, 8.6276e-02, 2.4809e-02,\n",
      "          4.9054e-01]],\n",
      "\n",
      "        [[2.7820e-03, 2.7822e-03, 3.1706e-03, 2.2215e-01, 2.5350e-02,\n",
      "          4.8907e-01],\n",
      "         [2.9000e-03, 2.3366e-03, 1.8433e-03, 1.8552e-01, 2.5605e-02,\n",
      "          4.7900e-01],\n",
      "         [1.3886e-03, 1.4835e-03, 1.9183e-03, 1.1176e-01, 2.4545e-02,\n",
      "          4.8915e-01],\n",
      "         [1.3862e-03, 1.4716e-03, 2.1073e-03, 8.6276e-02, 2.4809e-02,\n",
      "          4.9054e-01],\n",
      "         [2.1443e-03, 1.6304e-03, 2.3568e-03, 7.5525e-02, 2.5256e-02,\n",
      "          4.8397e-01]],\n",
      "\n",
      "        [[2.9000e-03, 2.3366e-03, 1.8433e-03, 1.8552e-01, 2.5605e-02,\n",
      "          4.7900e-01],\n",
      "         [1.3886e-03, 1.4835e-03, 1.9183e-03, 1.1176e-01, 2.4545e-02,\n",
      "          4.8915e-01],\n",
      "         [1.3862e-03, 1.4716e-03, 2.1073e-03, 8.6276e-02, 2.4809e-02,\n",
      "          4.9054e-01],\n",
      "         [2.1443e-03, 1.6304e-03, 2.3568e-03, 7.5525e-02, 2.5256e-02,\n",
      "          4.8397e-01],\n",
      "         [1.4656e-03, 8.8868e-04, 1.4654e-03, 6.3169e-02, 2.4845e-02,\n",
      "          4.7970e-01]],\n",
      "\n",
      "        [[1.3886e-03, 1.4835e-03, 1.9183e-03, 1.1176e-01, 2.4545e-02,\n",
      "          4.8915e-01],\n",
      "         [1.3862e-03, 1.4716e-03, 2.1073e-03, 8.6276e-02, 2.4809e-02,\n",
      "          4.9054e-01],\n",
      "         [2.1443e-03, 1.6304e-03, 2.3568e-03, 7.5525e-02, 2.5256e-02,\n",
      "          4.8397e-01],\n",
      "         [1.4656e-03, 8.8868e-04, 1.4654e-03, 6.3169e-02, 2.4845e-02,\n",
      "          4.7970e-01],\n",
      "         [7.4846e-04, 4.6685e-04, 1.5017e-03, 5.9774e-02, 2.3876e-02,\n",
      "          4.8777e-01]],\n",
      "\n",
      "        [[1.3862e-03, 1.4716e-03, 2.1073e-03, 8.6276e-02, 2.4809e-02,\n",
      "          4.9054e-01],\n",
      "         [2.1443e-03, 1.6304e-03, 2.3568e-03, 7.5525e-02, 2.5256e-02,\n",
      "          4.8397e-01],\n",
      "         [1.4656e-03, 8.8868e-04, 1.4654e-03, 6.3169e-02, 2.4845e-02,\n",
      "          4.7970e-01],\n",
      "         [7.4846e-04, 4.6685e-04, 1.5017e-03, 5.9774e-02, 2.3876e-02,\n",
      "          4.8777e-01],\n",
      "         [8.4471e-04, 2.9148e-04, 8.9862e-04, 1.1115e-01, 2.3960e-02,\n",
      "          4.8332e-01]],\n",
      "\n",
      "        [[2.1443e-03, 1.6304e-03, 2.3568e-03, 7.5525e-02, 2.5256e-02,\n",
      "          4.8397e-01],\n",
      "         [1.4656e-03, 8.8868e-04, 1.4654e-03, 6.3169e-02, 2.4845e-02,\n",
      "          4.7970e-01],\n",
      "         [7.4846e-04, 4.6685e-04, 1.5017e-03, 5.9774e-02, 2.3876e-02,\n",
      "          4.8777e-01],\n",
      "         [8.4471e-04, 2.9148e-04, 8.9862e-04, 1.1115e-01, 2.3960e-02,\n",
      "          4.8332e-01],\n",
      "         [0.0000e+00, 1.4930e-04, 7.2180e-04, 1.8396e-01, 2.3464e-02,\n",
      "          4.8938e-01]],\n",
      "\n",
      "        [[1.4656e-03, 8.8868e-04, 1.4654e-03, 6.3169e-02, 2.4845e-02,\n",
      "          4.7970e-01],\n",
      "         [7.4846e-04, 4.6685e-04, 1.5017e-03, 5.9774e-02, 2.3876e-02,\n",
      "          4.8777e-01],\n",
      "         [8.4471e-04, 2.9148e-04, 8.9862e-04, 1.1115e-01, 2.3960e-02,\n",
      "          4.8332e-01],\n",
      "         [0.0000e+00, 1.4930e-04, 7.2180e-04, 1.8396e-01, 2.3464e-02,\n",
      "          4.8938e-01],\n",
      "         [4.2356e-04, 0.0000e+00, 8.1384e-04, 6.2629e-02, 2.3759e-02,\n",
      "          4.8443e-01]],\n",
      "\n",
      "        [[7.4846e-04, 4.6685e-04, 1.5017e-03, 5.9774e-02, 2.3876e-02,\n",
      "          4.8777e-01],\n",
      "         [8.4471e-04, 2.9148e-04, 8.9862e-04, 1.1115e-01, 2.3960e-02,\n",
      "          4.8332e-01],\n",
      "         [0.0000e+00, 1.4930e-04, 7.2180e-04, 1.8396e-01, 2.3464e-02,\n",
      "          4.8938e-01],\n",
      "         [4.2356e-04, 0.0000e+00, 8.1384e-04, 6.2629e-02, 2.3759e-02,\n",
      "          4.8443e-01],\n",
      "         [4.3800e-04, 6.1618e-05, 8.8409e-04, 7.1091e-02, 2.3407e-02,\n",
      "          4.8993e-01]],\n",
      "\n",
      "        [[8.4471e-04, 2.9148e-04, 8.9862e-04, 1.1115e-01, 2.3960e-02,\n",
      "          4.8332e-01],\n",
      "         [0.0000e+00, 1.4930e-04, 7.2180e-04, 1.8396e-01, 2.3464e-02,\n",
      "          4.8938e-01],\n",
      "         [4.2356e-04, 0.0000e+00, 8.1384e-04, 6.2629e-02, 2.3759e-02,\n",
      "          4.8443e-01],\n",
      "         [4.3800e-04, 6.1618e-05, 8.8409e-04, 7.1091e-02, 2.3407e-02,\n",
      "          4.8993e-01],\n",
      "         [3.7302e-04, 3.0571e-04, 1.0997e-03, 6.0598e-02, 2.3775e-02,\n",
      "          4.8841e-01]],\n",
      "\n",
      "        [[0.0000e+00, 1.4930e-04, 7.2180e-04, 1.8396e-01, 2.3464e-02,\n",
      "          4.8938e-01],\n",
      "         [4.2356e-04, 0.0000e+00, 8.1384e-04, 6.2629e-02, 2.3759e-02,\n",
      "          4.8443e-01],\n",
      "         [4.3800e-04, 6.1618e-05, 8.8409e-04, 7.1091e-02, 2.3407e-02,\n",
      "          4.8993e-01],\n",
      "         [3.7302e-04, 3.0571e-04, 1.0997e-03, 6.0598e-02, 2.3775e-02,\n",
      "          4.8841e-01],\n",
      "         [8.0380e-04, 2.2987e-04, 1.2208e-03, 4.9351e-02, 2.3943e-02,\n",
      "          4.8714e-01]],\n",
      "\n",
      "        [[4.2356e-04, 0.0000e+00, 8.1384e-04, 6.2629e-02, 2.3759e-02,\n",
      "          4.8443e-01],\n",
      "         [4.3800e-04, 6.1618e-05, 8.8409e-04, 7.1091e-02, 2.3407e-02,\n",
      "          4.8993e-01],\n",
      "         [3.7302e-04, 3.0571e-04, 1.0997e-03, 6.0598e-02, 2.3775e-02,\n",
      "          4.8841e-01],\n",
      "         [8.0380e-04, 2.2987e-04, 1.2208e-03, 4.9351e-02, 2.3943e-02,\n",
      "          4.8714e-01],\n",
      "         [5.7999e-04, 1.1422e-03, 1.2934e-03, 1.0580e-01, 2.3946e-02,\n",
      "          4.9253e-01]],\n",
      "\n",
      "        [[4.3800e-04, 6.1618e-05, 8.8409e-04, 7.1091e-02, 2.3407e-02,\n",
      "          4.8993e-01],\n",
      "         [3.7302e-04, 3.0571e-04, 1.0997e-03, 6.0598e-02, 2.3775e-02,\n",
      "          4.8841e-01],\n",
      "         [8.0380e-04, 2.2987e-04, 1.2208e-03, 4.9351e-02, 2.3943e-02,\n",
      "          4.8714e-01],\n",
      "         [5.7999e-04, 1.1422e-03, 1.2934e-03, 1.0580e-01, 2.3946e-02,\n",
      "          4.9253e-01],\n",
      "         [1.7905e-03, 1.5807e-03, 2.5433e-03, 9.5398e-02, 2.4653e-02,\n",
      "          4.9101e-01]],\n",
      "\n",
      "        [[3.7302e-04, 3.0571e-04, 1.0997e-03, 6.0598e-02, 2.3775e-02,\n",
      "          4.8841e-01],\n",
      "         [8.0380e-04, 2.2987e-04, 1.2208e-03, 4.9351e-02, 2.3943e-02,\n",
      "          4.8714e-01],\n",
      "         [5.7999e-04, 1.1422e-03, 1.2934e-03, 1.0580e-01, 2.3946e-02,\n",
      "          4.9253e-01],\n",
      "         [1.7905e-03, 1.5807e-03, 2.5433e-03, 9.5398e-02, 2.4653e-02,\n",
      "          4.9101e-01],\n",
      "         [1.9879e-03, 2.4314e-03, 2.6232e-03, 1.3173e-01, 2.5160e-02,\n",
      "          4.9427e-01]],\n",
      "\n",
      "        [[8.0380e-04, 2.2987e-04, 1.2208e-03, 4.9351e-02, 2.3943e-02,\n",
      "          4.8714e-01],\n",
      "         [5.7999e-04, 1.1422e-03, 1.2934e-03, 1.0580e-01, 2.3946e-02,\n",
      "          4.9253e-01],\n",
      "         [1.7905e-03, 1.5807e-03, 2.5433e-03, 9.5398e-02, 2.4653e-02,\n",
      "          4.9101e-01],\n",
      "         [1.9879e-03, 2.4314e-03, 2.6232e-03, 1.3173e-01, 2.5160e-02,\n",
      "          4.9427e-01],\n",
      "         [2.7363e-03, 2.9599e-03, 3.4492e-03, 1.3032e-01, 2.6094e-02,\n",
      "          4.8803e-01]],\n",
      "\n",
      "        [[5.7999e-04, 1.1422e-03, 1.2934e-03, 1.0580e-01, 2.3946e-02,\n",
      "          4.9253e-01],\n",
      "         [1.7905e-03, 1.5807e-03, 2.5433e-03, 9.5398e-02, 2.4653e-02,\n",
      "          4.9101e-01],\n",
      "         [1.9879e-03, 2.4314e-03, 2.6232e-03, 1.3173e-01, 2.5160e-02,\n",
      "          4.9427e-01],\n",
      "         [2.7363e-03, 2.9599e-03, 3.4492e-03, 1.3032e-01, 2.6094e-02,\n",
      "          4.8803e-01],\n",
      "         [3.1647e-03, 3.3319e-03, 3.8004e-03, 1.1271e-01, 2.6214e-02,\n",
      "          4.9065e-01]],\n",
      "\n",
      "        [[1.7905e-03, 1.5807e-03, 2.5433e-03, 9.5398e-02, 2.4653e-02,\n",
      "          4.9101e-01],\n",
      "         [1.9879e-03, 2.4314e-03, 2.6232e-03, 1.3173e-01, 2.5160e-02,\n",
      "          4.9427e-01],\n",
      "         [2.7363e-03, 2.9599e-03, 3.4492e-03, 1.3032e-01, 2.6094e-02,\n",
      "          4.8803e-01],\n",
      "         [3.1647e-03, 3.3319e-03, 3.8004e-03, 1.1271e-01, 2.6214e-02,\n",
      "          4.9065e-01],\n",
      "         [3.6653e-03, 3.7324e-03, 4.2606e-03, 1.1522e-01, 2.6675e-02,\n",
      "          4.9343e-01]],\n",
      "\n",
      "        [[1.9879e-03, 2.4314e-03, 2.6232e-03, 1.3173e-01, 2.5160e-02,\n",
      "          4.9427e-01],\n",
      "         [2.7363e-03, 2.9599e-03, 3.4492e-03, 1.3032e-01, 2.6094e-02,\n",
      "          4.8803e-01],\n",
      "         [3.1647e-03, 3.3319e-03, 3.8004e-03, 1.1271e-01, 2.6214e-02,\n",
      "          4.9065e-01],\n",
      "         [3.6653e-03, 3.7324e-03, 4.2606e-03, 1.1522e-01, 2.6675e-02,\n",
      "          4.9343e-01],\n",
      "         [4.2741e-03, 4.7064e-03, 5.0405e-03, 1.2930e-01, 2.7499e-02,\n",
      "          4.9047e-01]],\n",
      "\n",
      "        [[2.7363e-03, 2.9599e-03, 3.4492e-03, 1.3032e-01, 2.6094e-02,\n",
      "          4.8803e-01],\n",
      "         [3.1647e-03, 3.3319e-03, 3.8004e-03, 1.1271e-01, 2.6214e-02,\n",
      "          4.9065e-01],\n",
      "         [3.6653e-03, 3.7324e-03, 4.2606e-03, 1.1522e-01, 2.6675e-02,\n",
      "          4.9343e-01],\n",
      "         [4.2741e-03, 4.7064e-03, 5.0405e-03, 1.2930e-01, 2.7499e-02,\n",
      "          4.9047e-01],\n",
      "         [4.9624e-03, 4.4268e-03, 5.2198e-03, 8.7906e-02, 2.7936e-02,\n",
      "          4.8440e-01]],\n",
      "\n",
      "        [[3.1647e-03, 3.3319e-03, 3.8004e-03, 1.1271e-01, 2.6214e-02,\n",
      "          4.9065e-01],\n",
      "         [3.6653e-03, 3.7324e-03, 4.2606e-03, 1.1522e-01, 2.6675e-02,\n",
      "          4.9343e-01],\n",
      "         [4.2741e-03, 4.7064e-03, 5.0405e-03, 1.2930e-01, 2.7499e-02,\n",
      "          4.9047e-01],\n",
      "         [4.9624e-03, 4.4268e-03, 5.2198e-03, 8.7906e-02, 2.7936e-02,\n",
      "          4.8440e-01],\n",
      "         [4.3824e-03, 4.2491e-03, 5.0502e-03, 9.2197e-02, 2.7581e-02,\n",
      "          4.8809e-01]],\n",
      "\n",
      "        [[3.6653e-03, 3.7324e-03, 4.2606e-03, 1.1522e-01, 2.6675e-02,\n",
      "          4.9343e-01],\n",
      "         [4.2741e-03, 4.7064e-03, 5.0405e-03, 1.2930e-01, 2.7499e-02,\n",
      "          4.9047e-01],\n",
      "         [4.9624e-03, 4.4268e-03, 5.2198e-03, 8.7906e-02, 2.7936e-02,\n",
      "          4.8440e-01],\n",
      "         [4.3824e-03, 4.2491e-03, 5.0502e-03, 9.2197e-02, 2.7581e-02,\n",
      "          4.8809e-01],\n",
      "         [4.7290e-03, 4.9505e-03, 5.1011e-03, 1.0382e-01, 2.7707e-02,\n",
      "          4.9149e-01]],\n",
      "\n",
      "        [[4.2741e-03, 4.7064e-03, 5.0405e-03, 1.2930e-01, 2.7499e-02,\n",
      "          4.9047e-01],\n",
      "         [4.9624e-03, 4.4268e-03, 5.2198e-03, 8.7906e-02, 2.7936e-02,\n",
      "          4.8440e-01],\n",
      "         [4.3824e-03, 4.2491e-03, 5.0502e-03, 9.2197e-02, 2.7581e-02,\n",
      "          4.8809e-01],\n",
      "         [4.7290e-03, 4.9505e-03, 5.1011e-03, 1.0382e-01, 2.7707e-02,\n",
      "          4.9149e-01],\n",
      "         [5.2344e-03, 5.2989e-03, 5.7648e-03, 1.1097e-01, 2.8278e-02,\n",
      "          4.8535e-01]],\n",
      "\n",
      "        [[4.9624e-03, 4.4268e-03, 5.2198e-03, 8.7906e-02, 2.7936e-02,\n",
      "          4.8440e-01],\n",
      "         [4.3824e-03, 4.2491e-03, 5.0502e-03, 9.2197e-02, 2.7581e-02,\n",
      "          4.8809e-01],\n",
      "         [4.7290e-03, 4.9505e-03, 5.1011e-03, 1.0382e-01, 2.7707e-02,\n",
      "          4.9149e-01],\n",
      "         [5.2344e-03, 5.2989e-03, 5.7648e-03, 1.1097e-01, 2.8278e-02,\n",
      "          4.8535e-01],\n",
      "         [4.9022e-03, 4.5358e-03, 5.2900e-03, 8.5926e-02, 2.8046e-02,\n",
      "          4.8431e-01]],\n",
      "\n",
      "        [[4.3824e-03, 4.2491e-03, 5.0502e-03, 9.2197e-02, 2.7581e-02,\n",
      "          4.8809e-01],\n",
      "         [4.7290e-03, 4.9505e-03, 5.1011e-03, 1.0382e-01, 2.7707e-02,\n",
      "          4.9149e-01],\n",
      "         [5.2344e-03, 5.2989e-03, 5.7648e-03, 1.1097e-01, 2.8278e-02,\n",
      "          4.8535e-01],\n",
      "         [4.9022e-03, 4.5358e-03, 5.2900e-03, 8.5926e-02, 2.8046e-02,\n",
      "          4.8431e-01],\n",
      "         [5.3210e-03, 6.0809e-03, 5.8738e-03, 2.0600e-01, 2.7679e-02,\n",
      "          5.0253e-01]],\n",
      "\n",
      "        [[4.7290e-03, 4.9505e-03, 5.1011e-03, 1.0382e-01, 2.7707e-02,\n",
      "          4.9149e-01],\n",
      "         [5.2344e-03, 5.2989e-03, 5.7648e-03, 1.1097e-01, 2.8278e-02,\n",
      "          4.8535e-01],\n",
      "         [4.9022e-03, 4.5358e-03, 5.2900e-03, 8.5926e-02, 2.8046e-02,\n",
      "          4.8431e-01],\n",
      "         [5.3210e-03, 6.0809e-03, 5.8738e-03, 2.0600e-01, 2.7679e-02,\n",
      "          5.0253e-01],\n",
      "         [6.6206e-03, 7.8867e-03, 7.3319e-03, 3.7141e-01, 2.9692e-02,\n",
      "          4.9468e-01]],\n",
      "\n",
      "        [[5.2344e-03, 5.2989e-03, 5.7648e-03, 1.1097e-01, 2.8278e-02,\n",
      "          4.8535e-01],\n",
      "         [4.9022e-03, 4.5358e-03, 5.2900e-03, 8.5926e-02, 2.8046e-02,\n",
      "          4.8431e-01],\n",
      "         [5.3210e-03, 6.0809e-03, 5.8738e-03, 2.0600e-01, 2.7679e-02,\n",
      "          5.0253e-01],\n",
      "         [6.6206e-03, 7.8867e-03, 7.3319e-03, 3.7141e-01, 2.9692e-02,\n",
      "          4.9468e-01],\n",
      "         [7.3907e-03, 7.2421e-03, 8.0028e-03, 1.6739e-01, 3.0679e-02,\n",
      "          4.8447e-01]],\n",
      "\n",
      "        [[4.9022e-03, 4.5358e-03, 5.2900e-03, 8.5926e-02, 2.8046e-02,\n",
      "          4.8431e-01],\n",
      "         [5.3210e-03, 6.0809e-03, 5.8738e-03, 2.0600e-01, 2.7679e-02,\n",
      "          5.0253e-01],\n",
      "         [6.6206e-03, 7.8867e-03, 7.3319e-03, 3.7141e-01, 2.9692e-02,\n",
      "          4.9468e-01],\n",
      "         [7.3907e-03, 7.2421e-03, 8.0028e-03, 1.6739e-01, 3.0679e-02,\n",
      "          4.8447e-01],\n",
      "         [7.6073e-03, 7.7019e-03, 7.9786e-03, 1.8403e-01, 3.0333e-02,\n",
      "          4.9246e-01]],\n",
      "\n",
      "        [[5.3210e-03, 6.0809e-03, 5.8738e-03, 2.0600e-01, 2.7679e-02,\n",
      "          5.0253e-01],\n",
      "         [6.6206e-03, 7.8867e-03, 7.3319e-03, 3.7141e-01, 2.9692e-02,\n",
      "          4.9468e-01],\n",
      "         [7.3907e-03, 7.2421e-03, 8.0028e-03, 1.6739e-01, 3.0679e-02,\n",
      "          4.8447e-01],\n",
      "         [7.6073e-03, 7.7019e-03, 7.9786e-03, 1.8403e-01, 3.0333e-02,\n",
      "          4.9246e-01],\n",
      "         [8.6782e-03, 8.3251e-03, 9.2212e-03, 1.5844e-01, 3.1030e-02,\n",
      "          4.9156e-01]],\n",
      "\n",
      "        [[6.6206e-03, 7.8867e-03, 7.3319e-03, 3.7141e-01, 2.9692e-02,\n",
      "          4.9468e-01],\n",
      "         [7.3907e-03, 7.2421e-03, 8.0028e-03, 1.6739e-01, 3.0679e-02,\n",
      "          4.8447e-01],\n",
      "         [7.6073e-03, 7.7019e-03, 7.9786e-03, 1.8403e-01, 3.0333e-02,\n",
      "          4.9246e-01],\n",
      "         [8.6782e-03, 8.3251e-03, 9.2212e-03, 1.5844e-01, 3.1030e-02,\n",
      "          4.9156e-01],\n",
      "         [8.5362e-03, 8.7185e-03, 8.7876e-03, 1.8219e-01, 3.1611e-02,\n",
      "          4.9305e-01]],\n",
      "\n",
      "        [[7.3907e-03, 7.2421e-03, 8.0028e-03, 1.6739e-01, 3.0679e-02,\n",
      "          4.8447e-01],\n",
      "         [7.6073e-03, 7.7019e-03, 7.9786e-03, 1.8403e-01, 3.0333e-02,\n",
      "          4.9246e-01],\n",
      "         [8.6782e-03, 8.3251e-03, 9.2212e-03, 1.5844e-01, 3.1030e-02,\n",
      "          4.9156e-01],\n",
      "         [8.5362e-03, 8.7185e-03, 8.7876e-03, 1.8219e-01, 3.1611e-02,\n",
      "          4.9305e-01],\n",
      "         [9.2317e-03, 8.6996e-03, 9.6984e-03, 1.6281e-01, 3.2386e-02,\n",
      "          4.8481e-01]],\n",
      "\n",
      "        [[7.6073e-03, 7.7019e-03, 7.9786e-03, 1.8403e-01, 3.0333e-02,\n",
      "          4.9246e-01],\n",
      "         [8.6782e-03, 8.3251e-03, 9.2212e-03, 1.5844e-01, 3.1030e-02,\n",
      "          4.9156e-01],\n",
      "         [8.5362e-03, 8.7185e-03, 8.7876e-03, 1.8219e-01, 3.1611e-02,\n",
      "          4.9305e-01],\n",
      "         [9.2317e-03, 8.6996e-03, 9.6984e-03, 1.6281e-01, 3.2386e-02,\n",
      "          4.8481e-01],\n",
      "         [9.0801e-03, 9.0384e-03, 9.8316e-03, 1.7174e-01, 3.2084e-02,\n",
      "          4.9029e-01]],\n",
      "\n",
      "        [[8.6782e-03, 8.3251e-03, 9.2212e-03, 1.5844e-01, 3.1030e-02,\n",
      "          4.9156e-01],\n",
      "         [8.5362e-03, 8.7185e-03, 8.7876e-03, 1.8219e-01, 3.1611e-02,\n",
      "          4.9305e-01],\n",
      "         [9.2317e-03, 8.6996e-03, 9.6984e-03, 1.6281e-01, 3.2386e-02,\n",
      "          4.8481e-01],\n",
      "         [9.0801e-03, 9.0384e-03, 9.8316e-03, 1.7174e-01, 3.2084e-02,\n",
      "          4.9029e-01],\n",
      "         [9.5133e-03, 8.9910e-03, 9.9454e-03, 1.3466e-01, 3.2498e-02,\n",
      "          4.8511e-01]]]), 'label': tensor([1.8933e-03, 1.4715e-03, 4.7933e-04, 5.6560e-04, 5.7522e-05, 3.5949e-04,\n",
      "        0.0000e+00, 3.7627e-04, 5.4883e-04, 5.5123e-04, 1.2750e-03, 1.7951e-03,\n",
      "        2.7513e-03, 2.8735e-03, 3.3457e-03, 4.1893e-03, 4.6374e-03, 4.2732e-03,\n",
      "        4.4026e-03, 4.9873e-03, 4.7501e-03, 4.3738e-03, 6.4349e-03, 7.4463e-03,\n",
      "        7.0916e-03, 7.8058e-03, 8.4001e-03, 9.1934e-03, 8.8842e-03, 9.3084e-03,\n",
      "        9.0400e-03, 8.4480e-03])}\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "for item in data_module.train_dataloader():\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b8dbd0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricePredictionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features=6, n_hidden=128, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = n_features,\n",
    "            hidden_size = n_hidden,\n",
    "            batch_first = True,\n",
    "            num_layers = n_layers, # Stack LSTMs\n",
    "            dropout = 0.2\n",
    "                            )\n",
    "        self.regressor = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = hidden[-1] \n",
    "\n",
    "        return self.regressor(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8c67e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class just_GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features=6, n_hidden=128, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size = n_features,\n",
    "            hidden_size = n_hidden,\n",
    "            batch_first = True,\n",
    "            num_layers = n_layers, # Stack GRUs\n",
    "            dropout = 0.2\n",
    "                            )\n",
    "        self.regressor = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.gru.flatten_parameters()\n",
    "\n",
    "        _, (hidden, _) = self.gru(x)\n",
    "        out = hidden[-1] \n",
    "\n",
    "        return self.regressor(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "911b953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridGRU_LSTM_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features=6, n_hidden=100, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = n_features,\n",
    "            hidden_size = 100,\n",
    "            batch_first = True,\n",
    "            num_layers = 1, #1 LSTM\n",
    "            dropout = 0.2\n",
    "                            )\n",
    "        self.regressor = nn.Linear(n_hidden, 1)\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "                    input_size = 100,\n",
    "                    hidden_size = 50,\n",
    "                    batch_first = True,\n",
    "                    num_layers = 1,  #1 GRU with dropout\n",
    "                    dropout = 0.2)\n",
    "        \n",
    "        self.linear = nn.Linear(6,1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        a = self.lstm(x)\n",
    "        b = self.gru(a)\n",
    "        out = self.linear(b,1) \n",
    "        return out\n",
    "        #return self.regressor(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "fdff244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class akbankPricePredictor(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_features: int):\n",
    "        super().__init__()\n",
    "        self.model = PricePredictionModel(n_features)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels.unsqueeze(dim=1))\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sequences = batch[\"sequence\"]\n",
    "        labels = batch[\"label\"]\n",
    "        loss, output = self.forward(sequences, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        sequences = batch[\"sequence\"]\n",
    "        labels = batch[\"label\"]\n",
    "        loss, output = self.forward(sequences, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "  \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        sequences = batch[\"sequence\"]\n",
    "        labels = batch[\"label\"]\n",
    "        loss, output = self.forward(sequences, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "555681df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = akbankPricePredictor(n_features = 6)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ed6929b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': tensor([[[1.9493e-04, 5.4979e-04, 0.0000e+00, 5.4358e-01, 0.0000e+00,\n",
      "          6.6694e-01],\n",
      "         [4.3800e-04, 1.7394e-03, 1.0997e-03, 2.7789e-01, 2.3485e-02,\n",
      "          5.0140e-01],\n",
      "         [2.7820e-03, 2.7822e-03, 3.1706e-03, 2.2215e-01, 2.5350e-02,\n",
      "          4.8907e-01],\n",
      "         [2.9000e-03, 2.3366e-03, 1.8433e-03, 1.8552e-01, 2.5605e-02,\n",
      "          4.7900e-01],\n",
      "         [1.3886e-03, 1.4835e-03, 1.9183e-03, 1.1176e-01, 2.4545e-02,\n",
      "          4.8915e-01]],\n",
      "\n",
      "        [[4.3800e-04, 1.7394e-03, 1.0997e-03, 2.7789e-01, 2.3485e-02,\n",
      "          5.0140e-01],\n",
      "         [2.7820e-03, 2.7822e-03, 3.1706e-03, 2.2215e-01, 2.5350e-02,\n",
      "          4.8907e-01],\n",
      "         [2.9000e-03, 2.3366e-03, 1.8433e-03, 1.8552e-01, 2.5605e-02,\n",
      "          4.7900e-01],\n",
      "         [1.3886e-03, 1.4835e-03, 1.9183e-03, 1.1176e-01, 2.4545e-02,\n",
      "          4.8915e-01],\n",
      "         [1.3862e-03, 1.4716e-03, 2.1073e-03, 8.6276e-02, 2.4809e-02,\n",
      "          4.9054e-01]],\n",
      "\n",
      "        [[2.7820e-03, 2.7822e-03, 3.1706e-03, 2.2215e-01, 2.5350e-02,\n",
      "          4.8907e-01],\n",
      "         [2.9000e-03, 2.3366e-03, 1.8433e-03, 1.8552e-01, 2.5605e-02,\n",
      "          4.7900e-01],\n",
      "         [1.3886e-03, 1.4835e-03, 1.9183e-03, 1.1176e-01, 2.4545e-02,\n",
      "          4.8915e-01],\n",
      "         [1.3862e-03, 1.4716e-03, 2.1073e-03, 8.6276e-02, 2.4809e-02,\n",
      "          4.9054e-01],\n",
      "         [2.1443e-03, 1.6304e-03, 2.3568e-03, 7.5525e-02, 2.5256e-02,\n",
      "          4.8397e-01]],\n",
      "\n",
      "        [[2.9000e-03, 2.3366e-03, 1.8433e-03, 1.8552e-01, 2.5605e-02,\n",
      "          4.7900e-01],\n",
      "         [1.3886e-03, 1.4835e-03, 1.9183e-03, 1.1176e-01, 2.4545e-02,\n",
      "          4.8915e-01],\n",
      "         [1.3862e-03, 1.4716e-03, 2.1073e-03, 8.6276e-02, 2.4809e-02,\n",
      "          4.9054e-01],\n",
      "         [2.1443e-03, 1.6304e-03, 2.3568e-03, 7.5525e-02, 2.5256e-02,\n",
      "          4.8397e-01],\n",
      "         [1.4656e-03, 8.8868e-04, 1.4654e-03, 6.3169e-02, 2.4845e-02,\n",
      "          4.7970e-01]],\n",
      "\n",
      "        [[1.3886e-03, 1.4835e-03, 1.9183e-03, 1.1176e-01, 2.4545e-02,\n",
      "          4.8915e-01],\n",
      "         [1.3862e-03, 1.4716e-03, 2.1073e-03, 8.6276e-02, 2.4809e-02,\n",
      "          4.9054e-01],\n",
      "         [2.1443e-03, 1.6304e-03, 2.3568e-03, 7.5525e-02, 2.5256e-02,\n",
      "          4.8397e-01],\n",
      "         [1.4656e-03, 8.8868e-04, 1.4654e-03, 6.3169e-02, 2.4845e-02,\n",
      "          4.7970e-01],\n",
      "         [7.4846e-04, 4.6685e-04, 1.5017e-03, 5.9774e-02, 2.3876e-02,\n",
      "          4.8777e-01]],\n",
      "\n",
      "        [[1.3862e-03, 1.4716e-03, 2.1073e-03, 8.6276e-02, 2.4809e-02,\n",
      "          4.9054e-01],\n",
      "         [2.1443e-03, 1.6304e-03, 2.3568e-03, 7.5525e-02, 2.5256e-02,\n",
      "          4.8397e-01],\n",
      "         [1.4656e-03, 8.8868e-04, 1.4654e-03, 6.3169e-02, 2.4845e-02,\n",
      "          4.7970e-01],\n",
      "         [7.4846e-04, 4.6685e-04, 1.5017e-03, 5.9774e-02, 2.3876e-02,\n",
      "          4.8777e-01],\n",
      "         [8.4471e-04, 2.9148e-04, 8.9862e-04, 1.1115e-01, 2.3960e-02,\n",
      "          4.8332e-01]],\n",
      "\n",
      "        [[2.1443e-03, 1.6304e-03, 2.3568e-03, 7.5525e-02, 2.5256e-02,\n",
      "          4.8397e-01],\n",
      "         [1.4656e-03, 8.8868e-04, 1.4654e-03, 6.3169e-02, 2.4845e-02,\n",
      "          4.7970e-01],\n",
      "         [7.4846e-04, 4.6685e-04, 1.5017e-03, 5.9774e-02, 2.3876e-02,\n",
      "          4.8777e-01],\n",
      "         [8.4471e-04, 2.9148e-04, 8.9862e-04, 1.1115e-01, 2.3960e-02,\n",
      "          4.8332e-01],\n",
      "         [0.0000e+00, 1.4930e-04, 7.2180e-04, 1.8396e-01, 2.3464e-02,\n",
      "          4.8938e-01]],\n",
      "\n",
      "        [[1.4656e-03, 8.8868e-04, 1.4654e-03, 6.3169e-02, 2.4845e-02,\n",
      "          4.7970e-01],\n",
      "         [7.4846e-04, 4.6685e-04, 1.5017e-03, 5.9774e-02, 2.3876e-02,\n",
      "          4.8777e-01],\n",
      "         [8.4471e-04, 2.9148e-04, 8.9862e-04, 1.1115e-01, 2.3960e-02,\n",
      "          4.8332e-01],\n",
      "         [0.0000e+00, 1.4930e-04, 7.2180e-04, 1.8396e-01, 2.3464e-02,\n",
      "          4.8938e-01],\n",
      "         [4.2356e-04, 0.0000e+00, 8.1384e-04, 6.2629e-02, 2.3759e-02,\n",
      "          4.8443e-01]],\n",
      "\n",
      "        [[7.4846e-04, 4.6685e-04, 1.5017e-03, 5.9774e-02, 2.3876e-02,\n",
      "          4.8777e-01],\n",
      "         [8.4471e-04, 2.9148e-04, 8.9862e-04, 1.1115e-01, 2.3960e-02,\n",
      "          4.8332e-01],\n",
      "         [0.0000e+00, 1.4930e-04, 7.2180e-04, 1.8396e-01, 2.3464e-02,\n",
      "          4.8938e-01],\n",
      "         [4.2356e-04, 0.0000e+00, 8.1384e-04, 6.2629e-02, 2.3759e-02,\n",
      "          4.8443e-01],\n",
      "         [4.3800e-04, 6.1618e-05, 8.8409e-04, 7.1091e-02, 2.3407e-02,\n",
      "          4.8993e-01]],\n",
      "\n",
      "        [[8.4471e-04, 2.9148e-04, 8.9862e-04, 1.1115e-01, 2.3960e-02,\n",
      "          4.8332e-01],\n",
      "         [0.0000e+00, 1.4930e-04, 7.2180e-04, 1.8396e-01, 2.3464e-02,\n",
      "          4.8938e-01],\n",
      "         [4.2356e-04, 0.0000e+00, 8.1384e-04, 6.2629e-02, 2.3759e-02,\n",
      "          4.8443e-01],\n",
      "         [4.3800e-04, 6.1618e-05, 8.8409e-04, 7.1091e-02, 2.3407e-02,\n",
      "          4.8993e-01],\n",
      "         [3.7302e-04, 3.0571e-04, 1.0997e-03, 6.0598e-02, 2.3775e-02,\n",
      "          4.8841e-01]],\n",
      "\n",
      "        [[0.0000e+00, 1.4930e-04, 7.2180e-04, 1.8396e-01, 2.3464e-02,\n",
      "          4.8938e-01],\n",
      "         [4.2356e-04, 0.0000e+00, 8.1384e-04, 6.2629e-02, 2.3759e-02,\n",
      "          4.8443e-01],\n",
      "         [4.3800e-04, 6.1618e-05, 8.8409e-04, 7.1091e-02, 2.3407e-02,\n",
      "          4.8993e-01],\n",
      "         [3.7302e-04, 3.0571e-04, 1.0997e-03, 6.0598e-02, 2.3775e-02,\n",
      "          4.8841e-01],\n",
      "         [8.0380e-04, 2.2987e-04, 1.2208e-03, 4.9351e-02, 2.3943e-02,\n",
      "          4.8714e-01]],\n",
      "\n",
      "        [[4.2356e-04, 0.0000e+00, 8.1384e-04, 6.2629e-02, 2.3759e-02,\n",
      "          4.8443e-01],\n",
      "         [4.3800e-04, 6.1618e-05, 8.8409e-04, 7.1091e-02, 2.3407e-02,\n",
      "          4.8993e-01],\n",
      "         [3.7302e-04, 3.0571e-04, 1.0997e-03, 6.0598e-02, 2.3775e-02,\n",
      "          4.8841e-01],\n",
      "         [8.0380e-04, 2.2987e-04, 1.2208e-03, 4.9351e-02, 2.3943e-02,\n",
      "          4.8714e-01],\n",
      "         [5.7999e-04, 1.1422e-03, 1.2934e-03, 1.0580e-01, 2.3946e-02,\n",
      "          4.9253e-01]],\n",
      "\n",
      "        [[4.3800e-04, 6.1618e-05, 8.8409e-04, 7.1091e-02, 2.3407e-02,\n",
      "          4.8993e-01],\n",
      "         [3.7302e-04, 3.0571e-04, 1.0997e-03, 6.0598e-02, 2.3775e-02,\n",
      "          4.8841e-01],\n",
      "         [8.0380e-04, 2.2987e-04, 1.2208e-03, 4.9351e-02, 2.3943e-02,\n",
      "          4.8714e-01],\n",
      "         [5.7999e-04, 1.1422e-03, 1.2934e-03, 1.0580e-01, 2.3946e-02,\n",
      "          4.9253e-01],\n",
      "         [1.7905e-03, 1.5807e-03, 2.5433e-03, 9.5398e-02, 2.4653e-02,\n",
      "          4.9101e-01]],\n",
      "\n",
      "        [[3.7302e-04, 3.0571e-04, 1.0997e-03, 6.0598e-02, 2.3775e-02,\n",
      "          4.8841e-01],\n",
      "         [8.0380e-04, 2.2987e-04, 1.2208e-03, 4.9351e-02, 2.3943e-02,\n",
      "          4.8714e-01],\n",
      "         [5.7999e-04, 1.1422e-03, 1.2934e-03, 1.0580e-01, 2.3946e-02,\n",
      "          4.9253e-01],\n",
      "         [1.7905e-03, 1.5807e-03, 2.5433e-03, 9.5398e-02, 2.4653e-02,\n",
      "          4.9101e-01],\n",
      "         [1.9879e-03, 2.4314e-03, 2.6232e-03, 1.3173e-01, 2.5160e-02,\n",
      "          4.9427e-01]],\n",
      "\n",
      "        [[8.0380e-04, 2.2987e-04, 1.2208e-03, 4.9351e-02, 2.3943e-02,\n",
      "          4.8714e-01],\n",
      "         [5.7999e-04, 1.1422e-03, 1.2934e-03, 1.0580e-01, 2.3946e-02,\n",
      "          4.9253e-01],\n",
      "         [1.7905e-03, 1.5807e-03, 2.5433e-03, 9.5398e-02, 2.4653e-02,\n",
      "          4.9101e-01],\n",
      "         [1.9879e-03, 2.4314e-03, 2.6232e-03, 1.3173e-01, 2.5160e-02,\n",
      "          4.9427e-01],\n",
      "         [2.7363e-03, 2.9599e-03, 3.4492e-03, 1.3032e-01, 2.6094e-02,\n",
      "          4.8803e-01]],\n",
      "\n",
      "        [[5.7999e-04, 1.1422e-03, 1.2934e-03, 1.0580e-01, 2.3946e-02,\n",
      "          4.9253e-01],\n",
      "         [1.7905e-03, 1.5807e-03, 2.5433e-03, 9.5398e-02, 2.4653e-02,\n",
      "          4.9101e-01],\n",
      "         [1.9879e-03, 2.4314e-03, 2.6232e-03, 1.3173e-01, 2.5160e-02,\n",
      "          4.9427e-01],\n",
      "         [2.7363e-03, 2.9599e-03, 3.4492e-03, 1.3032e-01, 2.6094e-02,\n",
      "          4.8803e-01],\n",
      "         [3.1647e-03, 3.3319e-03, 3.8004e-03, 1.1271e-01, 2.6214e-02,\n",
      "          4.9065e-01]],\n",
      "\n",
      "        [[1.7905e-03, 1.5807e-03, 2.5433e-03, 9.5398e-02, 2.4653e-02,\n",
      "          4.9101e-01],\n",
      "         [1.9879e-03, 2.4314e-03, 2.6232e-03, 1.3173e-01, 2.5160e-02,\n",
      "          4.9427e-01],\n",
      "         [2.7363e-03, 2.9599e-03, 3.4492e-03, 1.3032e-01, 2.6094e-02,\n",
      "          4.8803e-01],\n",
      "         [3.1647e-03, 3.3319e-03, 3.8004e-03, 1.1271e-01, 2.6214e-02,\n",
      "          4.9065e-01],\n",
      "         [3.6653e-03, 3.7324e-03, 4.2606e-03, 1.1522e-01, 2.6675e-02,\n",
      "          4.9343e-01]],\n",
      "\n",
      "        [[1.9879e-03, 2.4314e-03, 2.6232e-03, 1.3173e-01, 2.5160e-02,\n",
      "          4.9427e-01],\n",
      "         [2.7363e-03, 2.9599e-03, 3.4492e-03, 1.3032e-01, 2.6094e-02,\n",
      "          4.8803e-01],\n",
      "         [3.1647e-03, 3.3319e-03, 3.8004e-03, 1.1271e-01, 2.6214e-02,\n",
      "          4.9065e-01],\n",
      "         [3.6653e-03, 3.7324e-03, 4.2606e-03, 1.1522e-01, 2.6675e-02,\n",
      "          4.9343e-01],\n",
      "         [4.2741e-03, 4.7064e-03, 5.0405e-03, 1.2930e-01, 2.7499e-02,\n",
      "          4.9047e-01]],\n",
      "\n",
      "        [[2.7363e-03, 2.9599e-03, 3.4492e-03, 1.3032e-01, 2.6094e-02,\n",
      "          4.8803e-01],\n",
      "         [3.1647e-03, 3.3319e-03, 3.8004e-03, 1.1271e-01, 2.6214e-02,\n",
      "          4.9065e-01],\n",
      "         [3.6653e-03, 3.7324e-03, 4.2606e-03, 1.1522e-01, 2.6675e-02,\n",
      "          4.9343e-01],\n",
      "         [4.2741e-03, 4.7064e-03, 5.0405e-03, 1.2930e-01, 2.7499e-02,\n",
      "          4.9047e-01],\n",
      "         [4.9624e-03, 4.4268e-03, 5.2198e-03, 8.7906e-02, 2.7936e-02,\n",
      "          4.8440e-01]],\n",
      "\n",
      "        [[3.1647e-03, 3.3319e-03, 3.8004e-03, 1.1271e-01, 2.6214e-02,\n",
      "          4.9065e-01],\n",
      "         [3.6653e-03, 3.7324e-03, 4.2606e-03, 1.1522e-01, 2.6675e-02,\n",
      "          4.9343e-01],\n",
      "         [4.2741e-03, 4.7064e-03, 5.0405e-03, 1.2930e-01, 2.7499e-02,\n",
      "          4.9047e-01],\n",
      "         [4.9624e-03, 4.4268e-03, 5.2198e-03, 8.7906e-02, 2.7936e-02,\n",
      "          4.8440e-01],\n",
      "         [4.3824e-03, 4.2491e-03, 5.0502e-03, 9.2197e-02, 2.7581e-02,\n",
      "          4.8809e-01]],\n",
      "\n",
      "        [[3.6653e-03, 3.7324e-03, 4.2606e-03, 1.1522e-01, 2.6675e-02,\n",
      "          4.9343e-01],\n",
      "         [4.2741e-03, 4.7064e-03, 5.0405e-03, 1.2930e-01, 2.7499e-02,\n",
      "          4.9047e-01],\n",
      "         [4.9624e-03, 4.4268e-03, 5.2198e-03, 8.7906e-02, 2.7936e-02,\n",
      "          4.8440e-01],\n",
      "         [4.3824e-03, 4.2491e-03, 5.0502e-03, 9.2197e-02, 2.7581e-02,\n",
      "          4.8809e-01],\n",
      "         [4.7290e-03, 4.9505e-03, 5.1011e-03, 1.0382e-01, 2.7707e-02,\n",
      "          4.9149e-01]],\n",
      "\n",
      "        [[4.2741e-03, 4.7064e-03, 5.0405e-03, 1.2930e-01, 2.7499e-02,\n",
      "          4.9047e-01],\n",
      "         [4.9624e-03, 4.4268e-03, 5.2198e-03, 8.7906e-02, 2.7936e-02,\n",
      "          4.8440e-01],\n",
      "         [4.3824e-03, 4.2491e-03, 5.0502e-03, 9.2197e-02, 2.7581e-02,\n",
      "          4.8809e-01],\n",
      "         [4.7290e-03, 4.9505e-03, 5.1011e-03, 1.0382e-01, 2.7707e-02,\n",
      "          4.9149e-01],\n",
      "         [5.2344e-03, 5.2989e-03, 5.7648e-03, 1.1097e-01, 2.8278e-02,\n",
      "          4.8535e-01]],\n",
      "\n",
      "        [[4.9624e-03, 4.4268e-03, 5.2198e-03, 8.7906e-02, 2.7936e-02,\n",
      "          4.8440e-01],\n",
      "         [4.3824e-03, 4.2491e-03, 5.0502e-03, 9.2197e-02, 2.7581e-02,\n",
      "          4.8809e-01],\n",
      "         [4.7290e-03, 4.9505e-03, 5.1011e-03, 1.0382e-01, 2.7707e-02,\n",
      "          4.9149e-01],\n",
      "         [5.2344e-03, 5.2989e-03, 5.7648e-03, 1.1097e-01, 2.8278e-02,\n",
      "          4.8535e-01],\n",
      "         [4.9022e-03, 4.5358e-03, 5.2900e-03, 8.5926e-02, 2.8046e-02,\n",
      "          4.8431e-01]],\n",
      "\n",
      "        [[4.3824e-03, 4.2491e-03, 5.0502e-03, 9.2197e-02, 2.7581e-02,\n",
      "          4.8809e-01],\n",
      "         [4.7290e-03, 4.9505e-03, 5.1011e-03, 1.0382e-01, 2.7707e-02,\n",
      "          4.9149e-01],\n",
      "         [5.2344e-03, 5.2989e-03, 5.7648e-03, 1.1097e-01, 2.8278e-02,\n",
      "          4.8535e-01],\n",
      "         [4.9022e-03, 4.5358e-03, 5.2900e-03, 8.5926e-02, 2.8046e-02,\n",
      "          4.8431e-01],\n",
      "         [5.3210e-03, 6.0809e-03, 5.8738e-03, 2.0600e-01, 2.7679e-02,\n",
      "          5.0253e-01]],\n",
      "\n",
      "        [[4.7290e-03, 4.9505e-03, 5.1011e-03, 1.0382e-01, 2.7707e-02,\n",
      "          4.9149e-01],\n",
      "         [5.2344e-03, 5.2989e-03, 5.7648e-03, 1.1097e-01, 2.8278e-02,\n",
      "          4.8535e-01],\n",
      "         [4.9022e-03, 4.5358e-03, 5.2900e-03, 8.5926e-02, 2.8046e-02,\n",
      "          4.8431e-01],\n",
      "         [5.3210e-03, 6.0809e-03, 5.8738e-03, 2.0600e-01, 2.7679e-02,\n",
      "          5.0253e-01],\n",
      "         [6.6206e-03, 7.8867e-03, 7.3319e-03, 3.7141e-01, 2.9692e-02,\n",
      "          4.9468e-01]],\n",
      "\n",
      "        [[5.2344e-03, 5.2989e-03, 5.7648e-03, 1.1097e-01, 2.8278e-02,\n",
      "          4.8535e-01],\n",
      "         [4.9022e-03, 4.5358e-03, 5.2900e-03, 8.5926e-02, 2.8046e-02,\n",
      "          4.8431e-01],\n",
      "         [5.3210e-03, 6.0809e-03, 5.8738e-03, 2.0600e-01, 2.7679e-02,\n",
      "          5.0253e-01],\n",
      "         [6.6206e-03, 7.8867e-03, 7.3319e-03, 3.7141e-01, 2.9692e-02,\n",
      "          4.9468e-01],\n",
      "         [7.3907e-03, 7.2421e-03, 8.0028e-03, 1.6739e-01, 3.0679e-02,\n",
      "          4.8447e-01]],\n",
      "\n",
      "        [[4.9022e-03, 4.5358e-03, 5.2900e-03, 8.5926e-02, 2.8046e-02,\n",
      "          4.8431e-01],\n",
      "         [5.3210e-03, 6.0809e-03, 5.8738e-03, 2.0600e-01, 2.7679e-02,\n",
      "          5.0253e-01],\n",
      "         [6.6206e-03, 7.8867e-03, 7.3319e-03, 3.7141e-01, 2.9692e-02,\n",
      "          4.9468e-01],\n",
      "         [7.3907e-03, 7.2421e-03, 8.0028e-03, 1.6739e-01, 3.0679e-02,\n",
      "          4.8447e-01],\n",
      "         [7.6073e-03, 7.7019e-03, 7.9786e-03, 1.8403e-01, 3.0333e-02,\n",
      "          4.9246e-01]],\n",
      "\n",
      "        [[5.3210e-03, 6.0809e-03, 5.8738e-03, 2.0600e-01, 2.7679e-02,\n",
      "          5.0253e-01],\n",
      "         [6.6206e-03, 7.8867e-03, 7.3319e-03, 3.7141e-01, 2.9692e-02,\n",
      "          4.9468e-01],\n",
      "         [7.3907e-03, 7.2421e-03, 8.0028e-03, 1.6739e-01, 3.0679e-02,\n",
      "          4.8447e-01],\n",
      "         [7.6073e-03, 7.7019e-03, 7.9786e-03, 1.8403e-01, 3.0333e-02,\n",
      "          4.9246e-01],\n",
      "         [8.6782e-03, 8.3251e-03, 9.2212e-03, 1.5844e-01, 3.1030e-02,\n",
      "          4.9156e-01]],\n",
      "\n",
      "        [[6.6206e-03, 7.8867e-03, 7.3319e-03, 3.7141e-01, 2.9692e-02,\n",
      "          4.9468e-01],\n",
      "         [7.3907e-03, 7.2421e-03, 8.0028e-03, 1.6739e-01, 3.0679e-02,\n",
      "          4.8447e-01],\n",
      "         [7.6073e-03, 7.7019e-03, 7.9786e-03, 1.8403e-01, 3.0333e-02,\n",
      "          4.9246e-01],\n",
      "         [8.6782e-03, 8.3251e-03, 9.2212e-03, 1.5844e-01, 3.1030e-02,\n",
      "          4.9156e-01],\n",
      "         [8.5362e-03, 8.7185e-03, 8.7876e-03, 1.8219e-01, 3.1611e-02,\n",
      "          4.9305e-01]],\n",
      "\n",
      "        [[7.3907e-03, 7.2421e-03, 8.0028e-03, 1.6739e-01, 3.0679e-02,\n",
      "          4.8447e-01],\n",
      "         [7.6073e-03, 7.7019e-03, 7.9786e-03, 1.8403e-01, 3.0333e-02,\n",
      "          4.9246e-01],\n",
      "         [8.6782e-03, 8.3251e-03, 9.2212e-03, 1.5844e-01, 3.1030e-02,\n",
      "          4.9156e-01],\n",
      "         [8.5362e-03, 8.7185e-03, 8.7876e-03, 1.8219e-01, 3.1611e-02,\n",
      "          4.9305e-01],\n",
      "         [9.2317e-03, 8.6996e-03, 9.6984e-03, 1.6281e-01, 3.2386e-02,\n",
      "          4.8481e-01]],\n",
      "\n",
      "        [[7.6073e-03, 7.7019e-03, 7.9786e-03, 1.8403e-01, 3.0333e-02,\n",
      "          4.9246e-01],\n",
      "         [8.6782e-03, 8.3251e-03, 9.2212e-03, 1.5844e-01, 3.1030e-02,\n",
      "          4.9156e-01],\n",
      "         [8.5362e-03, 8.7185e-03, 8.7876e-03, 1.8219e-01, 3.1611e-02,\n",
      "          4.9305e-01],\n",
      "         [9.2317e-03, 8.6996e-03, 9.6984e-03, 1.6281e-01, 3.2386e-02,\n",
      "          4.8481e-01],\n",
      "         [9.0801e-03, 9.0384e-03, 9.8316e-03, 1.7174e-01, 3.2084e-02,\n",
      "          4.9029e-01]],\n",
      "\n",
      "        [[8.6782e-03, 8.3251e-03, 9.2212e-03, 1.5844e-01, 3.1030e-02,\n",
      "          4.9156e-01],\n",
      "         [8.5362e-03, 8.7185e-03, 8.7876e-03, 1.8219e-01, 3.1611e-02,\n",
      "          4.9305e-01],\n",
      "         [9.2317e-03, 8.6996e-03, 9.6984e-03, 1.6281e-01, 3.2386e-02,\n",
      "          4.8481e-01],\n",
      "         [9.0801e-03, 9.0384e-03, 9.8316e-03, 1.7174e-01, 3.2084e-02,\n",
      "          4.9029e-01],\n",
      "         [9.5133e-03, 8.9910e-03, 9.9454e-03, 1.3466e-01, 3.2498e-02,\n",
      "          4.8511e-01]]]), 'label': tensor([1.8933e-03, 1.4715e-03, 4.7933e-04, 5.6560e-04, 5.7522e-05, 3.5949e-04,\n",
      "        0.0000e+00, 3.7627e-04, 5.4883e-04, 5.5123e-04, 1.2750e-03, 1.7951e-03,\n",
      "        2.7513e-03, 2.8735e-03, 3.3457e-03, 4.1893e-03, 4.6374e-03, 4.2732e-03,\n",
      "        4.4026e-03, 4.9873e-03, 4.7501e-03, 4.3738e-03, 6.4349e-03, 7.4463e-03,\n",
      "        7.0916e-03, 7.8058e-03, 8.4001e-03, 9.1934e-03, 8.8842e-03, 9.3084e-03,\n",
      "        9.0400e-03, 8.4480e-03])}\n"
     ]
    }
   ],
   "source": [
    "for item in data_module.train_dataloader():\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7e763943",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoints2\",monitor = \"val_loss\",save_top_k = 1,verbose = 1, mode = \"min\")\n",
    "#logger = TensorBoardLogger(\"new_logs\", name = \"akbank_last\")\n",
    "\n",
    "#early_stopping_callback = EarlyStopping(monitor = \"val_loss\", patience = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "96b86bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    #logger,\n",
    "    callbacks = [checkpoint_callback],\n",
    "    #callbacks = [],\n",
    "    max_epochs = 512,\n",
    "    accelerator = 'gpu',\n",
    "    devices = 1\n",
    "    #progress_bar_refresh_rate = 30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "51a8c7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type     | Params\n",
      "---------------------------------------\n",
      "0 | model     | just_GRU | 151 K \n",
      "1 | criterion | MSELoss  | 0     \n",
      "---------------------------------------\n",
      "151 K     Trainable params\n",
      "0         Non-trainable params\n",
      "151 K     Total params\n",
      "0.606     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hazre\\anaconda3\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\hazre\\anaconda3\\envs\\yolov7\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  90%|████████▉ | 131/146 [00:02<00:00, 56.08it/s, loss=0.00225, v_num=8, train_loss=0.000593]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hazre\\anaconda3\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([6, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  90%|█████████ | 132/146 [00:02<00:00, 56.01it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0:  91%|█████████ | 133/146 [00:02<00:00, 56.19it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0:  92%|█████████▏| 134/146 [00:02<00:00, 56.36it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0:  92%|█████████▏| 135/146 [00:02<00:00, 56.51it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0:  93%|█████████▎| 136/146 [00:02<00:00, 56.74it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0:  94%|█████████▍| 137/146 [00:02<00:00, 56.95it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0:  95%|█████████▍| 138/146 [00:02<00:00, 57.15it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0:  95%|█████████▌| 139/146 [00:02<00:00, 57.33it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0:  96%|█████████▌| 140/146 [00:02<00:00, 57.48it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0:  97%|█████████▋| 141/146 [00:02<00:00, 57.63it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0:  97%|█████████▋| 142/146 [00:02<00:00, 57.82it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0:  98%|█████████▊| 143/146 [00:02<00:00, 58.02it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0:  99%|█████████▊| 144/146 [00:02<00:00, 58.16it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0:  99%|█████████▉| 145/146 [00:02<00:00, 58.36it/s, loss=0.00225, v_num=8, train_loss=0.000593]\n",
      "Epoch 0: 100%|██████████| 146/146 [00:02<00:00, 58.38it/s, loss=0.00225, v_num=8, train_loss=0.000593, val_loss=0.0481]\n",
      "Epoch 0: 100%|██████████| 146/146 [00:02<00:00, 58.29it/s, loss=0.00225, v_num=8, train_loss=0.000593, val_loss=0.0481]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hazre\\anaconda3\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([11, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 0, global step 131: 'val_loss' reached 0.04810 (best 0.04810), saving model to 'C:\\\\Users\\\\hazre\\\\Desktop\\\\checkpoints2\\\\epoch=0-step=131.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  90%|████████▉ | 131/146 [00:02<00:00, 55.58it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  90%|█████████ | 132/146 [00:02<00:00, 55.32it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1:  91%|█████████ | 133/146 [00:02<00:00, 55.46it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1:  92%|█████████▏| 134/146 [00:02<00:00, 55.60it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1:  92%|█████████▏| 135/146 [00:02<00:00, 55.71it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1:  93%|█████████▎| 136/146 [00:02<00:00, 55.76it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1:  94%|█████████▍| 137/146 [00:02<00:00, 55.89it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1:  95%|█████████▍| 138/146 [00:02<00:00, 56.00it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1:  95%|█████████▌| 139/146 [00:02<00:00, 56.16it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1:  96%|█████████▌| 140/146 [00:02<00:00, 56.29it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1:  97%|█████████▋| 141/146 [00:02<00:00, 56.44it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1:  97%|█████████▋| 142/146 [00:02<00:00, 56.52it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1:  98%|█████████▊| 143/146 [00:02<00:00, 56.65it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1:  99%|█████████▊| 144/146 [00:02<00:00, 56.78it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1:  99%|█████████▉| 145/146 [00:02<00:00, 56.84it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0481]\n",
      "Epoch 1: 100%|██████████| 146/146 [00:02<00:00, 56.61it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0129]\n",
      "Epoch 1: 100%|██████████| 146/146 [00:02<00:00, 56.32it/s, loss=0.00148, v_num=8, train_loss=0.000681, val_loss=0.0129]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 262: 'val_loss' reached 0.01294 (best 0.01294), saving model to 'C:\\\\Users\\\\hazre\\\\Desktop\\\\checkpoints2\\\\epoch=1-step=262.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  90%|████████▉ | 131/146 [00:02<00:00, 50.97it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  90%|█████████ | 132/146 [00:02<00:00, 50.92it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2:  91%|█████████ | 133/146 [00:02<00:00, 51.13it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2:  92%|█████████▏| 134/146 [00:02<00:00, 51.32it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2:  92%|█████████▏| 135/146 [00:02<00:00, 51.48it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2:  93%|█████████▎| 136/146 [00:02<00:00, 51.69it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2:  94%|█████████▍| 137/146 [00:02<00:00, 51.87it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2:  95%|█████████▍| 138/146 [00:02<00:00, 52.05it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2:  95%|█████████▌| 139/146 [00:02<00:00, 52.21it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2:  96%|█████████▌| 140/146 [00:02<00:00, 52.39it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2:  97%|█████████▋| 141/146 [00:02<00:00, 52.57it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2:  97%|█████████▋| 142/146 [00:02<00:00, 52.71it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2:  98%|█████████▊| 143/146 [00:02<00:00, 52.88it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2:  99%|█████████▊| 144/146 [00:02<00:00, 53.03it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2:  99%|█████████▉| 145/146 [00:02<00:00, 53.23it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0129]\n",
      "Epoch 2: 100%|██████████| 146/146 [00:02<00:00, 53.28it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0214]\n",
      "Epoch 2: 100%|██████████| 146/146 [00:02<00:00, 53.20it/s, loss=0.0011, v_num=8, train_loss=0.000319, val_loss=0.0214]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 393: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  90%|████████▉ | 131/146 [00:02<00:00, 59.27it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  90%|█████████ | 132/146 [00:02<00:00, 59.19it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3:  91%|█████████ | 133/146 [00:02<00:00, 59.37it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3:  92%|█████████▏| 134/146 [00:02<00:00, 59.52it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3:  92%|█████████▏| 135/146 [00:02<00:00, 59.68it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3:  93%|█████████▎| 136/146 [00:02<00:00, 59.91it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3:  94%|█████████▍| 137/146 [00:02<00:00, 60.08it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3:  95%|█████████▍| 138/146 [00:02<00:00, 60.26it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3:  95%|█████████▌| 139/146 [00:02<00:00, 60.46it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3:  96%|█████████▌| 140/146 [00:02<00:00, 60.63it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3:  97%|█████████▋| 141/146 [00:02<00:00, 60.82it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3:  97%|█████████▋| 142/146 [00:02<00:00, 60.99it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3:  98%|█████████▊| 143/146 [00:02<00:00, 61.21it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3:  99%|█████████▊| 144/146 [00:02<00:00, 61.35it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3:  99%|█████████▉| 145/146 [00:02<00:00, 61.54it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0214]\n",
      "Epoch 3: 100%|██████████| 146/146 [00:02<00:00, 61.57it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0152]\n",
      "Epoch 3: 100%|██████████| 146/146 [00:02<00:00, 61.47it/s, loss=0.00122, v_num=8, train_loss=0.000169, val_loss=0.0152]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 524: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  90%|████████▉ | 131/146 [00:02<00:00, 62.26it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  90%|█████████ | 132/146 [00:02<00:00, 62.14it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4:  91%|█████████ | 133/146 [00:02<00:00, 62.35it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4:  92%|█████████▏| 134/146 [00:02<00:00, 62.52it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4:  92%|█████████▏| 135/146 [00:02<00:00, 62.73it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4:  93%|█████████▎| 136/146 [00:02<00:00, 62.84it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4:  94%|█████████▍| 137/146 [00:02<00:00, 63.07it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4:  95%|█████████▍| 138/146 [00:02<00:00, 63.27it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4:  95%|█████████▌| 139/146 [00:02<00:00, 63.47it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [00:02<00:00, 63.63it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4:  97%|█████████▋| 141/146 [00:02<00:00, 63.83it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4:  97%|█████████▋| 142/146 [00:02<00:00, 64.00it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4:  98%|█████████▊| 143/146 [00:02<00:00, 64.09it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4:  99%|█████████▊| 144/146 [00:02<00:00, 64.22it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4:  99%|█████████▉| 145/146 [00:02<00:00, 64.41it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0152]\n",
      "Epoch 4: 100%|██████████| 146/146 [00:02<00:00, 64.45it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0194]\n",
      "Epoch 4: 100%|██████████| 146/146 [00:02<00:00, 64.31it/s, loss=0.00111, v_num=8, train_loss=0.000275, val_loss=0.0194]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 655: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  90%|████████▉ | 131/146 [00:02<00:00, 57.76it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  90%|█████████ | 132/146 [00:02<00:00, 57.71it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5:  91%|█████████ | 133/146 [00:02<00:00, 57.87it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5:  92%|█████████▏| 134/146 [00:02<00:00, 58.03it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5:  92%|█████████▏| 135/146 [00:02<00:00, 58.21it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5:  93%|█████████▎| 136/146 [00:02<00:00, 58.39it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5:  94%|█████████▍| 137/146 [00:02<00:00, 58.59it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5:  95%|█████████▍| 138/146 [00:02<00:00, 58.74it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5:  95%|█████████▌| 139/146 [00:02<00:00, 58.87it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5:  96%|█████████▌| 140/146 [00:02<00:00, 59.04it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5:  97%|█████████▋| 141/146 [00:02<00:00, 59.21it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5:  97%|█████████▋| 142/146 [00:02<00:00, 59.41it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5:  98%|█████████▊| 143/146 [00:02<00:00, 59.55it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5:  99%|█████████▊| 144/146 [00:02<00:00, 59.67it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5:  99%|█████████▉| 145/146 [00:02<00:00, 59.86it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0194]\n",
      "Epoch 5: 100%|██████████| 146/146 [00:02<00:00, 59.83it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0158]\n",
      "Epoch 5: 100%|██████████| 146/146 [00:02<00:00, 59.68it/s, loss=0.00117, v_num=8, train_loss=0.000276, val_loss=0.0158]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 786: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  90%|████████▉ | 131/146 [00:02<00:00, 57.58it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]   \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  90%|█████████ | 132/146 [00:02<00:00, 57.51it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6:  91%|█████████ | 133/146 [00:02<00:00, 57.62it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6:  92%|█████████▏| 134/146 [00:02<00:00, 57.70it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6:  92%|█████████▏| 135/146 [00:02<00:00, 57.91it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6:  93%|█████████▎| 136/146 [00:02<00:00, 58.07it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6:  94%|█████████▍| 137/146 [00:02<00:00, 58.19it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6:  95%|█████████▍| 138/146 [00:02<00:00, 58.35it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6:  95%|█████████▌| 139/146 [00:02<00:00, 58.45it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6:  96%|█████████▌| 140/146 [00:02<00:00, 58.60it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6:  97%|█████████▋| 141/146 [00:02<00:00, 58.70it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6:  97%|█████████▋| 142/146 [00:02<00:00, 58.82it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6:  98%|█████████▊| 143/146 [00:02<00:00, 58.94it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6:  99%|█████████▊| 144/146 [00:02<00:00, 59.08it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6:  99%|█████████▉| 145/146 [00:02<00:00, 59.25it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0158]\n",
      "Epoch 6: 100%|██████████| 146/146 [00:02<00:00, 59.25it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0181]\n",
      "Epoch 6: 100%|██████████| 146/146 [00:02<00:00, 59.13it/s, loss=0.0011, v_num=8, train_loss=0.00042, val_loss=0.0181]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 917: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  90%|████████▉ | 131/146 [00:02<00:00, 59.73it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  90%|█████████ | 132/146 [00:02<00:00, 59.40it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7:  91%|█████████ | 133/146 [00:02<00:00, 59.58it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7:  92%|█████████▏| 134/146 [00:02<00:00, 59.79it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7:  92%|█████████▏| 135/146 [00:02<00:00, 59.97it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7:  93%|█████████▎| 136/146 [00:02<00:00, 60.12it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7:  94%|█████████▍| 137/146 [00:02<00:00, 60.22it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7:  95%|█████████▍| 138/146 [00:02<00:00, 60.42it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7:  95%|█████████▌| 139/146 [00:02<00:00, 60.54it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7:  96%|█████████▌| 140/146 [00:02<00:00, 60.76it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7:  97%|█████████▋| 141/146 [00:02<00:00, 60.93it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7:  97%|█████████▋| 142/146 [00:02<00:00, 61.12it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7:  98%|█████████▊| 143/146 [00:02<00:00, 61.32it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7:  99%|█████████▊| 144/146 [00:02<00:00, 61.46it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7:  99%|█████████▉| 145/146 [00:02<00:00, 61.65it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0181]\n",
      "Epoch 7: 100%|██████████| 146/146 [00:02<00:00, 61.70it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0159]\n",
      "Epoch 7: 100%|██████████| 146/146 [00:02<00:00, 61.55it/s, loss=0.00113, v_num=8, train_loss=0.000438, val_loss=0.0159]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1048: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  90%|████████▉ | 131/146 [00:02<00:00, 55.75it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  90%|█████████ | 132/146 [00:02<00:00, 55.56it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8:  91%|█████████ | 133/146 [00:02<00:00, 55.72it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8:  92%|█████████▏| 134/146 [00:02<00:00, 55.88it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8:  92%|█████████▏| 135/146 [00:02<00:00, 56.00it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8:  93%|█████████▎| 136/146 [00:02<00:00, 56.11it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8:  94%|█████████▍| 137/146 [00:02<00:00, 56.24it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8:  95%|█████████▍| 138/146 [00:02<00:00, 56.35it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8:  95%|█████████▌| 139/146 [00:02<00:00, 56.49it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8:  96%|█████████▌| 140/146 [00:02<00:00, 56.64it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8:  97%|█████████▋| 141/146 [00:02<00:00, 56.72it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8:  97%|█████████▋| 142/146 [00:02<00:00, 56.85it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8:  98%|█████████▊| 143/146 [00:02<00:00, 57.02it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8:  99%|█████████▊| 144/146 [00:02<00:00, 57.12it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8:  99%|█████████▉| 145/146 [00:02<00:00, 57.23it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0159]\n",
      "Epoch 8: 100%|██████████| 146/146 [00:02<00:00, 57.19it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0172]\n",
      "Epoch 8: 100%|██████████| 146/146 [00:02<00:00, 57.08it/s, loss=0.00109, v_num=8, train_loss=0.00052, val_loss=0.0172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1179: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  90%|████████▉ | 131/146 [00:02<00:00, 54.24it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  90%|█████████ | 132/146 [00:02<00:00, 54.13it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9:  91%|█████████ | 133/146 [00:02<00:00, 54.28it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9:  92%|█████████▏| 134/146 [00:02<00:00, 54.44it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9:  92%|█████████▏| 135/146 [00:02<00:00, 54.60it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9:  93%|█████████▎| 136/146 [00:02<00:00, 54.72it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9:  94%|█████████▍| 137/146 [00:02<00:00, 54.88it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9:  95%|█████████▍| 138/146 [00:02<00:00, 55.04it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9:  95%|█████████▌| 139/146 [00:02<00:00, 55.15it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9:  96%|█████████▌| 140/146 [00:02<00:00, 55.33it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9:  97%|█████████▋| 141/146 [00:02<00:00, 55.42it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9:  97%|█████████▋| 142/146 [00:02<00:00, 55.55it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9:  98%|█████████▊| 143/146 [00:02<00:00, 55.63it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9:  99%|█████████▊| 144/146 [00:02<00:00, 55.78it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9:  99%|█████████▉| 145/146 [00:02<00:00, 55.89it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0172]\n",
      "Epoch 9: 100%|██████████| 146/146 [00:02<00:00, 55.91it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0159]\n",
      "Epoch 9: 100%|██████████| 146/146 [00:02<00:00, 55.74it/s, loss=0.00111, v_num=8, train_loss=0.000545, val_loss=0.0159]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1310: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  90%|████████▉ | 131/146 [00:02<00:00, 58.37it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  90%|█████████ | 132/146 [00:02<00:00, 58.25it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10:  91%|█████████ | 133/146 [00:02<00:00, 58.43it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10:  92%|█████████▏| 134/146 [00:02<00:00, 58.61it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10:  92%|█████████▏| 135/146 [00:02<00:00, 58.74it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10:  93%|█████████▎| 136/146 [00:02<00:00, 58.90it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10:  94%|█████████▍| 137/146 [00:02<00:00, 59.05it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10:  95%|█████████▍| 138/146 [00:02<00:00, 59.22it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10:  95%|█████████▌| 139/146 [00:02<00:00, 59.35it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10:  96%|█████████▌| 140/146 [00:02<00:00, 59.47it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10:  97%|█████████▋| 141/146 [00:02<00:00, 59.59it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10:  97%|█████████▋| 142/146 [00:02<00:00, 59.76it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10:  98%|█████████▊| 143/146 [00:02<00:00, 59.90it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10:  99%|█████████▊| 144/146 [00:02<00:00, 60.02it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10:  99%|█████████▉| 145/146 [00:02<00:00, 60.16it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0159]\n",
      "Epoch 10: 100%|██████████| 146/146 [00:02<00:00, 60.08it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0167]\n",
      "Epoch 10: 100%|██████████| 146/146 [00:02<00:00, 59.98it/s, loss=0.00108, v_num=8, train_loss=0.000579, val_loss=0.0167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 1441: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  90%|████████▉ | 131/146 [00:02<00:00, 59.90it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  90%|█████████ | 132/146 [00:02<00:00, 59.83it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11:  91%|█████████ | 133/146 [00:02<00:00, 60.07it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11:  92%|█████████▏| 134/146 [00:02<00:00, 60.19it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11:  92%|█████████▏| 135/146 [00:02<00:00, 60.32it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11:  93%|█████████▎| 136/146 [00:02<00:00, 60.44it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11:  94%|█████████▍| 137/146 [00:02<00:00, 60.62it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11:  95%|█████████▍| 138/146 [00:02<00:00, 60.76it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11:  95%|█████████▌| 139/146 [00:02<00:00, 60.93it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11:  96%|█████████▌| 140/146 [00:02<00:00, 61.08it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11:  97%|█████████▋| 141/146 [00:02<00:00, 61.19it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11:  97%|█████████▋| 142/146 [00:02<00:00, 61.28it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11:  98%|█████████▊| 143/146 [00:02<00:00, 61.47it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11:  99%|█████████▊| 144/146 [00:02<00:00, 61.53it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11:  99%|█████████▉| 145/146 [00:02<00:00, 61.67it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.0167]\n",
      "Epoch 11: 100%|██████████| 146/146 [00:02<00:00, 61.62it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.016] \n",
      "Epoch 11: 100%|██████████| 146/146 [00:02<00:00, 61.52it/s, loss=0.00108, v_num=8, train_loss=0.000597, val_loss=0.016]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 1572: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  90%|████████▉ | 131/146 [00:02<00:00, 59.48it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  90%|█████████ | 132/146 [00:02<00:00, 59.34it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12:  91%|█████████ | 133/146 [00:02<00:00, 59.47it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12:  92%|█████████▏| 134/146 [00:02<00:00, 59.60it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12:  92%|█████████▏| 135/146 [00:02<00:00, 59.78it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12:  93%|█████████▎| 136/146 [00:02<00:00, 59.93it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12:  94%|█████████▍| 137/146 [00:02<00:00, 60.08it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12:  95%|█████████▍| 138/146 [00:02<00:00, 60.25it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12:  95%|█████████▌| 139/146 [00:02<00:00, 60.37it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12:  96%|█████████▌| 140/146 [00:02<00:00, 60.52it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12:  97%|█████████▋| 141/146 [00:02<00:00, 60.58it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12:  97%|█████████▋| 142/146 [00:02<00:00, 60.75it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12:  98%|█████████▊| 143/146 [00:02<00:00, 60.87it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12:  99%|█████████▊| 144/146 [00:02<00:00, 60.98it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12:  99%|█████████▉| 145/146 [00:02<00:00, 61.12it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.016]\n",
      "Epoch 12: 100%|██████████| 146/146 [00:02<00:00, 61.10it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.0163]\n",
      "Epoch 12: 100%|██████████| 146/146 [00:02<00:00, 60.92it/s, loss=0.00107, v_num=8, train_loss=0.000613, val_loss=0.0163]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 1703: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  90%|████████▉ | 131/146 [00:02<00:00, 62.12it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  90%|█████████ | 132/146 [00:02<00:00, 61.85it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13:  91%|█████████ | 133/146 [00:02<00:00, 61.91it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13:  92%|█████████▏| 134/146 [00:02<00:00, 61.97it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13:  92%|█████████▏| 135/146 [00:02<00:00, 62.12it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13:  93%|█████████▎| 136/146 [00:02<00:00, 62.23it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13:  94%|█████████▍| 137/146 [00:02<00:00, 62.38it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13:  95%|█████████▍| 138/146 [00:02<00:00, 62.52it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13:  95%|█████████▌| 139/146 [00:02<00:00, 62.66it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13:  96%|█████████▌| 140/146 [00:02<00:00, 62.77it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13:  97%|█████████▋| 141/146 [00:02<00:00, 62.91it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13:  97%|█████████▋| 142/146 [00:02<00:00, 63.05it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13:  98%|█████████▊| 143/146 [00:02<00:00, 63.12it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13:  99%|█████████▊| 144/146 [00:02<00:00, 63.26it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13:  99%|█████████▉| 145/146 [00:02<00:00, 63.36it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.0163]\n",
      "Epoch 13: 100%|██████████| 146/146 [00:02<00:00, 63.41it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.016] \n",
      "Epoch 13: 100%|██████████| 146/146 [00:02<00:00, 63.22it/s, loss=0.00107, v_num=8, train_loss=0.000622, val_loss=0.016]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 1834: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  90%|████████▉ | 131/146 [00:02<00:00, 58.64it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  90%|█████████ | 132/146 [00:02<00:00, 58.43it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14:  91%|█████████ | 133/146 [00:02<00:00, 58.59it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14:  92%|█████████▏| 134/146 [00:02<00:00, 58.72it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14:  92%|█████████▏| 135/146 [00:02<00:00, 58.87it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14:  93%|█████████▎| 136/146 [00:02<00:00, 59.02it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14:  94%|█████████▍| 137/146 [00:02<00:00, 59.15it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14:  95%|█████████▍| 138/146 [00:02<00:00, 59.27it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14:  95%|█████████▌| 139/146 [00:02<00:00, 59.42it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14:  96%|█████████▌| 140/146 [00:02<00:00, 59.52it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14:  97%|█████████▋| 141/146 [00:02<00:00, 59.64it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14:  97%|█████████▋| 142/146 [00:02<00:00, 59.73it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14:  98%|█████████▊| 143/146 [00:02<00:00, 59.83it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14:  99%|█████████▊| 144/146 [00:02<00:00, 59.85it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14:  99%|█████████▉| 145/146 [00:02<00:00, 59.94it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.016]\n",
      "Epoch 14: 100%|██████████| 146/146 [00:02<00:00, 59.88it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.0162]\n",
      "Epoch 14: 100%|██████████| 146/146 [00:02<00:00, 59.61it/s, loss=0.00105, v_num=8, train_loss=0.000629, val_loss=0.0162]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 1965: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  90%|████████▉ | 131/146 [00:02<00:00, 48.35it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  90%|█████████ | 132/146 [00:02<00:00, 48.30it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15:  91%|█████████ | 133/146 [00:02<00:00, 48.38it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15:  92%|█████████▏| 134/146 [00:02<00:00, 48.51it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15:  92%|█████████▏| 135/146 [00:02<00:00, 48.66it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15:  93%|█████████▎| 136/146 [00:02<00:00, 48.78it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15:  94%|█████████▍| 137/146 [00:02<00:00, 48.91it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15:  95%|█████████▍| 138/146 [00:02<00:00, 49.00it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15:  95%|█████████▌| 139/146 [00:02<00:00, 49.11it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15:  96%|█████████▌| 140/146 [00:02<00:00, 49.24it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15:  97%|█████████▋| 141/146 [00:02<00:00, 49.38it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15:  97%|█████████▋| 142/146 [00:02<00:00, 49.51it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15:  98%|█████████▊| 143/146 [00:02<00:00, 49.63it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15:  99%|█████████▊| 144/146 [00:02<00:00, 49.76it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15:  99%|█████████▉| 145/146 [00:02<00:00, 49.79it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.0162]\n",
      "Epoch 15: 100%|██████████| 146/146 [00:02<00:00, 49.86it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.016] \n",
      "Epoch 15: 100%|██████████| 146/146 [00:02<00:00, 49.76it/s, loss=0.00105, v_num=8, train_loss=0.000633, val_loss=0.016]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 2096: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  90%|████████▉ | 131/146 [00:02<00:00, 57.23it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  90%|█████████ | 132/146 [00:02<00:00, 57.17it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16:  91%|█████████ | 133/146 [00:02<00:00, 57.25it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16:  92%|█████████▏| 134/146 [00:02<00:00, 57.44it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16:  92%|█████████▏| 135/146 [00:02<00:00, 57.57it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16:  93%|█████████▎| 136/146 [00:02<00:00, 57.75it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16:  94%|█████████▍| 137/146 [00:02<00:00, 57.90it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16:  95%|█████████▍| 138/146 [00:02<00:00, 58.06it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16:  95%|█████████▌| 139/146 [00:02<00:00, 58.23it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16:  96%|█████████▌| 140/146 [00:02<00:00, 58.36it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16:  97%|█████████▋| 141/146 [00:02<00:00, 58.48it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16:  97%|█████████▋| 142/146 [00:02<00:00, 58.61it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16:  98%|█████████▊| 143/146 [00:02<00:00, 58.73it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16:  99%|█████████▊| 144/146 [00:02<00:00, 58.82it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16:  99%|█████████▉| 145/146 [00:02<00:00, 58.99it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.016]\n",
      "Epoch 16: 100%|██████████| 146/146 [00:02<00:00, 58.99it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 16: 100%|██████████| 146/146 [00:02<00:00, 58.85it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 2227: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  90%|████████▉ | 131/146 [00:02<00:00, 60.63it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  90%|█████████ | 132/146 [00:02<00:00, 60.61it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17:  91%|█████████ | 133/146 [00:02<00:00, 60.76it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17:  92%|█████████▏| 134/146 [00:02<00:00, 60.94it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17:  92%|█████████▏| 135/146 [00:02<00:00, 61.09it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17:  93%|█████████▎| 136/146 [00:02<00:00, 61.24it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17:  94%|█████████▍| 137/146 [00:02<00:00, 61.41it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17:  95%|█████████▍| 138/146 [00:02<00:00, 61.58it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17:  95%|█████████▌| 139/146 [00:02<00:00, 61.73it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17:  96%|█████████▌| 140/146 [00:02<00:00, 61.90it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17:  97%|█████████▋| 141/146 [00:02<00:00, 62.01it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17:  97%|█████████▋| 142/146 [00:02<00:00, 62.18it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17:  98%|█████████▊| 143/146 [00:02<00:00, 62.34it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17:  99%|█████████▊| 144/146 [00:02<00:00, 62.45it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17:  99%|█████████▉| 145/146 [00:02<00:00, 62.61it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17: 100%|██████████| 146/146 [00:02<00:00, 62.64it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]\n",
      "Epoch 17: 100%|██████████| 146/146 [00:02<00:00, 62.53it/s, loss=0.00104, v_num=8, train_loss=0.000635, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 2358: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  90%|████████▉ | 131/146 [00:02<00:00, 58.85it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  90%|█████████ | 132/146 [00:02<00:00, 58.79it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18:  91%|█████████ | 133/146 [00:02<00:00, 58.95it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18:  92%|█████████▏| 134/146 [00:02<00:00, 59.13it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18:  92%|█████████▏| 135/146 [00:02<00:00, 59.18it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18:  93%|█████████▎| 136/146 [00:02<00:00, 59.25it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18:  94%|█████████▍| 137/146 [00:02<00:00, 59.38it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18:  95%|█████████▍| 138/146 [00:02<00:00, 59.50it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18:  95%|█████████▌| 139/146 [00:02<00:00, 59.60it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18:  96%|█████████▌| 140/146 [00:02<00:00, 59.65it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18:  97%|█████████▋| 141/146 [00:02<00:00, 59.77it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18:  97%|█████████▋| 142/146 [00:02<00:00, 59.86it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18:  98%|█████████▊| 143/146 [00:02<00:00, 59.98it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18:  99%|█████████▊| 144/146 [00:02<00:00, 59.80it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18:  99%|█████████▉| 145/146 [00:02<00:00, 59.91it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18: 100%|██████████| 146/146 [00:02<00:00, 59.83it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]\n",
      "Epoch 18: 100%|██████████| 146/146 [00:02<00:00, 59.64it/s, loss=0.00103, v_num=8, train_loss=0.000634, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 2489: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:  90%|████████▉ | 131/146 [00:02<00:00, 54.38it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  90%|█████████ | 132/146 [00:02<00:00, 54.25it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19:  91%|█████████ | 133/146 [00:02<00:00, 54.37it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19:  92%|█████████▏| 134/146 [00:02<00:00, 54.51it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19:  92%|█████████▏| 135/146 [00:02<00:00, 54.67it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19:  93%|█████████▎| 136/146 [00:02<00:00, 54.81it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19:  94%|█████████▍| 137/146 [00:02<00:00, 54.97it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19:  95%|█████████▍| 138/146 [00:02<00:00, 55.11it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19:  95%|█████████▌| 139/146 [00:02<00:00, 55.24it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19:  96%|█████████▌| 140/146 [00:02<00:00, 55.33it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19:  97%|█████████▋| 141/146 [00:02<00:00, 55.46it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19:  97%|█████████▋| 142/146 [00:02<00:00, 55.57it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19:  98%|█████████▊| 143/146 [00:02<00:00, 55.75it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19:  99%|█████████▊| 144/146 [00:02<00:00, 55.87it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19:  99%|█████████▉| 145/146 [00:02<00:00, 56.05it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19: 100%|██████████| 146/146 [00:02<00:00, 56.09it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]\n",
      "Epoch 19: 100%|██████████| 146/146 [00:02<00:00, 55.98it/s, loss=0.00103, v_num=8, train_loss=0.000632, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 2620: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  90%|████████▉ | 131/146 [00:02<00:00, 56.95it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  90%|█████████ | 132/146 [00:02<00:00, 56.94it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20:  91%|█████████ | 133/146 [00:02<00:00, 57.15it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20:  92%|█████████▏| 134/146 [00:02<00:00, 57.36it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20:  92%|█████████▏| 135/146 [00:02<00:00, 57.57it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20:  93%|█████████▎| 136/146 [00:02<00:00, 57.75it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20:  94%|█████████▍| 137/146 [00:02<00:00, 57.95it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20:  95%|█████████▍| 138/146 [00:02<00:00, 58.15it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20:  95%|█████████▌| 139/146 [00:02<00:00, 58.35it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20:  96%|█████████▌| 140/146 [00:02<00:00, 58.52it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20:  97%|█████████▋| 141/146 [00:02<00:00, 58.65it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20:  97%|█████████▋| 142/146 [00:02<00:00, 58.82it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20:  98%|█████████▊| 143/146 [00:02<00:00, 58.99it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20:  99%|█████████▊| 144/146 [00:02<00:00, 59.13it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20:  99%|█████████▉| 145/146 [00:02<00:00, 59.30it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20: 100%|██████████| 146/146 [00:02<00:00, 59.35it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]\n",
      "Epoch 20: 100%|██████████| 146/146 [00:02<00:00, 59.20it/s, loss=0.00102, v_num=8, train_loss=0.00063, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 2751: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:  90%|████████▉ | 131/146 [00:02<00:00, 55.72it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  90%|█████████ | 132/146 [00:02<00:00, 55.67it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21:  91%|█████████ | 133/146 [00:02<00:00, 55.83it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21:  92%|█████████▏| 134/146 [00:02<00:00, 55.95it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21:  92%|█████████▏| 135/146 [00:02<00:00, 56.11it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21:  93%|█████████▎| 136/146 [00:02<00:00, 56.22it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21:  94%|█████████▍| 137/146 [00:02<00:00, 56.35it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21:  95%|█████████▍| 138/146 [00:02<00:00, 56.46it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21:  95%|█████████▌| 139/146 [00:02<00:00, 56.57it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21:  96%|█████████▌| 140/146 [00:02<00:00, 56.70it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21:  97%|█████████▋| 141/146 [00:02<00:00, 56.85it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21:  97%|█████████▋| 142/146 [00:02<00:00, 56.98it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21:  98%|█████████▊| 143/146 [00:02<00:00, 57.11it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21:  99%|█████████▊| 144/146 [00:02<00:00, 57.23it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21:  99%|█████████▉| 145/146 [00:02<00:00, 57.31it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21: 100%|██████████| 146/146 [00:02<00:00, 57.32it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]\n",
      "Epoch 21: 100%|██████████| 146/146 [00:02<00:00, 57.23it/s, loss=0.00102, v_num=8, train_loss=0.000627, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 2882: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:  90%|████████▉ | 131/146 [00:02<00:00, 61.30it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  90%|█████████ | 132/146 [00:02<00:00, 61.22it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22:  91%|█████████ | 133/146 [00:02<00:00, 61.34it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22:  92%|█████████▏| 134/146 [00:02<00:00, 61.49it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22:  92%|█████████▏| 135/146 [00:02<00:00, 61.67it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22:  93%|█████████▎| 136/146 [00:02<00:00, 61.84it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22:  94%|█████████▍| 137/146 [00:02<00:00, 62.01it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22:  95%|█████████▍| 138/146 [00:02<00:00, 62.19it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22:  95%|█████████▌| 139/146 [00:02<00:00, 62.36it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22:  96%|█████████▌| 140/146 [00:02<00:00, 62.55it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22:  97%|█████████▋| 141/146 [00:02<00:00, 62.66it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22:  97%|█████████▋| 142/146 [00:02<00:00, 62.83it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22:  98%|█████████▊| 143/146 [00:02<00:00, 62.96it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22:  99%|█████████▊| 144/146 [00:02<00:00, 63.15it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22:  99%|█████████▉| 145/146 [00:02<00:00, 63.29it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22: 100%|██████████| 146/146 [00:02<00:00, 63.34it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]\n",
      "Epoch 22: 100%|██████████| 146/146 [00:02<00:00, 63.23it/s, loss=0.00101, v_num=8, train_loss=0.000625, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 3013: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:  90%|████████▉ | 131/146 [00:02<00:00, 60.31it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  90%|█████████ | 132/146 [00:02<00:00, 60.21it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23:  91%|█████████ | 133/146 [00:02<00:00, 60.42it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23:  92%|█████████▏| 134/146 [00:02<00:00, 60.60it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23:  92%|█████████▏| 135/146 [00:02<00:00, 60.78it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23:  93%|█████████▎| 136/146 [00:02<00:00, 60.95it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23:  94%|█████████▍| 137/146 [00:02<00:00, 61.13it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23:  95%|█████████▍| 138/146 [00:02<00:00, 61.36it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23:  95%|█████████▌| 139/146 [00:02<00:00, 61.50it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23:  96%|█████████▌| 140/146 [00:02<00:00, 61.64it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23:  97%|█████████▋| 141/146 [00:02<00:00, 61.81it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23:  97%|█████████▋| 142/146 [00:02<00:00, 61.98it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23:  98%|█████████▊| 143/146 [00:02<00:00, 62.17it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23:  99%|█████████▊| 144/146 [00:02<00:00, 62.36it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23:  99%|█████████▉| 145/146 [00:02<00:00, 62.52it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23: 100%|██████████| 146/146 [00:02<00:00, 62.58it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]\n",
      "Epoch 23: 100%|██████████| 146/146 [00:02<00:00, 62.44it/s, loss=0.00101, v_num=8, train_loss=0.000622, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 3144: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:  90%|████████▉ | 131/146 [00:02<00:00, 60.17it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  90%|█████████ | 132/146 [00:02<00:00, 60.08it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24:  91%|█████████ | 133/146 [00:02<00:00, 60.29it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24:  92%|█████████▏| 134/146 [00:02<00:00, 60.44it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24:  92%|█████████▏| 135/146 [00:02<00:00, 60.64it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24:  93%|█████████▎| 136/146 [00:02<00:00, 60.82it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24:  94%|█████████▍| 137/146 [00:02<00:00, 60.99it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24:  95%|█████████▍| 138/146 [00:02<00:00, 61.19it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24:  95%|█████████▌| 139/146 [00:02<00:00, 61.31it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24:  96%|█████████▌| 140/146 [00:02<00:00, 61.45it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24:  97%|█████████▋| 141/146 [00:02<00:00, 61.65it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24:  97%|█████████▋| 142/146 [00:02<00:00, 61.79it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24:  98%|█████████▊| 143/146 [00:02<00:00, 61.98it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24:  99%|█████████▊| 144/146 [00:02<00:00, 62.12it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24:  99%|█████████▉| 145/146 [00:02<00:00, 62.28it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24: 100%|██████████| 146/146 [00:02<00:00, 62.34it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 24: 100%|██████████| 146/146 [00:02<00:00, 62.23it/s, loss=0.00101, v_num=8, train_loss=0.00062, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 3275: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:  90%|████████▉ | 131/146 [00:02<00:00, 60.11it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]   \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25:  90%|█████████ | 132/146 [00:02<00:00, 60.02it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25:  91%|█████████ | 133/146 [00:02<00:00, 60.15it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25:  92%|█████████▏| 134/146 [00:02<00:00, 60.30it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25:  92%|█████████▏| 135/146 [00:02<00:00, 60.43it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25:  93%|█████████▎| 136/146 [00:02<00:00, 60.60it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25:  94%|█████████▍| 137/146 [00:02<00:00, 60.72it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25:  95%|█████████▍| 138/146 [00:02<00:00, 60.92it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25:  95%|█████████▌| 139/146 [00:02<00:00, 61.09it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25:  96%|█████████▌| 140/146 [00:02<00:00, 61.24it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25:  97%|█████████▋| 141/146 [00:02<00:00, 61.43it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25:  97%|█████████▋| 142/146 [00:02<00:00, 61.57it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25:  98%|█████████▊| 143/146 [00:02<00:00, 61.77it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25:  99%|█████████▊| 144/146 [00:02<00:00, 61.96it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25:  99%|█████████▉| 145/146 [00:02<00:00, 62.12it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25: 100%|██████████| 146/146 [00:02<00:00, 62.18it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]\n",
      "Epoch 25: 100%|██████████| 146/146 [00:02<00:00, 62.04it/s, loss=0.001, v_num=8, train_loss=0.000619, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 3406: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26:  90%|████████▉ | 131/146 [00:02<00:00, 60.95it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26:  90%|█████████ | 132/146 [00:02<00:00, 60.91it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26:  91%|█████████ | 133/146 [00:02<00:00, 61.14it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26:  92%|█████████▏| 134/146 [00:02<00:00, 61.29it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26:  92%|█████████▏| 135/146 [00:02<00:00, 61.50it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26:  93%|█████████▎| 136/146 [00:02<00:00, 61.62it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26:  94%|█████████▍| 137/146 [00:02<00:00, 61.76it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26:  95%|█████████▍| 138/146 [00:02<00:00, 61.96it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26:  95%|█████████▌| 139/146 [00:02<00:00, 62.13it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26:  96%|█████████▌| 140/146 [00:02<00:00, 62.22it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26:  97%|█████████▋| 141/146 [00:02<00:00, 62.41it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26:  97%|█████████▋| 142/146 [00:02<00:00, 62.55it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26:  98%|█████████▊| 143/146 [00:02<00:00, 62.74it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26:  99%|█████████▊| 144/146 [00:02<00:00, 62.90it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26:  99%|█████████▉| 145/146 [00:02<00:00, 63.07it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26: 100%|██████████| 146/146 [00:02<00:00, 63.14it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 26: 100%|██████████| 146/146 [00:02<00:00, 62.95it/s, loss=0.000999, v_num=8, train_loss=0.000618, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 3537: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27:  90%|████████▉ | 131/146 [00:02<00:00, 59.81it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27:  90%|█████████ | 132/146 [00:02<00:00, 59.64it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27:  91%|█████████ | 133/146 [00:02<00:00, 59.53it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27:  92%|█████████▏| 134/146 [00:02<00:00, 59.60it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27:  92%|█████████▏| 135/146 [00:02<00:00, 59.73it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27:  93%|█████████▎| 136/146 [00:02<00:00, 59.83it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27:  94%|█████████▍| 137/146 [00:02<00:00, 59.93it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27:  95%|█████████▍| 138/146 [00:02<00:00, 60.00it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27:  95%|█████████▌| 139/146 [00:02<00:00, 60.19it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27:  96%|█████████▌| 140/146 [00:02<00:00, 60.21it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27:  97%|█████████▋| 141/146 [00:02<00:00, 60.28it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27:  97%|█████████▋| 142/146 [00:02<00:00, 60.37it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27:  98%|█████████▊| 143/146 [00:02<00:00, 60.51it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27:  99%|█████████▊| 144/146 [00:02<00:00, 60.65it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27:  99%|█████████▉| 145/146 [00:02<00:00, 60.74it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27: 100%|██████████| 146/146 [00:02<00:00, 60.75it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]\n",
      "Epoch 27: 100%|██████████| 146/146 [00:02<00:00, 60.58it/s, loss=0.000995, v_num=8, train_loss=0.000618, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 3668: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28:  90%|████████▉ | 131/146 [00:02<00:00, 55.81it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28:  90%|█████████ | 132/146 [00:02<00:00, 55.69it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28:  91%|█████████ | 133/146 [00:02<00:00, 55.90it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28:  92%|█████████▏| 134/146 [00:02<00:00, 56.06it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28:  92%|█████████▏| 135/146 [00:02<00:00, 56.29it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28:  93%|█████████▎| 136/146 [00:02<00:00, 56.45it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28:  94%|█████████▍| 137/146 [00:02<00:00, 56.58it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28:  95%|█████████▍| 138/146 [00:02<00:00, 56.79it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28:  95%|█████████▌| 139/146 [00:02<00:00, 56.96it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28:  96%|█████████▌| 140/146 [00:02<00:00, 57.16it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28:  97%|█████████▋| 141/146 [00:02<00:00, 57.34it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28:  97%|█████████▋| 142/146 [00:02<00:00, 57.49it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28:  98%|█████████▊| 143/146 [00:02<00:00, 57.66it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28:  99%|█████████▊| 144/146 [00:02<00:00, 57.83it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28:  99%|█████████▉| 145/146 [00:02<00:00, 58.02it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28: 100%|██████████| 146/146 [00:02<00:00, 58.12it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]\n",
      "Epoch 28: 100%|██████████| 146/146 [00:02<00:00, 57.98it/s, loss=0.000992, v_num=8, train_loss=0.00062, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 3799: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29:  90%|████████▉ | 131/146 [00:02<00:00, 60.36it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29:  90%|█████████ | 132/146 [00:02<00:00, 60.24it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29:  91%|█████████ | 133/146 [00:02<00:00, 60.37it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29:  92%|█████████▏| 134/146 [00:02<00:00, 60.55it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29:  92%|█████████▏| 135/146 [00:02<00:00, 60.75it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29:  93%|█████████▎| 136/146 [00:02<00:00, 60.95it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29:  94%|█████████▍| 137/146 [00:02<00:00, 61.13it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29:  95%|█████████▍| 138/146 [00:02<00:00, 61.33it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29:  95%|█████████▌| 139/146 [00:02<00:00, 61.47it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29:  96%|█████████▌| 140/146 [00:02<00:00, 61.67it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29:  97%|█████████▋| 141/146 [00:02<00:00, 61.81it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29:  97%|█████████▋| 142/146 [00:02<00:00, 61.98it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29:  98%|█████████▊| 143/146 [00:02<00:00, 62.12it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29:  99%|█████████▊| 144/146 [00:02<00:00, 62.33it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29:  99%|█████████▉| 145/146 [00:02<00:00, 62.47it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29: 100%|██████████| 146/146 [00:02<00:00, 62.47it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]\n",
      "Epoch 29: 100%|██████████| 146/146 [00:02<00:00, 62.31it/s, loss=0.000989, v_num=8, train_loss=0.000626, val_loss=0.0161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 3930: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:  90%|████████▉ | 131/146 [00:02<00:00, 60.93it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30:  90%|█████████ | 132/146 [00:02<00:00, 60.88it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30:  91%|█████████ | 133/146 [00:02<00:00, 61.03it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30:  92%|█████████▏| 134/146 [00:02<00:00, 61.15it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30:  92%|█████████▏| 135/146 [00:02<00:00, 61.36it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30:  93%|█████████▎| 136/146 [00:02<00:00, 61.53it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30:  94%|█████████▍| 137/146 [00:02<00:00, 61.65it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30:  95%|█████████▍| 138/146 [00:02<00:00, 61.82it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30:  95%|█████████▌| 139/146 [00:02<00:00, 61.99it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30:  96%|█████████▌| 140/146 [00:02<00:00, 62.19it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30:  97%|█████████▋| 141/146 [00:02<00:00, 62.33it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30:  97%|█████████▋| 142/146 [00:02<00:00, 62.52it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30:  98%|█████████▊| 143/146 [00:02<00:00, 62.66it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30:  99%|█████████▊| 144/146 [00:02<00:00, 62.85it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30:  99%|█████████▉| 145/146 [00:02<00:00, 63.01it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.0161]\n",
      "Epoch 30: 100%|██████████| 146/146 [00:02<00:00, 63.04it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.016] \n",
      "Epoch 30: 100%|██████████| 146/146 [00:02<00:00, 62.87it/s, loss=0.000988, v_num=8, train_loss=0.000629, val_loss=0.016]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30, global step 4061: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31:  90%|████████▉ | 131/146 [00:02<00:00, 59.90it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31:  90%|█████████ | 132/146 [00:02<00:00, 59.83it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31:  91%|█████████ | 133/146 [00:02<00:00, 60.04it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31:  92%|█████████▏| 134/146 [00:02<00:00, 60.25it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31:  92%|█████████▏| 135/146 [00:02<00:00, 60.40it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31:  93%|█████████▎| 136/146 [00:02<00:00, 60.60it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31:  94%|█████████▍| 137/146 [00:02<00:00, 60.78it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31:  95%|█████████▍| 138/146 [00:02<00:00, 60.92it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31:  95%|█████████▌| 139/146 [00:02<00:00, 61.12it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31:  96%|█████████▌| 140/146 [00:02<00:00, 61.24it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31:  97%|█████████▋| 141/146 [00:02<00:00, 61.41it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31:  97%|█████████▋| 142/146 [00:02<00:00, 61.57it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31:  98%|█████████▊| 143/146 [00:02<00:00, 61.74it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31:  99%|█████████▊| 144/146 [00:02<00:00, 61.93it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31:  99%|█████████▉| 145/146 [00:02<00:00, 62.09it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31: 100%|██████████| 146/146 [00:02<00:00, 62.12it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]\n",
      "Epoch 31: 100%|██████████| 146/146 [00:02<00:00, 61.99it/s, loss=0.000983, v_num=8, train_loss=0.000623, val_loss=0.016]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31, global step 4192: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32:  90%|████████▉ | 131/146 [00:02<00:00, 60.95it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32:  90%|█████████ | 132/146 [00:02<00:00, 60.85it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32:  91%|█████████ | 133/146 [00:02<00:00, 61.00it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32:  92%|█████████▏| 134/146 [00:02<00:00, 61.21it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32:  92%|█████████▏| 135/146 [00:02<00:00, 61.33it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32:  93%|█████████▎| 136/146 [00:02<00:00, 61.51it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32:  94%|█████████▍| 137/146 [00:02<00:00, 61.68it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32:  95%|█████████▍| 138/146 [00:02<00:00, 61.85it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32:  95%|█████████▌| 139/146 [00:02<00:00, 62.02it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32:  96%|█████████▌| 140/146 [00:02<00:00, 62.22it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32:  97%|█████████▋| 141/146 [00:02<00:00, 62.41it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32:  97%|█████████▋| 142/146 [00:02<00:00, 62.58it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32:  98%|█████████▊| 143/146 [00:02<00:00, 62.77it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32:  99%|█████████▊| 144/146 [00:02<00:00, 62.93it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32:  99%|█████████▉| 145/146 [00:02<00:00, 63.04it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32: 100%|██████████| 146/146 [00:02<00:00, 63.12it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]\n",
      "Epoch 32: 100%|██████████| 146/146 [00:02<00:00, 63.01it/s, loss=0.000979, v_num=8, train_loss=0.000628, val_loss=0.016]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32, global step 4323: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33:  90%|████████▉ | 131/146 [00:02<00:00, 61.29it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33:  90%|█████████ | 132/146 [00:02<00:00, 61.21it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33:  91%|█████████ | 133/146 [00:02<00:00, 61.36it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33:  92%|█████████▏| 134/146 [00:02<00:00, 61.45it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33:  92%|█████████▏| 135/146 [00:02<00:00, 61.60it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33:  93%|█████████▎| 136/146 [00:02<00:00, 61.75it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33:  94%|█████████▍| 137/146 [00:02<00:00, 61.92it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33:  95%|█████████▍| 138/146 [00:02<00:00, 62.12it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33:  95%|█████████▌| 139/146 [00:02<00:00, 62.21it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33:  96%|█████████▌| 140/146 [00:02<00:00, 62.35it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33:  97%|█████████▋| 141/146 [00:02<00:00, 62.49it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33:  97%|█████████▋| 142/146 [00:02<00:00, 62.60it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33:  98%|█████████▊| 143/146 [00:02<00:00, 62.79it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33:  99%|█████████▊| 144/146 [00:02<00:00, 62.95it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33:  99%|█████████▉| 145/146 [00:02<00:00, 63.08it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.016]\n",
      "Epoch 33: 100%|██████████| 146/146 [00:02<00:00, 63.14it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.0159]\n",
      "Epoch 33: 100%|██████████| 146/146 [00:02<00:00, 63.00it/s, loss=0.000978, v_num=8, train_loss=0.000638, val_loss=0.0159]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33, global step 4454: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34:  90%|████████▉ | 131/146 [00:02<00:00, 58.35it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34:  90%|█████████ | 132/146 [00:02<00:00, 58.17it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34:  91%|█████████ | 133/146 [00:02<00:00, 58.28it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34:  92%|█████████▏| 134/146 [00:02<00:00, 58.39it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34:  92%|█████████▏| 135/146 [00:02<00:00, 58.49it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34:  93%|█████████▎| 136/146 [00:02<00:00, 58.62it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34:  94%|█████████▍| 137/146 [00:02<00:00, 58.72it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34:  95%|█████████▍| 138/146 [00:02<00:00, 58.85it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34:  95%|█████████▌| 139/146 [00:02<00:00, 58.97it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34:  96%|█████████▌| 140/146 [00:02<00:00, 59.05it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34:  97%|█████████▋| 141/146 [00:02<00:00, 59.14it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34:  97%|█████████▋| 142/146 [00:02<00:00, 59.26it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34:  98%|█████████▊| 143/146 [00:02<00:00, 59.38it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34:  99%|█████████▊| 144/146 [00:02<00:00, 59.55it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34:  99%|█████████▉| 145/146 [00:02<00:00, 59.69it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34: 100%|██████████| 146/146 [00:02<00:00, 59.69it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]\n",
      "Epoch 34: 100%|██████████| 146/146 [00:02<00:00, 59.57it/s, loss=0.000976, v_num=8, train_loss=0.000636, val_loss=0.0159]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34, global step 4585: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:  90%|████████▉ | 131/146 [00:02<00:00, 57.50it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 35:  90%|█████████ | 132/146 [00:02<00:00, 57.44it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35:  91%|█████████ | 133/146 [00:02<00:00, 57.65it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35:  92%|█████████▏| 134/146 [00:02<00:00, 57.78it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35:  92%|█████████▏| 135/146 [00:02<00:00, 57.94it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35:  93%|█████████▎| 136/146 [00:02<00:00, 58.14it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35:  94%|█████████▍| 137/146 [00:02<00:00, 58.34it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35:  95%|█████████▍| 138/146 [00:02<00:00, 58.54it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35:  95%|█████████▌| 139/146 [00:02<00:00, 58.72it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35:  96%|█████████▌| 140/146 [00:02<00:00, 58.89it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35:  97%|█████████▋| 141/146 [00:02<00:00, 59.04it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35:  97%|█████████▋| 142/146 [00:02<00:00, 59.19it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35:  98%|█████████▊| 143/146 [00:02<00:00, 59.33it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35:  99%|█████████▊| 144/146 [00:02<00:00, 59.52it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35:  99%|█████████▉| 145/146 [00:02<00:00, 59.67it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.0159]\n",
      "Epoch 35: 100%|██████████| 146/146 [00:02<00:00, 59.78it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.016] \n",
      "Epoch 35: 100%|██████████| 146/146 [00:02<00:00, 59.68it/s, loss=0.000969, v_num=8, train_loss=0.000634, val_loss=0.016]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35, global step 4716: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36:  90%|████████▉ | 131/146 [00:02<00:00, 62.48it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36:  90%|█████████ | 132/146 [00:02<00:00, 62.42it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36:  91%|█████████ | 133/146 [00:02<00:00, 62.60it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36:  92%|█████████▏| 134/146 [00:02<00:00, 62.77it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36:  92%|█████████▏| 135/146 [00:02<00:00, 62.71it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36:  93%|█████████▎| 136/146 [00:02<00:00, 62.86it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36:  94%|█████████▍| 137/146 [00:02<00:00, 63.03it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36:  95%|█████████▍| 138/146 [00:02<00:00, 63.23it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36:  95%|█████████▌| 139/146 [00:02<00:00, 63.39it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36:  96%|█████████▌| 140/146 [00:02<00:00, 63.59it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36:  97%|█████████▋| 141/146 [00:02<00:00, 63.70it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36:  97%|█████████▋| 142/146 [00:02<00:00, 63.86it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36:  98%|█████████▊| 143/146 [00:02<00:00, 64.02it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36:  99%|█████████▊| 144/146 [00:02<00:00, 64.15it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36:  99%|█████████▉| 145/146 [00:02<00:00, 64.31it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.016]\n",
      "Epoch 36: 100%|██████████| 146/146 [00:02<00:00, 64.33it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.0159]\n",
      "Epoch 36: 100%|██████████| 146/146 [00:02<00:00, 64.19it/s, loss=0.000967, v_num=8, train_loss=0.000643, val_loss=0.0159]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36, global step 4847: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37:  90%|████████▉ | 131/146 [00:02<00:00, 61.15it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 37:  90%|█████████ | 132/146 [00:02<00:00, 61.13it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37:  91%|█████████ | 133/146 [00:02<00:00, 61.31it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37:  92%|█████████▏| 134/146 [00:02<00:00, 61.46it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37:  92%|█████████▏| 135/146 [00:02<00:00, 61.58it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37:  93%|█████████▎| 136/146 [00:02<00:00, 61.70it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37:  94%|█████████▍| 137/146 [00:02<00:00, 61.85it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37:  95%|█████████▍| 138/146 [00:02<00:00, 62.05it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37:  95%|█████████▌| 139/146 [00:02<00:00, 62.22it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37:  96%|█████████▌| 140/146 [00:02<00:00, 62.38it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37:  97%|█████████▋| 141/146 [00:02<00:00, 62.58it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37:  97%|█████████▋| 142/146 [00:02<00:00, 62.74it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37:  98%|█████████▊| 143/146 [00:02<00:00, 62.91it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37:  99%|█████████▊| 144/146 [00:02<00:00, 63.07it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37:  99%|█████████▉| 145/146 [00:02<00:00, 63.26it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37: 100%|██████████| 146/146 [00:02<00:00, 63.28it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]\n",
      "Epoch 37: 100%|██████████| 146/146 [00:02<00:00, 63.12it/s, loss=0.000963, v_num=8, train_loss=0.000656, val_loss=0.0159]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37, global step 4978: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38:  90%|████████▉ | 131/146 [00:02<00:00, 60.34it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 38:  90%|█████████ | 132/146 [00:02<00:00, 60.27it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38:  91%|█████████ | 133/146 [00:02<00:00, 60.45it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38:  92%|█████████▏| 134/146 [00:02<00:00, 60.57it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38:  92%|█████████▏| 135/146 [00:02<00:00, 60.75it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38:  93%|█████████▎| 136/146 [00:02<00:00, 60.87it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38:  94%|█████████▍| 137/146 [00:02<00:00, 61.07it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38:  95%|█████████▍| 138/146 [00:02<00:00, 61.19it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38:  95%|█████████▌| 139/146 [00:02<00:00, 61.34it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38:  96%|█████████▌| 140/146 [00:02<00:00, 61.51it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38:  97%|█████████▋| 141/146 [00:02<00:00, 61.68it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38:  97%|█████████▋| 142/146 [00:02<00:00, 61.82it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38:  98%|█████████▊| 143/146 [00:02<00:00, 61.98it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38:  99%|█████████▊| 144/146 [00:02<00:00, 62.14it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38:  99%|█████████▉| 145/146 [00:02<00:00, 62.33it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0159]\n",
      "Epoch 38: 100%|██████████| 146/146 [00:02<00:00, 62.28it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0158]\n",
      "Epoch 38: 100%|██████████| 146/146 [00:02<00:00, 62.15it/s, loss=0.000961, v_num=8, train_loss=0.000667, val_loss=0.0158]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38, global step 5109: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39:  90%|████████▉ | 131/146 [00:02<00:00, 61.73it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39:  90%|█████████ | 132/146 [00:02<00:00, 61.62it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39:  91%|█████████ | 133/146 [00:02<00:00, 61.83it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39:  92%|█████████▏| 134/146 [00:02<00:00, 62.00it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39:  92%|█████████▏| 135/146 [00:02<00:00, 62.24it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39:  93%|█████████▎| 136/146 [00:02<00:00, 62.35it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39:  94%|█████████▍| 137/146 [00:02<00:00, 62.55it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39:  95%|█████████▍| 138/146 [00:02<00:00, 62.72it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39:  95%|█████████▌| 139/146 [00:02<00:00, 62.89it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39:  96%|█████████▌| 140/146 [00:02<00:00, 63.06it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39:  97%|█████████▋| 141/146 [00:02<00:00, 63.22it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39:  97%|█████████▋| 142/146 [00:02<00:00, 63.39it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39:  98%|█████████▊| 143/146 [00:02<00:00, 63.52it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39:  99%|█████████▊| 144/146 [00:02<00:00, 63.68it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39:  99%|█████████▉| 145/146 [00:02<00:00, 63.87it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39: 100%|██████████| 146/146 [00:02<00:00, 63.95it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 39: 100%|██████████| 146/146 [00:02<00:00, 63.67it/s, loss=0.000957, v_num=8, train_loss=0.000684, val_loss=0.0158]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39, global step 5240: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40:  90%|████████▉ | 131/146 [00:02<00:00, 59.24it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 40:  90%|█████████ | 132/146 [00:02<00:00, 59.16it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40:  91%|█████████ | 133/146 [00:02<00:00, 59.37it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40:  92%|█████████▏| 134/146 [00:02<00:00, 59.50it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40:  92%|█████████▏| 135/146 [00:02<00:00, 59.68it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40:  93%|█████████▎| 136/146 [00:02<00:00, 59.85it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40:  94%|█████████▍| 137/146 [00:02<00:00, 60.03it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40:  95%|█████████▍| 138/146 [00:02<00:00, 60.23it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40:  95%|█████████▌| 139/146 [00:02<00:00, 60.38it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40:  96%|█████████▌| 140/146 [00:02<00:00, 60.58it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40:  97%|█████████▋| 141/146 [00:02<00:00, 60.72it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40:  97%|█████████▋| 142/146 [00:02<00:00, 60.94it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40:  98%|█████████▊| 143/146 [00:02<00:00, 61.13it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40:  99%|█████████▊| 144/146 [00:02<00:00, 61.25it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40:  99%|█████████▉| 145/146 [00:02<00:00, 61.36it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0158]\n",
      "Epoch 40: 100%|██████████| 146/146 [00:02<00:00, 61.42it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0157]\n",
      "Epoch 40: 100%|██████████| 146/146 [00:02<00:00, 61.31it/s, loss=0.000955, v_num=8, train_loss=0.000697, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40, global step 5371: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41:  90%|████████▉ | 131/146 [00:02<00:00, 56.85it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 41:  90%|█████████ | 132/146 [00:02<00:00, 56.72it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41:  91%|█████████ | 133/146 [00:02<00:00, 56.83it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41:  92%|█████████▏| 134/146 [00:02<00:00, 56.94it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41:  92%|█████████▏| 135/146 [00:02<00:00, 57.08it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41:  93%|█████████▎| 136/146 [00:02<00:00, 57.26it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41:  94%|█████████▍| 137/146 [00:02<00:00, 57.41it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41:  95%|█████████▍| 138/146 [00:02<00:00, 57.54it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41:  95%|█████████▌| 139/146 [00:02<00:00, 57.67it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41:  96%|█████████▌| 140/146 [00:02<00:00, 57.78it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41:  97%|█████████▋| 141/146 [00:02<00:00, 57.90it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41:  97%|█████████▋| 142/146 [00:02<00:00, 57.96it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41:  98%|█████████▊| 143/146 [00:02<00:00, 58.06it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41:  99%|█████████▊| 144/146 [00:02<00:00, 58.18it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41:  99%|█████████▉| 145/146 [00:02<00:00, 58.30it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41: 100%|██████████| 146/146 [00:02<00:00, 58.28it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]\n",
      "Epoch 41: 100%|██████████| 146/146 [00:02<00:00, 58.14it/s, loss=0.000951, v_num=8, train_loss=0.000709, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41, global step 5502: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42:  90%|████████▉ | 131/146 [00:02<00:00, 59.79it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 42:  90%|█████████ | 132/146 [00:02<00:00, 59.72it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42:  91%|█████████ | 133/146 [00:02<00:00, 59.82it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42:  92%|█████████▏| 134/146 [00:02<00:00, 60.00it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42:  92%|█████████▏| 135/146 [00:02<00:00, 60.16it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42:  93%|█████████▎| 136/146 [00:02<00:00, 60.33it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42:  94%|█████████▍| 137/146 [00:02<00:00, 60.51it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42:  95%|█████████▍| 138/146 [00:02<00:00, 60.66it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42:  95%|█████████▌| 139/146 [00:02<00:00, 60.61it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42:  96%|█████████▌| 140/146 [00:02<00:00, 60.81it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42:  97%|█████████▋| 141/146 [00:02<00:00, 60.98it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42:  97%|█████████▋| 142/146 [00:02<00:00, 61.12it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42:  98%|█████████▊| 143/146 [00:02<00:00, 61.29it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42:  99%|█████████▊| 144/146 [00:02<00:00, 61.43it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42:  99%|█████████▉| 145/146 [00:02<00:00, 61.59it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0157]\n",
      "Epoch 42: 100%|██████████| 146/146 [00:02<00:00, 61.62it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0156]\n",
      "Epoch 42: 100%|██████████| 146/146 [00:02<00:00, 61.52it/s, loss=0.000948, v_num=8, train_loss=0.000724, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42, global step 5633: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43:  90%|████████▉ | 131/146 [00:02<00:00, 60.28it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 43:  90%|█████████ | 132/146 [00:02<00:00, 60.24it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43:  91%|█████████ | 133/146 [00:02<00:00, 60.42it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43:  92%|█████████▏| 134/146 [00:02<00:00, 60.60it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43:  92%|█████████▏| 135/146 [00:02<00:00, 60.81it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43:  93%|█████████▎| 136/146 [00:02<00:00, 60.95it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43:  94%|█████████▍| 137/146 [00:02<00:00, 61.16it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43:  95%|█████████▍| 138/146 [00:02<00:00, 61.36it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43:  95%|█████████▌| 139/146 [00:02<00:00, 61.55it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43:  96%|█████████▌| 140/146 [00:02<00:00, 61.75it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43:  97%|█████████▋| 141/146 [00:02<00:00, 61.89it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43:  97%|█████████▋| 142/146 [00:02<00:00, 62.06it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43:  98%|█████████▊| 143/146 [00:02<00:00, 62.20it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43:  99%|█████████▊| 144/146 [00:02<00:00, 62.39it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43:  99%|█████████▉| 145/146 [00:02<00:00, 62.52it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43: 100%|██████████| 146/146 [00:02<00:00, 62.52it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]\n",
      "Epoch 43: 100%|██████████| 146/146 [00:02<00:00, 62.39it/s, loss=0.000945, v_num=8, train_loss=0.000735, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43, global step 5764: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44:  90%|████████▉ | 131/146 [00:02<00:00, 61.41it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 44:  90%|█████████ | 132/146 [00:02<00:00, 61.33it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44:  91%|█████████ | 133/146 [00:02<00:00, 61.51it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44:  92%|█████████▏| 134/146 [00:02<00:00, 61.63it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44:  92%|█████████▏| 135/146 [00:02<00:00, 61.81it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44:  93%|█████████▎| 136/146 [00:02<00:00, 61.95it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44:  94%|█████████▍| 137/146 [00:02<00:00, 62.16it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44:  95%|█████████▍| 138/146 [00:02<00:00, 62.33it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44:  95%|█████████▌| 139/146 [00:02<00:00, 62.50it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44:  96%|█████████▌| 140/146 [00:02<00:00, 62.69it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44:  97%|█████████▋| 141/146 [00:02<00:00, 62.83it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44:  97%|█████████▋| 142/146 [00:02<00:00, 62.99it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44:  98%|█████████▊| 143/146 [00:02<00:00, 63.10it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44:  99%|█████████▊| 144/146 [00:02<00:00, 63.26it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44:  99%|█████████▉| 145/146 [00:02<00:00, 63.40it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44: 100%|██████████| 146/146 [00:02<00:00, 63.45it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]\n",
      "Epoch 44: 100%|██████████| 146/146 [00:02<00:00, 63.31it/s, loss=0.000941, v_num=8, train_loss=0.000742, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44, global step 5895: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45:  90%|████████▉ | 131/146 [00:02<00:00, 61.12it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45:  90%|█████████ | 132/146 [00:02<00:00, 61.02it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45:  91%|█████████ | 133/146 [00:02<00:00, 61.20it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45:  92%|█████████▏| 134/146 [00:02<00:00, 61.38it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45:  92%|█████████▏| 135/146 [00:02<00:00, 61.58it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45:  93%|█████████▎| 136/146 [00:02<00:00, 61.79it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45:  94%|█████████▍| 137/146 [00:02<00:00, 61.99it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45:  95%|█████████▍| 138/146 [00:02<00:00, 62.21it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45:  95%|█████████▌| 139/146 [00:02<00:00, 62.36it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45:  96%|█████████▌| 140/146 [00:02<00:00, 62.47it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45:  97%|█████████▋| 141/146 [00:02<00:00, 62.61it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45:  97%|█████████▋| 142/146 [00:02<00:00, 62.80it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45:  98%|█████████▊| 143/146 [00:02<00:00, 62.99it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45:  99%|█████████▊| 144/146 [00:02<00:00, 63.15it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45:  99%|█████████▉| 145/146 [00:02<00:00, 63.34it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45: 100%|██████████| 146/146 [00:02<00:00, 63.36it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 45: 100%|██████████| 146/146 [00:02<00:00, 63.23it/s, loss=0.000938, v_num=8, train_loss=0.000754, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45, global step 6026: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46:  90%|████████▉ | 131/146 [00:02<00:00, 61.64it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46:  90%|█████████ | 132/146 [00:02<00:00, 61.53it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46:  91%|█████████ | 133/146 [00:02<00:00, 61.74it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46:  92%|█████████▏| 134/146 [00:02<00:00, 61.95it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46:  92%|█████████▏| 135/146 [00:02<00:00, 62.06it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46:  93%|█████████▎| 136/146 [00:02<00:00, 62.15it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46:  94%|█████████▍| 137/146 [00:02<00:00, 62.30it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46:  95%|█████████▍| 138/146 [00:02<00:00, 62.47it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46:  95%|█████████▌| 139/146 [00:02<00:00, 62.58it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46:  96%|█████████▌| 140/146 [00:02<00:00, 62.72it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46:  97%|█████████▋| 141/146 [00:02<00:00, 62.91it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46:  97%|█████████▋| 142/146 [00:02<00:00, 63.08it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46:  98%|█████████▊| 143/146 [00:02<00:00, 63.27it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46:  99%|█████████▊| 144/146 [00:02<00:00, 63.40it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46:  99%|█████████▉| 145/146 [00:02<00:00, 63.59it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46: 100%|██████████| 146/146 [00:02<00:00, 63.61it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]\n",
      "Epoch 46: 100%|██████████| 146/146 [00:02<00:00, 63.47it/s, loss=0.000936, v_num=8, train_loss=0.000768, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46, global step 6157: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47:  90%|████████▉ | 131/146 [00:02<00:00, 60.67it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 47:  90%|█████████ | 132/146 [00:02<00:00, 60.55it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47:  91%|█████████ | 133/146 [00:02<00:00, 60.78it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47:  92%|█████████▏| 134/146 [00:02<00:00, 60.96it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47:  92%|█████████▏| 135/146 [00:02<00:00, 61.14it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47:  93%|█████████▎| 136/146 [00:02<00:00, 61.28it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47:  94%|█████████▍| 137/146 [00:02<00:00, 61.49it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47:  95%|█████████▍| 138/146 [00:02<00:00, 61.66it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47:  95%|█████████▌| 139/146 [00:02<00:00, 61.83it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47:  96%|█████████▌| 140/146 [00:02<00:00, 61.97it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47:  97%|█████████▋| 141/146 [00:02<00:00, 62.14it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47:  97%|█████████▋| 142/146 [00:02<00:00, 62.25it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47:  98%|█████████▊| 143/146 [00:02<00:00, 62.44it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47:  99%|█████████▊| 144/146 [00:02<00:00, 62.63it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47:  99%|█████████▉| 145/146 [00:02<00:00, 62.77it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47: 100%|██████████| 146/146 [00:02<00:00, 62.85it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]\n",
      "Epoch 47: 100%|██████████| 146/146 [00:02<00:00, 62.74it/s, loss=0.000934, v_num=8, train_loss=0.00078, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47, global step 6288: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48:  90%|████████▉ | 131/146 [00:02<00:00, 54.81it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 48:  90%|█████████ | 132/146 [00:02<00:00, 54.65it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48:  91%|█████████ | 133/146 [00:02<00:00, 54.75it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48:  92%|█████████▏| 134/146 [00:02<00:00, 54.89it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48:  92%|█████████▏| 135/146 [00:02<00:00, 55.01it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48:  93%|█████████▎| 136/146 [00:02<00:00, 55.12it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48:  94%|█████████▍| 137/146 [00:02<00:00, 55.22it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48:  95%|█████████▍| 138/146 [00:02<00:00, 55.33it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48:  95%|█████████▌| 139/146 [00:02<00:00, 55.37it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48:  96%|█████████▌| 140/146 [00:02<00:00, 55.53it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48:  97%|█████████▋| 141/146 [00:02<00:00, 55.64it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48:  97%|█████████▋| 142/146 [00:02<00:00, 55.79it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48:  98%|█████████▊| 143/146 [00:02<00:00, 55.75it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48:  99%|█████████▊| 144/146 [00:02<00:00, 55.87it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48:  99%|█████████▉| 145/146 [00:02<00:00, 55.98it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0156]\n",
      "Epoch 48: 100%|██████████| 146/146 [00:02<00:00, 56.00it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0155]\n",
      "Epoch 48: 100%|██████████| 146/146 [00:02<00:00, 55.91it/s, loss=0.000932, v_num=8, train_loss=0.000787, val_loss=0.0155]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48, global step 6419: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49:  90%|████████▉ | 131/146 [00:02<00:00, 59.20it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49:  90%|█████████ | 132/146 [00:02<00:00, 59.12it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49:  91%|█████████ | 133/146 [00:02<00:00, 59.27it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49:  92%|█████████▏| 134/146 [00:02<00:00, 59.45it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49:  92%|█████████▏| 135/146 [00:02<00:00, 59.58it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49:  93%|█████████▎| 136/146 [00:02<00:00, 59.76it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49:  94%|█████████▍| 137/146 [00:02<00:00, 59.91it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49:  95%|█████████▍| 138/146 [00:02<00:00, 60.08it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49:  95%|█████████▌| 139/146 [00:02<00:00, 60.25it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49:  96%|█████████▌| 140/146 [00:02<00:00, 60.37it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49:  97%|█████████▋| 141/146 [00:02<00:00, 60.49it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49:  97%|█████████▋| 142/146 [00:02<00:00, 60.66it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49:  98%|█████████▊| 143/146 [00:02<00:00, 60.83it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49:  99%|█████████▊| 144/146 [00:02<00:00, 61.02it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49:  99%|█████████▉| 145/146 [00:02<00:00, 61.18it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0155]\n",
      "Epoch 49: 100%|██████████| 146/146 [00:02<00:00, 61.24it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 49: 100%|██████████| 146/146 [00:02<00:00, 61.09it/s, loss=0.00093, v_num=8, train_loss=0.000789, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49, global step 6550: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50:  90%|████████▉ | 131/146 [00:02<00:00, 61.21it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 50:  90%|█████████ | 132/146 [00:02<00:00, 61.16it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50:  91%|█████████ | 133/146 [00:02<00:00, 61.34it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50:  92%|█████████▏| 134/146 [00:02<00:00, 61.52it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50:  92%|█████████▏| 135/146 [00:02<00:00, 61.67it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50:  93%|█████████▎| 136/146 [00:02<00:00, 61.90it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50:  94%|█████████▍| 137/146 [00:02<00:00, 62.07it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50:  95%|█████████▍| 138/146 [00:02<00:00, 62.27it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50:  95%|█████████▌| 139/146 [00:02<00:00, 62.41it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50:  96%|█████████▌| 140/146 [00:02<00:00, 62.58it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50:  97%|█████████▋| 141/146 [00:02<00:00, 62.72it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50:  97%|█████████▋| 142/146 [00:02<00:00, 62.91it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50:  98%|█████████▊| 143/146 [00:02<00:00, 63.05it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50:  99%|█████████▊| 144/146 [00:02<00:00, 63.24it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50:  99%|█████████▉| 145/146 [00:02<00:00, 63.40it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50: 100%|██████████| 146/146 [00:02<00:00, 63.47it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]\n",
      "Epoch 50: 100%|██████████| 146/146 [00:02<00:00, 63.36it/s, loss=0.000926, v_num=8, train_loss=0.000778, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50, global step 6681: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51:  90%|████████▉ | 131/146 [00:02<00:00, 60.64it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 51:  90%|█████████ | 132/146 [00:02<00:00, 60.49it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51:  91%|█████████ | 133/146 [00:02<00:00, 60.70it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51:  92%|█████████▏| 134/146 [00:02<00:00, 60.88it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51:  92%|█████████▏| 135/146 [00:02<00:00, 61.08it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51:  93%|█████████▎| 136/146 [00:02<00:00, 61.20it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51:  94%|█████████▍| 137/146 [00:02<00:00, 61.40it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51:  95%|█████████▍| 138/146 [00:02<00:00, 61.60it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51:  95%|█████████▌| 139/146 [00:02<00:00, 61.77it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51:  96%|█████████▌| 140/146 [00:02<00:00, 61.94it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51:  97%|█████████▋| 141/146 [00:02<00:00, 62.08it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51:  97%|█████████▋| 142/146 [00:02<00:00, 62.25it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51:  98%|█████████▊| 143/146 [00:02<00:00, 62.39it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51:  99%|█████████▊| 144/146 [00:02<00:00, 62.52it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51:  99%|█████████▉| 145/146 [00:02<00:00, 62.66it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51: 100%|██████████| 146/146 [00:02<00:00, 62.66it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 51: 100%|██████████| 146/146 [00:02<00:00, 62.55it/s, loss=0.000921, v_num=8, train_loss=0.000784, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51, global step 6812: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52:  90%|████████▉ | 131/146 [00:02<00:00, 60.34it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 52:  90%|█████████ | 132/146 [00:02<00:00, 60.27it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52:  91%|█████████ | 133/146 [00:02<00:00, 60.45it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52:  92%|█████████▏| 134/146 [00:02<00:00, 60.60it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52:  92%|█████████▏| 135/146 [00:02<00:00, 60.72it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52:  93%|█████████▎| 136/146 [00:02<00:00, 60.87it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52:  94%|█████████▍| 137/146 [00:02<00:00, 60.99it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52:  95%|█████████▍| 138/146 [00:02<00:00, 61.06it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52:  95%|█████████▌| 139/146 [00:02<00:00, 61.20it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52:  96%|█████████▌| 140/146 [00:02<00:00, 61.35it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52:  97%|█████████▋| 141/146 [00:02<00:00, 61.51it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52:  97%|█████████▋| 142/146 [00:02<00:00, 61.65it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52:  98%|█████████▊| 143/146 [00:02<00:00, 61.79it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52:  99%|█████████▊| 144/146 [00:02<00:00, 61.98it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52:  99%|█████████▉| 145/146 [00:02<00:00, 62.15it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52: 100%|██████████| 146/146 [00:02<00:00, 62.18it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 52: 100%|██████████| 146/146 [00:02<00:00, 62.10it/s, loss=0.000919, v_num=8, train_loss=0.000784, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52, global step 6943: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53:  90%|████████▉ | 131/146 [00:02<00:00, 60.70it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 53:  90%|█████████ | 132/146 [00:02<00:00, 60.63it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53:  91%|█████████ | 133/146 [00:02<00:00, 60.81it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53:  92%|█████████▏| 134/146 [00:02<00:00, 61.04it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53:  92%|█████████▏| 135/146 [00:02<00:00, 61.19it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53:  93%|█████████▎| 136/146 [00:02<00:00, 61.40it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53:  94%|█████████▍| 137/146 [00:02<00:00, 61.57it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53:  95%|█████████▍| 138/146 [00:02<00:00, 61.69it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53:  95%|█████████▌| 139/146 [00:02<00:00, 61.91it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53:  96%|█████████▌| 140/146 [00:02<00:00, 62.05it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53:  97%|█████████▋| 141/146 [00:02<00:00, 62.27it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53:  97%|█████████▋| 142/146 [00:02<00:00, 62.36it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53:  98%|█████████▊| 143/146 [00:02<00:00, 62.55it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53:  99%|█████████▊| 144/146 [00:02<00:00, 62.74it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53:  99%|█████████▉| 145/146 [00:02<00:00, 62.87it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53: 100%|██████████| 146/146 [00:02<00:00, 62.90it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]\n",
      "Epoch 53: 100%|██████████| 146/146 [00:02<00:00, 62.79it/s, loss=0.000917, v_num=8, train_loss=0.000789, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53, global step 7074: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54:  90%|████████▉ | 131/146 [00:02<00:00, 59.79it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 54:  90%|█████████ | 132/146 [00:02<00:00, 59.59it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54:  91%|█████████ | 133/146 [00:02<00:00, 59.72it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54:  92%|█████████▏| 134/146 [00:02<00:00, 59.84it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54:  92%|█████████▏| 135/146 [00:02<00:00, 59.97it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54:  93%|█████████▎| 136/146 [00:02<00:00, 60.15it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54:  94%|█████████▍| 137/146 [00:02<00:00, 60.29it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54:  95%|█████████▍| 138/146 [00:02<00:00, 60.50it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54:  95%|█████████▌| 139/146 [00:02<00:00, 60.72it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54:  96%|█████████▌| 140/146 [00:02<00:00, 60.92it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54:  97%|█████████▋| 141/146 [00:02<00:00, 61.11it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54:  97%|█████████▋| 142/146 [00:02<00:00, 61.28it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54:  98%|█████████▊| 143/146 [00:02<00:00, 61.42it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54:  99%|█████████▊| 144/146 [00:02<00:00, 61.53it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54:  99%|█████████▉| 145/146 [00:02<00:00, 61.70it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54: 100%|██████████| 146/146 [00:02<00:00, 61.62it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]\n",
      "Epoch 54: 100%|██████████| 146/146 [00:02<00:00, 61.50it/s, loss=0.000915, v_num=8, train_loss=0.000784, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54, global step 7205: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55:  90%|████████▉ | 131/146 [00:02<00:00, 55.04it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 55:  90%|█████████ | 132/146 [00:02<00:00, 55.04it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55:  91%|█████████ | 133/146 [00:02<00:00, 55.23it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55:  92%|█████████▏| 134/146 [00:02<00:00, 55.41it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55:  92%|█████████▏| 135/146 [00:02<00:00, 55.60it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55:  93%|█████████▎| 136/146 [00:02<00:00, 55.80it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55:  94%|█████████▍| 137/146 [00:02<00:00, 55.96it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55:  95%|█████████▍| 138/146 [00:02<00:00, 56.14it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55:  95%|█████████▌| 139/146 [00:02<00:00, 56.29it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55:  96%|█████████▌| 140/146 [00:02<00:00, 56.47it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55:  97%|█████████▋| 141/146 [00:02<00:00, 56.65it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55:  97%|█████████▋| 142/146 [00:02<00:00, 56.82it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55:  98%|█████████▊| 143/146 [00:02<00:00, 57.01it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55:  99%|█████████▊| 144/146 [00:02<00:00, 57.18it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55:  99%|█████████▉| 145/146 [00:02<00:00, 57.33it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55: 100%|██████████| 146/146 [00:02<00:00, 57.41it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]\n",
      "Epoch 55: 100%|██████████| 146/146 [00:02<00:00, 57.21it/s, loss=0.000912, v_num=8, train_loss=0.000786, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55, global step 7336: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56:  90%|████████▉ | 131/146 [00:02<00:00, 60.20it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 56:  90%|█████████ | 132/146 [00:02<00:00, 60.16it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56:  91%|█████████ | 133/146 [00:02<00:00, 60.34it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56:  92%|█████████▏| 134/146 [00:02<00:00, 60.57it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56:  92%|█████████▏| 135/146 [00:02<00:00, 60.70it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56:  93%|█████████▎| 136/146 [00:02<00:00, 60.85it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56:  94%|█████████▍| 137/146 [00:02<00:00, 61.02it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56:  95%|█████████▍| 138/146 [00:02<00:00, 61.17it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56:  95%|█████████▌| 139/146 [00:02<00:00, 61.36it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56:  96%|█████████▌| 140/146 [00:02<00:00, 61.53it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56:  97%|█████████▋| 141/146 [00:02<00:00, 61.73it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56:  97%|█████████▋| 142/146 [00:02<00:00, 61.90it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56:  98%|█████████▊| 143/146 [00:02<00:00, 62.09it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56:  99%|█████████▊| 144/146 [00:02<00:00, 62.28it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56:  99%|█████████▉| 145/146 [00:02<00:00, 62.44it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56: 100%|██████████| 146/146 [00:02<00:00, 62.47it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 56: 100%|██████████| 146/146 [00:02<00:00, 62.36it/s, loss=0.000911, v_num=8, train_loss=0.000771, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56, global step 7467: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57:  90%|████████▉ | 131/146 [00:02<00:00, 60.95it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 57:  90%|█████████ | 132/146 [00:02<00:00, 60.85it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57:  91%|█████████ | 133/146 [00:02<00:00, 61.03it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57:  92%|█████████▏| 134/146 [00:02<00:00, 61.15it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57:  92%|█████████▏| 135/146 [00:02<00:00, 61.33it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57:  93%|█████████▎| 136/146 [00:02<00:00, 61.51it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57:  94%|█████████▍| 137/146 [00:02<00:00, 61.65it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57:  95%|█████████▍| 138/146 [00:02<00:00, 61.85it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57:  95%|█████████▌| 139/146 [00:02<00:00, 62.02it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57:  96%|█████████▌| 140/146 [00:02<00:00, 62.19it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57:  97%|█████████▋| 141/146 [00:02<00:00, 62.36it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57:  97%|█████████▋| 142/146 [00:02<00:00, 62.55it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57:  98%|█████████▊| 143/146 [00:02<00:00, 62.71it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57:  99%|█████████▊| 144/146 [00:02<00:00, 62.91it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57:  99%|█████████▉| 145/146 [00:02<00:00, 63.04it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0156]\n",
      "Epoch 57: 100%|██████████| 146/146 [00:02<00:00, 63.12it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0157]\n",
      "Epoch 57: 100%|██████████| 146/146 [00:02<00:00, 62.98it/s, loss=0.000907, v_num=8, train_loss=0.000771, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57, global step 7598: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58:  90%|████████▉ | 131/146 [00:02<00:00, 62.17it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 58:  90%|█████████ | 132/146 [00:02<00:00, 62.08it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58:  91%|█████████ | 133/146 [00:02<00:00, 62.29it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58:  92%|█████████▏| 134/146 [00:02<00:00, 62.47it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58:  92%|█████████▏| 135/146 [00:02<00:00, 62.64it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58:  93%|█████████▎| 136/146 [00:02<00:00, 62.84it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58:  94%|█████████▍| 137/146 [00:02<00:00, 63.01it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58:  95%|█████████▍| 138/146 [00:02<00:00, 63.27it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58:  95%|█████████▌| 139/146 [00:02<00:00, 63.44it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58:  96%|█████████▌| 140/146 [00:02<00:00, 63.63it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58:  97%|█████████▋| 141/146 [00:02<00:00, 63.68it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58:  97%|█████████▋| 142/146 [00:02<00:00, 63.87it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58:  98%|█████████▊| 143/146 [00:02<00:00, 64.03it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58:  99%|█████████▊| 144/146 [00:02<00:00, 64.22it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58:  99%|█████████▉| 145/146 [00:02<00:00, 64.38it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0157]\n",
      "Epoch 58: 100%|██████████| 146/146 [00:02<00:00, 64.45it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0156]\n",
      "Epoch 58: 100%|██████████| 146/146 [00:02<00:00, 64.28it/s, loss=0.000905, v_num=8, train_loss=0.000757, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58, global step 7729: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59:  90%|████████▉ | 131/146 [00:02<00:00, 59.70it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 59:  90%|█████████ | 132/146 [00:02<00:00, 59.62it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59:  91%|█████████ | 133/146 [00:02<00:00, 59.82it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59:  92%|█████████▏| 134/146 [00:02<00:00, 59.98it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59:  92%|█████████▏| 135/146 [00:02<00:00, 60.18it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59:  93%|█████████▎| 136/146 [00:02<00:00, 60.39it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59:  94%|█████████▍| 137/146 [00:02<00:00, 60.53it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59:  95%|█████████▍| 138/146 [00:02<00:00, 60.71it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59:  95%|█████████▌| 139/146 [00:02<00:00, 60.88it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59:  96%|█████████▌| 140/146 [00:02<00:00, 61.02it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59:  97%|█████████▋| 141/146 [00:02<00:00, 61.17it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59:  97%|█████████▋| 142/146 [00:02<00:00, 61.36it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59:  98%|█████████▊| 143/146 [00:02<00:00, 61.53it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59:  99%|█████████▊| 144/146 [00:02<00:00, 61.67it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59:  99%|█████████▉| 145/146 [00:02<00:00, 61.83it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0156]\n",
      "Epoch 59: 100%|██████████| 146/146 [00:02<00:00, 61.81it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0157]\n",
      "Epoch 59: 100%|██████████| 146/146 [00:02<00:00, 61.70it/s, loss=0.000902, v_num=8, train_loss=0.000754, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59, global step 7860: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60:  90%|████████▉ | 131/146 [00:02<00:00, 60.59it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 60:  90%|█████████ | 132/146 [00:02<00:00, 60.52it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60:  91%|█████████ | 133/146 [00:02<00:00, 60.67it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60:  92%|█████████▏| 134/146 [00:02<00:00, 60.82it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60:  92%|█████████▏| 135/146 [00:02<00:00, 61.00it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60:  93%|█████████▎| 136/146 [00:02<00:00, 61.17it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60:  94%|█████████▍| 137/146 [00:02<00:00, 61.38it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60:  95%|█████████▍| 138/146 [00:02<00:00, 61.55it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60:  95%|█████████▌| 139/146 [00:02<00:00, 61.75it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60:  96%|█████████▌| 140/146 [00:02<00:00, 61.94it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60:  97%|█████████▋| 141/146 [00:02<00:00, 62.14it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60:  97%|█████████▋| 142/146 [00:02<00:00, 62.30it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60:  98%|█████████▊| 143/146 [00:02<00:00, 62.41it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60:  99%|█████████▊| 144/146 [00:02<00:00, 62.55it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60:  99%|█████████▉| 145/146 [00:02<00:00, 62.63it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0157]\n",
      "Epoch 60: 100%|██████████| 146/146 [00:02<00:00, 62.66it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0156]\n",
      "Epoch 60: 100%|██████████| 146/146 [00:02<00:00, 62.52it/s, loss=0.0009, v_num=8, train_loss=0.000743, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60, global step 7991: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61:  90%|████████▉ | 131/146 [00:02<00:00, 58.37it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 61:  90%|█████████ | 132/146 [00:02<00:00, 58.25it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61:  91%|█████████ | 133/146 [00:02<00:00, 58.41it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61:  92%|█████████▏| 134/146 [00:02<00:00, 58.51it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61:  92%|█████████▏| 135/146 [00:02<00:00, 58.67it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61:  93%|█████████▎| 136/146 [00:02<00:00, 58.79it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61:  94%|█████████▍| 137/146 [00:02<00:00, 58.92it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61:  95%|█████████▍| 138/146 [00:02<00:00, 59.02it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61:  95%|█████████▌| 139/146 [00:02<00:00, 59.17it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61:  96%|█████████▌| 140/146 [00:02<00:00, 59.27it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61:  97%|█████████▋| 141/146 [00:02<00:00, 59.39it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61:  97%|█████████▋| 142/146 [00:02<00:00, 59.48it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61:  98%|█████████▊| 143/146 [00:02<00:00, 59.58it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61:  99%|█████████▊| 144/146 [00:02<00:00, 59.70it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61:  99%|█████████▉| 145/146 [00:02<00:00, 59.81it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0156]\n",
      "Epoch 61: 100%|██████████| 146/146 [00:02<00:00, 59.83it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0157]\n",
      "Epoch 61: 100%|██████████| 146/146 [00:02<00:00, 59.66it/s, loss=0.000898, v_num=8, train_loss=0.000732, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61, global step 8122: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62:  90%|████████▉ | 131/146 [00:02<00:00, 56.50it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 62:  90%|█████████ | 132/146 [00:02<00:00, 56.42it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62:  91%|█████████ | 133/146 [00:02<00:00, 56.58it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62:  92%|█████████▏| 134/146 [00:02<00:00, 56.77it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62:  92%|█████████▏| 135/146 [00:02<00:00, 56.97it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62:  93%|█████████▎| 136/146 [00:02<00:00, 57.10it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62:  94%|█████████▍| 137/146 [00:02<00:00, 57.26it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62:  95%|█████████▍| 138/146 [00:02<00:00, 57.41it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62:  95%|█████████▌| 139/146 [00:02<00:00, 57.57it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62:  96%|█████████▌| 140/146 [00:02<00:00, 57.74it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62:  97%|█████████▋| 141/146 [00:02<00:00, 57.94it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62:  97%|█████████▋| 142/146 [00:02<00:00, 58.09it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62:  98%|█████████▊| 143/146 [00:02<00:00, 58.23it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62:  99%|█████████▊| 144/146 [00:02<00:00, 58.45it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62:  99%|█████████▉| 145/146 [00:02<00:00, 58.62it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62: 100%|██████████| 146/146 [00:02<00:00, 58.69it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]\n",
      "Epoch 62: 100%|██████████| 146/146 [00:02<00:00, 58.57it/s, loss=0.000896, v_num=8, train_loss=0.000723, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62, global step 8253: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63:  90%|████████▉ | 131/146 [00:02<00:00, 57.68it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 63:  90%|█████████ | 132/146 [00:02<00:00, 57.56it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63:  91%|█████████ | 133/146 [00:02<00:00, 57.65it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63:  92%|█████████▏| 134/146 [00:02<00:00, 57.73it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63:  92%|█████████▏| 135/146 [00:02<00:00, 57.84it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63:  93%|█████████▎| 136/146 [00:02<00:00, 57.92it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63:  94%|█████████▍| 137/146 [00:02<00:00, 57.92it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63:  95%|█████████▍| 138/146 [00:02<00:00, 58.05it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63:  95%|█████████▌| 139/146 [00:02<00:00, 58.13it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63:  96%|█████████▌| 140/146 [00:02<00:00, 58.26it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63:  97%|█████████▋| 141/146 [00:02<00:00, 58.41it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63:  97%|█████████▋| 142/146 [00:02<00:00, 58.53it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63:  98%|█████████▊| 143/146 [00:02<00:00, 58.65it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63:  99%|█████████▊| 144/146 [00:02<00:00, 58.72it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63:  99%|█████████▉| 145/146 [00:02<00:00, 58.77it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63: 100%|██████████| 146/146 [00:02<00:00, 58.77it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]\n",
      "Epoch 63: 100%|██████████| 146/146 [00:02<00:00, 58.63it/s, loss=0.000894, v_num=8, train_loss=0.000719, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63, global step 8384: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64:  90%|████████▉ | 131/146 [00:02<00:00, 53.40it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 64:  90%|█████████ | 132/146 [00:02<00:00, 53.26it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64:  91%|█████████ | 133/146 [00:02<00:00, 53.43it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64:  92%|█████████▏| 134/146 [00:02<00:00, 53.57it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64:  92%|█████████▏| 135/146 [00:02<00:00, 53.74it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64:  93%|█████████▎| 136/146 [00:02<00:00, 53.90it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64:  94%|█████████▍| 137/146 [00:02<00:00, 54.04it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64:  95%|█████████▍| 138/146 [00:02<00:00, 54.16it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64:  95%|█████████▌| 139/146 [00:02<00:00, 54.25it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64:  96%|█████████▌| 140/146 [00:02<00:00, 54.41it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64:  97%|█████████▋| 141/146 [00:02<00:00, 54.52it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64:  97%|█████████▋| 142/146 [00:02<00:00, 54.67it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64:  98%|█████████▊| 143/146 [00:02<00:00, 54.76it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64:  99%|█████████▊| 144/146 [00:02<00:00, 54.92it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64:  99%|█████████▉| 145/146 [00:02<00:00, 55.00it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64: 100%|██████████| 146/146 [00:02<00:00, 54.99it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]\n",
      "Epoch 64: 100%|██████████| 146/146 [00:02<00:00, 54.86it/s, loss=0.000892, v_num=8, train_loss=0.000695, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64, global step 8515: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65:  90%|████████▉ | 131/146 [00:02<00:00, 55.91it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 65:  90%|█████████ | 132/146 [00:02<00:00, 55.88it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65:  91%|█████████ | 133/146 [00:02<00:00, 56.07it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65:  92%|█████████▏| 134/146 [00:02<00:00, 56.25it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65:  92%|█████████▏| 135/146 [00:02<00:00, 56.46it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65:  93%|█████████▎| 136/146 [00:02<00:00, 56.64it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65:  94%|█████████▍| 137/146 [00:02<00:00, 56.84it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65:  95%|█████████▍| 138/146 [00:02<00:00, 57.04it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65:  95%|█████████▌| 139/146 [00:02<00:00, 57.15it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65:  96%|█████████▌| 140/146 [00:02<00:00, 57.35it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65:  97%|█████████▋| 141/146 [00:02<00:00, 57.50it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65:  97%|█████████▋| 142/146 [00:02<00:00, 57.70it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65:  98%|█████████▊| 143/146 [00:02<00:00, 57.91it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65:  99%|█████████▊| 144/146 [00:02<00:00, 58.06it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65:  99%|█████████▉| 145/146 [00:02<00:00, 58.21it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0157]\n",
      "Epoch 65: 100%|██████████| 146/146 [00:02<00:00, 58.28it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0158]\n",
      "Epoch 65: 100%|██████████| 146/146 [00:02<00:00, 58.14it/s, loss=0.000888, v_num=8, train_loss=0.00069, val_loss=0.0158]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65, global step 8646: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66:  90%|████████▉ | 131/146 [00:02<00:00, 61.53it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 66:  90%|█████████ | 132/146 [00:02<00:00, 61.48it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66:  91%|█████████ | 133/146 [00:02<00:00, 61.68it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66:  92%|█████████▏| 134/146 [00:02<00:00, 61.89it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66:  92%|█████████▏| 135/146 [00:02<00:00, 62.09it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66:  93%|█████████▎| 136/146 [00:02<00:00, 62.30it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66:  94%|█████████▍| 137/146 [00:02<00:00, 62.44it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66:  95%|█████████▍| 138/146 [00:02<00:00, 62.61it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66:  95%|█████████▌| 139/146 [00:02<00:00, 62.78it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66:  96%|█████████▌| 140/146 [00:02<00:00, 62.95it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66:  97%|█████████▋| 141/146 [00:02<00:00, 63.14it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66:  97%|█████████▋| 142/146 [00:02<00:00, 63.28it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66:  98%|█████████▊| 143/146 [00:02<00:00, 63.44it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66:  99%|█████████▊| 144/146 [00:02<00:00, 63.66it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66:  99%|█████████▉| 145/146 [00:02<00:00, 63.82it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0158]\n",
      "Epoch 66: 100%|██████████| 146/146 [00:02<00:00, 63.75it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0157]\n",
      "Epoch 66: 100%|██████████| 146/146 [00:02<00:00, 63.61it/s, loss=0.000886, v_num=8, train_loss=0.000684, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66, global step 8777: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67:  90%|████████▉ | 131/146 [00:02<00:00, 60.14it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 67:  90%|█████████ | 132/146 [00:02<00:00, 60.02it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67:  91%|█████████ | 133/146 [00:02<00:00, 60.20it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67:  92%|█████████▏| 134/146 [00:02<00:00, 60.41it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67:  92%|█████████▏| 135/146 [00:02<00:00, 60.59it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67:  93%|█████████▎| 136/146 [00:02<00:00, 60.79it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67:  94%|█████████▍| 137/146 [00:02<00:00, 60.99it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67:  95%|█████████▍| 138/146 [00:02<00:00, 61.19it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67:  95%|█████████▌| 139/146 [00:02<00:00, 61.34it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67:  96%|█████████▌| 140/146 [00:02<00:00, 61.51it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67:  97%|█████████▋| 141/146 [00:02<00:00, 61.70it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67:  97%|█████████▋| 142/146 [00:02<00:00, 61.65it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67:  98%|█████████▊| 143/146 [00:02<00:00, 61.77it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67:  99%|█████████▊| 144/146 [00:02<00:00, 61.93it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67:  99%|█████████▉| 145/146 [00:02<00:00, 62.12it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0157]\n",
      "Epoch 67: 100%|██████████| 146/146 [00:02<00:00, 62.07it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0158]\n",
      "Epoch 67: 100%|██████████| 146/146 [00:02<00:00, 61.89it/s, loss=0.000887, v_num=8, train_loss=0.000664, val_loss=0.0158]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67, global step 8908: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68:  90%|████████▉ | 131/146 [00:02<00:00, 54.08it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 68:  90%|█████████ | 132/146 [00:02<00:00, 54.01it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68:  91%|█████████ | 133/146 [00:02<00:00, 54.15it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68:  92%|█████████▏| 134/146 [00:02<00:00, 54.31it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68:  92%|█████████▏| 135/146 [00:02<00:00, 54.48it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68:  93%|█████████▎| 136/146 [00:02<00:00, 54.66it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68:  94%|█████████▍| 137/146 [00:02<00:00, 54.88it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68:  95%|█████████▍| 138/146 [00:02<00:00, 55.06it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68:  95%|█████████▌| 139/146 [00:02<00:00, 55.22it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68:  96%|█████████▌| 140/146 [00:02<00:00, 55.38it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68:  97%|█████████▋| 141/146 [00:02<00:00, 55.57it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68:  97%|█████████▋| 142/146 [00:02<00:00, 55.66it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68:  98%|█████████▊| 143/146 [00:02<00:00, 55.86it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68:  99%|█████████▊| 144/146 [00:02<00:00, 55.98it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68:  99%|█████████▉| 145/146 [00:02<00:00, 56.18it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68: 100%|██████████| 146/146 [00:02<00:00, 56.15it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]\n",
      "Epoch 68: 100%|██████████| 146/146 [00:02<00:00, 56.04it/s, loss=0.00088, v_num=8, train_loss=0.000652, val_loss=0.0158]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68, global step 9039: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69:  90%|████████▉ | 131/146 [00:02<00:00, 60.42it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 69:  90%|█████████ | 132/146 [00:02<00:00, 60.30it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69:  91%|█████████ | 133/146 [00:02<00:00, 60.53it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69:  92%|█████████▏| 134/146 [00:02<00:00, 60.71it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69:  92%|█████████▏| 135/146 [00:02<00:00, 60.89it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69:  93%|█████████▎| 136/146 [00:02<00:00, 61.06it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69:  94%|█████████▍| 137/146 [00:02<00:00, 61.18it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69:  95%|█████████▍| 138/146 [00:02<00:00, 61.38it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69:  95%|█████████▌| 139/146 [00:02<00:00, 61.55it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69:  96%|█████████▌| 140/146 [00:02<00:00, 61.70it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69:  97%|█████████▋| 141/146 [00:02<00:00, 61.92it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69:  97%|█████████▋| 142/146 [00:02<00:00, 62.11it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69:  98%|█████████▊| 143/146 [00:02<00:00, 62.28it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69:  99%|█████████▊| 144/146 [00:02<00:00, 62.39it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69:  99%|█████████▉| 145/146 [00:02<00:00, 62.55it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0158]\n",
      "Epoch 69: 100%|██████████| 146/146 [00:02<00:00, 62.58it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0157]\n",
      "Epoch 69: 100%|██████████| 146/146 [00:02<00:00, 62.47it/s, loss=0.000879, v_num=8, train_loss=0.000633, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69, global step 9170: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70:  90%|████████▉ | 131/146 [00:02<00:00, 62.17it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 70:  90%|█████████ | 132/146 [00:02<00:00, 62.00it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70:  91%|█████████ | 133/146 [00:02<00:00, 62.17it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70:  92%|█████████▏| 134/146 [00:02<00:00, 62.38it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70:  92%|█████████▏| 135/146 [00:02<00:00, 62.50it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70:  93%|█████████▎| 136/146 [00:02<00:00, 62.67it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70:  94%|█████████▍| 137/146 [00:02<00:00, 62.81it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70:  95%|█████████▍| 138/146 [00:02<00:00, 62.98it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70:  95%|█████████▌| 139/146 [00:02<00:00, 63.15it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70:  96%|█████████▌| 140/146 [00:02<00:00, 63.29it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70:  97%|█████████▋| 141/146 [00:02<00:00, 63.45it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70:  97%|█████████▋| 142/146 [00:02<00:00, 63.56it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70:  98%|█████████▊| 143/146 [00:02<00:00, 63.69it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70:  99%|█████████▊| 144/146 [00:02<00:00, 63.85it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70:  99%|█████████▉| 145/146 [00:02<00:00, 63.98it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0157]\n",
      "Epoch 70: 100%|██████████| 146/146 [00:02<00:00, 63.97it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0158]\n",
      "Epoch 70: 100%|██████████| 146/146 [00:02<00:00, 63.86it/s, loss=0.000875, v_num=8, train_loss=0.000628, val_loss=0.0158]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70, global step 9301: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71:  90%|████████▉ | 131/146 [00:02<00:00, 59.95it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 71:  90%|█████████ | 132/146 [00:02<00:00, 59.81it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71:  91%|█████████ | 133/146 [00:02<00:00, 59.99it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71:  92%|█████████▏| 134/146 [00:02<00:00, 60.17it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71:  92%|█████████▏| 135/146 [00:02<00:00, 60.34it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71:  93%|█████████▎| 136/146 [00:02<00:00, 60.52it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71:  94%|█████████▍| 137/146 [00:02<00:00, 60.72it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71:  95%|█████████▍| 138/146 [00:02<00:00, 60.87it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71:  95%|█████████▌| 139/146 [00:02<00:00, 61.01it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71:  96%|█████████▌| 140/146 [00:02<00:00, 61.21it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71:  97%|█████████▋| 141/146 [00:02<00:00, 61.30it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71:  97%|█████████▋| 142/146 [00:02<00:00, 61.47it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71:  98%|█████████▊| 143/146 [00:02<00:00, 61.63it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71:  99%|█████████▊| 144/146 [00:02<00:00, 61.82it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71:  99%|█████████▉| 145/146 [00:02<00:00, 61.96it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0158]\n",
      "Epoch 71: 100%|██████████| 146/146 [00:02<00:00, 62.02it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0157]\n",
      "Epoch 71: 100%|██████████| 146/146 [00:02<00:00, 61.89it/s, loss=0.000874, v_num=8, train_loss=0.000611, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71, global step 9432: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72:  90%|████████▉ | 131/146 [00:02<00:00, 60.98it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 72:  90%|█████████ | 132/146 [00:02<00:00, 60.91it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72:  91%|█████████ | 133/146 [00:02<00:00, 61.06it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72:  92%|█████████▏| 134/146 [00:02<00:00, 61.27it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72:  92%|█████████▏| 135/146 [00:02<00:00, 61.44it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72:  93%|█████████▎| 136/146 [00:02<00:00, 61.59it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72:  94%|█████████▍| 137/146 [00:02<00:00, 61.71it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72:  95%|█████████▍| 138/146 [00:02<00:00, 61.88it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72:  95%|█████████▌| 139/146 [00:02<00:00, 62.05it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72:  96%|█████████▌| 140/146 [00:02<00:00, 62.22it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72:  97%|█████████▋| 141/146 [00:02<00:00, 62.38it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72:  97%|█████████▋| 142/146 [00:02<00:00, 62.52it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72:  98%|█████████▊| 143/146 [00:02<00:00, 62.69it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72:  99%|█████████▊| 144/146 [00:02<00:00, 62.85it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72:  99%|█████████▉| 145/146 [00:02<00:00, 63.04it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0157]\n",
      "Epoch 72: 100%|██████████| 146/146 [00:02<00:00, 63.04it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0158]\n",
      "Epoch 72: 100%|██████████| 146/146 [00:02<00:00, 62.93it/s, loss=0.000871, v_num=8, train_loss=0.0006, val_loss=0.0158]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72, global step 9563: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73:  90%|████████▉ | 131/146 [00:02<00:00, 60.90it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 73:  90%|█████████ | 132/146 [00:02<00:00, 60.83it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73:  91%|█████████ | 133/146 [00:02<00:00, 60.92it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73:  92%|█████████▏| 134/146 [00:02<00:00, 61.15it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73:  92%|█████████▏| 135/146 [00:02<00:00, 61.30it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73:  93%|█████████▎| 136/146 [00:02<00:00, 61.48it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73:  94%|█████████▍| 137/146 [00:02<00:00, 61.68it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73:  95%|█████████▍| 138/146 [00:02<00:00, 61.82it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73:  95%|█████████▌| 139/146 [00:02<00:00, 62.05it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73:  96%|█████████▌| 140/146 [00:02<00:00, 62.19it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73:  97%|█████████▋| 141/146 [00:02<00:00, 62.33it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73:  97%|█████████▋| 142/146 [00:02<00:00, 62.52it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73:  98%|█████████▊| 143/146 [00:02<00:00, 62.69it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73:  99%|█████████▊| 144/146 [00:02<00:00, 62.91it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73:  99%|█████████▉| 145/146 [00:02<00:00, 63.01it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73: 100%|██████████| 146/146 [00:02<00:00, 63.04it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]\n",
      "Epoch 73: 100%|██████████| 146/146 [00:02<00:00, 62.93it/s, loss=0.000869, v_num=8, train_loss=0.000586, val_loss=0.0158]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73, global step 9694: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74:  90%|████████▉ | 131/146 [00:02<00:00, 60.34it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 74:  90%|█████████ | 132/146 [00:02<00:00, 60.19it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74:  91%|█████████ | 133/146 [00:02<00:00, 60.34it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74:  92%|█████████▏| 134/146 [00:02<00:00, 60.49it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74:  92%|█████████▏| 135/146 [00:02<00:00, 60.64it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74:  93%|█████████▎| 136/146 [00:02<00:00, 60.74it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74:  94%|█████████▍| 137/146 [00:02<00:00, 60.83it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74:  95%|█████████▍| 138/146 [00:02<00:00, 60.92it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74:  95%|█████████▌| 139/146 [00:02<00:00, 61.04it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74:  96%|█████████▌| 140/146 [00:02<00:00, 61.16it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74:  97%|█████████▋| 141/146 [00:02<00:00, 61.25it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74:  97%|█████████▋| 142/146 [00:02<00:00, 61.36it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74:  98%|█████████▊| 143/146 [00:02<00:00, 61.47it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74:  99%|█████████▊| 144/146 [00:02<00:00, 61.59it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74:  99%|█████████▉| 145/146 [00:02<00:00, 61.70it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74: 100%|██████████| 146/146 [00:02<00:00, 61.63it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]\n",
      "Epoch 74: 100%|██████████| 146/146 [00:02<00:00, 61.47it/s, loss=0.000866, v_num=8, train_loss=0.000576, val_loss=0.0158]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74, global step 9825: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75:  90%|████████▉ | 131/146 [00:02<00:00, 55.04it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 75:  90%|█████████ | 132/146 [00:02<00:00, 55.00it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75:  91%|█████████ | 133/146 [00:02<00:00, 55.16it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75:  92%|█████████▏| 134/146 [00:02<00:00, 55.39it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75:  92%|█████████▏| 135/146 [00:02<00:00, 55.55it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75:  93%|█████████▎| 136/146 [00:02<00:00, 55.67it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75:  94%|█████████▍| 137/146 [00:02<00:00, 55.78it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75:  95%|█████████▍| 138/146 [00:02<00:00, 55.87it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75:  95%|█████████▌| 139/146 [00:02<00:00, 56.02it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75:  96%|█████████▌| 140/146 [00:02<00:00, 56.20it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75:  97%|█████████▋| 141/146 [00:02<00:00, 56.37it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75:  97%|█████████▋| 142/146 [00:02<00:00, 56.48it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75:  98%|█████████▊| 143/146 [00:02<00:00, 56.65it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75:  99%|█████████▊| 144/146 [00:02<00:00, 56.82it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75:  99%|█████████▉| 145/146 [00:02<00:00, 57.02it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0158]\n",
      "Epoch 75: 100%|██████████| 146/146 [00:02<00:00, 57.07it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0157]\n",
      "Epoch 75: 100%|██████████| 146/146 [00:02<00:00, 56.96it/s, loss=0.000864, v_num=8, train_loss=0.000563, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75, global step 9956: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76:  90%|████████▉ | 131/146 [00:02<00:00, 61.30it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 76:  90%|█████████ | 132/146 [00:02<00:00, 61.19it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76:  91%|█████████ | 133/146 [00:02<00:00, 61.34it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76:  92%|█████████▏| 134/146 [00:02<00:00, 61.55it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76:  92%|█████████▏| 135/146 [00:02<00:00, 61.67it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76:  93%|█████████▎| 136/146 [00:02<00:00, 61.79it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76:  94%|█████████▍| 137/146 [00:02<00:00, 61.96it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76:  95%|█████████▍| 138/146 [00:02<00:00, 62.13it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76:  95%|█████████▌| 139/146 [00:02<00:00, 62.24it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76:  96%|█████████▌| 140/146 [00:02<00:00, 62.41it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76:  97%|█████████▋| 141/146 [00:02<00:00, 62.50it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76:  97%|█████████▋| 142/146 [00:02<00:00, 62.66it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76:  98%|█████████▊| 143/146 [00:02<00:00, 62.85it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76:  99%|█████████▊| 144/146 [00:02<00:00, 63.04it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76:  99%|█████████▉| 145/146 [00:02<00:00, 63.20it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0157]\n",
      "Epoch 76: 100%|██████████| 146/146 [00:02<00:00, 63.25it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0158]\n",
      "Epoch 76: 100%|██████████| 146/146 [00:02<00:00, 63.14it/s, loss=0.000861, v_num=8, train_loss=0.000548, val_loss=0.0158]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76, global step 10087: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77:  90%|████████▉ | 131/146 [00:02<00:00, 60.70it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 77:  90%|█████████ | 132/146 [00:02<00:00, 60.66it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77:  91%|█████████ | 133/146 [00:02<00:00, 60.84it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77:  92%|█████████▏| 134/146 [00:02<00:00, 60.99it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77:  92%|█████████▏| 135/146 [00:02<00:00, 61.16it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77:  93%|█████████▎| 136/146 [00:02<00:00, 61.37it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77:  94%|█████████▍| 137/146 [00:02<00:00, 61.54it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77:  95%|█████████▍| 138/146 [00:02<00:00, 61.71it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77:  95%|█████████▌| 139/146 [00:02<00:00, 61.88it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77:  96%|█████████▌| 140/146 [00:02<00:00, 62.00it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77:  97%|█████████▋| 141/146 [00:02<00:00, 62.16it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77:  97%|█████████▋| 142/146 [00:02<00:00, 62.36it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77:  98%|█████████▊| 143/146 [00:02<00:00, 62.52it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77:  99%|█████████▊| 144/146 [00:02<00:00, 62.69it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77:  99%|█████████▉| 145/146 [00:02<00:00, 62.79it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0158]\n",
      "Epoch 77: 100%|██████████| 146/146 [00:02<00:00, 62.82it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0157]\n",
      "Epoch 77: 100%|██████████| 146/146 [00:02<00:00, 62.66it/s, loss=0.000858, v_num=8, train_loss=0.000533, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77, global step 10218: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78:  90%|████████▉ | 131/146 [00:02<00:00, 60.81it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 78:  90%|█████████ | 132/146 [00:02<00:00, 60.74it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78:  91%|█████████ | 133/146 [00:02<00:00, 60.89it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78:  92%|█████████▏| 134/146 [00:02<00:00, 61.07it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78:  92%|█████████▏| 135/146 [00:02<00:00, 61.25it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78:  93%|█████████▎| 136/146 [00:02<00:00, 61.42it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78:  94%|█████████▍| 137/146 [00:02<00:00, 61.62it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78:  95%|█████████▍| 138/146 [00:02<00:00, 61.80it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78:  95%|█████████▌| 139/146 [00:02<00:00, 61.94it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78:  96%|█████████▌| 140/146 [00:02<00:00, 62.11it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78:  97%|█████████▋| 141/146 [00:02<00:00, 62.27it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78:  97%|█████████▋| 142/146 [00:02<00:00, 62.39it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78:  98%|█████████▊| 143/146 [00:02<00:00, 62.58it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78:  99%|█████████▊| 144/146 [00:02<00:00, 62.71it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78:  99%|█████████▉| 145/146 [00:02<00:00, 62.90it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0157]\n",
      "Epoch 78: 100%|██████████| 146/146 [00:02<00:00, 62.95it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0158]\n",
      "Epoch 78: 100%|██████████| 146/146 [00:02<00:00, 62.85it/s, loss=0.000856, v_num=8, train_loss=0.000516, val_loss=0.0158]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78, global step 10349: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79:  90%|████████▉ | 131/146 [00:02<00:00, 59.73it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 79:  90%|█████████ | 132/146 [00:02<00:00, 59.67it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79:  91%|█████████ | 133/146 [00:02<00:00, 59.88it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79:  92%|█████████▏| 134/146 [00:02<00:00, 60.06it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79:  92%|█████████▏| 135/146 [00:02<00:00, 60.24it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79:  93%|█████████▎| 136/146 [00:02<00:00, 60.44it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79:  94%|█████████▍| 137/146 [00:02<00:00, 60.62it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79:  95%|█████████▍| 138/146 [00:02<00:00, 60.79it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79:  95%|█████████▌| 139/146 [00:02<00:00, 60.96it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79:  96%|█████████▌| 140/146 [00:02<00:00, 61.10it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79:  97%|█████████▋| 141/146 [00:02<00:00, 61.25it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79:  97%|█████████▋| 142/146 [00:02<00:00, 61.41it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79:  98%|█████████▊| 143/146 [00:02<00:00, 61.58it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79:  99%|█████████▊| 144/146 [00:02<00:00, 61.72it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79:  99%|█████████▉| 145/146 [00:02<00:00, 61.88it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0158]\n",
      "Epoch 79: 100%|██████████| 146/146 [00:02<00:00, 61.91it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0157]\n",
      "Epoch 79: 100%|██████████| 146/146 [00:02<00:00, 61.81it/s, loss=0.000853, v_num=8, train_loss=0.000496, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79, global step 10480: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80:  90%|████████▉ | 131/146 [00:02<00:00, 60.31it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 80:  90%|█████████ | 132/146 [00:02<00:00, 60.24it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80:  91%|█████████ | 133/146 [00:02<00:00, 60.40it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80:  92%|█████████▏| 134/146 [00:02<00:00, 60.60it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80:  92%|█████████▏| 135/146 [00:02<00:00, 60.78it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80:  93%|█████████▎| 136/146 [00:02<00:00, 60.95it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80:  94%|█████████▍| 137/146 [00:02<00:00, 61.16it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80:  95%|█████████▍| 138/146 [00:02<00:00, 61.27it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80:  95%|█████████▌| 139/146 [00:02<00:00, 61.45it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80:  96%|█████████▌| 140/146 [00:02<00:00, 61.59it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80:  97%|█████████▋| 141/146 [00:02<00:00, 61.78it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80:  97%|█████████▋| 142/146 [00:02<00:00, 61.95it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80:  98%|█████████▊| 143/146 [00:02<00:00, 62.12it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80:  99%|█████████▊| 144/146 [00:02<00:00, 62.25it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80:  99%|█████████▉| 145/146 [00:02<00:00, 62.47it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80: 100%|██████████| 146/146 [00:02<00:00, 62.52it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]\n",
      "Epoch 80: 100%|██████████| 146/146 [00:02<00:00, 62.39it/s, loss=0.00085, v_num=8, train_loss=0.000473, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80, global step 10611: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81:  90%|████████▉ | 131/146 [00:02<00:00, 57.05it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 81:  90%|█████████ | 132/146 [00:02<00:00, 56.92it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81:  91%|█████████ | 133/146 [00:02<00:00, 57.08it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81:  92%|█████████▏| 134/146 [00:02<00:00, 57.19it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81:  92%|█████████▏| 135/146 [00:02<00:00, 57.35it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81:  93%|█████████▎| 136/146 [00:02<00:00, 57.45it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81:  94%|█████████▍| 137/146 [00:02<00:00, 57.53it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81:  95%|█████████▍| 138/146 [00:02<00:00, 57.62it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81:  95%|█████████▌| 139/146 [00:02<00:00, 57.70it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81:  96%|█████████▌| 140/146 [00:02<00:00, 57.80it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81:  97%|█████████▋| 141/146 [00:02<00:00, 57.93it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81:  97%|█████████▋| 142/146 [00:02<00:00, 58.05it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81:  98%|█████████▊| 143/146 [00:02<00:00, 58.20it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81:  99%|█████████▊| 144/146 [00:02<00:00, 58.30it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81:  99%|█████████▉| 145/146 [00:02<00:00, 58.44it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81: 100%|██████████| 146/146 [00:02<00:00, 58.44it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]\n",
      "Epoch 81: 100%|██████████| 146/146 [00:02<00:00, 58.30it/s, loss=0.000846, v_num=8, train_loss=0.000447, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81, global step 10742: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82:  90%|████████▉ | 131/146 [00:02<00:00, 56.61it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 82:  90%|█████████ | 132/146 [00:02<00:00, 56.58it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82:  91%|█████████ | 133/146 [00:02<00:00, 56.76it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82:  92%|█████████▏| 134/146 [00:02<00:00, 56.94it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82:  92%|█████████▏| 135/146 [00:02<00:00, 57.10it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82:  93%|█████████▎| 136/146 [00:02<00:00, 57.26it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82:  94%|█████████▍| 137/146 [00:02<00:00, 57.44it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82:  95%|█████████▍| 138/146 [00:02<00:00, 57.64it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82:  95%|█████████▌| 139/146 [00:02<00:00, 57.82it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82:  96%|█████████▌| 140/146 [00:02<00:00, 57.97it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82:  97%|█████████▋| 141/146 [00:02<00:00, 58.14it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82:  97%|█████████▋| 142/146 [00:02<00:00, 58.31it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82:  98%|█████████▊| 143/146 [00:02<00:00, 58.53it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82:  99%|█████████▊| 144/146 [00:02<00:00, 58.70it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82:  99%|█████████▉| 145/146 [00:02<00:00, 58.82it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82: 100%|██████████| 146/146 [00:02<00:00, 58.91it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]\n",
      "Epoch 82: 100%|██████████| 146/146 [00:02<00:00, 58.75it/s, loss=0.000843, v_num=8, train_loss=0.000419, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82, global step 10873: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83:  90%|████████▉ | 131/146 [00:02<00:00, 61.27it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 83:  90%|█████████ | 132/146 [00:02<00:00, 61.14it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83:  91%|█████████ | 133/146 [00:02<00:00, 61.31it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83:  92%|█████████▏| 134/146 [00:02<00:00, 61.52it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83:  92%|█████████▏| 135/146 [00:02<00:00, 61.61it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83:  93%|█████████▎| 136/146 [00:02<00:00, 61.67it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83:  94%|█████████▍| 137/146 [00:02<00:00, 61.82it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83:  95%|█████████▍| 138/146 [00:02<00:00, 61.91it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83:  95%|█████████▌| 139/146 [00:02<00:00, 62.05it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83:  96%|█████████▌| 140/146 [00:02<00:00, 62.19it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83:  97%|█████████▋| 141/146 [00:02<00:00, 62.36it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83:  97%|█████████▋| 142/146 [00:02<00:00, 62.52it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83:  98%|█████████▊| 143/146 [00:02<00:00, 62.61it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83:  99%|█████████▊| 144/146 [00:02<00:00, 62.74it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83:  99%|█████████▉| 145/146 [00:02<00:00, 62.88it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0157]\n",
      "Epoch 83: 100%|██████████| 146/146 [00:02<00:00, 62.95it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0156]\n",
      "Epoch 83: 100%|██████████| 146/146 [00:02<00:00, 62.66it/s, loss=0.00084, v_num=8, train_loss=0.000391, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83, global step 11004: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84:  90%|████████▉ | 131/146 [00:02<00:00, 60.59it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 84:  90%|█████████ | 132/146 [00:02<00:00, 60.50it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84:  91%|█████████ | 133/146 [00:02<00:00, 60.68it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84:  92%|█████████▏| 134/146 [00:02<00:00, 60.91it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84:  92%|█████████▏| 135/146 [00:02<00:00, 61.09it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84:  93%|█████████▎| 136/146 [00:02<00:00, 61.26it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84:  94%|█████████▍| 137/146 [00:02<00:00, 61.44it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84:  95%|█████████▍| 138/146 [00:02<00:00, 61.64it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84:  95%|█████████▌| 139/146 [00:02<00:00, 61.84it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84:  96%|█████████▌| 140/146 [00:02<00:00, 62.00it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84:  97%|█████████▋| 141/146 [00:02<00:00, 62.20it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84:  97%|█████████▋| 142/146 [00:02<00:00, 62.37it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84:  98%|█████████▊| 143/146 [00:02<00:00, 62.53it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84:  99%|█████████▊| 144/146 [00:02<00:00, 62.75it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84:  99%|█████████▉| 145/146 [00:02<00:00, 62.91it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84: 100%|██████████| 146/146 [00:02<00:00, 62.96it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]\n",
      "Epoch 84: 100%|██████████| 146/146 [00:02<00:00, 62.74it/s, loss=0.000837, v_num=8, train_loss=0.00036, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84, global step 11135: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85:  90%|████████▉ | 131/146 [00:02<00:00, 59.57it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 85:  90%|█████████ | 132/146 [00:02<00:00, 59.51it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85:  91%|█████████ | 133/146 [00:02<00:00, 59.69it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85:  92%|█████████▏| 134/146 [00:02<00:00, 59.84it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85:  92%|█████████▏| 135/146 [00:02<00:00, 60.00it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85:  93%|█████████▎| 136/146 [00:02<00:00, 60.12it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85:  94%|█████████▍| 137/146 [00:02<00:00, 60.29it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85:  95%|█████████▍| 138/146 [00:02<00:00, 60.47it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85:  95%|█████████▌| 139/146 [00:02<00:00, 60.67it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85:  96%|█████████▌| 140/146 [00:02<00:00, 60.81it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85:  97%|█████████▋| 141/146 [00:02<00:00, 60.75it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85:  97%|█████████▋| 142/146 [00:02<00:00, 60.89it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85:  98%|█████████▊| 143/146 [00:02<00:00, 61.08it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85:  99%|█████████▊| 144/146 [00:02<00:00, 61.19it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85:  99%|█████████▉| 145/146 [00:02<00:00, 61.33it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85: 100%|██████████| 146/146 [00:02<00:00, 61.37it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]\n",
      "Epoch 85: 100%|██████████| 146/146 [00:02<00:00, 61.19it/s, loss=0.000834, v_num=8, train_loss=0.000332, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85, global step 11266: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86:  90%|████████▉ | 131/146 [00:02<00:00, 60.09it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 86:  90%|█████████ | 132/146 [00:02<00:00, 60.02it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86:  91%|█████████ | 133/146 [00:02<00:00, 60.15it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86:  92%|█████████▏| 134/146 [00:02<00:00, 60.30it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86:  92%|█████████▏| 135/146 [00:02<00:00, 60.51it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86:  93%|█████████▎| 136/146 [00:02<00:00, 60.68it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86:  94%|█████████▍| 137/146 [00:02<00:00, 60.86it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86:  95%|█████████▍| 138/146 [00:02<00:00, 61.00it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86:  95%|█████████▌| 139/146 [00:02<00:00, 61.18it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86:  96%|█████████▌| 140/146 [00:02<00:00, 61.35it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86:  97%|█████████▋| 141/146 [00:02<00:00, 61.51it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86:  97%|█████████▋| 142/146 [00:02<00:00, 61.68it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86:  98%|█████████▊| 143/146 [00:02<00:00, 61.85it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86:  99%|█████████▊| 144/146 [00:02<00:00, 61.96it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86:  99%|█████████▉| 145/146 [00:02<00:00, 62.09it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86: 100%|██████████| 146/146 [00:02<00:00, 62.15it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]\n",
      "Epoch 86: 100%|██████████| 146/146 [00:02<00:00, 62.04it/s, loss=0.000832, v_num=8, train_loss=0.000302, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86, global step 11397: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87:  90%|████████▉ | 131/146 [00:02<00:00, 61.29it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 87:  90%|█████████ | 132/146 [00:02<00:00, 61.24it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87:  91%|█████████ | 133/146 [00:02<00:00, 61.42it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87:  92%|█████████▏| 134/146 [00:02<00:00, 61.57it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87:  92%|█████████▏| 135/146 [00:02<00:00, 61.78it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87:  93%|█████████▎| 136/146 [00:02<00:00, 61.89it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87:  94%|█████████▍| 137/146 [00:02<00:00, 62.09it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87:  95%|█████████▍| 138/146 [00:02<00:00, 62.32it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87:  95%|█████████▌| 139/146 [00:02<00:00, 62.43it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87:  96%|█████████▌| 140/146 [00:02<00:00, 62.63it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87:  97%|█████████▋| 141/146 [00:02<00:00, 62.74it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87:  97%|█████████▋| 142/146 [00:02<00:00, 62.93it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87:  98%|█████████▊| 143/146 [00:02<00:00, 63.07it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87:  99%|█████████▊| 144/146 [00:02<00:00, 63.23it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87:  99%|█████████▉| 145/146 [00:02<00:00, 63.36it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0156]\n",
      "Epoch 87: 100%|██████████| 146/146 [00:02<00:00, 63.36it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0155]\n",
      "Epoch 87: 100%|██████████| 146/146 [00:02<00:00, 63.22it/s, loss=0.00083, v_num=8, train_loss=0.000277, val_loss=0.0155]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87, global step 11528: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88:  90%|████████▉ | 131/146 [00:02<00:00, 53.18it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 88:  90%|█████████ | 132/146 [00:02<00:00, 53.03it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88:  91%|█████████ | 133/146 [00:02<00:00, 53.20it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88:  92%|█████████▏| 134/146 [00:02<00:00, 53.36it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88:  92%|█████████▏| 135/146 [00:02<00:00, 53.48it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88:  93%|█████████▎| 136/146 [00:02<00:00, 53.62it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88:  94%|█████████▍| 137/146 [00:02<00:00, 53.74it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88:  95%|█████████▍| 138/146 [00:02<00:00, 53.84it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88:  95%|█████████▌| 139/146 [00:02<00:00, 53.98it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88:  96%|█████████▌| 140/146 [00:02<00:00, 54.11it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88:  97%|█████████▋| 141/146 [00:02<00:00, 54.27it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88:  97%|█████████▋| 142/146 [00:02<00:00, 54.38it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88:  98%|█████████▊| 143/146 [00:02<00:00, 54.49it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88:  99%|█████████▊| 144/146 [00:02<00:00, 54.60it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88:  99%|█████████▉| 145/146 [00:02<00:00, 54.69it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0155]\n",
      "Epoch 88: 100%|██████████| 146/146 [00:02<00:00, 54.72it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0154]\n",
      "Epoch 88: 100%|██████████| 146/146 [00:02<00:00, 54.62it/s, loss=0.000829, v_num=8, train_loss=0.000256, val_loss=0.0154]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88, global step 11659: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89:  90%|████████▉ | 131/146 [00:02<00:00, 60.56it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 89:  90%|█████████ | 132/146 [00:02<00:00, 60.35it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89:  91%|█████████ | 133/146 [00:02<00:00, 60.56it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89:  92%|█████████▏| 134/146 [00:02<00:00, 60.71it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89:  92%|█████████▏| 135/146 [00:02<00:00, 60.83it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89:  93%|█████████▎| 136/146 [00:02<00:00, 61.04it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89:  94%|█████████▍| 137/146 [00:02<00:00, 61.18it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89:  95%|█████████▍| 138/146 [00:02<00:00, 61.36it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89:  95%|█████████▌| 139/146 [00:02<00:00, 61.50it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89:  96%|█████████▌| 140/146 [00:02<00:00, 61.67it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89:  97%|█████████▋| 141/146 [00:02<00:00, 61.81it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89:  97%|█████████▋| 142/146 [00:02<00:00, 61.98it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89:  98%|█████████▊| 143/146 [00:02<00:00, 62.17it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89:  99%|█████████▊| 144/146 [00:02<00:00, 62.31it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89:  99%|█████████▉| 145/146 [00:02<00:00, 62.50it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89: 100%|██████████| 146/146 [00:02<00:00, 62.52it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]\n",
      "Epoch 89: 100%|██████████| 146/146 [00:02<00:00, 62.42it/s, loss=0.000828, v_num=8, train_loss=0.000239, val_loss=0.0154]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89, global step 11790: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90:  90%|████████▉ | 131/146 [00:02<00:00, 59.20it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 90:  90%|█████████ | 132/146 [00:02<00:00, 59.12it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90:  91%|█████████ | 133/146 [00:02<00:00, 59.28it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90:  92%|█████████▏| 134/146 [00:02<00:00, 59.43it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90:  92%|█████████▏| 135/146 [00:02<00:00, 59.64it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90:  93%|█████████▎| 136/146 [00:02<00:00, 59.81it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90:  94%|█████████▍| 137/146 [00:02<00:00, 60.02it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90:  95%|█████████▍| 138/146 [00:02<00:00, 60.19it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90:  95%|█████████▌| 139/146 [00:02<00:00, 60.39it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90:  96%|█████████▌| 140/146 [00:02<00:00, 60.53it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90:  97%|█████████▋| 141/146 [00:02<00:00, 60.73it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90:  97%|█████████▋| 142/146 [00:02<00:00, 60.87it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90:  98%|█████████▊| 143/146 [00:02<00:00, 61.01it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90:  99%|█████████▊| 144/146 [00:02<00:00, 61.18it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90:  99%|█████████▉| 145/146 [00:02<00:00, 61.32it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0154]\n",
      "Epoch 90: 100%|██████████| 146/146 [00:02<00:00, 61.35it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0153]\n",
      "Epoch 90: 100%|██████████| 146/146 [00:02<00:00, 61.22it/s, loss=0.000828, v_num=8, train_loss=0.000226, val_loss=0.0153]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90, global step 11921: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91:  90%|████████▉ | 131/146 [00:02<00:00, 58.90it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 91:  90%|█████████ | 132/146 [00:02<00:00, 58.82it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91:  91%|█████████ | 133/146 [00:02<00:00, 59.00it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91:  92%|█████████▏| 134/146 [00:02<00:00, 59.16it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91:  92%|█████████▏| 135/146 [00:02<00:00, 59.34it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91:  93%|█████████▎| 136/146 [00:02<00:00, 59.54it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91:  94%|█████████▍| 137/146 [00:02<00:00, 59.74it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91:  95%|█████████▍| 138/146 [00:02<00:00, 59.87it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91:  95%|█████████▌| 139/146 [00:02<00:00, 60.01it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91:  96%|█████████▌| 140/146 [00:02<00:00, 60.21it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91:  97%|█████████▋| 141/146 [00:02<00:00, 60.33it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91:  97%|█████████▋| 142/146 [00:02<00:00, 60.52it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91:  98%|█████████▊| 143/146 [00:02<00:00, 60.67it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91:  99%|█████████▊| 144/146 [00:02<00:00, 60.83it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91:  99%|█████████▉| 145/146 [00:02<00:00, 61.02it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0153]\n",
      "Epoch 91: 100%|██████████| 146/146 [00:02<00:00, 61.08it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0152]\n",
      "Epoch 91: 100%|██████████| 146/146 [00:02<00:00, 60.96it/s, loss=0.000828, v_num=8, train_loss=0.000217, val_loss=0.0152]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91, global step 12052: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92:  90%|████████▉ | 131/146 [00:02<00:00, 61.10it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 92:  90%|█████████ | 132/146 [00:02<00:00, 61.02it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92:  91%|█████████ | 133/146 [00:02<00:00, 61.20it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92:  92%|█████████▏| 134/146 [00:02<00:00, 61.35it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92:  92%|█████████▏| 135/146 [00:02<00:00, 61.50it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92:  93%|█████████▎| 136/146 [00:02<00:00, 61.70it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92:  94%|█████████▍| 137/146 [00:02<00:00, 61.87it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92:  95%|█████████▍| 138/146 [00:02<00:00, 62.05it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92:  95%|█████████▌| 139/146 [00:02<00:00, 62.19it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92:  96%|█████████▌| 140/146 [00:02<00:00, 62.36it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92:  97%|█████████▋| 141/146 [00:02<00:00, 62.55it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92:  97%|█████████▋| 142/146 [00:02<00:00, 62.74it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92:  98%|█████████▊| 143/146 [00:02<00:00, 62.88it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92:  99%|█████████▊| 144/146 [00:02<00:00, 63.10it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92:  99%|█████████▉| 145/146 [00:02<00:00, 63.23it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0152]\n",
      "Epoch 92: 100%|██████████| 146/146 [00:02<00:00, 63.25it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0151]\n",
      "Epoch 92: 100%|██████████| 146/146 [00:02<00:00, 63.17it/s, loss=0.000828, v_num=8, train_loss=0.000212, val_loss=0.0151]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92, global step 12183: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93:  90%|████████▉ | 131/146 [00:02<00:00, 60.64it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 93:  90%|█████████ | 132/146 [00:02<00:00, 60.52it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93:  91%|█████████ | 133/146 [00:02<00:00, 60.73it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93:  92%|█████████▏| 134/146 [00:02<00:00, 60.90it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93:  92%|█████████▏| 135/146 [00:02<00:00, 61.11it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93:  93%|█████████▎| 136/146 [00:02<00:00, 61.06it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93:  94%|█████████▍| 137/146 [00:02<00:00, 61.21it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93:  95%|█████████▍| 138/146 [00:02<00:00, 61.33it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93:  95%|█████████▌| 139/146 [00:02<00:00, 61.53it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93:  96%|█████████▌| 140/146 [00:02<00:00, 61.72it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93:  97%|█████████▋| 141/146 [00:02<00:00, 61.89it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93:  97%|█████████▋| 142/146 [00:02<00:00, 61.98it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93:  98%|█████████▊| 143/146 [00:02<00:00, 62.12it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93:  99%|█████████▊| 144/146 [00:02<00:00, 62.28it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93:  99%|█████████▉| 145/146 [00:02<00:00, 62.50it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93: 100%|██████████| 146/146 [00:02<00:00, 62.60it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]\n",
      "Epoch 93: 100%|██████████| 146/146 [00:02<00:00, 62.50it/s, loss=0.000829, v_num=8, train_loss=0.000207, val_loss=0.0151]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93, global step 12314: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94:  90%|████████▉ | 131/146 [00:02<00:00, 59.35it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 94:  90%|█████████ | 132/146 [00:02<00:00, 59.14it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94:  91%|█████████ | 133/146 [00:02<00:00, 59.26it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94:  92%|█████████▏| 134/146 [00:02<00:00, 59.37it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94:  92%|█████████▏| 135/146 [00:02<00:00, 59.52it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94:  93%|█████████▎| 136/146 [00:02<00:00, 59.62it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94:  94%|█████████▍| 137/146 [00:02<00:00, 59.74it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94:  95%|█████████▍| 138/146 [00:02<00:00, 59.87it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94:  95%|█████████▌| 139/146 [00:02<00:00, 60.01it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94:  96%|█████████▌| 140/146 [00:02<00:00, 60.16it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94:  97%|█████████▋| 141/146 [00:02<00:00, 60.30it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94:  97%|█████████▋| 142/146 [00:02<00:00, 60.37it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94:  98%|█████████▊| 143/146 [00:02<00:00, 60.49it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94:  99%|█████████▊| 144/146 [00:02<00:00, 60.55it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94:  99%|█████████▉| 145/146 [00:02<00:00, 60.69it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.0151]\n",
      "Epoch 94: 100%|██████████| 146/146 [00:02<00:00, 60.70it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.015] \n",
      "Epoch 94: 100%|██████████| 146/146 [00:02<00:00, 60.55it/s, loss=0.000828, v_num=8, train_loss=0.000206, val_loss=0.015]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94, global step 12445: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95:  90%|████████▉ | 131/146 [00:02<00:00, 53.79it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 95:  90%|█████████ | 132/146 [00:02<00:00, 53.81it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95:  91%|█████████ | 133/146 [00:02<00:00, 53.95it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95:  92%|█████████▏| 134/146 [00:02<00:00, 54.16it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95:  92%|█████████▏| 135/146 [00:02<00:00, 54.32it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95:  93%|█████████▎| 136/146 [00:02<00:00, 54.51it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95:  94%|█████████▍| 137/146 [00:02<00:00, 54.71it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95:  95%|█████████▍| 138/146 [00:02<00:00, 54.87it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95:  95%|█████████▌| 139/146 [00:02<00:00, 55.07it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95:  96%|█████████▌| 140/146 [00:02<00:00, 55.24it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95:  97%|█████████▋| 141/146 [00:02<00:00, 55.42it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95:  97%|█████████▋| 142/146 [00:02<00:00, 55.59it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95:  98%|█████████▊| 143/146 [00:02<00:00, 55.77it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95:  99%|█████████▊| 144/146 [00:02<00:00, 55.98it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95:  99%|█████████▉| 145/146 [00:02<00:00, 56.15it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95: 100%|██████████| 146/146 [00:02<00:00, 56.19it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]\n",
      "Epoch 95: 100%|██████████| 146/146 [00:02<00:00, 56.13it/s, loss=0.000829, v_num=8, train_loss=0.000202, val_loss=0.015]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95, global step 12576: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96:  90%|████████▉ | 131/146 [00:02<00:00, 60.76it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 96:  90%|█████████ | 132/146 [00:02<00:00, 60.66it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96:  91%|█████████ | 133/146 [00:02<00:00, 60.84it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96:  92%|█████████▏| 134/146 [00:02<00:00, 60.99it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96:  92%|█████████▏| 135/146 [00:02<00:00, 61.16it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96:  93%|█████████▎| 136/146 [00:02<00:00, 61.31it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96:  94%|█████████▍| 137/146 [00:02<00:00, 61.46it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96:  95%|█████████▍| 138/146 [00:02<00:00, 61.66it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96:  95%|█████████▌| 139/146 [00:02<00:00, 61.80it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96:  96%|█████████▌| 140/146 [00:02<00:00, 61.92it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96:  97%|█████████▋| 141/146 [00:02<00:00, 62.06it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96:  97%|█████████▋| 142/146 [00:02<00:00, 62.19it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96:  98%|█████████▊| 143/146 [00:02<00:00, 62.36it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96:  99%|█████████▊| 144/146 [00:02<00:00, 62.52it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96:  99%|█████████▉| 145/146 [00:02<00:00, 62.68it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.015]\n",
      "Epoch 96: 100%|██████████| 146/146 [00:02<00:00, 62.66it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0149]\n",
      "Epoch 96: 100%|██████████| 146/146 [00:02<00:00, 62.55it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96, global step 12707: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97:  90%|████████▉ | 131/146 [00:02<00:00, 61.76it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 97:  90%|█████████ | 132/146 [00:02<00:00, 61.65it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97:  91%|█████████ | 133/146 [00:02<00:00, 61.80it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97:  92%|█████████▏| 134/146 [00:02<00:00, 61.98it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97:  92%|█████████▏| 135/146 [00:02<00:00, 62.15it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97:  93%|█████████▎| 136/146 [00:02<00:00, 62.35it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97:  94%|█████████▍| 137/146 [00:02<00:00, 62.52it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97:  95%|█████████▍| 138/146 [00:02<00:00, 62.69it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97:  95%|█████████▌| 139/146 [00:02<00:00, 62.89it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97:  96%|█████████▌| 140/146 [00:02<00:00, 63.06it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97:  97%|█████████▋| 141/146 [00:02<00:00, 63.25it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97:  97%|█████████▋| 142/146 [00:02<00:00, 63.42it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97:  98%|█████████▊| 143/146 [00:02<00:00, 63.61it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97:  99%|█████████▊| 144/146 [00:02<00:00, 63.80it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97:  99%|█████████▉| 145/146 [00:02<00:00, 63.98it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97: 100%|██████████| 146/146 [00:02<00:00, 63.95it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]\n",
      "Epoch 97: 100%|██████████| 146/146 [00:02<00:00, 63.83it/s, loss=0.000829, v_num=8, train_loss=0.000201, val_loss=0.0149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97, global step 12838: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98:  90%|████████▉ | 131/146 [00:02<00:00, 59.16it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 98:  90%|█████████ | 132/146 [00:02<00:00, 59.14it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98:  91%|█████████ | 133/146 [00:02<00:00, 59.29it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98:  92%|█████████▏| 134/146 [00:02<00:00, 59.45it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98:  92%|█████████▏| 135/146 [00:02<00:00, 59.55it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98:  93%|█████████▎| 136/146 [00:02<00:00, 59.70it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98:  94%|█████████▍| 137/146 [00:02<00:00, 59.93it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98:  95%|█████████▍| 138/146 [00:02<00:00, 60.10it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98:  95%|█████████▌| 139/146 [00:02<00:00, 60.27it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98:  96%|█████████▌| 140/146 [00:02<00:00, 60.47it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98:  97%|█████████▋| 141/146 [00:02<00:00, 60.61it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98:  97%|█████████▋| 142/146 [00:02<00:00, 60.84it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98:  98%|█████████▊| 143/146 [00:02<00:00, 60.98it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98:  99%|█████████▊| 144/146 [00:02<00:00, 61.12it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98:  99%|█████████▉| 145/146 [00:02<00:00, 61.18it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0149]\n",
      "Epoch 98: 100%|██████████| 146/146 [00:02<00:00, 61.21it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0148]\n",
      "Epoch 98: 100%|██████████| 146/146 [00:02<00:00, 61.08it/s, loss=0.000828, v_num=8, train_loss=0.000202, val_loss=0.0148]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98, global step 12969: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99:  90%|████████▉ | 131/146 [00:02<00:00, 60.42it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 99:  90%|█████████ | 132/146 [00:02<00:00, 60.30it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99:  91%|█████████ | 133/146 [00:02<00:00, 60.42it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99:  92%|█████████▏| 134/146 [00:02<00:00, 60.63it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99:  92%|█████████▏| 135/146 [00:02<00:00, 60.78it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99:  93%|█████████▎| 136/146 [00:02<00:00, 60.98it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99:  94%|█████████▍| 137/146 [00:02<00:00, 61.18it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99:  95%|█████████▍| 138/146 [00:02<00:00, 61.33it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99:  95%|█████████▌| 139/146 [00:02<00:00, 61.47it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99:  96%|█████████▌| 140/146 [00:02<00:00, 61.62it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99:  97%|█████████▋| 141/146 [00:02<00:00, 61.78it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99:  97%|█████████▋| 142/146 [00:02<00:00, 61.95it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99:  98%|█████████▊| 143/146 [00:02<00:00, 62.14it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99:  99%|█████████▊| 144/146 [00:02<00:00, 62.25it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99:  99%|█████████▉| 145/146 [00:02<00:00, 62.44it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99: 100%|██████████| 146/146 [00:02<00:00, 62.42it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]\n",
      "Epoch 99: 100%|██████████| 146/146 [00:02<00:00, 62.20it/s, loss=0.000829, v_num=8, train_loss=0.0002, val_loss=0.0148]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99, global step 13100: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:  90%|████████▉ | 131/146 [00:02<00:00, 60.59it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 100:  90%|█████████ | 132/146 [00:02<00:00, 60.52it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100:  91%|█████████ | 133/146 [00:02<00:00, 60.73it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100:  92%|█████████▏| 134/146 [00:02<00:00, 60.93it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100:  92%|█████████▏| 135/146 [00:02<00:00, 61.05it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100:  93%|█████████▎| 136/146 [00:02<00:00, 61.23it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100:  94%|█████████▍| 137/146 [00:02<00:00, 61.43it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100:  95%|█████████▍| 138/146 [00:02<00:00, 61.60it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100:  95%|█████████▌| 139/146 [00:02<00:00, 61.80it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100:  96%|█████████▌| 140/146 [00:02<00:00, 61.97it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100:  97%|█████████▋| 141/146 [00:02<00:00, 62.08it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100:  97%|█████████▋| 142/146 [00:02<00:00, 62.22it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100:  98%|█████████▊| 143/146 [00:02<00:00, 62.41it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100:  99%|█████████▊| 144/146 [00:02<00:00, 62.58it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100:  99%|█████████▉| 145/146 [00:02<00:00, 62.71it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0148]\n",
      "Epoch 100: 100%|██████████| 146/146 [00:02<00:00, 62.79it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0147]\n",
      "Epoch 100: 100%|██████████| 146/146 [00:02<00:00, 62.66it/s, loss=0.000828, v_num=8, train_loss=0.000203, val_loss=0.0147]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100, global step 13231: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101:  90%|████████▉ | 131/146 [00:02<00:00, 56.29it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 101:  90%|█████████ | 132/146 [00:02<00:00, 56.17it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101:  91%|█████████ | 133/146 [00:02<00:00, 56.26it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101:  92%|█████████▏| 134/146 [00:02<00:00, 56.37it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101:  92%|█████████▏| 135/146 [00:02<00:00, 56.50it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101:  93%|█████████▎| 136/146 [00:02<00:00, 56.59it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101:  94%|█████████▍| 137/146 [00:02<00:00, 56.70it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101:  95%|█████████▍| 138/146 [00:02<00:00, 56.81it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101:  95%|█████████▌| 139/146 [00:02<00:00, 56.94it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101:  96%|█████████▌| 140/146 [00:02<00:00, 57.07it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101:  97%|█████████▋| 141/146 [00:02<00:00, 57.22it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101:  97%|█████████▋| 142/146 [00:02<00:00, 57.32it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101:  98%|█████████▊| 143/146 [00:02<00:00, 57.47it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101:  99%|█████████▊| 144/146 [00:02<00:00, 57.57it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101:  99%|█████████▉| 145/146 [00:02<00:00, 57.67it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101: 100%|██████████| 146/146 [00:02<00:00, 57.61it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]\n",
      "Epoch 101: 100%|██████████| 146/146 [00:02<00:00, 57.45it/s, loss=0.000828, v_num=8, train_loss=0.0002, val_loss=0.0147]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101, global step 13362: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102:  90%|████████▉ | 131/146 [00:02<00:00, 55.86it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 102:  90%|█████████ | 132/146 [00:02<00:00, 55.81it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102:  91%|█████████ | 133/146 [00:02<00:00, 56.04it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102:  92%|█████████▏| 134/146 [00:02<00:00, 56.23it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102:  92%|█████████▏| 135/146 [00:02<00:00, 56.41it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102:  93%|█████████▎| 136/146 [00:02<00:00, 56.59it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102:  94%|█████████▍| 137/146 [00:02<00:00, 56.75it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102:  95%|█████████▍| 138/146 [00:02<00:00, 56.95it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102:  95%|█████████▌| 139/146 [00:02<00:00, 57.13it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102:  96%|█████████▌| 140/146 [00:02<00:00, 57.33it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102:  97%|█████████▋| 141/146 [00:02<00:00, 57.48it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102:  97%|█████████▋| 142/146 [00:02<00:00, 57.65it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102:  98%|█████████▊| 143/146 [00:02<00:00, 57.80it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102:  99%|█████████▊| 144/146 [00:02<00:00, 57.97it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102:  99%|█████████▉| 145/146 [00:02<00:00, 58.18it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0147]\n",
      "Epoch 102: 100%|██████████| 146/146 [00:02<00:00, 58.26it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0146]\n",
      "Epoch 102: 100%|██████████| 146/146 [00:02<00:00, 58.09it/s, loss=0.000827, v_num=8, train_loss=0.000202, val_loss=0.0146]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102, global step 13493: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103:  90%|████████▉ | 131/146 [00:02<00:00, 61.50it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 103:  90%|█████████ | 132/146 [00:02<00:00, 61.42it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103:  91%|█████████ | 133/146 [00:02<00:00, 61.63it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103:  92%|█████████▏| 134/146 [00:02<00:00, 61.78it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103:  92%|█████████▏| 135/146 [00:02<00:00, 61.95it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103:  93%|█████████▎| 136/146 [00:02<00:00, 62.12it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103:  94%|█████████▍| 137/146 [00:02<00:00, 62.32it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103:  95%|█████████▍| 138/146 [00:02<00:00, 62.47it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103:  95%|█████████▌| 139/146 [00:02<00:00, 62.66it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103:  96%|█████████▌| 140/146 [00:02<00:00, 62.83it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103:  97%|█████████▋| 141/146 [00:02<00:00, 63.05it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103:  97%|█████████▋| 142/146 [00:02<00:00, 63.19it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103:  98%|█████████▊| 143/146 [00:02<00:00, 63.35it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103:  99%|█████████▊| 144/146 [00:02<00:00, 63.52it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103:  99%|█████████▉| 145/146 [00:02<00:00, 63.62it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103: 100%|██████████| 146/146 [00:02<00:00, 63.64it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]\n",
      "Epoch 103: 100%|██████████| 146/146 [00:02<00:00, 63.50it/s, loss=0.000827, v_num=8, train_loss=0.0002, val_loss=0.0146]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103, global step 13624: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104:  90%|████████▉ | 131/146 [00:02<00:00, 60.11it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 104:  90%|█████████ | 132/146 [00:02<00:00, 60.00it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104:  91%|█████████ | 133/146 [00:02<00:00, 60.20it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104:  92%|█████████▏| 134/146 [00:02<00:00, 60.06it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104:  92%|█████████▏| 135/146 [00:02<00:00, 60.21it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104:  93%|█████████▎| 136/146 [00:02<00:00, 60.39it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104:  94%|█████████▍| 137/146 [00:02<00:00, 60.56it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104:  95%|█████████▍| 138/146 [00:02<00:00, 60.65it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104:  95%|█████████▌| 139/146 [00:02<00:00, 60.85it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104:  96%|█████████▌| 140/146 [00:02<00:00, 61.00it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104:  97%|█████████▋| 141/146 [00:02<00:00, 61.09it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104:  97%|█████████▋| 142/146 [00:02<00:00, 61.28it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104:  98%|█████████▊| 143/146 [00:02<00:00, 61.45it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104:  99%|█████████▊| 144/146 [00:02<00:00, 61.61it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104:  99%|█████████▉| 145/146 [00:02<00:00, 61.72it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0146]\n",
      "Epoch 104: 100%|██████████| 146/146 [00:02<00:00, 61.81it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0145]\n",
      "Epoch 104: 100%|██████████| 146/146 [00:02<00:00, 61.65it/s, loss=0.000826, v_num=8, train_loss=0.000203, val_loss=0.0145]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104, global step 13755: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105:  90%|████████▉ | 131/146 [00:02<00:00, 60.03it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 105:  90%|█████████ | 132/146 [00:02<00:00, 59.94it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105:  91%|█████████ | 133/146 [00:02<00:00, 60.15it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105:  92%|█████████▏| 134/146 [00:02<00:00, 60.25it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105:  92%|█████████▏| 135/146 [00:02<00:00, 60.45it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105:  93%|█████████▎| 136/146 [00:02<00:00, 60.66it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105:  94%|█████████▍| 137/146 [00:02<00:00, 60.83it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105:  95%|█████████▍| 138/146 [00:02<00:00, 61.03it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105:  95%|█████████▌| 139/146 [00:02<00:00, 61.20it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105:  96%|█████████▌| 140/146 [00:02<00:00, 61.32it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105:  97%|█████████▋| 141/146 [00:02<00:00, 61.51it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105:  97%|█████████▋| 142/146 [00:02<00:00, 61.68it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105:  98%|█████████▊| 143/146 [00:02<00:00, 61.85it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105:  99%|█████████▊| 144/146 [00:02<00:00, 61.98it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105:  99%|█████████▉| 145/146 [00:02<00:00, 62.15it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105: 100%|██████████| 146/146 [00:02<00:00, 62.15it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]\n",
      "Epoch 105: 100%|██████████| 146/146 [00:02<00:00, 62.04it/s, loss=0.000826, v_num=8, train_loss=0.000198, val_loss=0.0145]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105, global step 13886: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106:  90%|████████▉ | 131/146 [00:02<00:00, 61.35it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 106:  90%|█████████ | 132/146 [00:02<00:00, 61.22it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106:  91%|█████████ | 133/146 [00:02<00:00, 61.40it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106:  92%|█████████▏| 134/146 [00:02<00:00, 61.55it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106:  92%|█████████▏| 135/146 [00:02<00:00, 61.81it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106:  93%|█████████▎| 136/146 [00:02<00:00, 61.95it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106:  94%|█████████▍| 137/146 [00:02<00:00, 62.13it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106:  95%|█████████▍| 138/146 [00:02<00:00, 62.30it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106:  95%|█████████▌| 139/146 [00:02<00:00, 62.47it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106:  96%|█████████▌| 140/146 [00:02<00:00, 62.61it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106:  97%|█████████▋| 141/146 [00:02<00:00, 62.75it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106:  97%|█████████▋| 142/146 [00:02<00:00, 62.91it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106:  98%|█████████▊| 143/146 [00:02<00:00, 63.05it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106:  99%|█████████▊| 144/146 [00:02<00:00, 63.13it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106:  99%|█████████▉| 145/146 [00:02<00:00, 63.31it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106: 100%|██████████| 146/146 [00:02<00:00, 63.31it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]\n",
      "Epoch 106: 100%|██████████| 146/146 [00:02<00:00, 63.20it/s, loss=0.000826, v_num=8, train_loss=0.000197, val_loss=0.0145]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106, global step 14017: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107:  90%|████████▉ | 131/146 [00:02<00:00, 61.44it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 107:  90%|█████████ | 132/146 [00:02<00:00, 61.39it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107:  91%|█████████ | 133/146 [00:02<00:00, 61.54it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107:  92%|█████████▏| 134/146 [00:02<00:00, 61.75it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107:  92%|█████████▏| 135/146 [00:02<00:00, 61.92it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107:  93%|█████████▎| 136/146 [00:02<00:00, 62.12it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107:  94%|█████████▍| 137/146 [00:02<00:00, 62.27it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107:  95%|█████████▍| 138/146 [00:02<00:00, 62.50it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107:  95%|█████████▌| 139/146 [00:02<00:00, 62.61it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107:  96%|█████████▌| 140/146 [00:02<00:00, 62.78it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107:  97%|█████████▋| 141/146 [00:02<00:00, 62.94it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107:  97%|█████████▋| 142/146 [00:02<00:00, 63.08it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107:  98%|█████████▊| 143/146 [00:02<00:00, 63.24it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107:  99%|█████████▊| 144/146 [00:02<00:00, 63.40it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107:  99%|█████████▉| 145/146 [00:02<00:00, 63.54it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0145]\n",
      "Epoch 107: 100%|██████████| 146/146 [00:02<00:00, 63.50it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0144]\n",
      "Epoch 107: 100%|██████████| 146/146 [00:02<00:00, 63.34it/s, loss=0.000824, v_num=8, train_loss=0.000201, val_loss=0.0144]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107, global step 14148: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108:  90%|████████▉ | 131/146 [00:02<00:00, 53.40it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 108:  90%|█████████ | 132/146 [00:02<00:00, 53.26it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108:  91%|█████████ | 133/146 [00:02<00:00, 53.37it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108:  92%|█████████▏| 134/146 [00:02<00:00, 53.49it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108:  92%|█████████▏| 135/146 [00:02<00:00, 53.61it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108:  93%|█████████▎| 136/146 [00:02<00:00, 53.77it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108:  94%|█████████▍| 137/146 [00:02<00:00, 53.74it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108:  95%|█████████▍| 138/146 [00:02<00:00, 53.82it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108:  95%|█████████▌| 139/146 [00:02<00:00, 53.87it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108:  96%|█████████▌| 140/146 [00:02<00:00, 53.86it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108:  97%|█████████▋| 141/146 [00:02<00:00, 53.92it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108:  97%|█████████▋| 142/146 [00:02<00:00, 53.99it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108:  98%|█████████▊| 143/146 [00:02<00:00, 54.04it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108:  99%|█████████▊| 144/146 [00:02<00:00, 54.01it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108:  99%|█████████▉| 145/146 [00:02<00:00, 54.06it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0144]\n",
      "Epoch 108: 100%|██████████| 146/146 [00:02<00:00, 53.91it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0145]\n",
      "Epoch 108: 100%|██████████| 146/146 [00:02<00:00, 53.73it/s, loss=0.000825, v_num=8, train_loss=0.000194, val_loss=0.0145]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108, global step 14279: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109:  90%|████████▉ | 131/146 [00:02<00:00, 50.71it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 109:  90%|█████████ | 132/146 [00:02<00:00, 50.76it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109:  91%|█████████ | 133/146 [00:02<00:00, 50.97it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109:  92%|█████████▏| 134/146 [00:02<00:00, 51.16it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109:  92%|█████████▏| 135/146 [00:02<00:00, 51.34it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109:  93%|█████████▎| 136/146 [00:02<00:00, 51.53it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109:  94%|█████████▍| 137/146 [00:02<00:00, 51.75it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109:  95%|█████████▍| 138/146 [00:02<00:00, 51.95it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109:  95%|█████████▌| 139/146 [00:02<00:00, 52.13it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109:  96%|█████████▌| 140/146 [00:02<00:00, 52.33it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109:  97%|█████████▋| 141/146 [00:02<00:00, 52.53it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109:  97%|█████████▋| 142/146 [00:02<00:00, 52.70it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109:  98%|█████████▊| 143/146 [00:02<00:00, 52.90it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109:  99%|█████████▊| 144/146 [00:02<00:00, 53.07it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109:  99%|█████████▉| 145/146 [00:02<00:00, 53.24it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0145]\n",
      "Epoch 109: 100%|██████████| 146/146 [00:02<00:00, 53.33it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0143]\n",
      "Epoch 109: 100%|██████████| 146/146 [00:02<00:00, 53.20it/s, loss=0.000823, v_num=8, train_loss=0.000204, val_loss=0.0143]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109, global step 14410: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110:  90%|████████▉ | 131/146 [00:02<00:00, 59.65it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 110:  90%|█████████ | 132/146 [00:02<00:00, 59.59it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110:  91%|█████████ | 133/146 [00:02<00:00, 59.80it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110:  92%|█████████▏| 134/146 [00:02<00:00, 60.00it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110:  92%|█████████▏| 135/146 [00:02<00:00, 60.21it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110:  93%|█████████▎| 136/146 [00:02<00:00, 60.39it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110:  94%|█████████▍| 137/146 [00:02<00:00, 60.62it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110:  95%|█████████▍| 138/146 [00:02<00:00, 60.79it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110:  95%|█████████▌| 139/146 [00:02<00:00, 60.93it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110:  96%|█████████▌| 140/146 [00:02<00:00, 61.10it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110:  97%|█████████▋| 141/146 [00:02<00:00, 61.30it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110:  97%|█████████▋| 142/146 [00:02<00:00, 61.44it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110:  98%|█████████▊| 143/146 [00:02<00:00, 61.61it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110:  99%|█████████▊| 144/146 [00:02<00:00, 61.80it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110:  99%|█████████▉| 145/146 [00:02<00:00, 61.93it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0143]\n",
      "Epoch 110: 100%|██████████| 146/146 [00:02<00:00, 61.91it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0146]\n",
      "Epoch 110: 100%|██████████| 146/146 [00:02<00:00, 61.81it/s, loss=0.000826, v_num=8, train_loss=0.000188, val_loss=0.0146]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110, global step 14541: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111:  90%|████████▉ | 131/146 [00:02<00:00, 57.20it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 111:  90%|█████████ | 132/146 [00:02<00:00, 57.14it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111:  91%|█████████ | 133/146 [00:02<00:00, 57.37it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111:  92%|█████████▏| 134/146 [00:02<00:00, 57.56it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111:  92%|█████████▏| 135/146 [00:02<00:00, 57.76it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111:  93%|█████████▎| 136/146 [00:02<00:00, 57.94it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111:  94%|█████████▍| 137/146 [00:02<00:00, 58.12it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111:  95%|█████████▍| 138/146 [00:02<00:00, 58.32it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111:  95%|█████████▌| 139/146 [00:02<00:00, 58.50it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111:  96%|█████████▌| 140/146 [00:02<00:00, 58.65it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111:  97%|█████████▋| 141/146 [00:02<00:00, 58.82it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111:  97%|█████████▋| 142/146 [00:02<00:00, 59.01it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111:  98%|█████████▊| 143/146 [00:02<00:00, 59.16it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111:  99%|█████████▊| 144/146 [00:02<00:00, 59.33it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111:  99%|█████████▉| 145/146 [00:02<00:00, 59.45it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0146]\n",
      "Epoch 111: 100%|██████████| 146/146 [00:02<00:00, 59.51it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0142]\n",
      "Epoch 111: 100%|██████████| 146/146 [00:02<00:00, 59.42it/s, loss=0.000823, v_num=8, train_loss=0.000211, val_loss=0.0142]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111, global step 14672: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112:  90%|████████▉ | 131/146 [00:02<00:00, 61.73it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 112:  90%|█████████ | 132/146 [00:02<00:00, 61.68it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112:  91%|█████████ | 133/146 [00:02<00:00, 61.86it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112:  92%|█████████▏| 134/146 [00:02<00:00, 62.04it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112:  92%|█████████▏| 135/146 [00:02<00:00, 62.24it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112:  93%|█████████▎| 136/146 [00:02<00:00, 62.44it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112:  94%|█████████▍| 137/146 [00:02<00:00, 62.61it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112:  95%|█████████▍| 138/146 [00:02<00:00, 62.81it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112:  95%|█████████▌| 139/146 [00:02<00:00, 63.07it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112:  96%|█████████▌| 140/146 [00:02<00:00, 63.29it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112:  97%|█████████▋| 141/146 [00:02<00:00, 63.46it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112:  97%|█████████▋| 142/146 [00:02<00:00, 63.62it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112:  98%|█████████▊| 143/146 [00:02<00:00, 63.81it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112:  99%|█████████▊| 144/146 [00:02<00:00, 64.00it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112:  99%|█████████▉| 145/146 [00:02<00:00, 64.16it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0142]\n",
      "Epoch 112: 100%|██████████| 146/146 [00:02<00:00, 64.12it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0148]\n",
      "Epoch 112: 100%|██████████| 146/146 [00:02<00:00, 63.95it/s, loss=0.000829, v_num=8, train_loss=0.000181, val_loss=0.0148]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112, global step 14803: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113:  90%|████████▉ | 131/146 [00:02<00:00, 59.62it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 113:  90%|█████████ | 132/146 [00:02<00:00, 59.54it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113:  91%|█████████ | 133/146 [00:02<00:00, 59.69it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113:  92%|█████████▏| 134/146 [00:02<00:00, 59.82it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113:  92%|█████████▏| 135/146 [00:02<00:00, 59.94it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113:  93%|█████████▎| 136/146 [00:02<00:00, 60.09it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113:  94%|█████████▍| 137/146 [00:02<00:00, 60.24it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113:  95%|█████████▍| 138/146 [00:02<00:00, 60.42it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113:  95%|█████████▌| 139/146 [00:02<00:00, 60.56it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113:  96%|█████████▌| 140/146 [00:02<00:00, 60.73it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113:  97%|█████████▋| 141/146 [00:02<00:00, 60.85it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113:  97%|█████████▋| 142/146 [00:02<00:00, 61.02it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113:  98%|█████████▊| 143/146 [00:02<00:00, 61.13it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113:  99%|█████████▊| 144/146 [00:02<00:00, 61.25it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113:  99%|█████████▉| 145/146 [00:02<00:00, 61.33it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0148]\n",
      "Epoch 113: 100%|██████████| 146/146 [00:02<00:00, 61.26it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0141]\n",
      "Epoch 113: 100%|██████████| 146/146 [00:02<00:00, 61.11it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0141]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113, global step 14934: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114:  90%|████████▉ | 131/146 [00:02<00:00, 48.03it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 114:  90%|█████████ | 132/146 [00:02<00:00, 47.81it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114:  91%|█████████ | 133/146 [00:02<00:00, 47.94it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114:  92%|█████████▏| 134/146 [00:02<00:00, 48.06it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114:  92%|█████████▏| 135/146 [00:02<00:00, 48.19it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114:  93%|█████████▎| 136/146 [00:02<00:00, 48.26it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114:  94%|█████████▍| 137/146 [00:02<00:00, 48.36it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114:  95%|█████████▍| 138/146 [00:02<00:00, 48.43it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114:  95%|█████████▌| 139/146 [00:02<00:00, 48.51it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114:  96%|█████████▌| 140/146 [00:02<00:00, 48.62it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114:  97%|█████████▋| 141/146 [00:02<00:00, 48.68it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114:  97%|█████████▋| 142/146 [00:02<00:00, 48.79it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114:  98%|█████████▊| 143/146 [00:02<00:00, 48.92it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114:  99%|█████████▊| 144/146 [00:02<00:00, 49.03it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114:  99%|█████████▉| 145/146 [00:02<00:00, 49.10it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0141]\n",
      "Epoch 114: 100%|██████████| 146/146 [00:02<00:00, 48.96it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0151]\n",
      "Epoch 114: 100%|██████████| 146/146 [00:02<00:00, 48.83it/s, loss=0.000833, v_num=8, train_loss=0.000176, val_loss=0.0151]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114, global step 15065: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115:  90%|████████▉ | 131/146 [00:02<00:00, 46.68it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 115:  90%|█████████ | 132/146 [00:02<00:00, 46.71it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115:  91%|█████████ | 133/146 [00:02<00:00, 46.93it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115:  92%|█████████▏| 134/146 [00:02<00:00, 47.06it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115:  92%|█████████▏| 135/146 [00:02<00:00, 47.27it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115:  93%|█████████▎| 136/146 [00:02<00:00, 47.42it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115:  94%|█████████▍| 137/146 [00:02<00:00, 47.57it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115:  95%|█████████▍| 138/146 [00:02<00:00, 47.78it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115:  95%|█████████▌| 139/146 [00:02<00:00, 47.96it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115:  96%|█████████▌| 140/146 [00:02<00:00, 48.12it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115:  97%|█████████▋| 141/146 [00:02<00:00, 48.28it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115:  97%|█████████▋| 142/146 [00:02<00:00, 48.43it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115:  98%|█████████▊| 143/146 [00:02<00:00, 48.62it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115:  99%|█████████▊| 144/146 [00:02<00:00, 48.78it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115:  99%|█████████▉| 145/146 [00:02<00:00, 48.90it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.0151]\n",
      "Epoch 115: 100%|██████████| 146/146 [00:02<00:00, 48.99it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.014] \n",
      "Epoch 115: 100%|██████████| 146/146 [00:02<00:00, 48.91it/s, loss=0.000824, v_num=8, train_loss=0.000216, val_loss=0.014]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115, global step 15196: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116:  90%|████████▉ | 131/146 [00:02<00:00, 59.87it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 116:  90%|█████████ | 132/146 [00:02<00:00, 59.75it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116:  91%|█████████ | 133/146 [00:02<00:00, 59.93it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116:  92%|█████████▏| 134/146 [00:02<00:00, 60.14it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116:  92%|█████████▏| 135/146 [00:02<00:00, 60.32it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116:  93%|█████████▎| 136/146 [00:02<00:00, 60.52it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116:  94%|█████████▍| 137/146 [00:02<00:00, 60.72it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116:  95%|█████████▍| 138/146 [00:02<00:00, 60.90it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116:  95%|█████████▌| 139/146 [00:02<00:00, 61.01it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116:  96%|█████████▌| 140/146 [00:02<00:00, 61.18it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116:  97%|█████████▋| 141/146 [00:02<00:00, 61.30it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116:  97%|█████████▋| 142/146 [00:02<00:00, 61.47it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116:  98%|█████████▊| 143/146 [00:02<00:00, 61.61it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116:  99%|█████████▊| 144/146 [00:02<00:00, 61.77it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116:  99%|█████████▉| 145/146 [00:02<00:00, 61.88it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 116: 100%|██████████| 146/146 [00:02<00:00, 61.91it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.0153]\n",
      "Epoch 116: 100%|██████████| 146/146 [00:02<00:00, 61.81it/s, loss=0.000835, v_num=8, train_loss=0.000172, val_loss=0.0153]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116, global step 15327: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117:  90%|████████▉ | 131/146 [00:02<00:00, 61.56it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 117:  90%|█████████ | 132/146 [00:02<00:00, 61.42it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117:  91%|█████████ | 133/146 [00:02<00:00, 61.66it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117:  92%|█████████▏| 134/146 [00:02<00:00, 61.83it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117:  92%|█████████▏| 135/146 [00:02<00:00, 62.01it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117:  93%|█████████▎| 136/146 [00:02<00:00, 62.21it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117:  94%|█████████▍| 137/146 [00:02<00:00, 62.38it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117:  95%|█████████▍| 138/146 [00:02<00:00, 62.55it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117:  95%|█████████▌| 139/146 [00:02<00:00, 62.72it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117:  96%|█████████▌| 140/146 [00:02<00:00, 62.86it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117:  97%|█████████▋| 141/146 [00:02<00:00, 63.03it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117:  97%|█████████▋| 142/146 [00:02<00:00, 63.13it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117:  98%|█████████▊| 143/146 [00:02<00:00, 63.33it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117:  99%|█████████▊| 144/146 [00:02<00:00, 63.52it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117:  99%|█████████▉| 145/146 [00:02<00:00, 63.70it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.0153]\n",
      "Epoch 117: 100%|██████████| 146/146 [00:02<00:00, 63.75it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.014] \n",
      "Epoch 117: 100%|██████████| 146/146 [00:02<00:00, 63.58it/s, loss=0.000823, v_num=8, train_loss=0.000216, val_loss=0.014]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117, global step 15458: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118:  90%|████████▉ | 131/146 [00:02<00:00, 59.76it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 118:  90%|█████████ | 132/146 [00:02<00:00, 59.70it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118:  91%|█████████ | 133/146 [00:02<00:00, 59.91it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118:  92%|█████████▏| 134/146 [00:02<00:00, 60.03it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118:  92%|█████████▏| 135/146 [00:02<00:00, 60.21it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118:  93%|█████████▎| 136/146 [00:02<00:00, 60.41it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118:  94%|█████████▍| 137/146 [00:02<00:00, 60.56it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118:  95%|█████████▍| 138/146 [00:02<00:00, 60.71it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118:  95%|█████████▌| 139/146 [00:02<00:00, 60.88it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118:  96%|█████████▌| 140/146 [00:02<00:00, 61.00it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118:  97%|█████████▋| 141/146 [00:02<00:00, 61.19it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118:  97%|█████████▋| 142/146 [00:02<00:00, 61.33it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118:  98%|█████████▊| 143/146 [00:02<00:00, 61.47it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118:  99%|█████████▊| 144/146 [00:02<00:00, 61.61it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118:  99%|█████████▉| 145/146 [00:02<00:00, 61.75it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 118: 100%|██████████| 146/146 [00:02<00:00, 61.73it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.0152]\n",
      "Epoch 118: 100%|██████████| 146/146 [00:02<00:00, 61.62it/s, loss=0.000831, v_num=8, train_loss=0.000171, val_loss=0.0152]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118, global step 15589: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119:  90%|████████▉ | 131/146 [00:02<00:00, 60.76it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 119:  90%|█████████ | 132/146 [00:02<00:00, 60.69it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119:  91%|█████████ | 133/146 [00:02<00:00, 60.84it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119:  92%|█████████▏| 134/146 [00:02<00:00, 60.96it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119:  92%|█████████▏| 135/146 [00:02<00:00, 61.14it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119:  93%|█████████▎| 136/146 [00:02<00:00, 61.34it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119:  94%|█████████▍| 137/146 [00:02<00:00, 61.51it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119:  95%|█████████▍| 138/146 [00:02<00:00, 61.66it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119:  95%|█████████▌| 139/146 [00:02<00:00, 61.83it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119:  96%|█████████▌| 140/146 [00:02<00:00, 62.00it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119:  97%|█████████▋| 141/146 [00:02<00:00, 62.11it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119:  97%|█████████▋| 142/146 [00:02<00:00, 62.22it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119:  98%|█████████▊| 143/146 [00:02<00:00, 62.41it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119:  99%|█████████▊| 144/146 [00:02<00:00, 62.55it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119:  99%|█████████▉| 145/146 [00:02<00:00, 62.71it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.0152]\n",
      "Epoch 119: 100%|██████████| 146/146 [00:02<00:00, 62.74it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.014] \n",
      "Epoch 119: 100%|██████████| 146/146 [00:02<00:00, 62.58it/s, loss=0.000814, v_num=8, train_loss=0.000213, val_loss=0.014]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119, global step 15720: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120:  90%|████████▉ | 131/146 [00:02<00:00, 60.00it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 120:  90%|█████████ | 132/146 [00:02<00:00, 59.89it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120:  91%|█████████ | 133/146 [00:02<00:00, 60.01it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120:  92%|█████████▏| 134/146 [00:02<00:00, 60.09it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120:  92%|█████████▏| 135/146 [00:02<00:00, 60.26it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120:  93%|█████████▎| 136/146 [00:02<00:00, 60.41it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120:  94%|█████████▍| 137/146 [00:02<00:00, 60.48it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120:  95%|█████████▍| 138/146 [00:02<00:00, 60.65it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120:  95%|█████████▌| 139/146 [00:02<00:00, 60.69it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120:  96%|█████████▌| 140/146 [00:02<00:00, 60.81it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120:  97%|█████████▋| 141/146 [00:02<00:00, 60.88it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120:  97%|█████████▋| 142/146 [00:02<00:00, 60.97it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120:  98%|█████████▊| 143/146 [00:02<00:00, 60.92it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120:  99%|█████████▊| 144/146 [00:02<00:00, 61.04it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120:  99%|█████████▉| 145/146 [00:02<00:00, 61.13it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.014]\n",
      "Epoch 120: 100%|██████████| 146/146 [00:02<00:00, 61.19it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.015]\n",
      "Epoch 120: 100%|██████████| 146/146 [00:02<00:00, 61.03it/s, loss=0.000821, v_num=8, train_loss=0.000172, val_loss=0.015]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120, global step 15851: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121:  90%|████████▉ | 131/146 [00:02<00:00, 49.71it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 121:  90%|█████████ | 132/146 [00:02<00:00, 49.73it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121:  91%|█████████ | 133/146 [00:02<00:00, 49.92it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121:  92%|█████████▏| 134/146 [00:02<00:00, 50.16it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121:  92%|█████████▏| 135/146 [00:02<00:00, 50.29it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121:  93%|█████████▎| 136/146 [00:02<00:00, 50.48it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121:  94%|█████████▍| 137/146 [00:02<00:00, 50.64it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121:  95%|█████████▍| 138/146 [00:02<00:00, 50.83it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121:  95%|█████████▌| 139/146 [00:02<00:00, 51.01it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121:  96%|█████████▌| 140/146 [00:02<00:00, 51.20it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121:  97%|█████████▋| 141/146 [00:02<00:00, 51.40it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121:  97%|█████████▋| 142/146 [00:02<00:00, 51.60it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121:  98%|█████████▊| 143/146 [00:02<00:00, 51.77it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121:  99%|█████████▊| 144/146 [00:02<00:00, 51.98it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121:  99%|█████████▉| 145/146 [00:02<00:00, 52.12it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.015]\n",
      "Epoch 121: 100%|██████████| 146/146 [00:02<00:00, 52.25it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.0142]\n",
      "Epoch 121: 100%|██████████| 146/146 [00:02<00:00, 52.16it/s, loss=0.000809, v_num=8, train_loss=0.000206, val_loss=0.0142]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121, global step 15982: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122:  90%|████████▉ | 131/146 [00:02<00:00, 60.20it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 122:  90%|█████████ | 132/146 [00:02<00:00, 60.13it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122:  91%|█████████ | 133/146 [00:02<00:00, 60.26it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122:  92%|█████████▏| 134/146 [00:02<00:00, 60.44it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122:  92%|█████████▏| 135/146 [00:02<00:00, 60.62it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122:  93%|█████████▎| 136/146 [00:02<00:00, 60.82it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122:  94%|█████████▍| 137/146 [00:02<00:00, 60.97it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122:  95%|█████████▍| 138/146 [00:02<00:00, 61.14it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122:  95%|█████████▌| 139/146 [00:02<00:00, 61.34it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122:  96%|█████████▌| 140/146 [00:02<00:00, 61.51it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122:  97%|█████████▋| 141/146 [00:02<00:00, 61.65it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122:  97%|█████████▋| 142/146 [00:02<00:00, 61.82it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122:  98%|█████████▊| 143/146 [00:02<00:00, 62.03it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122:  99%|█████████▊| 144/146 [00:02<00:00, 62.17it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122:  99%|█████████▉| 145/146 [00:02<00:00, 62.33it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0142]\n",
      "Epoch 122: 100%|██████████| 146/146 [00:02<00:00, 62.28it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0146]\n",
      "Epoch 122: 100%|██████████| 146/146 [00:02<00:00, 62.18it/s, loss=0.000811, v_num=8, train_loss=0.000176, val_loss=0.0146]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122, global step 16113: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123:  90%|████████▉ | 131/146 [00:02<00:00, 61.10it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 123:  90%|█████████ | 132/146 [00:02<00:00, 61.02it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123:  91%|█████████ | 133/146 [00:02<00:00, 61.20it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123:  92%|█████████▏| 134/146 [00:02<00:00, 61.32it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123:  92%|█████████▏| 135/146 [00:02<00:00, 61.50it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123:  93%|█████████▎| 136/146 [00:02<00:00, 61.70it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123:  94%|█████████▍| 137/146 [00:02<00:00, 61.90it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123:  95%|█████████▍| 138/146 [00:02<00:00, 62.05it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123:  95%|█████████▌| 139/146 [00:02<00:00, 62.22it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123:  96%|█████████▌| 140/146 [00:02<00:00, 62.38it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123:  97%|█████████▋| 141/146 [00:02<00:00, 62.50it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123:  97%|█████████▋| 142/146 [00:02<00:00, 62.66it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123:  98%|█████████▊| 143/146 [00:02<00:00, 62.83it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123:  99%|█████████▊| 144/146 [00:02<00:00, 62.99it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123:  99%|█████████▉| 145/146 [00:02<00:00, 63.18it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0146]\n",
      "Epoch 123: 100%|██████████| 146/146 [00:02<00:00, 63.23it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0143]\n",
      "Epoch 123: 100%|██████████| 146/146 [00:02<00:00, 63.09it/s, loss=0.000809, v_num=8, train_loss=0.000194, val_loss=0.0143]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 123, global step 16244: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124:  90%|████████▉ | 131/146 [00:02<00:00, 59.22it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 124:  90%|█████████ | 132/146 [00:02<00:00, 59.16it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124:  91%|█████████ | 133/146 [00:02<00:00, 59.34it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124:  92%|█████████▏| 134/146 [00:02<00:00, 59.52it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124:  92%|█████████▏| 135/146 [00:02<00:00, 59.73it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124:  93%|█████████▎| 136/146 [00:02<00:00, 59.88it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124:  94%|█████████▍| 137/146 [00:02<00:00, 60.08it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124:  95%|█████████▍| 138/146 [00:02<00:00, 60.26it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124:  95%|█████████▌| 139/146 [00:02<00:00, 60.46it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124:  96%|█████████▌| 140/146 [00:02<00:00, 60.63it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124:  97%|█████████▋| 141/146 [00:02<00:00, 60.75it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124:  97%|█████████▋| 142/146 [00:02<00:00, 60.91it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124:  98%|█████████▊| 143/146 [00:02<00:00, 61.11it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124:  99%|█████████▊| 144/146 [00:02<00:00, 61.22it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124:  99%|█████████▉| 145/146 [00:02<00:00, 61.38it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124: 100%|██████████| 146/146 [00:02<00:00, 61.42it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]\n",
      "Epoch 124: 100%|██████████| 146/146 [00:02<00:00, 61.29it/s, loss=0.000807, v_num=8, train_loss=0.000182, val_loss=0.0143]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 124, global step 16375: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125:  90%|████████▉ | 131/146 [00:02<00:00, 61.04it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 125:  90%|█████████ | 132/146 [00:02<00:00, 60.97it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125:  91%|█████████ | 133/146 [00:02<00:00, 61.14it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125:  92%|█████████▏| 134/146 [00:02<00:00, 61.35it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125:  92%|█████████▏| 135/146 [00:02<00:00, 61.50it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125:  93%|█████████▎| 136/146 [00:02<00:00, 61.67it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125:  94%|█████████▍| 137/146 [00:02<00:00, 61.76it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125:  95%|█████████▍| 138/146 [00:02<00:00, 61.96it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125:  95%|█████████▌| 139/146 [00:02<00:00, 62.16it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125:  96%|█████████▌| 140/146 [00:02<00:00, 62.30it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125:  97%|█████████▋| 141/146 [00:02<00:00, 62.41it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125:  97%|█████████▋| 142/146 [00:02<00:00, 62.55it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125:  98%|█████████▊| 143/146 [00:02<00:00, 62.74it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125:  99%|█████████▊| 144/146 [00:02<00:00, 62.90it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125:  99%|█████████▉| 145/146 [00:02<00:00, 63.09it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0143]\n",
      "Epoch 125: 100%|██████████| 146/146 [00:02<00:00, 63.14it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0144]\n",
      "Epoch 125: 100%|██████████| 146/146 [00:02<00:00, 62.87it/s, loss=0.000811, v_num=8, train_loss=0.000184, val_loss=0.0144]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 125, global step 16506: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126:  90%|████████▉ | 131/146 [00:02<00:00, 59.51it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 126:  90%|█████████ | 132/146 [00:02<00:00, 59.43it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126:  91%|█████████ | 133/146 [00:02<00:00, 59.61it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126:  92%|█████████▏| 134/146 [00:02<00:00, 59.79it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126:  92%|█████████▏| 135/146 [00:02<00:00, 59.97it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126:  93%|█████████▎| 136/146 [00:02<00:00, 60.17it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126:  94%|█████████▍| 137/146 [00:02<00:00, 60.35it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126:  95%|█████████▍| 138/146 [00:02<00:00, 60.52it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126:  95%|█████████▌| 139/146 [00:02<00:00, 60.67it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126:  96%|█████████▌| 140/146 [00:02<00:00, 60.87it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126:  97%|█████████▋| 141/146 [00:02<00:00, 61.03it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126:  97%|█████████▋| 142/146 [00:02<00:00, 61.28it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126:  98%|█████████▊| 143/146 [00:02<00:00, 61.42it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126:  99%|█████████▊| 144/146 [00:02<00:00, 61.53it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126:  99%|█████████▉| 145/146 [00:02<00:00, 61.70it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0144]\n",
      "Epoch 126: 100%|██████████| 146/146 [00:02<00:00, 61.73it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0142]\n",
      "Epoch 126: 100%|██████████| 146/146 [00:02<00:00, 61.60it/s, loss=0.000807, v_num=8, train_loss=0.000189, val_loss=0.0142]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 126, global step 16637: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127:  90%|████████▉ | 131/146 [00:02<00:00, 50.52it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 127:  90%|█████████ | 132/146 [00:02<00:00, 50.38it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127:  91%|█████████ | 133/146 [00:02<00:00, 50.53it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127:  92%|█████████▏| 134/146 [00:02<00:00, 50.73it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127:  92%|█████████▏| 135/146 [00:02<00:00, 50.79it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127:  93%|█████████▎| 136/146 [00:02<00:00, 50.93it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127:  94%|█████████▍| 137/146 [00:02<00:00, 51.04it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127:  95%|█████████▍| 138/146 [00:02<00:00, 51.13it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127:  95%|█████████▌| 139/146 [00:02<00:00, 51.21it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127:  96%|█████████▌| 140/146 [00:02<00:00, 51.30it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127:  97%|█████████▋| 141/146 [00:02<00:00, 51.42it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127:  97%|█████████▋| 142/146 [00:02<00:00, 51.56it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127:  98%|█████████▊| 143/146 [00:02<00:00, 51.68it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127:  99%|█████████▊| 144/146 [00:02<00:00, 51.83it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127:  99%|█████████▉| 145/146 [00:02<00:00, 52.02it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 127: 100%|██████████| 146/146 [00:02<00:00, 52.03it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0144]\n",
      "Epoch 127: 100%|██████████| 146/146 [00:02<00:00, 51.84it/s, loss=0.000811, v_num=8, train_loss=0.000179, val_loss=0.0144]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 127, global step 16768: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128:  90%|████████▉ | 131/146 [00:02<00:00, 55.01it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 128:  90%|█████████ | 132/146 [00:02<00:00, 54.97it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128:  91%|█████████ | 133/146 [00:02<00:00, 55.16it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128:  92%|█████████▏| 134/146 [00:02<00:00, 55.34it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128:  92%|█████████▏| 135/146 [00:02<00:00, 55.57it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128:  93%|█████████▎| 136/146 [00:02<00:00, 55.76it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128:  94%|█████████▍| 137/146 [00:02<00:00, 55.91it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128:  95%|█████████▍| 138/146 [00:02<00:00, 55.84it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128:  95%|█████████▌| 139/146 [00:02<00:00, 56.02it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128:  96%|█████████▌| 140/146 [00:02<00:00, 56.22it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128:  97%|█████████▋| 141/146 [00:02<00:00, 56.37it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128:  97%|█████████▋| 142/146 [00:02<00:00, 56.50it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128:  98%|█████████▊| 143/146 [00:02<00:00, 56.72it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128:  99%|█████████▊| 144/146 [00:02<00:00, 56.91it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128:  99%|█████████▉| 145/146 [00:02<00:00, 57.02it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0144]\n",
      "Epoch 128: 100%|██████████| 146/146 [00:02<00:00, 57.09it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0141]\n",
      "Epoch 128: 100%|██████████| 146/146 [00:02<00:00, 57.00it/s, loss=0.00081, v_num=8, train_loss=0.000196, val_loss=0.0141]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 128, global step 16899: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129:  90%|████████▉ | 131/146 [00:02<00:00, 59.57it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 129:  90%|█████████ | 132/146 [00:02<00:00, 59.48it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129:  91%|█████████ | 133/146 [00:02<00:00, 59.69it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129:  92%|█████████▏| 134/146 [00:02<00:00, 59.84it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129:  92%|█████████▏| 135/146 [00:02<00:00, 60.02it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129:  93%|█████████▎| 136/146 [00:02<00:00, 60.23it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129:  94%|█████████▍| 137/146 [00:02<00:00, 60.37it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129:  95%|█████████▍| 138/146 [00:02<00:00, 60.55it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129:  95%|█████████▌| 139/146 [00:02<00:00, 60.75it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129:  96%|█████████▌| 140/146 [00:02<00:00, 60.87it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129:  97%|█████████▋| 141/146 [00:02<00:00, 61.01it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129:  97%|█████████▋| 142/146 [00:02<00:00, 61.15it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129:  98%|█████████▊| 143/146 [00:02<00:00, 61.37it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129:  99%|█████████▊| 144/146 [00:02<00:00, 61.48it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129:  99%|█████████▉| 145/146 [00:02<00:00, 61.59it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 129: 100%|██████████| 146/146 [00:02<00:00, 61.55it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0144]\n",
      "Epoch 129: 100%|██████████| 146/146 [00:02<00:00, 61.44it/s, loss=0.00081, v_num=8, train_loss=0.000174, val_loss=0.0144]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 129, global step 17030: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130:  90%|████████▉ | 131/146 [00:02<00:00, 61.21it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 130:  90%|█████████ | 132/146 [00:02<00:00, 61.19it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130:  91%|█████████ | 133/146 [00:02<00:00, 61.37it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130:  92%|█████████▏| 134/146 [00:02<00:00, 61.58it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130:  92%|█████████▏| 135/146 [00:02<00:00, 61.75it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130:  93%|█████████▎| 136/146 [00:02<00:00, 61.90it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130:  94%|█████████▍| 137/146 [00:02<00:00, 62.13it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130:  95%|█████████▍| 138/146 [00:02<00:00, 62.30it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130:  95%|█████████▌| 139/146 [00:02<00:00, 62.44it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130:  96%|█████████▌| 140/146 [00:02<00:00, 62.61it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130:  97%|█████████▋| 141/146 [00:02<00:00, 62.80it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130:  97%|█████████▋| 142/146 [00:02<00:00, 62.94it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130:  98%|█████████▊| 143/146 [00:02<00:00, 63.13it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130:  99%|█████████▊| 144/146 [00:02<00:00, 63.32it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130:  99%|█████████▉| 145/146 [00:02<00:00, 63.51it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0144]\n",
      "Epoch 130: 100%|██████████| 146/146 [00:02<00:00, 63.47it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0141]\n",
      "Epoch 130: 100%|██████████| 146/146 [00:02<00:00, 63.34it/s, loss=0.000808, v_num=8, train_loss=0.000195, val_loss=0.0141]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 130, global step 17161: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131:  90%|████████▉ | 131/146 [00:02<00:00, 60.20it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 131:  90%|█████████ | 132/146 [00:02<00:00, 60.16it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131:  91%|█████████ | 133/146 [00:02<00:00, 60.34it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131:  92%|█████████▏| 134/146 [00:02<00:00, 60.55it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131:  92%|█████████▏| 135/146 [00:02<00:00, 60.70it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131:  93%|█████████▎| 136/146 [00:02<00:00, 60.87it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131:  94%|█████████▍| 137/146 [00:02<00:00, 61.05it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131:  95%|█████████▍| 138/146 [00:02<00:00, 61.22it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131:  95%|█████████▌| 139/146 [00:02<00:00, 61.39it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131:  96%|█████████▌| 140/146 [00:02<00:00, 61.56it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131:  97%|█████████▋| 141/146 [00:02<00:00, 61.70it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131:  97%|█████████▋| 142/146 [00:02<00:00, 61.87it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131:  98%|█████████▊| 143/146 [00:02<00:00, 62.06it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131:  99%|█████████▊| 144/146 [00:02<00:00, 62.17it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131:  99%|█████████▉| 145/146 [00:02<00:00, 62.39it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0141]\n",
      "Epoch 131: 100%|██████████| 146/146 [00:02<00:00, 62.44it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0144]\n",
      "Epoch 131: 100%|██████████| 146/146 [00:02<00:00, 62.31it/s, loss=0.000807, v_num=8, train_loss=0.000174, val_loss=0.0144]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131, global step 17292: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132:  90%|████████▉ | 131/146 [00:02<00:00, 60.36it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 132:  90%|█████████ | 132/146 [00:02<00:00, 60.24it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132:  91%|█████████ | 133/146 [00:02<00:00, 60.45it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132:  92%|█████████▏| 134/146 [00:02<00:00, 60.60it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132:  92%|█████████▏| 135/146 [00:02<00:00, 60.83it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132:  93%|█████████▎| 136/146 [00:02<00:00, 60.98it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132:  94%|█████████▍| 137/146 [00:02<00:00, 61.18it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132:  95%|█████████▍| 138/146 [00:02<00:00, 61.33it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132:  95%|█████████▌| 139/146 [00:02<00:00, 61.50it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132:  96%|█████████▌| 140/146 [00:02<00:00, 61.59it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132:  97%|█████████▋| 141/146 [00:02<00:00, 61.78it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132:  97%|█████████▋| 142/146 [00:02<00:00, 61.90it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132:  98%|█████████▊| 143/146 [00:02<00:00, 62.03it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132:  99%|█████████▊| 144/146 [00:02<00:00, 62.25it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132:  99%|█████████▉| 145/146 [00:02<00:00, 62.39it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0144]\n",
      "Epoch 132: 100%|██████████| 146/146 [00:02<00:00, 62.42it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0142]\n",
      "Epoch 132: 100%|██████████| 146/146 [00:02<00:00, 62.20it/s, loss=0.000806, v_num=8, train_loss=0.000187, val_loss=0.0142]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132, global step 17423: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133:  90%|████████▉ | 131/146 [00:02<00:00, 57.19it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 133:  90%|█████████ | 132/146 [00:02<00:00, 57.16it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133:  91%|█████████ | 133/146 [00:02<00:00, 57.32it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133:  92%|█████████▏| 134/146 [00:02<00:00, 57.48it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133:  92%|█████████▏| 135/146 [00:02<00:00, 57.58it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133:  93%|█████████▎| 136/146 [00:02<00:00, 57.74it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133:  94%|█████████▍| 137/146 [00:02<00:00, 57.87it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133:  95%|█████████▍| 138/146 [00:02<00:00, 57.90it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133:  95%|█████████▌| 139/146 [00:02<00:00, 58.03it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133:  96%|█████████▌| 140/146 [00:02<00:00, 58.13it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133:  97%|█████████▋| 141/146 [00:02<00:00, 58.28it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133:  97%|█████████▋| 142/146 [00:02<00:00, 58.38it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133:  98%|█████████▊| 143/146 [00:02<00:00, 58.45it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133:  99%|█████████▊| 144/146 [00:02<00:00, 58.55it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133:  99%|█████████▉| 145/146 [00:02<00:00, 58.62it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0142]\n",
      "Epoch 133: 100%|██████████| 146/146 [00:02<00:00, 58.41it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0141]\n",
      "Epoch 133: 100%|██████████| 146/146 [00:02<00:00, 58.30it/s, loss=0.000803, v_num=8, train_loss=0.000177, val_loss=0.0141]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 133, global step 17554: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134:  90%|████████▉ | 131/146 [00:02<00:00, 47.70it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 134:  90%|█████████ | 132/146 [00:02<00:00, 47.70it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134:  91%|█████████ | 133/146 [00:02<00:00, 47.87it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134:  92%|█████████▏| 134/146 [00:02<00:00, 48.06it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134:  92%|█████████▏| 135/146 [00:02<00:00, 48.21it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134:  93%|█████████▎| 136/146 [00:02<00:00, 48.38it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134:  94%|█████████▍| 137/146 [00:02<00:00, 48.53it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134:  95%|█████████▍| 138/146 [00:02<00:00, 48.64it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134:  95%|█████████▌| 139/146 [00:02<00:00, 48.82it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134:  96%|█████████▌| 140/146 [00:02<00:00, 48.96it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134:  97%|█████████▋| 141/146 [00:02<00:00, 49.09it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134:  97%|█████████▋| 142/146 [00:02<00:00, 49.20it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134:  98%|█████████▊| 143/146 [00:02<00:00, 49.34it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134:  99%|█████████▊| 144/146 [00:02<00:00, 49.45it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134:  99%|█████████▉| 145/146 [00:02<00:00, 49.55it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0141]\n",
      "Epoch 134: 100%|██████████| 146/146 [00:02<00:00, 49.54it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0142]\n",
      "Epoch 134: 100%|██████████| 146/146 [00:02<00:00, 49.35it/s, loss=0.000806, v_num=8, train_loss=0.000179, val_loss=0.0142]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 134, global step 17685: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135:  90%|████████▉ | 131/146 [00:02<00:00, 58.87it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 135:  90%|█████████ | 132/146 [00:02<00:00, 58.77it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135:  91%|█████████ | 133/146 [00:02<00:00, 58.98it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135:  92%|█████████▏| 134/146 [00:02<00:00, 59.10it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135:  92%|█████████▏| 135/146 [00:02<00:00, 59.31it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135:  93%|█████████▎| 136/146 [00:02<00:00, 59.49it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135:  94%|█████████▍| 137/146 [00:02<00:00, 59.66it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135:  95%|█████████▍| 138/146 [00:02<00:00, 59.84it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135:  95%|█████████▌| 139/146 [00:02<00:00, 59.99it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135:  96%|█████████▌| 140/146 [00:02<00:00, 60.13it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135:  97%|█████████▋| 141/146 [00:02<00:00, 60.28it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135:  97%|█████████▋| 142/146 [00:02<00:00, 60.45it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135:  98%|█████████▊| 143/146 [00:02<00:00, 60.61it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135:  99%|█████████▊| 144/146 [00:02<00:00, 60.81it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135:  99%|█████████▉| 145/146 [00:02<00:00, 60.89it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0142]\n",
      "Epoch 135: 100%|██████████| 146/146 [00:02<00:00, 60.96it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0139]\n",
      "Epoch 135: 100%|██████████| 146/146 [00:02<00:00, 60.83it/s, loss=0.0008, v_num=8, train_loss=0.000183, val_loss=0.0139]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 135, global step 17816: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136:  90%|████████▉ | 131/146 [00:02<00:00, 60.05it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 136:  90%|█████████ | 132/146 [00:02<00:00, 60.02it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136:  91%|█████████ | 133/146 [00:02<00:00, 60.12it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136:  92%|█████████▏| 134/146 [00:02<00:00, 60.32it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136:  92%|█████████▏| 135/146 [00:02<00:00, 60.47it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136:  93%|█████████▎| 136/146 [00:02<00:00, 60.62it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136:  94%|█████████▍| 137/146 [00:02<00:00, 60.74it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136:  95%|█████████▍| 138/146 [00:02<00:00, 60.92it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136:  95%|█████████▌| 139/146 [00:02<00:00, 61.06it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136:  96%|█████████▌| 140/146 [00:02<00:00, 61.21it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136:  97%|█████████▋| 141/146 [00:02<00:00, 61.40it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136:  97%|█████████▋| 142/146 [00:02<00:00, 61.57it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136:  98%|█████████▊| 143/146 [00:02<00:00, 61.73it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136:  99%|█████████▊| 144/146 [00:02<00:00, 61.85it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136:  99%|█████████▉| 145/146 [00:02<00:00, 61.93it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0139]\n",
      "Epoch 136: 100%|██████████| 146/146 [00:02<00:00, 61.96it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0141]\n",
      "Epoch 136: 100%|██████████| 146/146 [00:02<00:00, 61.78it/s, loss=0.000804, v_num=8, train_loss=0.000173, val_loss=0.0141]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 136, global step 17947: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137:  90%|████████▉ | 131/146 [00:02<00:00, 58.50it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 137:  90%|█████████ | 132/146 [00:02<00:00, 58.45it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137:  91%|█████████ | 133/146 [00:02<00:00, 58.66it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137:  92%|█████████▏| 134/146 [00:02<00:00, 58.77it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137:  92%|█████████▏| 135/146 [00:02<00:00, 58.95it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137:  93%|█████████▎| 136/146 [00:02<00:00, 59.13it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137:  94%|█████████▍| 137/146 [00:02<00:00, 59.33it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137:  95%|█████████▍| 138/146 [00:02<00:00, 59.45it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137:  95%|█████████▌| 139/146 [00:02<00:00, 59.65it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137:  96%|█████████▌| 140/146 [00:02<00:00, 59.82it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137:  97%|█████████▋| 141/146 [00:02<00:00, 59.94it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137:  97%|█████████▋| 142/146 [00:02<00:00, 60.11it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137:  98%|█████████▊| 143/146 [00:02<00:00, 60.28it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137:  99%|█████████▊| 144/146 [00:02<00:00, 60.45it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137:  99%|█████████▉| 145/146 [00:02<00:00, 60.59it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0141]\n",
      "Epoch 137: 100%|██████████| 146/146 [00:02<00:00, 60.60it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0138]\n",
      "Epoch 137: 100%|██████████| 146/146 [00:02<00:00, 60.50it/s, loss=0.000801, v_num=8, train_loss=0.00019, val_loss=0.0138]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137, global step 18078: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138:  90%|████████▉ | 131/146 [00:02<00:00, 60.17it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 138:  90%|█████████ | 132/146 [00:02<00:00, 60.10it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138:  91%|█████████ | 133/146 [00:02<00:00, 60.31it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138:  92%|█████████▏| 134/146 [00:02<00:00, 60.46it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138:  92%|█████████▏| 135/146 [00:02<00:00, 60.64it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138:  93%|█████████▎| 136/146 [00:02<00:00, 60.79it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138:  94%|█████████▍| 137/146 [00:02<00:00, 60.94it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138:  95%|█████████▍| 138/146 [00:02<00:00, 61.06it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138:  95%|█████████▌| 139/146 [00:02<00:00, 61.20it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138:  96%|█████████▌| 140/146 [00:02<00:00, 61.37it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138:  97%|█████████▋| 141/146 [00:02<00:00, 61.49it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138:  97%|█████████▋| 142/146 [00:02<00:00, 61.68it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138:  98%|█████████▊| 143/146 [00:02<00:00, 61.71it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138:  99%|█████████▊| 144/146 [00:02<00:00, 61.85it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138:  99%|█████████▉| 145/146 [00:02<00:00, 61.99it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 138: 100%|██████████| 146/146 [00:02<00:00, 62.04it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.014] \n",
      "Epoch 138: 100%|██████████| 146/146 [00:02<00:00, 61.91it/s, loss=0.000798, v_num=8, train_loss=0.000171, val_loss=0.014]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 138, global step 18209: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139:  90%|████████▉ | 131/146 [00:02<00:00, 59.00it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 139:  90%|█████████ | 132/146 [00:02<00:00, 58.82it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139:  91%|█████████ | 133/146 [00:02<00:00, 58.98it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139:  92%|█████████▏| 134/146 [00:02<00:00, 59.13it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139:  92%|█████████▏| 135/146 [00:02<00:00, 59.28it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139:  93%|█████████▎| 136/146 [00:02<00:00, 59.49it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139:  94%|█████████▍| 137/146 [00:02<00:00, 59.66it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139:  95%|█████████▍| 138/146 [00:02<00:00, 59.81it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139:  95%|█████████▌| 139/146 [00:02<00:00, 59.99it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139:  96%|█████████▌| 140/146 [00:02<00:00, 60.18it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139:  97%|█████████▋| 141/146 [00:02<00:00, 60.30it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139:  97%|█████████▋| 142/146 [00:02<00:00, 60.42it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139:  98%|█████████▊| 143/146 [00:02<00:00, 60.54it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139:  99%|█████████▊| 144/146 [00:02<00:00, 60.73it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139:  99%|█████████▉| 145/146 [00:02<00:00, 60.82it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139: 100%|██████████| 146/146 [00:02<00:00, 60.80it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]\n",
      "Epoch 139: 100%|██████████| 146/146 [00:02<00:00, 60.70it/s, loss=0.000805, v_num=8, train_loss=0.000194, val_loss=0.014]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 139, global step 18340: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140:  90%|████████▉ | 131/146 [00:02<00:00, 51.17it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 140:  90%|█████████ | 132/146 [00:02<00:00, 51.12it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140:  91%|█████████ | 133/146 [00:02<00:00, 51.25it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140:  92%|█████████▏| 134/146 [00:02<00:00, 51.42it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140:  92%|█████████▏| 135/146 [00:02<00:00, 51.58it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140:  93%|█████████▎| 136/146 [00:02<00:00, 51.73it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140:  94%|█████████▍| 137/146 [00:02<00:00, 51.85it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140:  95%|█████████▍| 138/146 [00:02<00:00, 51.93it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140:  95%|█████████▌| 139/146 [00:02<00:00, 52.06it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140:  96%|█████████▌| 140/146 [00:02<00:00, 52.10it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140:  97%|█████████▋| 141/146 [00:02<00:00, 52.24it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140:  97%|█████████▋| 142/146 [00:02<00:00, 52.39it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140:  98%|█████████▊| 143/146 [00:02<00:00, 52.53it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140:  99%|█████████▊| 144/146 [00:02<00:00, 52.69it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140:  99%|█████████▉| 145/146 [00:02<00:00, 52.78it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140: 100%|██████████| 146/146 [00:02<00:00, 52.82it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]\n",
      "Epoch 140: 100%|██████████| 146/146 [00:02<00:00, 52.68it/s, loss=0.000791, v_num=8, train_loss=0.000171, val_loss=0.014]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 140, global step 18471: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141:  90%|████████▉ | 131/146 [00:02<00:00, 56.12it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 141:  90%|█████████ | 132/146 [00:02<00:00, 56.07it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141:  91%|█████████ | 133/146 [00:02<00:00, 56.28it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141:  92%|█████████▏| 134/146 [00:02<00:00, 56.46it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141:  92%|█████████▏| 135/146 [00:02<00:00, 56.65it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141:  93%|█████████▎| 136/146 [00:02<00:00, 56.83it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141:  94%|█████████▍| 137/146 [00:02<00:00, 57.01it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141:  95%|█████████▍| 138/146 [00:02<00:00, 57.16it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141:  95%|█████████▌| 139/146 [00:02<00:00, 57.36it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141:  96%|█████████▌| 140/146 [00:02<00:00, 57.33it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141:  97%|█████████▋| 141/146 [00:02<00:00, 57.43it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141:  97%|█████████▋| 142/146 [00:02<00:00, 57.53it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141:  98%|█████████▊| 143/146 [00:02<00:00, 57.70it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141:  99%|█████████▊| 144/146 [00:02<00:00, 57.85it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141:  99%|█████████▉| 145/146 [00:02<00:00, 58.07it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.014]\n",
      "Epoch 141: 100%|██████████| 146/146 [00:02<00:00, 58.12it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.0141]\n",
      "Epoch 141: 100%|██████████| 146/146 [00:02<00:00, 57.96it/s, loss=0.000802, v_num=8, train_loss=0.000173, val_loss=0.0141]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141, global step 18602: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142:  90%|████████▉ | 131/146 [00:02<00:00, 59.62it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 142:  90%|█████████ | 132/146 [00:02<00:00, 59.56it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142:  91%|█████████ | 133/146 [00:02<00:00, 59.77it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142:  92%|█████████▏| 134/146 [00:02<00:00, 59.90it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142:  92%|█████████▏| 135/146 [00:02<00:00, 60.08it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142:  93%|█████████▎| 136/146 [00:02<00:00, 60.25it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142:  94%|█████████▍| 137/146 [00:02<00:00, 60.43it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142:  95%|█████████▍| 138/146 [00:02<00:00, 60.63it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142:  95%|█████████▌| 139/146 [00:02<00:00, 60.80it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142:  96%|█████████▌| 140/146 [00:02<00:00, 60.97it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142:  97%|█████████▋| 141/146 [00:02<00:00, 61.09it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142:  97%|█████████▋| 142/146 [00:02<00:00, 61.26it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142:  98%|█████████▊| 143/146 [00:02<00:00, 61.32it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142:  99%|█████████▊| 144/146 [00:02<00:00, 61.43it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142:  99%|█████████▉| 145/146 [00:02<00:00, 61.59it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0141]\n",
      "Epoch 142: 100%|██████████| 146/146 [00:02<00:00, 61.68it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0134]\n",
      "Epoch 142: 100%|██████████| 146/146 [00:02<00:00, 61.55it/s, loss=0.00079, v_num=8, train_loss=0.000206, val_loss=0.0134]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142, global step 18733: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143:  90%|████████▉ | 131/146 [00:02<00:00, 58.98it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 143:  90%|█████████ | 132/146 [00:02<00:00, 58.90it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143:  91%|█████████ | 133/146 [00:02<00:00, 59.11it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143:  92%|█████████▏| 134/146 [00:02<00:00, 59.26it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143:  92%|█████████▏| 135/146 [00:02<00:00, 59.41it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143:  93%|█████████▎| 136/146 [00:02<00:00, 59.59it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143:  94%|█████████▍| 137/146 [00:02<00:00, 59.77it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143:  95%|█████████▍| 138/146 [00:02<00:00, 59.94it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143:  95%|█████████▌| 139/146 [00:02<00:00, 60.09it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143:  96%|█████████▌| 140/146 [00:02<00:00, 60.21it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143:  97%|█████████▋| 141/146 [00:02<00:00, 60.38it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143:  97%|█████████▋| 142/146 [00:02<00:00, 60.58it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143:  98%|█████████▊| 143/146 [00:02<00:00, 60.69it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143:  99%|█████████▊| 144/146 [00:02<00:00, 60.81it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143:  99%|█████████▉| 145/146 [00:02<00:00, 61.05it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0134]\n",
      "Epoch 143: 100%|██████████| 146/146 [00:02<00:00, 61.11it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.014] \n",
      "Epoch 143: 100%|██████████| 146/146 [00:02<00:00, 61.01it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.014]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 143, global step 18864: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144:  90%|████████▉ | 131/146 [00:02<00:00, 58.98it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 144:  90%|█████████ | 132/146 [00:02<00:00, 58.92it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144:  91%|█████████ | 133/146 [00:02<00:00, 59.11it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144:  92%|█████████▏| 134/146 [00:02<00:00, 59.31it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144:  92%|█████████▏| 135/146 [00:02<00:00, 59.52it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144:  93%|█████████▎| 136/146 [00:02<00:00, 59.70it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144:  94%|█████████▍| 137/146 [00:02<00:00, 59.85it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144:  95%|█████████▍| 138/146 [00:02<00:00, 59.94it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144:  95%|█████████▌| 139/146 [00:02<00:00, 60.12it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144:  96%|█████████▌| 140/146 [00:02<00:00, 60.34it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144:  97%|█████████▋| 141/146 [00:02<00:00, 60.51it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144:  97%|█████████▋| 142/146 [00:02<00:00, 60.71it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144:  98%|█████████▊| 143/146 [00:02<00:00, 60.85it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144:  99%|█████████▊| 144/146 [00:02<00:00, 61.01it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144:  99%|█████████▉| 145/146 [00:02<00:00, 61.15it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.014]\n",
      "Epoch 144: 100%|██████████| 146/146 [00:02<00:00, 61.11it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.0135]\n",
      "Epoch 144: 100%|██████████| 146/146 [00:02<00:00, 60.98it/s, loss=0.000809, v_num=8, train_loss=0.00036, val_loss=0.0135]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 144, global step 18995: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145:  90%|████████▉ | 131/146 [00:02<00:00, 59.03it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 145:  90%|█████████ | 132/146 [00:02<00:00, 58.98it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145:  91%|█████████ | 133/146 [00:02<00:00, 59.16it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145:  92%|█████████▏| 134/146 [00:02<00:00, 59.37it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145:  92%|█████████▏| 135/146 [00:02<00:00, 59.57it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145:  93%|█████████▎| 136/146 [00:02<00:00, 59.75it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145:  94%|█████████▍| 137/146 [00:02<00:00, 59.95it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145:  95%|█████████▍| 138/146 [00:02<00:00, 60.13it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145:  95%|█████████▌| 139/146 [00:02<00:00, 60.22it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145:  96%|█████████▌| 140/146 [00:02<00:00, 60.44it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145:  97%|█████████▋| 141/146 [00:02<00:00, 60.64it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145:  97%|█████████▋| 142/146 [00:02<00:00, 60.78it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145:  98%|█████████▊| 143/146 [00:02<00:00, 60.95it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145:  99%|█████████▊| 144/146 [00:02<00:00, 61.04it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145:  99%|█████████▉| 145/146 [00:02<00:00, 61.23it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.0135]\n",
      "Epoch 145: 100%|██████████| 146/146 [00:02<00:00, 61.26it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.014] \n",
      "Epoch 145: 100%|██████████| 146/146 [00:02<00:00, 61.08it/s, loss=0.00078, v_num=8, train_loss=0.000208, val_loss=0.014]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 145, global step 19126: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146:  90%|████████▉ | 131/146 [00:02<00:00, 53.14it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 146:  90%|█████████ | 132/146 [00:02<00:00, 52.94it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146:  91%|█████████ | 133/146 [00:02<00:00, 52.90it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146:  92%|█████████▏| 134/146 [00:02<00:00, 52.98it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146:  92%|█████████▏| 135/146 [00:02<00:00, 53.08it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146:  93%|█████████▎| 136/146 [00:02<00:00, 53.23it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146:  94%|█████████▍| 137/146 [00:02<00:00, 53.26it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146:  95%|█████████▍| 138/146 [00:02<00:00, 53.34it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146:  95%|█████████▌| 139/146 [00:02<00:00, 53.33it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146:  96%|█████████▌| 140/146 [00:02<00:00, 53.39it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146:  97%|█████████▋| 141/146 [00:02<00:00, 53.51it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146:  97%|█████████▋| 142/146 [00:02<00:00, 53.58it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146:  98%|█████████▊| 143/146 [00:02<00:00, 53.57it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146:  99%|█████████▊| 144/146 [00:02<00:00, 53.63it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146:  99%|█████████▉| 145/146 [00:02<00:00, 53.76it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.014]\n",
      "Epoch 146: 100%|██████████| 146/146 [00:02<00:00, 53.61it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.0154]\n",
      "Epoch 146: 100%|██████████| 146/146 [00:02<00:00, 53.50it/s, loss=0.000863, v_num=8, train_loss=0.000215, val_loss=0.0154]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 146, global step 19257: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147:  90%|████████▉ | 131/146 [00:02<00:00, 50.23it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 147:  90%|█████████ | 132/146 [00:02<00:00, 50.19it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147:  91%|█████████ | 133/146 [00:02<00:00, 50.41it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147:  92%|█████████▏| 134/146 [00:02<00:00, 50.58it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147:  92%|█████████▏| 135/146 [00:02<00:00, 50.73it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147:  93%|█████████▎| 136/146 [00:02<00:00, 50.88it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147:  94%|█████████▍| 137/146 [00:02<00:00, 51.06it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147:  95%|█████████▍| 138/146 [00:02<00:00, 51.24it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147:  95%|█████████▌| 139/146 [00:02<00:00, 51.40it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147:  96%|█████████▌| 140/146 [00:02<00:00, 51.52it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147:  97%|█████████▋| 141/146 [00:02<00:00, 51.70it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147:  97%|█████████▋| 142/146 [00:02<00:00, 51.88it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147:  98%|█████████▊| 143/146 [00:02<00:00, 52.05it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147:  99%|█████████▊| 144/146 [00:02<00:00, 52.21it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147:  99%|█████████▉| 145/146 [00:02<00:00, 52.40it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0154]\n",
      "Epoch 147: 100%|██████████| 146/146 [00:02<00:00, 52.50it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0135]\n",
      "Epoch 147: 100%|██████████| 146/146 [00:02<00:00, 52.40it/s, loss=0.00095, v_num=8, train_loss=0.00038, val_loss=0.0135]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 147, global step 19388: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148:  90%|████████▉ | 131/146 [00:02<00:00, 54.38it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 148:  90%|█████████ | 132/146 [00:02<00:00, 54.25it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148:  91%|█████████ | 133/146 [00:02<00:00, 54.37it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148:  92%|█████████▏| 134/146 [00:02<00:00, 54.51it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148:  92%|█████████▏| 135/146 [00:02<00:00, 54.63it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148:  93%|█████████▎| 136/146 [00:02<00:00, 54.68it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148:  94%|█████████▍| 137/146 [00:02<00:00, 54.80it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148:  95%|█████████▍| 138/146 [00:02<00:00, 54.93it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148:  95%|█████████▌| 139/146 [00:02<00:00, 55.09it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148:  96%|█████████▌| 140/146 [00:02<00:00, 55.20it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148:  97%|█████████▋| 141/146 [00:02<00:00, 55.33it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148:  97%|█████████▋| 142/146 [00:02<00:00, 55.42it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148:  98%|█████████▊| 143/146 [00:02<00:00, 55.49it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148:  99%|█████████▊| 144/146 [00:02<00:00, 55.64it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148:  99%|█████████▉| 145/146 [00:02<00:00, 55.77it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0135]\n",
      "Epoch 148: 100%|██████████| 146/146 [00:02<00:00, 55.72it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0159]\n",
      "Epoch 148: 100%|██████████| 146/146 [00:02<00:00, 55.57it/s, loss=0.000837, v_num=8, train_loss=0.000273, val_loss=0.0159]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 148, global step 19519: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149:  90%|████████▉ | 131/146 [00:02<00:00, 57.91it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 149:  90%|█████████ | 132/146 [00:02<00:00, 57.76it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149:  91%|█████████ | 133/146 [00:02<00:00, 57.92it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149:  92%|█████████▏| 134/146 [00:02<00:00, 58.11it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149:  92%|█████████▏| 135/146 [00:02<00:00, 58.29it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149:  93%|█████████▎| 136/146 [00:02<00:00, 58.41it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149:  94%|█████████▍| 137/146 [00:02<00:00, 58.57it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149:  95%|█████████▍| 138/146 [00:02<00:00, 58.69it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149:  95%|█████████▌| 139/146 [00:02<00:00, 58.89it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149:  96%|█████████▌| 140/146 [00:02<00:00, 59.04it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149:  97%|█████████▋| 141/146 [00:02<00:00, 59.16it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149:  97%|█████████▋| 142/146 [00:02<00:00, 59.31it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149:  98%|█████████▊| 143/146 [00:02<00:00, 59.43it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149:  99%|█████████▊| 144/146 [00:02<00:00, 59.55it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149:  99%|█████████▉| 145/146 [00:02<00:00, 59.69it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0159]\n",
      "Epoch 149: 100%|██████████| 146/146 [00:02<00:00, 59.73it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0148]\n",
      "Epoch 149: 100%|██████████| 146/146 [00:02<00:00, 59.61it/s, loss=0.000835, v_num=8, train_loss=0.000348, val_loss=0.0148]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 149, global step 19650: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150:  90%|████████▉ | 131/146 [00:02<00:00, 58.74it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 150:  90%|█████████ | 132/146 [00:02<00:00, 58.64it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150:  91%|█████████ | 133/146 [00:02<00:00, 58.85it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150:  92%|█████████▏| 134/146 [00:02<00:00, 59.05it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150:  92%|█████████▏| 135/146 [00:02<00:00, 59.26it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150:  93%|█████████▎| 136/146 [00:02<00:00, 59.44it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150:  94%|█████████▍| 137/146 [00:02<00:00, 59.56it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150:  95%|█████████▍| 138/146 [00:02<00:00, 59.71it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150:  95%|█████████▌| 139/146 [00:02<00:00, 59.86it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150:  96%|█████████▌| 140/146 [00:02<00:00, 60.06it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150:  97%|█████████▋| 141/146 [00:02<00:00, 60.25it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150:  97%|█████████▋| 142/146 [00:02<00:00, 60.42it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150:  98%|█████████▊| 143/146 [00:02<00:00, 60.61it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150:  99%|█████████▊| 144/146 [00:02<00:00, 60.76it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150:  99%|█████████▉| 145/146 [00:02<00:00, 60.95it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0148]\n",
      "Epoch 150: 100%|██████████| 146/146 [00:02<00:00, 60.91it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0128]\n",
      "Epoch 150: 100%|██████████| 146/146 [00:02<00:00, 60.80it/s, loss=0.000766, v_num=8, train_loss=0.000178, val_loss=0.0128]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 150, global step 19781: 'val_loss' reached 0.01276 (best 0.01276), saving model to 'C:\\\\Users\\\\hazre\\\\Desktop\\\\checkpoints2\\\\epoch=150-step=19781.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151:  90%|████████▉ | 131/146 [00:02<00:00, 56.21it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 151:  90%|█████████ | 132/146 [00:02<00:00, 56.13it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151:  91%|█████████ | 133/146 [00:02<00:00, 56.36it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151:  92%|█████████▏| 134/146 [00:02<00:00, 56.55it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151:  92%|█████████▏| 135/146 [00:02<00:00, 56.66it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151:  93%|█████████▎| 136/146 [00:02<00:00, 56.81it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151:  94%|█████████▍| 137/146 [00:02<00:00, 56.92it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151:  95%|█████████▍| 138/146 [00:02<00:00, 57.05it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151:  95%|█████████▌| 139/146 [00:02<00:00, 57.14it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151:  96%|█████████▌| 140/146 [00:02<00:00, 57.22it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151:  97%|█████████▋| 141/146 [00:02<00:00, 57.37it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151:  97%|█████████▋| 142/146 [00:02<00:00, 57.45it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151:  98%|█████████▊| 143/146 [00:02<00:00, 57.53it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151:  99%|█████████▊| 144/146 [00:02<00:00, 57.67it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151:  99%|█████████▉| 145/146 [00:02<00:00, 57.75it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0128]\n",
      "Epoch 151: 100%|██████████| 146/146 [00:02<00:00, 57.69it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0148]\n",
      "Epoch 151: 100%|██████████| 146/146 [00:02<00:00, 57.55it/s, loss=0.0008, v_num=8, train_loss=0.000186, val_loss=0.0148]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 151, global step 19912: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152:  90%|████████▉ | 131/146 [00:02<00:00, 53.57it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 152:  90%|█████████ | 132/146 [00:02<00:00, 53.46it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152:  91%|█████████ | 133/146 [00:02<00:00, 53.67it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152:  92%|█████████▏| 134/146 [00:02<00:00, 53.83it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152:  92%|█████████▏| 135/146 [00:02<00:00, 53.93it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152:  93%|█████████▎| 136/146 [00:02<00:00, 54.03it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152:  94%|█████████▍| 137/146 [00:02<00:00, 54.08it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152:  95%|█████████▍| 138/146 [00:02<00:00, 54.20it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152:  95%|█████████▌| 139/146 [00:02<00:00, 54.36it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152:  96%|█████████▌| 140/146 [00:02<00:00, 54.47it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152:  97%|█████████▋| 141/146 [00:02<00:00, 54.65it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152:  97%|█████████▋| 142/146 [00:02<00:00, 54.78it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152:  98%|█████████▊| 143/146 [00:02<00:00, 54.93it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152:  99%|█████████▊| 144/146 [00:02<00:00, 55.08it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152:  99%|█████████▉| 145/146 [00:02<00:00, 55.26it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0148]\n",
      "Epoch 152: 100%|██████████| 146/146 [00:02<00:00, 55.30it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0137]\n",
      "Epoch 152: 100%|██████████| 146/146 [00:02<00:00, 55.17it/s, loss=0.000803, v_num=8, train_loss=0.000236, val_loss=0.0137]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 152, global step 20043: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153:  90%|████████▉ | 131/146 [00:02<00:00, 47.95it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 153:  90%|█████████ | 132/146 [00:02<00:00, 47.94it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153:  91%|█████████ | 133/146 [00:02<00:00, 48.17it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153:  92%|█████████▏| 134/146 [00:02<00:00, 48.34it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153:  92%|█████████▏| 135/146 [00:02<00:00, 48.51it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153:  93%|█████████▎| 136/146 [00:02<00:00, 48.67it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153:  94%|█████████▍| 137/146 [00:02<00:00, 48.80it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153:  95%|█████████▍| 138/146 [00:02<00:00, 48.97it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153:  95%|█████████▌| 139/146 [00:02<00:00, 49.13it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153:  96%|█████████▌| 140/146 [00:02<00:00, 49.31it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153:  97%|█████████▋| 141/146 [00:02<00:00, 49.44it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153:  97%|█████████▋| 142/146 [00:02<00:00, 49.51it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153:  98%|█████████▊| 143/146 [00:02<00:00, 49.58it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153:  99%|█████████▊| 144/146 [00:02<00:00, 49.72it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153:  99%|█████████▉| 145/146 [00:02<00:00, 49.81it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0137]\n",
      "Epoch 153: 100%|██████████| 146/146 [00:02<00:00, 49.83it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0131]\n",
      "Epoch 153: 100%|██████████| 146/146 [00:02<00:00, 49.72it/s, loss=0.000779, v_num=8, train_loss=0.00017, val_loss=0.0131]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 153, global step 20174: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154:  90%|████████▉ | 131/146 [00:02<00:00, 56.27it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 154:  90%|█████████ | 132/146 [00:02<00:00, 56.26it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154:  91%|█████████ | 133/146 [00:02<00:00, 56.42it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154:  92%|█████████▏| 134/146 [00:02<00:00, 56.61it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154:  92%|█████████▏| 135/146 [00:02<00:00, 56.72it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154:  93%|█████████▎| 136/146 [00:02<00:00, 56.88it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154:  94%|█████████▍| 137/146 [00:02<00:00, 57.01it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154:  95%|█████████▍| 138/146 [00:02<00:00, 57.09it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154:  95%|█████████▌| 139/146 [00:02<00:00, 57.29it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154:  96%|█████████▌| 140/146 [00:02<00:00, 57.40it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154:  97%|█████████▋| 141/146 [00:02<00:00, 57.55it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154:  97%|█████████▋| 142/146 [00:02<00:00, 57.70it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154:  98%|█████████▊| 143/146 [00:02<00:00, 57.84it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154:  99%|█████████▊| 144/146 [00:02<00:00, 58.01it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154:  99%|█████████▉| 145/146 [00:02<00:00, 58.18it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0131]\n",
      "Epoch 154: 100%|██████████| 146/146 [00:02<00:00, 58.23it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0139]\n",
      "Epoch 154: 100%|██████████| 146/146 [00:02<00:00, 58.14it/s, loss=0.000806, v_num=8, train_loss=0.000176, val_loss=0.0139]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 154, global step 20305: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155:  90%|████████▉ | 131/146 [00:02<00:00, 57.20it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 155:  90%|█████████ | 132/146 [00:02<00:00, 57.13it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155:  91%|█████████ | 133/146 [00:02<00:00, 57.27it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155:  92%|█████████▏| 134/146 [00:02<00:00, 57.43it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155:  92%|█████████▏| 135/146 [00:02<00:00, 57.59it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155:  93%|█████████▎| 136/146 [00:02<00:00, 57.72it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155:  94%|█████████▍| 137/146 [00:02<00:00, 57.87it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155:  95%|█████████▍| 138/146 [00:02<00:00, 58.10it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155:  95%|█████████▌| 139/146 [00:02<00:00, 58.22it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155:  96%|█████████▌| 140/146 [00:02<00:00, 58.37it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155:  97%|█████████▋| 141/146 [00:02<00:00, 58.57it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155:  97%|█████████▋| 142/146 [00:02<00:00, 58.62it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155:  98%|█████████▊| 143/146 [00:02<00:00, 58.67it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155:  99%|█████████▊| 144/146 [00:02<00:00, 58.77it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155:  99%|█████████▉| 145/146 [00:02<00:00, 58.94it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0139]\n",
      "Epoch 155: 100%|██████████| 146/146 [00:02<00:00, 58.89it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0127]\n",
      "Epoch 155: 100%|██████████| 146/146 [00:02<00:00, 58.74it/s, loss=0.000776, v_num=8, train_loss=0.0002, val_loss=0.0127]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 155, global step 20436: 'val_loss' reached 0.01266 (best 0.01266), saving model to 'C:\\\\Users\\\\hazre\\\\Desktop\\\\checkpoints2\\\\epoch=155-step=20436.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156:  90%|████████▉ | 131/146 [00:02<00:00, 54.51it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 156:  90%|█████████ | 132/146 [00:02<00:00, 54.45it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156:  91%|█████████ | 133/146 [00:02<00:00, 54.64it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156:  92%|█████████▏| 134/146 [00:02<00:00, 54.80it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156:  92%|█████████▏| 135/146 [00:02<00:00, 55.01it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156:  93%|█████████▎| 136/146 [00:02<00:00, 55.17it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156:  94%|█████████▍| 137/146 [00:02<00:00, 55.30it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156:  95%|█████████▍| 138/146 [00:02<00:00, 55.42it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156:  95%|█████████▌| 139/146 [00:02<00:00, 55.55it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156:  96%|█████████▌| 140/146 [00:02<00:00, 55.71it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156:  97%|█████████▋| 141/146 [00:02<00:00, 55.84it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156:  97%|█████████▋| 142/146 [00:02<00:00, 55.99it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156:  98%|█████████▊| 143/146 [00:02<00:00, 56.14it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156:  99%|█████████▊| 144/146 [00:02<00:00, 56.31it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156:  99%|█████████▉| 145/146 [00:02<00:00, 56.44it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.0127]\n",
      "Epoch 156: 100%|██████████| 146/146 [00:02<00:00, 56.48it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.013] \n",
      "Epoch 156: 100%|██████████| 146/146 [00:02<00:00, 56.34it/s, loss=0.000785, v_num=8, train_loss=0.000169, val_loss=0.013]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 156, global step 20567: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157:  90%|████████▉ | 131/146 [00:02<00:00, 58.02it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 157:  90%|█████████ | 132/146 [00:02<00:00, 57.95it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157:  91%|█████████ | 133/146 [00:02<00:00, 58.13it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157:  92%|█████████▏| 134/146 [00:02<00:00, 58.31it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157:  92%|█████████▏| 135/146 [00:02<00:00, 58.49it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157:  93%|█████████▎| 136/146 [00:02<00:00, 58.65it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157:  94%|█████████▍| 137/146 [00:02<00:00, 58.77it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157:  95%|█████████▍| 138/146 [00:02<00:00, 58.95it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157:  95%|█████████▌| 139/146 [00:02<00:00, 59.15it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157:  96%|█████████▌| 140/146 [00:02<00:00, 59.30it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157:  97%|█████████▋| 141/146 [00:02<00:00, 59.47it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157:  97%|█████████▋| 142/146 [00:02<00:00, 59.64it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157:  98%|█████████▊| 143/146 [00:02<00:00, 59.81it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157:  99%|█████████▊| 144/146 [00:02<00:00, 60.00it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157:  99%|█████████▉| 145/146 [00:02<00:00, 60.19it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.013]\n",
      "Epoch 157: 100%|██████████| 146/146 [00:02<00:00, 60.26it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.0128]\n",
      "Epoch 157: 100%|██████████| 146/146 [00:02<00:00, 60.13it/s, loss=0.000785, v_num=8, train_loss=0.000204, val_loss=0.0128]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 157, global step 20698: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158:  90%|████████▉ | 131/146 [00:02<00:00, 56.73it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 158:  90%|█████████ | 132/146 [00:02<00:00, 56.63it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158:  91%|█████████ | 133/146 [00:02<00:00, 56.74it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158:  92%|█████████▏| 134/146 [00:02<00:00, 56.92it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158:  92%|█████████▏| 135/146 [00:02<00:00, 57.06it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158:  93%|█████████▎| 136/146 [00:02<00:00, 57.12it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158:  94%|█████████▍| 137/146 [00:02<00:00, 57.25it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158:  95%|█████████▍| 138/146 [00:02<00:00, 57.43it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158:  95%|█████████▌| 139/146 [00:02<00:00, 57.56it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158:  96%|█████████▌| 140/146 [00:02<00:00, 57.75it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158:  97%|█████████▋| 141/146 [00:02<00:00, 57.90it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158:  97%|█████████▋| 142/146 [00:02<00:00, 58.12it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158:  98%|█████████▊| 143/146 [00:02<00:00, 58.25it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158:  99%|█████████▊| 144/146 [00:02<00:00, 58.37it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158:  99%|█████████▉| 145/146 [00:02<00:00, 58.51it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0128]\n",
      "Epoch 158: 100%|██████████| 146/146 [00:02<00:00, 58.59it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0122]\n",
      "Epoch 158: 100%|██████████| 146/146 [00:02<00:00, 58.47it/s, loss=0.00077, v_num=8, train_loss=0.000169, val_loss=0.0122]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 158, global step 20829: 'val_loss' reached 0.01223 (best 0.01223), saving model to 'C:\\\\Users\\\\hazre\\\\Desktop\\\\checkpoints2\\\\epoch=158-step=20829.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159:  90%|████████▉ | 131/146 [00:02<00:00, 45.19it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 159:  90%|█████████ | 132/146 [00:02<00:00, 45.11it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159:  91%|█████████ | 133/146 [00:02<00:00, 45.22it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159:  92%|█████████▏| 134/146 [00:02<00:00, 45.38it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159:  92%|█████████▏| 135/146 [00:02<00:00, 45.53it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159:  93%|█████████▎| 136/146 [00:02<00:00, 45.62it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159:  94%|█████████▍| 137/146 [00:02<00:00, 45.70it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159:  95%|█████████▍| 138/146 [00:03<00:00, 45.85it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159:  95%|█████████▌| 139/146 [00:03<00:00, 46.00it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159:  96%|█████████▌| 140/146 [00:03<00:00, 46.15it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159:  97%|█████████▋| 141/146 [00:03<00:00, 46.29it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159:  97%|█████████▋| 142/146 [00:03<00:00, 46.42it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159:  98%|█████████▊| 143/146 [00:03<00:00, 46.57it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159:  99%|█████████▊| 144/146 [00:03<00:00, 46.71it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159:  99%|█████████▉| 145/146 [00:03<00:00, 46.79it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0122]\n",
      "Epoch 159: 100%|██████████| 146/146 [00:03<00:00, 46.87it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0131]\n",
      "Epoch 159: 100%|██████████| 146/146 [00:03<00:00, 46.77it/s, loss=0.000802, v_num=8, train_loss=0.000188, val_loss=0.0131]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159, global step 20960: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160:  90%|████████▉ | 131/146 [00:02<00:00, 53.52it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 160:  90%|█████████ | 132/146 [00:02<00:00, 53.51it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160:  91%|█████████ | 133/146 [00:02<00:00, 53.70it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160:  92%|█████████▏| 134/146 [00:02<00:00, 53.88it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160:  92%|█████████▏| 135/146 [00:02<00:00, 54.07it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160:  93%|█████████▎| 136/146 [00:02<00:00, 54.27it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160:  94%|█████████▍| 137/146 [00:02<00:00, 54.43it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160:  95%|█████████▍| 138/146 [00:02<00:00, 54.57it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160:  95%|█████████▌| 139/146 [00:02<00:00, 54.75it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160:  96%|█████████▌| 140/146 [00:02<00:00, 54.88it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160:  97%|█████████▋| 141/146 [00:02<00:00, 55.00it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160:  97%|█████████▋| 142/146 [00:02<00:00, 55.19it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160:  98%|█████████▊| 143/146 [00:02<00:00, 55.30it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160:  99%|█████████▊| 144/146 [00:02<00:00, 55.49it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160:  99%|█████████▉| 145/146 [00:02<00:00, 55.67it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0131]\n",
      "Epoch 160: 100%|██████████| 146/146 [00:02<00:00, 55.73it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0117]\n",
      "Epoch 160: 100%|██████████| 146/146 [00:02<00:00, 55.60it/s, loss=0.000798, v_num=8, train_loss=0.00061, val_loss=0.0117]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 160, global step 21091: 'val_loss' reached 0.01167 (best 0.01167), saving model to 'C:\\\\Users\\\\hazre\\\\Desktop\\\\checkpoints2\\\\epoch=160-step=21091.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161:  90%|████████▉ | 131/146 [00:02<00:00, 46.83it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 161:  90%|█████████ | 132/146 [00:02<00:00, 46.81it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161:  91%|█████████ | 133/146 [00:02<00:00, 46.96it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161:  92%|█████████▏| 134/146 [00:02<00:00, 47.11it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161:  92%|█████████▏| 135/146 [00:02<00:00, 47.25it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161:  93%|█████████▎| 136/146 [00:02<00:00, 47.42it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161:  94%|█████████▍| 137/146 [00:02<00:00, 47.58it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161:  95%|█████████▍| 138/146 [00:02<00:00, 47.73it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161:  95%|█████████▌| 139/146 [00:02<00:00, 47.91it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161:  96%|█████████▌| 140/146 [00:02<00:00, 48.06it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161:  97%|█████████▋| 141/146 [00:02<00:00, 48.23it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161:  97%|█████████▋| 142/146 [00:02<00:00, 48.26it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161:  98%|█████████▊| 143/146 [00:02<00:00, 48.45it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161:  99%|█████████▊| 144/146 [00:02<00:00, 48.61it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161:  99%|█████████▉| 145/146 [00:02<00:00, 48.80it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0117]\n",
      "Epoch 161: 100%|██████████| 146/146 [00:02<00:00, 48.87it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0132]\n",
      "Epoch 161: 100%|██████████| 146/146 [00:02<00:00, 48.79it/s, loss=0.000804, v_num=8, train_loss=0.000536, val_loss=0.0132]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 161, global step 21222: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162:  90%|████████▉ | 131/146 [00:02<00:00, 52.15it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 162:  90%|█████████ | 132/146 [00:02<00:00, 52.09it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162:  91%|█████████ | 133/146 [00:02<00:00, 52.24it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162:  92%|█████████▏| 134/146 [00:02<00:00, 52.36it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162:  92%|█████████▏| 135/146 [00:02<00:00, 52.55it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162:  93%|█████████▎| 136/146 [00:02<00:00, 52.69it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162:  94%|█████████▍| 137/146 [00:02<00:00, 52.83it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162:  95%|█████████▍| 138/146 [00:02<00:00, 52.95it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162:  95%|█████████▌| 139/146 [00:02<00:00, 53.13it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162:  96%|█████████▌| 140/146 [00:02<00:00, 53.29it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162:  97%|█████████▋| 141/146 [00:02<00:00, 53.45it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162:  97%|█████████▋| 142/146 [00:02<00:00, 53.60it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162:  98%|█████████▊| 143/146 [00:02<00:00, 53.74it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162:  99%|█████████▊| 144/146 [00:02<00:00, 53.87it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162:  99%|█████████▉| 145/146 [00:02<00:00, 54.00it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0132]\n",
      "Epoch 162: 100%|██████████| 146/146 [00:02<00:00, 54.07it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0175]\n",
      "Epoch 162: 100%|██████████| 146/146 [00:02<00:00, 53.93it/s, loss=0.000958, v_num=8, train_loss=0.000224, val_loss=0.0175]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162, global step 21353: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163:  90%|████████▉ | 131/146 [00:02<00:00, 55.20it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 163:  90%|█████████ | 132/146 [00:02<00:00, 55.04it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163:  91%|█████████ | 133/146 [00:02<00:00, 55.23it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163:  92%|█████████▏| 134/146 [00:02<00:00, 55.37it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163:  92%|█████████▏| 135/146 [00:02<00:00, 55.48it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163:  93%|█████████▎| 136/146 [00:02<00:00, 55.64it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163:  94%|█████████▍| 137/146 [00:02<00:00, 55.73it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163:  95%|█████████▍| 138/146 [00:02<00:00, 55.87it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163:  95%|█████████▌| 139/146 [00:02<00:00, 56.00it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163:  96%|█████████▌| 140/146 [00:02<00:00, 56.09it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163:  97%|█████████▋| 141/146 [00:02<00:00, 56.22it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163:  97%|█████████▋| 142/146 [00:02<00:00, 56.28it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163:  98%|█████████▊| 143/146 [00:02<00:00, 56.38it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163:  99%|█████████▊| 144/146 [00:02<00:00, 56.42it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163:  99%|█████████▉| 145/146 [00:02<00:00, 56.48it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0175]\n",
      "Epoch 163: 100%|██████████| 146/146 [00:02<00:00, 56.48it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0128]\n",
      "Epoch 163: 100%|██████████| 146/146 [00:02<00:00, 56.32it/s, loss=0.00117, v_num=8, train_loss=0.000674, val_loss=0.0128]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 163, global step 21484: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164:  90%|████████▉ | 131/146 [00:02<00:00, 55.04it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 164:  90%|█████████ | 132/146 [00:02<00:00, 54.93it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164:  91%|█████████ | 133/146 [00:02<00:00, 55.07it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164:  92%|█████████▏| 134/146 [00:02<00:00, 55.21it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164:  92%|█████████▏| 135/146 [00:02<00:00, 55.32it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164:  93%|█████████▎| 136/146 [00:02<00:00, 55.44it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164:  94%|█████████▍| 137/146 [00:02<00:00, 55.51it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164:  95%|█████████▍| 138/146 [00:02<00:00, 55.57it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164:  95%|█████████▌| 139/146 [00:02<00:00, 55.66it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164:  96%|█████████▌| 140/146 [00:02<00:00, 55.75it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164:  97%|█████████▋| 141/146 [00:02<00:00, 55.82it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164:  97%|█████████▋| 142/146 [00:02<00:00, 55.95it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164:  98%|█████████▊| 143/146 [00:02<00:00, 56.05it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164:  99%|█████████▊| 144/146 [00:02<00:00, 56.16it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164:  99%|█████████▉| 145/146 [00:02<00:00, 56.22it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0128]\n",
      "Epoch 164: 100%|██████████| 146/146 [00:02<00:00, 56.13it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0147]\n",
      "Epoch 164: 100%|██████████| 146/146 [00:02<00:00, 56.00it/s, loss=0.000882, v_num=8, train_loss=0.000192, val_loss=0.0147]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 164, global step 21615: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165:  90%|████████▉ | 131/146 [00:02<00:00, 48.00it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 165:  90%|█████████ | 132/146 [00:02<00:00, 47.94it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165:  91%|█████████ | 133/146 [00:02<00:00, 48.10it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165:  92%|█████████▏| 134/146 [00:02<00:00, 48.23it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165:  92%|█████████▏| 135/146 [00:02<00:00, 48.40it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165:  93%|█████████▎| 136/146 [00:02<00:00, 48.57it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165:  94%|█████████▍| 137/146 [00:02<00:00, 48.68it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165:  95%|█████████▍| 138/146 [00:02<00:00, 48.81it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165:  95%|█████████▌| 139/146 [00:02<00:00, 48.96it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165:  96%|█████████▌| 140/146 [00:02<00:00, 49.10it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165:  97%|█████████▋| 141/146 [00:02<00:00, 49.26it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165:  97%|█████████▋| 142/146 [00:02<00:00, 49.40it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165:  98%|█████████▊| 143/146 [00:02<00:00, 49.41it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165:  99%|█████████▊| 144/146 [00:02<00:00, 49.57it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165:  99%|█████████▉| 145/146 [00:02<00:00, 49.71it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0147]\n",
      "Epoch 165: 100%|██████████| 146/146 [00:02<00:00, 49.76it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0129]\n",
      "Epoch 165: 100%|██████████| 146/146 [00:02<00:00, 49.67it/s, loss=0.00083, v_num=8, train_loss=0.000238, val_loss=0.0129]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165, global step 21746: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166:  90%|████████▉ | 131/146 [00:02<00:00, 52.28it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 166:  90%|█████████ | 132/146 [00:02<00:00, 52.26it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166:  91%|█████████ | 133/146 [00:02<00:00, 52.43it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166:  92%|█████████▏| 134/146 [00:02<00:00, 52.60it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166:  92%|█████████▏| 135/146 [00:02<00:00, 52.74it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166:  93%|█████████▎| 136/146 [00:02<00:00, 52.91it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166:  94%|█████████▍| 137/146 [00:02<00:00, 53.07it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166:  95%|█████████▍| 138/146 [00:02<00:00, 53.21it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166:  95%|█████████▌| 139/146 [00:02<00:00, 53.37it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166:  96%|█████████▌| 140/146 [00:02<00:00, 53.50it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166:  97%|█████████▋| 141/146 [00:02<00:00, 53.68it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166:  97%|█████████▋| 142/146 [00:02<00:00, 53.88it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166:  98%|█████████▊| 143/146 [00:02<00:00, 54.05it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166:  99%|█████████▊| 144/146 [00:02<00:00, 54.18it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166:  99%|█████████▉| 145/146 [00:02<00:00, 54.33it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0129]\n",
      "Epoch 166: 100%|██████████| 146/146 [00:02<00:00, 54.38it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0138]\n",
      "Epoch 166: 100%|██████████| 146/146 [00:02<00:00, 54.26it/s, loss=0.000904, v_num=8, train_loss=0.000171, val_loss=0.0138]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 166, global step 21877: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167:  90%|████████▉ | 131/146 [00:02<00:00, 56.24it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 167:  90%|█████████ | 132/146 [00:02<00:00, 56.14it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167:  91%|█████████ | 133/146 [00:02<00:00, 56.33it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167:  92%|█████████▏| 134/146 [00:02<00:00, 56.46it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167:  92%|█████████▏| 135/146 [00:02<00:00, 56.58it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167:  93%|█████████▎| 136/146 [00:02<00:00, 56.76it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167:  94%|█████████▍| 137/146 [00:02<00:00, 56.91it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167:  95%|█████████▍| 138/146 [00:02<00:00, 57.09it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167:  95%|█████████▌| 139/146 [00:02<00:00, 57.24it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167:  96%|█████████▌| 140/146 [00:02<00:00, 57.40it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167:  97%|█████████▋| 141/146 [00:02<00:00, 57.55it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167:  97%|█████████▋| 142/146 [00:02<00:00, 57.70it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167:  98%|█████████▊| 143/146 [00:02<00:00, 57.54it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167:  99%|█████████▊| 144/146 [00:02<00:00, 57.71it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167:  99%|█████████▉| 145/146 [00:02<00:00, 57.79it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.0138]\n",
      "Epoch 167: 100%|██████████| 146/146 [00:02<00:00, 57.77it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.014] \n",
      "Epoch 167: 100%|██████████| 146/146 [00:02<00:00, 57.66it/s, loss=0.000879, v_num=8, train_loss=0.000216, val_loss=0.014]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 167, global step 22008: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168:  90%|████████▉ | 131/146 [00:02<00:00, 55.79it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 168:  90%|█████████ | 132/146 [00:02<00:00, 55.72it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168:  91%|█████████ | 133/146 [00:02<00:00, 55.90it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168:  92%|█████████▏| 134/146 [00:02<00:00, 56.06it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168:  92%|█████████▏| 135/146 [00:02<00:00, 56.22it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168:  93%|█████████▎| 136/146 [00:02<00:00, 56.38it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168:  94%|█████████▍| 137/146 [00:02<00:00, 56.56it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168:  95%|█████████▍| 138/146 [00:02<00:00, 56.67it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168:  95%|█████████▌| 139/146 [00:02<00:00, 56.85it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168:  96%|█████████▌| 140/146 [00:02<00:00, 57.00it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168:  97%|█████████▋| 141/146 [00:02<00:00, 57.10it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168:  97%|█████████▋| 142/146 [00:02<00:00, 57.28it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168:  98%|█████████▊| 143/146 [00:02<00:00, 57.40it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168:  99%|█████████▊| 144/146 [00:02<00:00, 57.57it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168:  99%|█████████▉| 145/146 [00:02<00:00, 57.67it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.014]\n",
      "Epoch 168: 100%|██████████| 146/146 [00:02<00:00, 57.75it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.013]\n",
      "Epoch 168: 100%|██████████| 146/146 [00:02<00:00, 57.61it/s, loss=0.000887, v_num=8, train_loss=0.000199, val_loss=0.013]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 168, global step 22139: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169:  90%|████████▉ | 131/146 [00:02<00:00, 54.92it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 169:  90%|█████████ | 132/146 [00:02<00:00, 54.88it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169:  91%|█████████ | 133/146 [00:02<00:00, 55.05it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169:  92%|█████████▏| 134/146 [00:02<00:00, 55.16it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169:  92%|█████████▏| 135/146 [00:02<00:00, 55.30it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169:  93%|█████████▎| 136/146 [00:02<00:00, 55.46it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169:  94%|█████████▍| 137/146 [00:02<00:00, 55.64it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169:  95%|█████████▍| 138/146 [00:02<00:00, 55.80it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169:  95%|█████████▌| 139/146 [00:02<00:00, 55.93it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169:  96%|█████████▌| 140/146 [00:02<00:00, 56.11it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169:  97%|█████████▋| 141/146 [00:02<00:00, 56.28it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169:  97%|█████████▋| 142/146 [00:02<00:00, 56.41it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169:  98%|█████████▊| 143/146 [00:02<00:00, 56.56it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169:  99%|█████████▊| 144/146 [00:02<00:00, 56.73it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169:  99%|█████████▉| 145/146 [00:02<00:00, 56.86it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.013]\n",
      "Epoch 169: 100%|██████████| 146/146 [00:02<00:00, 56.76it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.0138]\n",
      "Epoch 169: 100%|██████████| 146/146 [00:02<00:00, 56.65it/s, loss=0.000897, v_num=8, train_loss=0.000219, val_loss=0.0138]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 169, global step 22270: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170:  90%|████████▉ | 131/146 [00:02<00:00, 55.88it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 170:  90%|█████████ | 132/146 [00:02<00:00, 55.72it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170:  91%|█████████ | 133/146 [00:02<00:00, 55.83it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170:  92%|█████████▏| 134/146 [00:02<00:00, 55.97it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170:  92%|█████████▏| 135/146 [00:02<00:00, 56.13it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170:  93%|█████████▎| 136/146 [00:02<00:00, 56.29it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170:  94%|█████████▍| 137/146 [00:02<00:00, 56.47it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170:  95%|█████████▍| 138/146 [00:02<00:00, 56.65it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170:  95%|█████████▌| 139/146 [00:02<00:00, 56.82it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170:  96%|█████████▌| 140/146 [00:02<00:00, 57.00it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170:  97%|█████████▋| 141/146 [00:02<00:00, 57.08it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170:  97%|█████████▋| 142/146 [00:02<00:00, 57.16it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170:  98%|█████████▊| 143/146 [00:02<00:00, 57.22it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170:  99%|█████████▊| 144/146 [00:02<00:00, 57.39it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170:  99%|█████████▉| 145/146 [00:02<00:00, 57.47it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0138]\n",
      "Epoch 170: 100%|██████████| 146/146 [00:02<00:00, 57.43it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0132]\n",
      "Epoch 170: 100%|██████████| 146/146 [00:02<00:00, 57.27it/s, loss=0.000873, v_num=8, train_loss=0.000202, val_loss=0.0132]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 170, global step 22401: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171:  90%|████████▉ | 131/146 [00:02<00:00, 48.39it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 171:  90%|█████████ | 132/146 [00:02<00:00, 48.35it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171:  91%|█████████ | 133/146 [00:02<00:00, 48.48it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171:  92%|█████████▏| 134/146 [00:02<00:00, 48.62it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171:  92%|█████████▏| 135/146 [00:02<00:00, 48.73it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171:  93%|█████████▎| 136/146 [00:02<00:00, 48.90it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171:  94%|█████████▍| 137/146 [00:02<00:00, 49.05it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171:  95%|█████████▍| 138/146 [00:02<00:00, 49.19it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171:  95%|█████████▌| 139/146 [00:02<00:00, 49.37it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171:  96%|█████████▌| 140/146 [00:02<00:00, 49.52it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171:  97%|█████████▋| 141/146 [00:02<00:00, 49.68it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171:  97%|█████████▋| 142/146 [00:02<00:00, 49.84it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171:  98%|█████████▊| 143/146 [00:02<00:00, 50.00it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171:  99%|█████████▊| 144/146 [00:02<00:00, 50.15it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171:  99%|█████████▉| 145/146 [00:02<00:00, 50.29it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0132]\n",
      "Epoch 171: 100%|██████████| 146/146 [00:02<00:00, 50.32it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0135]\n",
      "Epoch 171: 100%|██████████| 146/146 [00:02<00:00, 50.24it/s, loss=0.000884, v_num=8, train_loss=0.000253, val_loss=0.0135]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171, global step 22532: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172:  90%|████████▉ | 131/146 [00:02<00:00, 54.74it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 172:  90%|█████████ | 132/146 [00:02<00:00, 54.68it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172:  91%|█████████ | 133/146 [00:02<00:00, 54.84it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172:  92%|█████████▏| 134/146 [00:02<00:00, 55.03it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172:  92%|█████████▏| 135/146 [00:02<00:00, 55.17it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172:  93%|█████████▎| 136/146 [00:02<00:00, 55.33it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172:  94%|█████████▍| 137/146 [00:02<00:00, 55.46it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172:  95%|█████████▍| 138/146 [00:02<00:00, 55.60it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172:  95%|█████████▌| 139/146 [00:02<00:00, 55.77it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172:  96%|█████████▌| 140/146 [00:02<00:00, 55.93it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172:  97%|█████████▋| 141/146 [00:02<00:00, 56.13it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172:  97%|█████████▋| 142/146 [00:02<00:00, 56.26it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172:  98%|█████████▊| 143/146 [00:02<00:00, 56.41it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172:  99%|█████████▊| 144/146 [00:02<00:00, 56.51it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172:  99%|█████████▉| 145/146 [00:02<00:00, 56.64it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.0135]\n",
      "Epoch 172: 100%|██████████| 146/146 [00:02<00:00, 56.67it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.013] \n",
      "Epoch 172: 100%|██████████| 146/146 [00:02<00:00, 56.52it/s, loss=0.00087, v_num=8, train_loss=0.000191, val_loss=0.013]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172, global step 22663: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173:  90%|████████▉ | 131/146 [00:02<00:00, 56.36it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 173:  90%|█████████ | 132/146 [00:02<00:00, 56.29it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173:  91%|█████████ | 133/146 [00:02<00:00, 56.47it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173:  92%|█████████▏| 134/146 [00:02<00:00, 56.63it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173:  92%|█████████▏| 135/146 [00:02<00:00, 56.79it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173:  93%|█████████▎| 136/146 [00:02<00:00, 56.95it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173:  94%|█████████▍| 137/146 [00:02<00:00, 57.10it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173:  95%|█████████▍| 138/146 [00:02<00:00, 57.21it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173:  95%|█████████▌| 139/146 [00:02<00:00, 57.34it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173:  96%|█████████▌| 140/146 [00:02<00:00, 57.47it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173:  97%|█████████▋| 141/146 [00:02<00:00, 57.55it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173:  97%|█████████▋| 142/146 [00:02<00:00, 57.67it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173:  98%|█████████▊| 143/146 [00:02<00:00, 57.84it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173:  99%|█████████▊| 144/146 [00:02<00:00, 57.99it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173:  99%|█████████▉| 145/146 [00:02<00:00, 58.16it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.013]\n",
      "Epoch 173: 100%|██████████| 146/146 [00:02<00:00, 58.19it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.0137]\n",
      "Epoch 173: 100%|██████████| 146/146 [00:02<00:00, 58.02it/s, loss=0.000875, v_num=8, train_loss=0.00028, val_loss=0.0137]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 173, global step 22794: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174:  90%|████████▉ | 131/146 [00:02<00:00, 55.76it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 174:  90%|█████████ | 132/146 [00:02<00:00, 55.69it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174:  91%|█████████ | 133/146 [00:02<00:00, 55.88it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174:  92%|█████████▏| 134/146 [00:02<00:00, 56.04it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174:  92%|█████████▏| 135/146 [00:02<00:00, 56.20it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174:  93%|█████████▎| 136/146 [00:02<00:00, 56.36it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174:  94%|█████████▍| 137/146 [00:02<00:00, 56.51it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174:  95%|█████████▍| 138/146 [00:02<00:00, 56.65it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174:  95%|█████████▌| 139/146 [00:02<00:00, 56.80it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174:  96%|█████████▌| 140/146 [00:02<00:00, 56.95it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174:  97%|█████████▋| 141/146 [00:02<00:00, 57.10it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174:  97%|█████████▋| 142/146 [00:02<00:00, 57.23it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174:  98%|█████████▊| 143/146 [00:02<00:00, 57.40it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174:  99%|█████████▊| 144/146 [00:02<00:00, 57.50it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174:  99%|█████████▉| 145/146 [00:02<00:00, 57.67it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0137]\n",
      "Epoch 174: 100%|██████████| 146/146 [00:02<00:00, 57.70it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0125]\n",
      "Epoch 174: 100%|██████████| 146/146 [00:02<00:00, 57.57it/s, loss=0.000852, v_num=8, train_loss=0.000181, val_loss=0.0125]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 174, global step 22925: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175:  90%|████████▉ | 131/146 [00:02<00:00, 55.67it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 175:  90%|█████████ | 132/146 [00:02<00:00, 55.57it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175:  91%|█████████ | 133/146 [00:02<00:00, 55.71it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175:  92%|█████████▏| 134/146 [00:02<00:00, 55.90it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175:  92%|█████████▏| 135/146 [00:02<00:00, 56.06it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175:  93%|█████████▎| 136/146 [00:02<00:00, 56.19it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175:  94%|█████████▍| 137/146 [00:02<00:00, 56.37it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175:  95%|█████████▍| 138/146 [00:02<00:00, 56.55it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175:  95%|█████████▌| 139/146 [00:02<00:00, 56.68it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175:  96%|█████████▌| 140/146 [00:02<00:00, 56.88it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175:  97%|█████████▋| 141/146 [00:02<00:00, 57.01it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175:  97%|█████████▋| 142/146 [00:02<00:00, 57.12it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175:  98%|█████████▊| 143/146 [00:02<00:00, 57.29it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175:  99%|█████████▊| 144/146 [00:02<00:00, 57.46it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175:  99%|█████████▉| 145/146 [00:02<00:00, 57.63it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0125]\n",
      "Epoch 175: 100%|██████████| 146/146 [00:02<00:00, 57.64it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0139]\n",
      "Epoch 175: 100%|██████████| 146/146 [00:02<00:00, 57.48it/s, loss=0.000863, v_num=8, train_loss=0.000273, val_loss=0.0139]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 175, global step 23056: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176:  90%|████████▉ | 131/146 [00:02<00:00, 55.86it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 176:  90%|█████████ | 132/146 [00:02<00:00, 55.79it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176:  91%|█████████ | 133/146 [00:02<00:00, 55.97it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176:  92%|█████████▏| 134/146 [00:02<00:00, 56.13it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176:  92%|█████████▏| 135/146 [00:02<00:00, 56.27it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176:  93%|█████████▎| 136/146 [00:02<00:00, 56.45it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176:  94%|█████████▍| 137/146 [00:02<00:00, 56.58it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176:  95%|█████████▍| 138/146 [00:02<00:00, 56.72it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176:  95%|█████████▌| 139/146 [00:02<00:00, 56.89it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176:  96%|█████████▌| 140/146 [00:02<00:00, 57.05it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176:  97%|█████████▋| 141/146 [00:02<00:00, 57.20it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176:  97%|█████████▋| 142/146 [00:02<00:00, 57.32it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176:  98%|█████████▊| 143/146 [00:02<00:00, 57.47it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176:  99%|█████████▊| 144/146 [00:02<00:00, 57.62it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176:  99%|█████████▉| 145/146 [00:02<00:00, 57.74it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0139]\n",
      "Epoch 176: 100%|██████████| 146/146 [00:02<00:00, 57.79it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0117]\n",
      "Epoch 176: 100%|██████████| 146/146 [00:02<00:00, 57.66it/s, loss=0.000824, v_num=8, train_loss=0.000208, val_loss=0.0117]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 176, global step 23187: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177:  90%|████████▉ | 131/146 [00:02<00:00, 48.09it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 177:  90%|█████████ | 132/146 [00:02<00:00, 48.03it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177:  91%|█████████ | 133/146 [00:02<00:00, 48.15it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177:  92%|█████████▏| 134/146 [00:02<00:00, 48.28it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177:  92%|█████████▏| 135/146 [00:02<00:00, 48.37it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177:  93%|█████████▎| 136/146 [00:02<00:00, 48.52it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177:  94%|█████████▍| 137/146 [00:02<00:00, 48.66it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177:  95%|█████████▍| 138/146 [00:02<00:00, 48.66it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177:  95%|█████████▌| 139/146 [00:02<00:00, 48.80it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177:  96%|█████████▌| 140/146 [00:02<00:00, 48.96it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177:  97%|█████████▋| 141/146 [00:02<00:00, 49.09it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177:  97%|█████████▋| 142/146 [00:02<00:00, 49.23it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177:  98%|█████████▊| 143/146 [00:02<00:00, 49.37it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177:  99%|█████████▊| 144/146 [00:02<00:00, 49.51it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177:  99%|█████████▉| 145/146 [00:02<00:00, 49.67it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0117]\n",
      "Epoch 177: 100%|██████████| 146/146 [00:02<00:00, 49.69it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0135]\n",
      "Epoch 177: 100%|██████████| 146/146 [00:02<00:00, 49.62it/s, loss=0.000833, v_num=8, train_loss=0.000243, val_loss=0.0135]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 177, global step 23318: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178:  90%|████████▉ | 131/146 [00:02<00:00, 55.69it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 178:  90%|█████████ | 132/146 [00:02<00:00, 55.55it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178:  91%|█████████ | 133/146 [00:02<00:00, 55.71it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178:  92%|█████████▏| 134/146 [00:02<00:00, 55.85it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178:  92%|█████████▏| 135/146 [00:02<00:00, 56.01it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178:  93%|█████████▎| 136/146 [00:02<00:00, 56.06it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178:  94%|█████████▍| 137/146 [00:02<00:00, 56.19it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178:  95%|█████████▍| 138/146 [00:02<00:00, 56.35it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178:  95%|█████████▌| 139/146 [00:02<00:00, 56.50it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178:  96%|█████████▌| 140/146 [00:02<00:00, 56.63it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178:  97%|█████████▋| 141/146 [00:02<00:00, 56.78it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178:  97%|█████████▋| 142/146 [00:02<00:00, 56.93it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178:  98%|█████████▊| 143/146 [00:02<00:00, 57.08it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178:  99%|█████████▊| 144/146 [00:02<00:00, 57.25it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178:  99%|█████████▉| 145/146 [00:02<00:00, 57.38it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0135]\n",
      "Epoch 178: 100%|██████████| 146/146 [00:02<00:00, 57.36it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0127]\n",
      "Epoch 178: 100%|██████████| 146/146 [00:02<00:00, 57.27it/s, loss=0.000858, v_num=8, train_loss=0.000827, val_loss=0.0127]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 178, global step 23449: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179:  90%|████████▉ | 131/146 [00:02<00:00, 55.75it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 179:  90%|█████████ | 132/146 [00:02<00:00, 55.61it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179:  91%|█████████ | 133/146 [00:02<00:00, 55.77it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179:  92%|█████████▏| 134/146 [00:02<00:00, 55.96it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179:  92%|█████████▏| 135/146 [00:02<00:00, 56.07it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179:  93%|█████████▎| 136/146 [00:02<00:00, 56.18it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179:  94%|█████████▍| 137/146 [00:02<00:00, 56.36it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179:  95%|█████████▍| 138/146 [00:02<00:00, 56.27it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179:  95%|█████████▌| 139/146 [00:02<00:00, 56.42it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179:  96%|█████████▌| 140/146 [00:02<00:00, 56.53it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179:  97%|█████████▋| 141/146 [00:02<00:00, 56.63it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179:  97%|█████████▋| 142/146 [00:02<00:00, 56.79it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179:  98%|█████████▊| 143/146 [00:02<00:00, 56.96it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179:  99%|█████████▊| 144/146 [00:02<00:00, 57.08it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179:  99%|█████████▉| 145/146 [00:02<00:00, 57.23it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0127]\n",
      "Epoch 179: 100%|██████████| 146/146 [00:02<00:00, 57.24it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0119]\n",
      "Epoch 179: 100%|██████████| 146/146 [00:02<00:00, 57.13it/s, loss=0.000881, v_num=8, train_loss=0.000427, val_loss=0.0119]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 179, global step 23580: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180:  90%|████████▉ | 131/146 [00:02<00:00, 56.03it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 180:  90%|█████████ | 132/146 [00:02<00:00, 55.98it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180:  91%|█████████ | 133/146 [00:02<00:00, 56.11it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180:  92%|█████████▏| 134/146 [00:02<00:00, 56.27it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180:  92%|█████████▏| 135/146 [00:02<00:00, 56.43it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180:  93%|█████████▎| 136/146 [00:02<00:00, 56.57it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180:  94%|█████████▍| 137/146 [00:02<00:00, 56.72it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180:  95%|█████████▍| 138/146 [00:02<00:00, 56.88it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180:  95%|█████████▌| 139/146 [00:02<00:00, 57.03it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180:  96%|█████████▌| 140/146 [00:02<00:00, 57.21it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180:  97%|█████████▋| 141/146 [00:02<00:00, 57.34it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180:  97%|█████████▋| 142/146 [00:02<00:00, 57.51it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180:  98%|█████████▊| 143/146 [00:02<00:00, 57.61it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180:  99%|█████████▊| 144/146 [00:02<00:00, 57.76it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180:  99%|█████████▉| 145/146 [00:02<00:00, 57.90it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0119]\n",
      "Epoch 180: 100%|██████████| 146/146 [00:02<00:00, 57.93it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0164]\n",
      "Epoch 180: 100%|██████████| 146/146 [00:02<00:00, 57.77it/s, loss=0.000907, v_num=8, train_loss=0.000179, val_loss=0.0164]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 180, global step 23711: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181:  90%|████████▉ | 131/146 [00:02<00:00, 55.43it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 181:  90%|█████████ | 132/146 [00:02<00:00, 55.32it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181:  91%|█████████ | 133/146 [00:02<00:00, 55.46it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181:  92%|█████████▏| 134/146 [00:02<00:00, 55.62it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181:  92%|█████████▏| 135/146 [00:02<00:00, 55.76it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181:  93%|█████████▎| 136/146 [00:02<00:00, 55.94it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181:  94%|█████████▍| 137/146 [00:02<00:00, 56.10it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181:  95%|█████████▍| 138/146 [00:02<00:00, 56.25it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181:  95%|█████████▌| 139/146 [00:02<00:00, 56.39it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181:  96%|█████████▌| 140/146 [00:02<00:00, 56.54it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181:  97%|█████████▋| 141/146 [00:02<00:00, 56.42it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181:  97%|█████████▋| 142/146 [00:02<00:00, 56.55it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181:  98%|█████████▊| 143/146 [00:02<00:00, 56.72it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181:  99%|█████████▊| 144/146 [00:02<00:00, 56.82it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181:  99%|█████████▉| 145/146 [00:02<00:00, 56.99it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.0164]\n",
      "Epoch 181: 100%|██████████| 146/146 [00:02<00:00, 56.98it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.012] \n",
      "Epoch 181: 100%|██████████| 146/146 [00:02<00:00, 56.74it/s, loss=0.00107, v_num=8, train_loss=0.000519, val_loss=0.012]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181, global step 23842: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182:  90%|████████▉ | 131/146 [00:02<00:00, 55.43it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 182:  90%|█████████ | 132/146 [00:02<00:00, 55.17it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182:  91%|█████████ | 133/146 [00:02<00:00, 55.32it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182:  92%|█████████▏| 134/146 [00:02<00:00, 55.50it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182:  92%|█████████▏| 135/146 [00:02<00:00, 55.64it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182:  93%|█████████▎| 136/146 [00:02<00:00, 55.80it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182:  94%|█████████▍| 137/146 [00:02<00:00, 55.91it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182:  95%|█████████▍| 138/146 [00:02<00:00, 56.04it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182:  95%|█████████▌| 139/146 [00:02<00:00, 56.18it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182:  96%|█████████▌| 140/146 [00:02<00:00, 56.28it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182:  97%|█████████▋| 141/146 [00:02<00:00, 56.41it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182:  97%|█████████▋| 142/146 [00:02<00:00, 56.54it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182:  98%|█████████▊| 143/146 [00:02<00:00, 56.67it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182:  99%|█████████▊| 144/146 [00:02<00:00, 56.82it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182:  99%|█████████▉| 145/146 [00:02<00:00, 56.97it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.012]\n",
      "Epoch 182: 100%|██████████| 146/146 [00:02<00:00, 56.98it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.0143]\n",
      "Epoch 182: 100%|██████████| 146/146 [00:02<00:00, 56.89it/s, loss=0.000835, v_num=8, train_loss=0.000296, val_loss=0.0143]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182, global step 23973: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183:  90%|████████▉ | 131/146 [00:02<00:00, 47.74it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 183:  90%|█████████ | 132/146 [00:02<00:00, 47.65it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183:  91%|█████████ | 133/146 [00:02<00:00, 47.79it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183:  92%|█████████▏| 134/146 [00:02<00:00, 47.89it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183:  92%|█████████▏| 135/146 [00:02<00:00, 48.02it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183:  93%|█████████▎| 136/146 [00:02<00:00, 48.14it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183:  94%|█████████▍| 137/146 [00:02<00:00, 48.27it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183:  95%|█████████▍| 138/146 [00:02<00:00, 48.37it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183:  95%|█████████▌| 139/146 [00:02<00:00, 48.53it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183:  96%|█████████▌| 140/146 [00:02<00:00, 48.66it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183:  97%|█████████▋| 141/146 [00:02<00:00, 48.77it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183:  97%|█████████▋| 142/146 [00:02<00:00, 48.89it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183:  98%|█████████▊| 143/146 [00:02<00:00, 48.95it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183:  99%|█████████▊| 144/146 [00:02<00:00, 49.08it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183:  99%|█████████▉| 145/146 [00:02<00:00, 49.22it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0143]\n",
      "Epoch 183: 100%|██████████| 146/146 [00:02<00:00, 49.22it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0145]\n",
      "Epoch 183: 100%|██████████| 146/146 [00:02<00:00, 49.12it/s, loss=0.000916, v_num=8, train_loss=0.000555, val_loss=0.0145]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 183, global step 24104: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184:  90%|████████▉ | 131/146 [00:02<00:00, 54.99it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 184:  90%|█████████ | 132/146 [00:02<00:00, 54.93it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184:  91%|█████████ | 133/146 [00:02<00:00, 55.07it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184:  92%|█████████▏| 134/146 [00:02<00:00, 55.21it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184:  92%|█████████▏| 135/146 [00:02<00:00, 55.39it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184:  93%|█████████▎| 136/146 [00:02<00:00, 55.51it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184:  94%|█████████▍| 137/146 [00:02<00:00, 55.64it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184:  95%|█████████▍| 138/146 [00:02<00:00, 55.80it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184:  95%|█████████▌| 139/146 [00:02<00:00, 55.93it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184:  96%|█████████▌| 140/146 [00:02<00:00, 56.09it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184:  97%|█████████▋| 141/146 [00:02<00:00, 56.22it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184:  97%|█████████▋| 142/146 [00:02<00:00, 56.32it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184:  98%|█████████▊| 143/146 [00:02<00:00, 56.50it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184:  99%|█████████▊| 144/146 [00:02<00:00, 56.64it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184:  99%|█████████▉| 145/146 [00:02<00:00, 56.79it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0145]\n",
      "Epoch 184: 100%|██████████| 146/146 [00:02<00:00, 56.81it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0131]\n",
      "Epoch 184: 100%|██████████| 146/146 [00:02<00:00, 56.72it/s, loss=0.000932, v_num=8, train_loss=0.000225, val_loss=0.0131]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 184, global step 24235: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185:  90%|████████▉ | 131/146 [00:02<00:00, 54.11it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 185:  90%|█████████ | 132/146 [00:02<00:00, 54.03it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185:  91%|█████████ | 133/146 [00:02<00:00, 54.19it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185:  92%|█████████▏| 134/146 [00:02<00:00, 54.36it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185:  92%|█████████▏| 135/146 [00:02<00:00, 54.52it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185:  93%|█████████▎| 136/146 [00:02<00:00, 54.70it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185:  94%|█████████▍| 137/146 [00:02<00:00, 54.86it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185:  95%|█████████▍| 138/146 [00:02<00:00, 55.02it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185:  95%|█████████▌| 139/146 [00:02<00:00, 55.15it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185:  96%|█████████▌| 140/146 [00:02<00:00, 55.31it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185:  97%|█████████▋| 141/146 [00:02<00:00, 55.44it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185:  97%|█████████▋| 142/146 [00:02<00:00, 55.62it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185:  98%|█████████▊| 143/146 [00:02<00:00, 55.70it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185:  99%|█████████▊| 144/146 [00:02<00:00, 55.85it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185:  99%|█████████▉| 145/146 [00:02<00:00, 55.98it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0131]\n",
      "Epoch 185: 100%|██████████| 146/146 [00:02<00:00, 56.02it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0164]\n",
      "Epoch 185: 100%|██████████| 146/146 [00:02<00:00, 55.89it/s, loss=0.000885, v_num=8, train_loss=0.000257, val_loss=0.0164]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 185, global step 24366: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186:  90%|████████▉ | 131/146 [00:02<00:00, 55.67it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 186:  90%|█████████ | 132/146 [00:02<00:00, 55.57it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186:  91%|█████████ | 133/146 [00:02<00:00, 55.74it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186:  92%|█████████▏| 134/146 [00:02<00:00, 55.88it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186:  92%|█████████▏| 135/146 [00:02<00:00, 56.01it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186:  93%|█████████▎| 136/146 [00:02<00:00, 56.15it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186:  94%|█████████▍| 137/146 [00:02<00:00, 56.33it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186:  95%|█████████▍| 138/146 [00:02<00:00, 56.44it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186:  95%|█████████▌| 139/146 [00:02<00:00, 56.64it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186:  96%|█████████▌| 140/146 [00:02<00:00, 56.77it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186:  97%|█████████▋| 141/146 [00:02<00:00, 56.90it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186:  97%|█████████▋| 142/146 [00:02<00:00, 57.07it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186:  98%|█████████▊| 143/146 [00:02<00:00, 57.22it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186:  99%|█████████▊| 144/146 [00:02<00:00, 57.37it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186:  99%|█████████▉| 145/146 [00:02<00:00, 57.49it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0164]\n",
      "Epoch 186: 100%|██████████| 146/146 [00:02<00:00, 57.48it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0118]\n",
      "Epoch 186: 100%|██████████| 146/146 [00:02<00:00, 57.36it/s, loss=0.000925, v_num=8, train_loss=0.00032, val_loss=0.0118]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 186, global step 24497: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187:  90%|████████▉ | 131/146 [00:02<00:00, 55.32it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 187:  90%|█████████ | 132/146 [00:02<00:00, 55.16it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187:  91%|█████████ | 133/146 [00:02<00:00, 55.34it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187:  92%|█████████▏| 134/146 [00:02<00:00, 55.48it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187:  92%|█████████▏| 135/146 [00:02<00:00, 55.67it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187:  93%|█████████▎| 136/146 [00:02<00:00, 55.80it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187:  94%|█████████▍| 137/146 [00:02<00:00, 55.96it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187:  95%|█████████▍| 138/146 [00:02<00:00, 56.09it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187:  95%|█████████▌| 139/146 [00:02<00:00, 56.25it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187:  96%|█████████▌| 140/146 [00:02<00:00, 56.40it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187:  97%|█████████▋| 141/146 [00:02<00:00, 56.53it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187:  97%|█████████▋| 142/146 [00:02<00:00, 56.66it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187:  98%|█████████▊| 143/146 [00:02<00:00, 56.76it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187:  99%|█████████▊| 144/146 [00:02<00:00, 56.91it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187:  99%|█████████▉| 145/146 [00:02<00:00, 57.06it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0118]\n",
      "Epoch 187: 100%|██████████| 146/146 [00:02<00:00, 57.03it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0149]\n",
      "Epoch 187: 100%|██████████| 146/146 [00:02<00:00, 56.94it/s, loss=0.000874, v_num=8, train_loss=0.000237, val_loss=0.0149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 187, global step 24628: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188:  90%|████████▉ | 131/146 [00:02<00:00, 55.58it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 188:  90%|█████████ | 132/146 [00:02<00:00, 55.48it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188:  91%|█████████ | 133/146 [00:02<00:00, 55.62it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188:  92%|█████████▏| 134/146 [00:02<00:00, 55.81it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188:  92%|█████████▏| 135/146 [00:02<00:00, 55.92it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188:  93%|█████████▎| 136/146 [00:02<00:00, 56.08it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188:  94%|█████████▍| 137/146 [00:02<00:00, 56.24it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188:  95%|█████████▍| 138/146 [00:02<00:00, 56.39it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188:  95%|█████████▌| 139/146 [00:02<00:00, 56.52it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188:  96%|█████████▌| 140/146 [00:02<00:00, 56.65it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188:  97%|█████████▋| 141/146 [00:02<00:00, 56.83it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188:  97%|█████████▋| 142/146 [00:02<00:00, 56.91it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188:  98%|█████████▊| 143/146 [00:02<00:00, 57.04it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188:  99%|█████████▊| 144/146 [00:02<00:00, 57.18it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188:  99%|█████████▉| 145/146 [00:02<00:00, 57.33it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0149]\n",
      "Epoch 188: 100%|██████████| 146/146 [00:02<00:00, 57.32it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0151]\n",
      "Epoch 188: 100%|██████████| 146/146 [00:02<00:00, 57.23it/s, loss=0.000922, v_num=8, train_loss=0.000579, val_loss=0.0151]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 188, global step 24759: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189:  90%|████████▉ | 131/146 [00:02<00:00, 49.10it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 189:  90%|█████████ | 132/146 [00:02<00:00, 49.01it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189:  91%|█████████ | 133/146 [00:02<00:00, 49.13it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189:  92%|█████████▏| 134/146 [00:02<00:00, 49.22it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189:  92%|█████████▏| 135/146 [00:02<00:00, 49.36it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189:  93%|█████████▎| 136/146 [00:02<00:00, 49.47it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189:  94%|█████████▍| 137/146 [00:02<00:00, 49.60it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189:  95%|█████████▍| 138/146 [00:02<00:00, 49.74it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189:  95%|█████████▌| 139/146 [00:02<00:00, 49.87it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189:  96%|█████████▌| 140/146 [00:02<00:00, 49.98it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189:  97%|█████████▋| 141/146 [00:02<00:00, 50.09it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189:  97%|█████████▋| 142/146 [00:02<00:00, 50.21it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189:  98%|█████████▊| 143/146 [00:02<00:00, 50.37it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189:  99%|█████████▊| 144/146 [00:02<00:00, 50.52it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189:  99%|█████████▉| 145/146 [00:02<00:00, 50.62it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0151]\n",
      "Epoch 189: 100%|██████████| 146/146 [00:02<00:00, 50.64it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0119]\n",
      "Epoch 189: 100%|██████████| 146/146 [00:02<00:00, 50.53it/s, loss=0.000871, v_num=8, train_loss=0.000205, val_loss=0.0119]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 189, global step 24890: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190:  90%|████████▉ | 131/146 [00:02<00:00, 54.11it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 190:  90%|█████████ | 132/146 [00:02<00:00, 54.03it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190:  91%|█████████ | 133/146 [00:02<00:00, 54.19it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190:  92%|█████████▏| 134/146 [00:02<00:00, 54.38it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190:  92%|█████████▏| 135/146 [00:02<00:00, 54.54it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190:  93%|█████████▎| 136/146 [00:02<00:00, 54.75it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190:  94%|█████████▍| 137/146 [00:02<00:00, 54.91it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190:  95%|█████████▍| 138/146 [00:02<00:00, 55.09it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190:  95%|█████████▌| 139/146 [00:02<00:00, 55.24it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190:  96%|█████████▌| 140/146 [00:02<00:00, 55.40it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190:  97%|█████████▋| 141/146 [00:02<00:00, 55.53it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190:  97%|█████████▋| 142/146 [00:02<00:00, 55.70it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190:  98%|█████████▊| 143/146 [00:02<00:00, 55.83it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190:  99%|█████████▊| 144/146 [00:02<00:00, 55.96it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190:  99%|█████████▉| 145/146 [00:02<00:00, 56.09it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0119]\n",
      "Epoch 190: 100%|██████████| 146/146 [00:02<00:00, 56.17it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0152]\n",
      "Epoch 190: 100%|██████████| 146/146 [00:02<00:00, 56.04it/s, loss=0.000851, v_num=8, train_loss=0.000178, val_loss=0.0152]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 190, global step 25021: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191:  90%|████████▉ | 131/146 [00:02<00:00, 54.88it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 191:  90%|█████████ | 132/146 [00:02<00:00, 54.79it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191:  91%|█████████ | 133/146 [00:02<00:00, 54.98it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191:  92%|█████████▏| 134/146 [00:02<00:00, 55.16it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191:  92%|█████████▏| 135/146 [00:02<00:00, 55.28it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191:  93%|█████████▎| 136/146 [00:02<00:00, 55.46it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191:  94%|█████████▍| 137/146 [00:02<00:00, 55.64it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191:  95%|█████████▍| 138/146 [00:02<00:00, 55.82it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191:  95%|█████████▌| 139/146 [00:02<00:00, 55.91it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191:  96%|█████████▌| 140/146 [00:02<00:00, 56.04it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191:  97%|█████████▋| 141/146 [00:02<00:00, 56.22it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191:  97%|█████████▋| 142/146 [00:02<00:00, 56.30it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191:  98%|█████████▊| 143/146 [00:02<00:00, 56.43it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191:  99%|█████████▊| 144/146 [00:02<00:00, 56.56it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191:  99%|█████████▉| 145/146 [00:02<00:00, 56.68it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.0152]\n",
      "Epoch 191: 100%|██████████| 146/146 [00:02<00:00, 56.72it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.014] \n",
      "Epoch 191: 100%|██████████| 146/146 [00:02<00:00, 56.61it/s, loss=0.000881, v_num=8, train_loss=0.000487, val_loss=0.014]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 191, global step 25152: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192:  90%|████████▉ | 131/146 [00:02<00:00, 55.20it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 192:  90%|█████████ | 132/146 [00:02<00:00, 55.16it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192:  91%|█████████ | 133/146 [00:02<00:00, 55.30it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192:  92%|█████████▏| 134/146 [00:02<00:00, 55.46it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192:  92%|█████████▏| 135/146 [00:02<00:00, 55.62it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192:  93%|█████████▎| 136/146 [00:02<00:00, 55.80it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192:  94%|█████████▍| 137/146 [00:02<00:00, 55.96it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192:  95%|█████████▍| 138/146 [00:02<00:00, 56.14it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192:  95%|█████████▌| 139/146 [00:02<00:00, 56.29it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192:  96%|█████████▌| 140/146 [00:02<00:00, 56.45it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192:  97%|█████████▋| 141/146 [00:02<00:00, 56.58it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192:  97%|█████████▋| 142/146 [00:02<00:00, 56.73it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192:  98%|█████████▊| 143/146 [00:02<00:00, 56.90it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192:  99%|█████████▊| 144/146 [00:02<00:00, 56.98it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192:  99%|█████████▉| 145/146 [00:02<00:00, 57.13it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.014]\n",
      "Epoch 192: 100%|██████████| 146/146 [00:02<00:00, 57.12it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.0126]\n",
      "Epoch 192: 100%|██████████| 146/146 [00:02<00:00, 57.00it/s, loss=0.000877, v_num=8, train_loss=0.00017, val_loss=0.0126]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 192, global step 25283: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193:  90%|████████▉ | 131/146 [00:02<00:00, 55.04it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 193:  90%|█████████ | 132/146 [00:02<00:00, 54.97it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193:  91%|█████████ | 133/146 [00:02<00:00, 55.11it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193:  92%|█████████▏| 134/146 [00:02<00:00, 55.25it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193:  92%|█████████▏| 135/146 [00:02<00:00, 55.41it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193:  93%|█████████▎| 136/146 [00:02<00:00, 55.57it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193:  94%|█████████▍| 137/146 [00:02<00:00, 55.75it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193:  95%|█████████▍| 138/146 [00:02<00:00, 55.91it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193:  95%|█████████▌| 139/146 [00:02<00:00, 56.07it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193:  96%|█████████▌| 140/146 [00:02<00:00, 56.22it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193:  97%|█████████▋| 141/146 [00:02<00:00, 56.33it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193:  97%|█████████▋| 142/146 [00:02<00:00, 56.46it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193:  98%|█████████▊| 143/146 [00:02<00:00, 56.61it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193:  99%|█████████▊| 144/146 [00:02<00:00, 56.76it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193:  99%|█████████▉| 145/146 [00:02<00:00, 56.90it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0126]\n",
      "Epoch 193: 100%|██████████| 146/146 [00:02<00:00, 56.87it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0162]\n",
      "Epoch 193: 100%|██████████| 146/146 [00:02<00:00, 56.74it/s, loss=0.000847, v_num=8, train_loss=0.000169, val_loss=0.0162]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 193, global step 25414: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194:  90%|████████▉ | 131/146 [00:02<00:00, 54.60it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 194:  90%|█████████ | 132/146 [00:02<00:00, 54.59it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194:  91%|█████████ | 133/146 [00:02<00:00, 54.71it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194:  92%|█████████▏| 134/146 [00:02<00:00, 54.82it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194:  92%|█████████▏| 135/146 [00:02<00:00, 54.94it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194:  93%|█████████▎| 136/146 [00:02<00:00, 55.08it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194:  94%|█████████▍| 137/146 [00:02<00:00, 55.26it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194:  95%|█████████▍| 138/146 [00:02<00:00, 55.42it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194:  95%|█████████▌| 139/146 [00:02<00:00, 55.57it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194:  96%|█████████▌| 140/146 [00:02<00:00, 55.68it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194:  97%|█████████▋| 141/146 [00:02<00:00, 55.86it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194:  97%|█████████▋| 142/146 [00:02<00:00, 55.97it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194:  98%|█████████▊| 143/146 [00:02<00:00, 56.10it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194:  99%|█████████▊| 144/146 [00:02<00:00, 56.27it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194:  99%|█████████▉| 145/146 [00:02<00:00, 56.39it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0162]\n",
      "Epoch 194: 100%|██████████| 146/146 [00:02<00:00, 56.43it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0128]\n",
      "Epoch 194: 100%|██████████| 146/146 [00:02<00:00, 56.28it/s, loss=0.00082, v_num=8, train_loss=0.000521, val_loss=0.0128]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 194, global step 25545: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195:  90%|████████▉ | 131/146 [00:02<00:00, 50.03it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 195:  90%|█████████ | 132/146 [00:02<00:00, 49.94it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195:  91%|█████████ | 133/146 [00:02<00:00, 50.09it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195:  92%|█████████▏| 134/146 [00:02<00:00, 50.24it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195:  92%|█████████▏| 135/146 [00:02<00:00, 50.39it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195:  93%|█████████▎| 136/146 [00:02<00:00, 50.54it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195:  94%|█████████▍| 137/146 [00:02<00:00, 50.68it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195:  95%|█████████▍| 138/146 [00:02<00:00, 50.83it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195:  95%|█████████▌| 139/146 [00:02<00:00, 50.97it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195:  96%|█████████▌| 140/146 [00:02<00:00, 51.09it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195:  97%|█████████▋| 141/146 [00:02<00:00, 51.21it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195:  97%|█████████▋| 142/146 [00:02<00:00, 51.35it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195:  98%|█████████▊| 143/146 [00:02<00:00, 51.51it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195:  99%|█████████▊| 144/146 [00:02<00:00, 51.65it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195:  99%|█████████▉| 145/146 [00:02<00:00, 51.76it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0128]\n",
      "Epoch 195: 100%|██████████| 146/146 [00:02<00:00, 51.77it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0126]\n",
      "Epoch 195: 100%|██████████| 146/146 [00:02<00:00, 51.66it/s, loss=0.000821, v_num=8, train_loss=0.000209, val_loss=0.0126]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 195, global step 25676: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196:  90%|████████▉ | 131/146 [00:02<00:00, 53.51it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 196:  90%|█████████ | 132/146 [00:02<00:00, 53.46it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196:  91%|█████████ | 133/146 [00:02<00:00, 53.60it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196:  92%|█████████▏| 134/146 [00:02<00:00, 53.79it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196:  92%|█████████▏| 135/146 [00:02<00:00, 53.97it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196:  93%|█████████▎| 136/146 [00:02<00:00, 54.14it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196:  94%|█████████▍| 137/146 [00:02<00:00, 54.25it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196:  95%|█████████▍| 138/146 [00:02<00:00, 54.41it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196:  95%|█████████▌| 139/146 [00:02<00:00, 54.59it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196:  96%|█████████▌| 140/146 [00:02<00:00, 54.73it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196:  97%|█████████▋| 141/146 [00:02<00:00, 54.86it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196:  97%|█████████▋| 142/146 [00:02<00:00, 55.03it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196:  98%|█████████▊| 143/146 [00:02<00:00, 55.19it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196:  99%|█████████▊| 144/146 [00:02<00:00, 55.34it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196:  99%|█████████▉| 145/146 [00:02<00:00, 55.49it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.0126]\n",
      "Epoch 196: 100%|██████████| 146/146 [00:02<00:00, 55.51it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.017] \n",
      "Epoch 196: 100%|██████████| 146/146 [00:02<00:00, 55.40it/s, loss=0.000848, v_num=8, train_loss=0.000232, val_loss=0.017]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 196, global step 25807: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197:  90%|████████▉ | 131/146 [00:02<00:00, 56.24it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 197:  90%|█████████ | 132/146 [00:02<00:00, 56.14it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197:  91%|█████████ | 133/146 [00:02<00:00, 56.30it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197:  92%|█████████▏| 134/146 [00:02<00:00, 56.49it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197:  92%|█████████▏| 135/146 [00:02<00:00, 56.65it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197:  93%|█████████▎| 136/146 [00:02<00:00, 56.83it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197:  94%|█████████▍| 137/146 [00:02<00:00, 56.98it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197:  95%|█████████▍| 138/146 [00:02<00:00, 57.12it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197:  95%|█████████▌| 139/146 [00:02<00:00, 57.24it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197:  96%|█████████▌| 140/146 [00:02<00:00, 57.40it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197:  97%|█████████▋| 141/146 [00:02<00:00, 57.55it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197:  97%|█████████▋| 142/146 [00:02<00:00, 57.70it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197:  98%|█████████▊| 143/146 [00:02<00:00, 57.84it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197:  99%|█████████▊| 144/146 [00:02<00:00, 58.01it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197:  99%|█████████▉| 145/146 [00:02<00:00, 58.09it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.017]\n",
      "Epoch 197: 100%|██████████| 146/146 [00:02<00:00, 58.12it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.0122]\n",
      "Epoch 197: 100%|██████████| 146/146 [00:02<00:00, 58.02it/s, loss=0.000795, v_num=8, train_loss=0.000355, val_loss=0.0122]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 197, global step 25938: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198:  90%|████████▉ | 131/146 [00:02<00:00, 55.18it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 198:  90%|█████████ | 132/146 [00:02<00:00, 55.09it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198:  91%|█████████ | 133/146 [00:02<00:00, 55.25it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198:  92%|█████████▏| 134/146 [00:02<00:00, 55.41it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198:  92%|█████████▏| 135/146 [00:02<00:00, 55.55it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198:  93%|█████████▎| 136/146 [00:02<00:00, 55.73it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198:  94%|█████████▍| 137/146 [00:02<00:00, 55.87it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198:  95%|█████████▍| 138/146 [00:02<00:00, 56.03it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198:  95%|█████████▌| 139/146 [00:02<00:00, 56.18it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198:  96%|█████████▌| 140/146 [00:02<00:00, 56.29it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198:  97%|█████████▋| 141/146 [00:02<00:00, 56.44it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198:  97%|█████████▋| 142/146 [00:02<00:00, 56.59it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198:  98%|█████████▊| 143/146 [00:02<00:00, 56.47it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198:  99%|█████████▊| 144/146 [00:02<00:00, 56.62it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198:  99%|█████████▉| 145/146 [00:02<00:00, 56.73it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0122]\n",
      "Epoch 198: 100%|██████████| 146/146 [00:02<00:00, 56.74it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0137]\n",
      "Epoch 198: 100%|██████████| 146/146 [00:02<00:00, 56.63it/s, loss=0.000791, v_num=8, train_loss=0.000215, val_loss=0.0137]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 198, global step 26069: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199:  90%|████████▉ | 131/146 [00:02<00:00, 55.60it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 199:  90%|█████████ | 132/146 [00:02<00:00, 55.50it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199:  91%|█████████ | 133/146 [00:02<00:00, 55.67it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199:  92%|█████████▏| 134/146 [00:02<00:00, 55.81it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199:  92%|█████████▏| 135/146 [00:02<00:00, 55.97it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199:  93%|█████████▎| 136/146 [00:02<00:00, 56.15it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199:  94%|█████████▍| 137/146 [00:02<00:00, 56.33it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199:  95%|█████████▍| 138/146 [00:02<00:00, 56.48it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199:  95%|█████████▌| 139/146 [00:02<00:00, 56.64it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199:  96%|█████████▌| 140/146 [00:02<00:00, 56.81it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199:  97%|█████████▋| 141/146 [00:02<00:00, 56.97it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199:  97%|█████████▋| 142/146 [00:02<00:00, 57.12it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199:  98%|█████████▊| 143/146 [00:02<00:00, 57.26it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199:  99%|█████████▊| 144/146 [00:02<00:00, 57.41it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199:  99%|█████████▉| 145/146 [00:02<00:00, 57.58it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0137]\n",
      "Epoch 199: 100%|██████████| 146/146 [00:02<00:00, 57.57it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0173]\n",
      "Epoch 199: 100%|██████████| 146/146 [00:02<00:00, 57.48it/s, loss=0.000858, v_num=8, train_loss=0.000404, val_loss=0.0173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 199, global step 26200: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200:  90%|████████▉ | 131/146 [00:02<00:00, 55.86it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 200:  90%|█████████ | 132/146 [00:02<00:00, 55.79it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200:  91%|█████████ | 133/146 [00:02<00:00, 55.95it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200:  92%|█████████▏| 134/146 [00:02<00:00, 56.13it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200:  92%|█████████▏| 135/146 [00:02<00:00, 56.29it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200:  93%|█████████▎| 136/146 [00:02<00:00, 56.47it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200:  94%|█████████▍| 137/146 [00:02<00:00, 56.37it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200:  95%|█████████▍| 138/146 [00:02<00:00, 56.53it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200:  95%|█████████▌| 139/146 [00:02<00:00, 56.66it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200:  96%|█████████▌| 140/146 [00:02<00:00, 56.74it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200:  97%|█████████▋| 141/146 [00:02<00:00, 56.90it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200:  97%|█████████▋| 142/146 [00:02<00:00, 56.98it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200:  98%|█████████▊| 143/146 [00:02<00:00, 57.10it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200:  99%|█████████▊| 144/146 [00:02<00:00, 57.21it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200:  99%|█████████▉| 145/146 [00:02<00:00, 57.33it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0173]\n",
      "Epoch 200: 100%|██████████| 146/146 [00:02<00:00, 57.36it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0128]\n",
      "Epoch 200: 100%|██████████| 146/146 [00:02<00:00, 57.25it/s, loss=0.000813, v_num=8, train_loss=0.000214, val_loss=0.0128]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200, global step 26331: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201:  90%|████████▉ | 131/146 [00:02<00:00, 51.82it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 201:  90%|█████████ | 132/146 [00:02<00:00, 51.64it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201:  91%|█████████ | 133/146 [00:02<00:00, 51.77it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201:  92%|█████████▏| 134/146 [00:02<00:00, 51.87it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201:  92%|█████████▏| 135/146 [00:02<00:00, 51.98it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201:  93%|█████████▎| 136/146 [00:02<00:00, 52.12it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201:  94%|█████████▍| 137/146 [00:02<00:00, 52.27it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201:  95%|█████████▍| 138/146 [00:02<00:00, 52.39it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201:  95%|█████████▌| 139/146 [00:02<00:00, 52.51it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201:  96%|█████████▌| 140/146 [00:02<00:00, 52.61it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201:  97%|█████████▋| 141/146 [00:02<00:00, 52.73it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201:  97%|█████████▋| 142/146 [00:02<00:00, 52.86it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201:  98%|█████████▊| 143/146 [00:02<00:00, 52.96it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201:  99%|█████████▊| 144/146 [00:02<00:00, 53.07it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201:  99%|█████████▉| 145/146 [00:02<00:00, 53.15it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0128]\n",
      "Epoch 201: 100%|██████████| 146/146 [00:02<00:00, 53.15it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0154]\n",
      "Epoch 201: 100%|██████████| 146/146 [00:02<00:00, 53.03it/s, loss=0.000803, v_num=8, train_loss=0.000336, val_loss=0.0154]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 201, global step 26462: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202:  90%|████████▉ | 131/146 [00:02<00:00, 52.50it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 202:  90%|█████████ | 132/146 [00:02<00:00, 52.46it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202:  91%|█████████ | 133/146 [00:02<00:00, 52.63it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202:  92%|█████████▏| 134/146 [00:02<00:00, 52.79it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202:  92%|█████████▏| 135/146 [00:02<00:00, 52.96it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202:  93%|█████████▎| 136/146 [00:02<00:00, 53.12it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202:  94%|█████████▍| 137/146 [00:02<00:00, 53.28it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202:  95%|█████████▍| 138/146 [00:02<00:00, 53.44it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202:  95%|█████████▌| 139/146 [00:02<00:00, 53.60it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202:  96%|█████████▌| 140/146 [00:02<00:00, 53.76it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202:  97%|█████████▋| 141/146 [00:02<00:00, 53.92it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202:  97%|█████████▋| 142/146 [00:02<00:00, 54.03it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202:  98%|█████████▊| 143/146 [00:02<00:00, 54.16it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202:  99%|█████████▊| 144/146 [00:02<00:00, 54.31it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202:  99%|█████████▉| 145/146 [00:02<00:00, 54.47it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0154]\n",
      "Epoch 202: 100%|██████████| 146/146 [00:02<00:00, 54.55it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0165]\n",
      "Epoch 202: 100%|██████████| 146/146 [00:02<00:00, 54.37it/s, loss=0.000877, v_num=8, train_loss=0.00128, val_loss=0.0165]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 202, global step 26593: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203:  90%|████████▉ | 131/146 [00:02<00:00, 55.39it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 203:  90%|█████████ | 132/146 [00:02<00:00, 55.30it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203:  91%|█████████ | 133/146 [00:02<00:00, 55.48it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203:  92%|█████████▏| 134/146 [00:02<00:00, 55.64it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203:  92%|█████████▏| 135/146 [00:02<00:00, 55.83it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203:  93%|█████████▎| 136/146 [00:02<00:00, 55.96it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203:  94%|█████████▍| 137/146 [00:02<00:00, 56.17it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203:  95%|█████████▍| 138/146 [00:02<00:00, 56.32it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203:  95%|█████████▌| 139/146 [00:02<00:00, 56.48it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203:  96%|█████████▌| 140/146 [00:02<00:00, 56.63it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203:  97%|█████████▋| 141/146 [00:02<00:00, 56.71it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203:  97%|█████████▋| 142/146 [00:02<00:00, 56.86it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203:  98%|█████████▊| 143/146 [00:02<00:00, 56.99it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203:  99%|█████████▊| 144/146 [00:02<00:00, 57.12it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203:  99%|█████████▉| 145/146 [00:02<00:00, 57.31it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0165]\n",
      "Epoch 203: 100%|██████████| 146/146 [00:02<00:00, 57.32it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0122]\n",
      "Epoch 203: 100%|██████████| 146/146 [00:02<00:00, 57.18it/s, loss=0.000854, v_num=8, train_loss=0.000214, val_loss=0.0122]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 203, global step 26724: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204:  90%|████████▉ | 131/146 [00:02<00:00, 55.29it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 204:  90%|█████████ | 132/146 [00:02<00:00, 55.23it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204:  91%|█████████ | 133/146 [00:02<00:00, 55.34it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204:  92%|█████████▏| 134/146 [00:02<00:00, 55.51it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204:  92%|█████████▏| 135/146 [00:02<00:00, 55.67it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204:  93%|█████████▎| 136/146 [00:02<00:00, 55.80it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204:  94%|█████████▍| 137/146 [00:02<00:00, 55.94it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204:  95%|█████████▍| 138/146 [00:02<00:00, 56.09it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204:  95%|█████████▌| 139/146 [00:02<00:00, 56.27it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204:  96%|█████████▌| 140/146 [00:02<00:00, 56.42it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204:  97%|█████████▋| 141/146 [00:02<00:00, 56.55it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204:  97%|█████████▋| 142/146 [00:02<00:00, 56.68it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204:  98%|█████████▊| 143/146 [00:02<00:00, 56.85it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204:  99%|█████████▊| 144/146 [00:02<00:00, 57.00it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204:  99%|█████████▉| 145/146 [00:02<00:00, 57.15it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0122]\n",
      "Epoch 204: 100%|██████████| 146/146 [00:02<00:00, 57.16it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0165]\n",
      "Epoch 204: 100%|██████████| 146/146 [00:02<00:00, 57.07it/s, loss=0.000916, v_num=8, train_loss=0.000807, val_loss=0.0165]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 204, global step 26855: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205:  90%|████████▉ | 131/146 [00:02<00:00, 56.11it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 205:  90%|█████████ | 132/146 [00:02<00:00, 55.94it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205:  91%|█████████ | 133/146 [00:02<00:00, 56.11it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205:  92%|█████████▏| 134/146 [00:02<00:00, 56.27it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205:  92%|█████████▏| 135/146 [00:02<00:00, 56.43it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205:  93%|█████████▎| 136/146 [00:02<00:00, 56.56it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205:  94%|█████████▍| 137/146 [00:02<00:00, 56.69it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205:  95%|█████████▍| 138/146 [00:02<00:00, 56.85it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205:  95%|█████████▌| 139/146 [00:02<00:00, 57.00it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205:  96%|█████████▌| 140/146 [00:02<00:00, 57.15it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205:  97%|█████████▋| 141/146 [00:02<00:00, 57.31it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205:  97%|█████████▋| 142/146 [00:02<00:00, 57.45it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205:  98%|█████████▊| 143/146 [00:02<00:00, 57.53it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205:  99%|█████████▊| 144/146 [00:02<00:00, 57.68it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205:  99%|█████████▉| 145/146 [00:02<00:00, 57.85it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0165]\n",
      "Epoch 205: 100%|██████████| 146/146 [00:02<00:00, 57.88it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0146]\n",
      "Epoch 205: 100%|██████████| 146/146 [00:02<00:00, 57.74it/s, loss=0.00089, v_num=8, train_loss=0.00147, val_loss=0.0146]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 205, global step 26986: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206:  90%|████████▉ | 131/146 [00:02<00:00, 55.04it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 206:  90%|█████████ | 132/146 [00:02<00:00, 54.90it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206:  91%|█████████ | 133/146 [00:02<00:00, 55.05it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206:  92%|█████████▏| 134/146 [00:02<00:00, 55.19it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206:  92%|█████████▏| 135/146 [00:02<00:00, 55.37it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206:  93%|█████████▎| 136/146 [00:02<00:00, 55.53it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206:  94%|█████████▍| 137/146 [00:02<00:00, 55.66it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206:  95%|█████████▍| 138/146 [00:02<00:00, 55.84it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206:  95%|█████████▌| 139/146 [00:02<00:00, 56.00it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206:  96%|█████████▌| 140/146 [00:02<00:00, 56.13it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206:  97%|█████████▋| 141/146 [00:02<00:00, 56.28it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206:  97%|█████████▋| 142/146 [00:02<00:00, 56.43it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206:  98%|█████████▊| 143/146 [00:02<00:00, 56.61it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206:  99%|█████████▊| 144/146 [00:02<00:00, 56.76it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206:  99%|█████████▉| 145/146 [00:02<00:00, 56.90it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0146]\n",
      "Epoch 206: 100%|██████████| 146/146 [00:02<00:00, 56.81it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0128]\n",
      "Epoch 206: 100%|██████████| 146/146 [00:02<00:00, 56.72it/s, loss=0.000872, v_num=8, train_loss=0.000298, val_loss=0.0128]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 206, global step 27117: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207:  90%|████████▉ | 131/146 [00:02<00:00, 52.35it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 207:  90%|█████████ | 132/146 [00:02<00:00, 52.21it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207:  91%|█████████ | 133/146 [00:02<00:00, 52.32it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207:  92%|█████████▏| 134/146 [00:02<00:00, 52.46it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207:  92%|█████████▏| 135/146 [00:02<00:00, 52.57it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207:  93%|█████████▎| 136/146 [00:02<00:00, 52.69it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207:  94%|█████████▍| 137/146 [00:02<00:00, 52.81it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207:  95%|█████████▍| 138/146 [00:02<00:00, 52.93it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207:  95%|█████████▌| 139/146 [00:02<00:00, 53.07it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207:  96%|█████████▌| 140/146 [00:02<00:00, 53.21it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207:  97%|█████████▋| 141/146 [00:02<00:00, 53.32it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207:  97%|█████████▋| 142/146 [00:02<00:00, 53.42it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207:  98%|█████████▊| 143/146 [00:02<00:00, 53.57it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207:  99%|█████████▊| 144/146 [00:02<00:00, 53.61it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207:  99%|█████████▉| 145/146 [00:02<00:00, 53.68it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0128]\n",
      "Epoch 207: 100%|██████████| 146/146 [00:02<00:00, 53.67it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0171]\n",
      "Epoch 207: 100%|██████████| 146/146 [00:02<00:00, 53.53it/s, loss=0.000833, v_num=8, train_loss=0.000228, val_loss=0.0171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 207, global step 27248: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208:  90%|████████▉ | 131/146 [00:02<00:00, 51.61it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 208:  90%|█████████ | 132/146 [00:02<00:00, 51.42it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208:  91%|█████████ | 133/146 [00:02<00:00, 51.59it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208:  92%|█████████▏| 134/146 [00:02<00:00, 51.75it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208:  92%|█████████▏| 135/146 [00:02<00:00, 51.88it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208:  93%|█████████▎| 136/146 [00:02<00:00, 52.04it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208:  94%|█████████▍| 137/146 [00:02<00:00, 52.21it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208:  95%|█████████▍| 138/146 [00:02<00:00, 52.37it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208:  95%|█████████▌| 139/146 [00:02<00:00, 52.49it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208:  96%|█████████▌| 140/146 [00:02<00:00, 52.61it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208:  97%|█████████▋| 141/146 [00:02<00:00, 52.77it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208:  97%|█████████▋| 142/146 [00:02<00:00, 52.88it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208:  98%|█████████▊| 143/146 [00:02<00:00, 52.98it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208:  99%|█████████▊| 144/146 [00:02<00:00, 53.11it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208:  99%|█████████▉| 145/146 [00:02<00:00, 53.23it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0171]\n",
      "Epoch 208: 100%|██████████| 146/146 [00:02<00:00, 53.26it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0156]\n",
      "Epoch 208: 100%|██████████| 146/146 [00:02<00:00, 53.16it/s, loss=0.000804, v_num=8, train_loss=0.00019, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 208, global step 27379: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209:  90%|████████▉ | 131/146 [00:02<00:00, 54.78it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 209:  90%|█████████ | 132/146 [00:02<00:00, 54.75it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209:  91%|█████████ | 133/146 [00:02<00:00, 54.86it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209:  92%|█████████▏| 134/146 [00:02<00:00, 55.05it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209:  92%|█████████▏| 135/146 [00:02<00:00, 55.21it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209:  93%|█████████▎| 136/146 [00:02<00:00, 55.37it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209:  94%|█████████▍| 137/146 [00:02<00:00, 55.55it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209:  95%|█████████▍| 138/146 [00:02<00:00, 55.73it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209:  95%|█████████▌| 139/146 [00:02<00:00, 55.89it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209:  96%|█████████▌| 140/146 [00:02<00:00, 56.06it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209:  97%|█████████▋| 141/146 [00:02<00:00, 56.10it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209:  97%|█████████▋| 142/146 [00:02<00:00, 56.26it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209:  98%|█████████▊| 143/146 [00:02<00:00, 56.36it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209:  99%|█████████▊| 144/146 [00:02<00:00, 56.53it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209:  99%|█████████▉| 145/146 [00:02<00:00, 56.75it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0156]\n",
      "Epoch 209: 100%|██████████| 146/146 [00:02<00:00, 56.85it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0144]\n",
      "Epoch 209: 100%|██████████| 146/146 [00:02<00:00, 56.76it/s, loss=0.000866, v_num=8, train_loss=0.000288, val_loss=0.0144]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 209, global step 27510: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210:  90%|████████▉ | 131/146 [00:02<00:00, 55.81it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 210:  90%|█████████ | 132/146 [00:02<00:00, 55.74it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210:  91%|█████████ | 133/146 [00:02<00:00, 55.85it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210:  92%|█████████▏| 134/146 [00:02<00:00, 56.04it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210:  92%|█████████▏| 135/146 [00:02<00:00, 56.22it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210:  93%|█████████▎| 136/146 [00:02<00:00, 56.33it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210:  94%|█████████▍| 137/146 [00:02<00:00, 56.44it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210:  95%|█████████▍| 138/146 [00:02<00:00, 56.60it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210:  95%|█████████▌| 139/146 [00:02<00:00, 56.78it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210:  96%|█████████▌| 140/146 [00:02<00:00, 56.86it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210:  97%|█████████▋| 141/146 [00:02<00:00, 57.03it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210:  97%|█████████▋| 142/146 [00:02<00:00, 57.14it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210:  98%|█████████▊| 143/146 [00:02<00:00, 57.26it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210:  99%|█████████▊| 144/146 [00:02<00:00, 57.46it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210:  99%|█████████▉| 145/146 [00:02<00:00, 57.51it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0144]\n",
      "Epoch 210: 100%|██████████| 146/146 [00:02<00:00, 57.57it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0155]\n",
      "Epoch 210: 100%|██████████| 146/146 [00:02<00:00, 57.41it/s, loss=0.000792, v_num=8, train_loss=0.000207, val_loss=0.0155]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 210, global step 27641: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211:  90%|████████▉ | 131/146 [00:02<00:00, 55.79it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 211:  90%|█████████ | 132/146 [00:02<00:00, 55.69it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211:  91%|█████████ | 133/146 [00:02<00:00, 55.85it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211:  92%|█████████▏| 134/146 [00:02<00:00, 55.99it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211:  92%|█████████▏| 135/146 [00:02<00:00, 56.15it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211:  93%|█████████▎| 136/146 [00:02<00:00, 56.29it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211:  94%|█████████▍| 137/146 [00:02<00:00, 56.42it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211:  95%|█████████▍| 138/146 [00:02<00:00, 56.46it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211:  95%|█████████▌| 139/146 [00:02<00:00, 56.64it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211:  96%|█████████▌| 140/146 [00:02<00:00, 56.77it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211:  97%|█████████▋| 141/146 [00:02<00:00, 56.94it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211:  97%|█████████▋| 142/146 [00:02<00:00, 57.12it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211:  98%|█████████▊| 143/146 [00:02<00:00, 57.22it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211:  99%|█████████▊| 144/146 [00:02<00:00, 57.34it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211:  99%|█████████▉| 145/146 [00:02<00:00, 57.47it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0155]\n",
      "Epoch 211: 100%|██████████| 146/146 [00:02<00:00, 57.54it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0149]\n",
      "Epoch 211: 100%|██████████| 146/146 [00:02<00:00, 57.41it/s, loss=0.000801, v_num=8, train_loss=0.000417, val_loss=0.0149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 211, global step 27772: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212:  90%|████████▉ | 131/146 [00:02<00:00, 54.60it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 212:  90%|█████████ | 132/146 [00:02<00:00, 54.52it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212:  91%|█████████ | 133/146 [00:02<00:00, 54.68it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212:  92%|█████████▏| 134/146 [00:02<00:00, 54.80it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212:  92%|█████████▏| 135/146 [00:02<00:00, 54.99it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212:  93%|█████████▎| 136/146 [00:02<00:00, 55.12it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212:  94%|█████████▍| 137/146 [00:02<00:00, 55.30it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212:  95%|█████████▍| 138/146 [00:02<00:00, 55.46it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212:  95%|█████████▌| 139/146 [00:02<00:00, 55.62it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212:  96%|█████████▌| 140/146 [00:02<00:00, 55.77it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212:  97%|█████████▋| 141/146 [00:02<00:00, 55.97it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212:  97%|█████████▋| 142/146 [00:02<00:00, 56.10it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212:  98%|█████████▊| 143/146 [00:02<00:00, 56.23it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212:  99%|█████████▊| 144/146 [00:02<00:00, 56.36it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212:  99%|█████████▉| 145/146 [00:02<00:00, 56.48it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0149]\n",
      "Epoch 212: 100%|██████████| 146/146 [00:02<00:00, 56.45it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0137]\n",
      "Epoch 212: 100%|██████████| 146/146 [00:02<00:00, 56.34it/s, loss=0.000797, v_num=8, train_loss=0.000199, val_loss=0.0137]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 212, global step 27903: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213:  90%|████████▉ | 131/146 [00:02<00:00, 51.99it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 213:  90%|█████████ | 132/146 [00:02<00:00, 51.83it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213:  91%|█████████ | 133/146 [00:02<00:00, 51.96it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213:  92%|█████████▏| 134/146 [00:02<00:00, 52.08it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213:  92%|█████████▏| 135/146 [00:02<00:00, 52.21it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213:  93%|█████████▎| 136/146 [00:02<00:00, 52.33it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213:  94%|█████████▍| 137/146 [00:02<00:00, 52.43it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213:  95%|█████████▍| 138/146 [00:02<00:00, 52.54it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213:  95%|█████████▌| 139/146 [00:02<00:00, 52.64it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213:  96%|█████████▌| 140/146 [00:02<00:00, 52.74it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213:  97%|█████████▋| 141/146 [00:02<00:00, 52.85it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213:  97%|█████████▋| 142/146 [00:02<00:00, 52.95it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213:  98%|█████████▊| 143/146 [00:02<00:00, 53.09it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213:  99%|█████████▊| 144/146 [00:02<00:00, 53.22it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213:  99%|█████████▉| 145/146 [00:02<00:00, 53.31it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0137]\n",
      "Epoch 213: 100%|██████████| 146/146 [00:02<00:00, 53.35it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0152]\n",
      "Epoch 213: 100%|██████████| 146/146 [00:02<00:00, 53.23it/s, loss=0.00077, v_num=8, train_loss=0.000206, val_loss=0.0152]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 213, global step 28034: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214:  90%|████████▉ | 131/146 [00:02<00:00, 51.31it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 214:  90%|█████████ | 132/146 [00:02<00:00, 51.28it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214:  91%|█████████ | 133/146 [00:02<00:00, 51.41it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214:  92%|█████████▏| 134/146 [00:02<00:00, 51.57it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214:  92%|█████████▏| 135/146 [00:02<00:00, 51.76it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214:  93%|█████████▎| 136/146 [00:02<00:00, 51.79it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214:  94%|█████████▍| 137/146 [00:02<00:00, 51.97it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214:  95%|█████████▍| 138/146 [00:02<00:00, 52.09it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214:  95%|█████████▌| 139/146 [00:02<00:00, 52.25it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214:  96%|█████████▌| 140/146 [00:02<00:00, 52.41it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214:  97%|█████████▋| 141/146 [00:02<00:00, 52.57it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214:  97%|█████████▋| 142/146 [00:02<00:00, 52.73it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214:  98%|█████████▊| 143/146 [00:02<00:00, 52.90it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214:  99%|█████████▊| 144/146 [00:02<00:00, 53.03it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214:  99%|█████████▉| 145/146 [00:02<00:00, 53.19it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0152]\n",
      "Epoch 214: 100%|██████████| 146/146 [00:02<00:00, 53.24it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0144]\n",
      "Epoch 214: 100%|██████████| 146/146 [00:02<00:00, 53.11it/s, loss=0.000791, v_num=8, train_loss=0.000443, val_loss=0.0144]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 214, global step 28165: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215:  90%|████████▉ | 131/146 [00:02<00:00, 56.15it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 215:  90%|█████████ | 132/146 [00:02<00:00, 56.05it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215:  91%|█████████ | 133/146 [00:02<00:00, 56.19it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215:  92%|█████████▏| 134/146 [00:02<00:00, 56.39it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215:  92%|█████████▏| 135/146 [00:02<00:00, 56.55it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215:  93%|█████████▎| 136/146 [00:02<00:00, 56.71it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215:  94%|█████████▍| 137/146 [00:02<00:00, 56.84it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215:  95%|█████████▍| 138/146 [00:02<00:00, 56.97it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215:  95%|█████████▌| 139/146 [00:02<00:00, 57.13it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215:  96%|█████████▌| 140/146 [00:02<00:00, 57.30it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215:  97%|█████████▋| 141/146 [00:02<00:00, 57.45it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215:  97%|█████████▋| 142/146 [00:02<00:00, 57.58it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215:  98%|█████████▊| 143/146 [00:02<00:00, 57.68it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215:  99%|█████████▊| 144/146 [00:02<00:00, 57.78it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215:  99%|█████████▉| 145/146 [00:02<00:00, 57.90it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0144]\n",
      "Epoch 215: 100%|██████████| 146/146 [00:02<00:00, 57.79it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0143]\n",
      "Epoch 215: 100%|██████████| 146/146 [00:02<00:00, 57.68it/s, loss=0.00077, v_num=8, train_loss=0.000177, val_loss=0.0143]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 215, global step 28296: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216:  90%|████████▉ | 131/146 [00:02<00:00, 55.34it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 216:  90%|█████████ | 132/146 [00:02<00:00, 55.30it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216:  91%|█████████ | 133/146 [00:02<00:00, 55.48it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216:  92%|█████████▏| 134/146 [00:02<00:00, 55.67it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216:  92%|█████████▏| 135/146 [00:02<00:00, 55.85it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216:  93%|█████████▎| 136/146 [00:02<00:00, 56.03it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216:  94%|█████████▍| 137/146 [00:02<00:00, 56.17it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216:  95%|█████████▍| 138/146 [00:02<00:00, 56.35it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216:  95%|█████████▌| 139/146 [00:02<00:00, 56.50it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216:  96%|█████████▌| 140/146 [00:02<00:00, 56.65it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216:  97%|█████████▋| 141/146 [00:02<00:00, 56.80it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216:  97%|█████████▋| 142/146 [00:02<00:00, 56.98it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216:  98%|█████████▊| 143/146 [00:02<00:00, 57.13it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216:  99%|█████████▊| 144/146 [00:02<00:00, 57.28it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216:  99%|█████████▉| 145/146 [00:02<00:00, 57.40it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0143]\n",
      "Epoch 216: 100%|██████████| 146/146 [00:02<00:00, 57.39it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0156]\n",
      "Epoch 216: 100%|██████████| 146/146 [00:02<00:00, 57.27it/s, loss=0.000779, v_num=8, train_loss=0.000174, val_loss=0.0156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 216, global step 28427: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217:  90%|████████▉ | 131/146 [00:02<00:00, 55.34it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 217:  90%|█████████ | 132/146 [00:02<00:00, 55.23it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217:  91%|█████████ | 133/146 [00:02<00:00, 55.41it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217:  92%|█████████▏| 134/146 [00:02<00:00, 55.55it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217:  92%|█████████▏| 135/146 [00:02<00:00, 55.71it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217:  93%|█████████▎| 136/146 [00:02<00:00, 55.80it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217:  94%|█████████▍| 137/146 [00:02<00:00, 55.98it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217:  95%|█████████▍| 138/146 [00:02<00:00, 56.09it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217:  95%|█████████▌| 139/146 [00:02<00:00, 55.91it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217:  96%|█████████▌| 140/146 [00:02<00:00, 56.00it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217:  97%|█████████▋| 141/146 [00:02<00:00, 56.10it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217:  97%|█████████▋| 142/146 [00:02<00:00, 56.26it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217:  98%|█████████▊| 143/146 [00:02<00:00, 56.34it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217:  99%|█████████▊| 144/146 [00:02<00:00, 56.47it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217:  99%|█████████▉| 145/146 [00:02<00:00, 56.55it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0156]\n",
      "Epoch 217: 100%|██████████| 146/146 [00:02<00:00, 56.59it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0135]\n",
      "Epoch 217: 100%|██████████| 146/146 [00:02<00:00, 56.34it/s, loss=0.000816, v_num=8, train_loss=0.000533, val_loss=0.0135]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 217, global step 28558: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218:  90%|████████▉ | 131/146 [00:02<00:00, 55.95it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 218:  90%|█████████ | 132/146 [00:02<00:00, 55.83it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218:  91%|█████████ | 133/146 [00:02<00:00, 56.02it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218:  92%|█████████▏| 134/146 [00:02<00:00, 56.18it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218:  92%|█████████▏| 135/146 [00:02<00:00, 56.27it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218:  93%|█████████▎| 136/146 [00:02<00:00, 56.47it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218:  94%|█████████▍| 137/146 [00:02<00:00, 56.61it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218:  95%|█████████▍| 138/146 [00:02<00:00, 56.79it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218:  95%|█████████▌| 139/146 [00:02<00:00, 56.87it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218:  96%|█████████▌| 140/146 [00:02<00:00, 57.05it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218:  97%|█████████▋| 141/146 [00:02<00:00, 57.17it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218:  97%|█████████▋| 142/146 [00:02<00:00, 57.30it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218:  98%|█████████▊| 143/146 [00:02<00:00, 57.47it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218:  99%|█████████▊| 144/146 [00:02<00:00, 57.62it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218:  99%|█████████▉| 145/146 [00:02<00:00, 57.79it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0135]\n",
      "Epoch 218: 100%|██████████| 146/146 [00:02<00:00, 57.79it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0144]\n",
      "Epoch 218: 100%|██████████| 146/146 [00:02<00:00, 57.68it/s, loss=0.000761, v_num=8, train_loss=0.000322, val_loss=0.0144]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 218, global step 28689: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219:  90%|████████▉ | 131/146 [00:02<00:00, 53.38it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 219:  90%|█████████ | 132/146 [00:02<00:00, 53.20it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219:  91%|█████████ | 133/146 [00:02<00:00, 53.24it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219:  92%|█████████▏| 134/146 [00:02<00:00, 53.34it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219:  92%|█████████▏| 135/146 [00:02<00:00, 53.48it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219:  93%|█████████▎| 136/146 [00:02<00:00, 53.58it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219:  94%|█████████▍| 137/146 [00:02<00:00, 53.72it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219:  95%|█████████▍| 138/146 [00:02<00:00, 53.82it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219:  95%|█████████▌| 139/146 [00:02<00:00, 53.91it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219:  96%|█████████▌| 140/146 [00:02<00:00, 54.01it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219:  97%|█████████▋| 141/146 [00:02<00:00, 54.14it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219:  97%|█████████▋| 142/146 [00:02<00:00, 54.28it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219:  98%|█████████▊| 143/146 [00:02<00:00, 54.39it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219:  99%|█████████▊| 144/146 [00:02<00:00, 54.48it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219:  99%|█████████▉| 145/146 [00:02<00:00, 54.57it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0144]\n",
      "Epoch 219: 100%|██████████| 146/146 [00:02<00:00, 54.56it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0168]\n",
      "Epoch 219: 100%|██████████| 146/146 [00:02<00:00, 54.43it/s, loss=0.000769, v_num=8, train_loss=0.000215, val_loss=0.0168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 219, global step 28820: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220:  90%|████████▉ | 131/146 [00:02<00:00, 50.93it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 220:  90%|█████████ | 132/146 [00:02<00:00, 50.86it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220:  91%|█████████ | 133/146 [00:02<00:00, 51.05it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220:  92%|█████████▏| 134/146 [00:02<00:00, 51.20it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220:  92%|█████████▏| 135/146 [00:02<00:00, 51.37it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220:  93%|█████████▎| 136/146 [00:02<00:00, 51.51it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220:  94%|█████████▍| 137/146 [00:02<00:00, 51.66it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220:  95%|█████████▍| 138/146 [00:02<00:00, 51.80it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220:  95%|█████████▌| 139/146 [00:02<00:00, 51.96it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220:  96%|█████████▌| 140/146 [00:02<00:00, 52.10it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220:  97%|█████████▋| 141/146 [00:02<00:00, 52.28it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220:  97%|█████████▋| 142/146 [00:02<00:00, 52.39it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220:  98%|█████████▊| 143/146 [00:02<00:00, 52.55it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220:  99%|█████████▊| 144/146 [00:02<00:00, 52.72it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220:  99%|█████████▉| 145/146 [00:02<00:00, 52.88it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0168]\n",
      "Epoch 220: 100%|██████████| 146/146 [00:02<00:00, 52.91it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0127]\n",
      "Epoch 220: 100%|██████████| 146/146 [00:02<00:00, 52.82it/s, loss=0.000874, v_num=8, train_loss=0.000509, val_loss=0.0127]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 220, global step 28951: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221:  90%|████████▉ | 131/146 [00:02<00:00, 56.05it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 221:  90%|█████████ | 132/146 [00:02<00:00, 56.00it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221:  91%|█████████ | 133/146 [00:02<00:00, 56.19it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221:  92%|█████████▏| 134/146 [00:02<00:00, 56.25it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221:  92%|█████████▏| 135/146 [00:02<00:00, 56.43it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221:  93%|█████████▎| 136/146 [00:02<00:00, 56.47it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221:  94%|█████████▍| 137/146 [00:02<00:00, 56.63it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221:  95%|█████████▍| 138/146 [00:02<00:00, 56.76it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221:  95%|█████████▌| 139/146 [00:02<00:00, 56.94it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221:  96%|█████████▌| 140/146 [00:02<00:00, 57.12it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221:  97%|█████████▋| 141/146 [00:02<00:00, 57.27it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221:  97%|█████████▋| 142/146 [00:02<00:00, 57.42it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221:  98%|█████████▊| 143/146 [00:02<00:00, 57.56it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221:  99%|█████████▊| 144/146 [00:02<00:00, 57.71it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221:  99%|█████████▉| 145/146 [00:02<00:00, 57.83it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0127]\n",
      "Epoch 221: 100%|██████████| 146/146 [00:02<00:00, 57.86it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0147]\n",
      "Epoch 221: 100%|██████████| 146/146 [00:02<00:00, 57.75it/s, loss=0.000805, v_num=8, train_loss=0.000555, val_loss=0.0147]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 221, global step 29082: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222:  90%|████████▉ | 131/146 [00:02<00:00, 55.50it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 222:  90%|█████████ | 132/146 [00:02<00:00, 55.43it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222:  91%|█████████ | 133/146 [00:02<00:00, 55.60it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222:  92%|█████████▏| 134/146 [00:02<00:00, 55.74it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222:  92%|█████████▏| 135/146 [00:02<00:00, 55.94it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222:  93%|█████████▎| 136/146 [00:02<00:00, 56.10it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222:  94%|█████████▍| 137/146 [00:02<00:00, 56.24it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222:  95%|█████████▍| 138/146 [00:02<00:00, 56.41it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222:  95%|█████████▌| 139/146 [00:02<00:00, 56.55it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222:  96%|█████████▌| 140/146 [00:02<00:00, 56.68it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222:  97%|█████████▋| 141/146 [00:02<00:00, 56.83it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222:  97%|█████████▋| 142/146 [00:02<00:00, 57.00it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222:  98%|█████████▊| 143/146 [00:02<00:00, 57.08it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222:  99%|█████████▊| 144/146 [00:02<00:00, 57.16it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222:  99%|█████████▉| 145/146 [00:02<00:00, 57.26it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0147]\n",
      "Epoch 222: 100%|██████████| 146/146 [00:02<00:00, 57.23it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0174]\n",
      "Epoch 222: 100%|██████████| 146/146 [00:02<00:00, 57.09it/s, loss=0.00079, v_num=8, train_loss=0.000496, val_loss=0.0174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 222, global step 29213: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223:  90%|████████▉ | 131/146 [00:02<00:00, 55.46it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 223:  90%|█████████ | 132/146 [00:02<00:00, 55.34it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223:  91%|█████████ | 133/146 [00:02<00:00, 55.51it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223:  92%|█████████▏| 134/146 [00:02<00:00, 55.62it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223:  92%|█████████▏| 135/146 [00:02<00:00, 55.78it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223:  93%|█████████▎| 136/146 [00:02<00:00, 55.94it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223:  94%|█████████▍| 137/146 [00:02<00:00, 56.10it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223:  95%|█████████▍| 138/146 [00:02<00:00, 56.28it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223:  95%|█████████▌| 139/146 [00:02<00:00, 56.41it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223:  96%|█████████▌| 140/146 [00:02<00:00, 56.56it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223:  97%|█████████▋| 141/146 [00:02<00:00, 56.69it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223:  97%|█████████▋| 142/146 [00:02<00:00, 56.84it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223:  98%|█████████▊| 143/146 [00:02<00:00, 56.99it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223:  99%|█████████▊| 144/146 [00:02<00:00, 57.14it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223:  99%|█████████▉| 145/146 [00:02<00:00, 57.26it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0174]\n",
      "Epoch 223: 100%|██████████| 146/146 [00:02<00:00, 57.30it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0127]\n",
      "Epoch 223: 100%|██████████| 146/146 [00:02<00:00, 57.18it/s, loss=0.000841, v_num=8, train_loss=0.000182, val_loss=0.0127]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 223, global step 29344: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224:  90%|████████▉ | 131/146 [00:02<00:00, 55.60it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 224:  90%|█████████ | 132/146 [00:02<00:00, 55.53it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224:  91%|█████████ | 133/146 [00:02<00:00, 55.69it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224:  92%|█████████▏| 134/146 [00:02<00:00, 55.83it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224:  92%|█████████▏| 135/146 [00:02<00:00, 55.99it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224:  93%|█████████▎| 136/146 [00:02<00:00, 56.12it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224:  94%|█████████▍| 137/146 [00:02<00:00, 56.28it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224:  95%|█████████▍| 138/146 [00:02<00:00, 56.41it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224:  95%|█████████▌| 139/146 [00:02<00:00, 56.59it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224:  96%|█████████▌| 140/146 [00:02<00:00, 56.72it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224:  97%|█████████▋| 141/146 [00:02<00:00, 56.92it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224:  97%|█████████▋| 142/146 [00:02<00:00, 57.07it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224:  98%|█████████▊| 143/146 [00:02<00:00, 57.24it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224:  99%|█████████▊| 144/146 [00:02<00:00, 57.37it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224:  99%|█████████▉| 145/146 [00:02<00:00, 57.53it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.0127]\n",
      "Epoch 224: 100%|██████████| 146/146 [00:02<00:00, 57.57it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.016] \n",
      "Epoch 224: 100%|██████████| 146/146 [00:02<00:00, 57.45it/s, loss=0.000803, v_num=8, train_loss=0.000444, val_loss=0.016]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 224, global step 29475: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225:  90%|████████▉ | 131/146 [00:02<00:00, 53.32it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 225:  90%|█████████ | 132/146 [00:02<00:00, 53.20it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225:  91%|█████████ | 133/146 [00:02<00:00, 53.33it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225:  92%|█████████▏| 134/146 [00:02<00:00, 53.45it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225:  92%|█████████▏| 135/146 [00:02<00:00, 53.55it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225:  93%|█████████▎| 136/146 [00:02<00:00, 53.69it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225:  94%|█████████▍| 137/146 [00:02<00:00, 53.83it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225:  95%|█████████▍| 138/146 [00:02<00:00, 53.95it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225:  95%|█████████▌| 139/146 [00:02<00:00, 54.08it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225:  96%|█████████▌| 140/146 [00:02<00:00, 54.22it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225:  97%|█████████▋| 141/146 [00:02<00:00, 54.33it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225:  97%|█████████▋| 142/146 [00:02<00:00, 54.40it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225:  98%|█████████▊| 143/146 [00:02<00:00, 54.52it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225:  99%|█████████▊| 144/146 [00:02<00:00, 54.63it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225:  99%|█████████▉| 145/146 [00:02<00:00, 54.74it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.016]\n",
      "Epoch 225: 100%|██████████| 146/146 [00:02<00:00, 54.74it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.0154]\n",
      "Epoch 225: 100%|██████████| 146/146 [00:02<00:00, 54.58it/s, loss=0.000782, v_num=8, train_loss=0.00059, val_loss=0.0154]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 225, global step 29606: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226:  90%|████████▉ | 131/146 [00:02<00:00, 49.02it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 226:  90%|█████████ | 132/146 [00:02<00:00, 48.95it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226:  91%|█████████ | 133/146 [00:02<00:00, 49.11it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226:  92%|█████████▏| 134/146 [00:02<00:00, 49.26it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226:  92%|█████████▏| 135/146 [00:02<00:00, 49.43it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226:  93%|█████████▎| 136/146 [00:02<00:00, 49.61it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226:  94%|█████████▍| 137/146 [00:02<00:00, 49.79it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226:  95%|█████████▍| 138/146 [00:02<00:00, 49.94it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226:  95%|█████████▌| 139/146 [00:02<00:00, 50.10it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226:  96%|█████████▌| 140/146 [00:02<00:00, 50.26it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226:  97%|█████████▋| 141/146 [00:02<00:00, 50.42it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226:  97%|█████████▋| 142/146 [00:02<00:00, 50.58it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226:  98%|█████████▊| 143/146 [00:02<00:00, 50.76it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226:  99%|█████████▊| 144/146 [00:02<00:00, 50.88it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226:  99%|█████████▉| 145/146 [00:02<00:00, 51.01it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0154]\n",
      "Epoch 226: 100%|██████████| 146/146 [00:02<00:00, 51.10it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0131]\n",
      "Epoch 226: 100%|██████████| 146/146 [00:02<00:00, 50.99it/s, loss=0.000811, v_num=8, train_loss=0.000169, val_loss=0.0131]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 226, global step 29737: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227:  90%|████████▉ | 131/146 [00:02<00:00, 56.19it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 227:  90%|█████████ | 132/146 [00:02<00:00, 56.11it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227:  91%|█████████ | 133/146 [00:02<00:00, 56.28it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227:  92%|█████████▏| 134/146 [00:02<00:00, 56.44it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227:  92%|█████████▏| 135/146 [00:02<00:00, 56.60it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227:  93%|█████████▎| 136/146 [00:02<00:00, 56.75it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227:  94%|█████████▍| 137/146 [00:02<00:00, 56.91it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227:  95%|█████████▍| 138/146 [00:02<00:00, 57.09it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227:  95%|█████████▌| 139/146 [00:02<00:00, 57.19it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227:  96%|█████████▌| 140/146 [00:02<00:00, 57.35it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227:  97%|█████████▋| 141/146 [00:02<00:00, 57.50it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227:  97%|█████████▋| 142/146 [00:02<00:00, 57.60it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227:  98%|█████████▊| 143/146 [00:02<00:00, 57.79it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227:  99%|█████████▊| 144/146 [00:02<00:00, 57.94it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227:  99%|█████████▉| 145/146 [00:02<00:00, 58.11it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0131]\n",
      "Epoch 227: 100%|██████████| 146/146 [00:02<00:00, 58.18it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0162]\n",
      "Epoch 227: 100%|██████████| 146/146 [00:02<00:00, 58.04it/s, loss=0.000801, v_num=8, train_loss=0.000378, val_loss=0.0162]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 227, global step 29868: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228:  90%|████████▉ | 131/146 [00:02<00:00, 55.41it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 228:  90%|█████████ | 132/146 [00:02<00:00, 55.15it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228:  91%|█████████ | 133/146 [00:02<00:00, 55.29it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228:  92%|█████████▏| 134/146 [00:02<00:00, 55.43it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228:  92%|█████████▏| 135/146 [00:02<00:00, 55.59it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228:  93%|█████████▎| 136/146 [00:02<00:00, 55.73it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228:  94%|█████████▍| 137/146 [00:02<00:00, 55.86it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228:  95%|█████████▍| 138/146 [00:02<00:00, 56.04it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228:  95%|█████████▌| 139/146 [00:02<00:00, 56.18it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228:  96%|█████████▌| 140/146 [00:02<00:00, 56.33it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228:  97%|█████████▋| 141/146 [00:02<00:00, 56.41it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228:  97%|█████████▋| 142/146 [00:02<00:00, 56.54it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228:  98%|█████████▊| 143/146 [00:02<00:00, 56.63it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228:  99%|█████████▊| 144/146 [00:02<00:00, 56.77it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228:  99%|█████████▉| 145/146 [00:02<00:00, 56.88it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.0162]\n",
      "Epoch 228: 100%|██████████| 146/146 [00:02<00:00, 56.91it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.015] \n",
      "Epoch 228: 100%|██████████| 146/146 [00:02<00:00, 56.80it/s, loss=0.000772, v_num=8, train_loss=0.000386, val_loss=0.015]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 228, global step 29999: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229:  90%|████████▉ | 131/146 [00:02<00:00, 52.56it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 229:  90%|█████████ | 132/146 [00:02<00:00, 52.48it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229:  91%|█████████ | 133/146 [00:02<00:00, 52.57it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229:  92%|█████████▏| 134/146 [00:02<00:00, 52.71it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229:  92%|█████████▏| 135/146 [00:02<00:00, 52.83it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229:  93%|█████████▎| 136/146 [00:02<00:00, 53.00it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229:  94%|█████████▍| 137/146 [00:02<00:00, 53.18it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229:  95%|█████████▍| 138/146 [00:02<00:00, 53.30it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229:  95%|█████████▌| 139/146 [00:02<00:00, 53.42it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229:  96%|█████████▌| 140/146 [00:02<00:00, 53.59it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229:  97%|█████████▋| 141/146 [00:02<00:00, 53.73it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229:  97%|█████████▋| 142/146 [00:02<00:00, 53.87it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229:  98%|█████████▊| 143/146 [00:02<00:00, 54.04it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229:  99%|█████████▊| 144/146 [00:02<00:00, 54.17it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229:  99%|█████████▉| 145/146 [00:02<00:00, 54.34it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.015]\n",
      "Epoch 229: 100%|██████████| 146/146 [00:02<00:00, 54.35it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.0138]\n",
      "Epoch 229: 100%|██████████| 146/146 [00:02<00:00, 54.21it/s, loss=0.000768, v_num=8, train_loss=0.000169, val_loss=0.0138]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 229, global step 30130: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230:  90%|████████▉ | 131/146 [00:02<00:00, 49.01it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 230:  90%|█████████ | 132/146 [00:02<00:00, 48.87it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230:  91%|█████████ | 133/146 [00:02<00:00, 49.02it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230:  92%|█████████▏| 134/146 [00:02<00:00, 49.13it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230:  92%|█████████▏| 135/146 [00:02<00:00, 49.25it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230:  93%|█████████▎| 136/146 [00:02<00:00, 49.38it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230:  94%|█████████▍| 137/146 [00:02<00:00, 49.51it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230:  95%|█████████▍| 138/146 [00:02<00:00, 49.58it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230:  95%|█████████▌| 139/146 [00:02<00:00, 49.67it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230:  96%|█████████▌| 140/146 [00:02<00:00, 49.80it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230:  97%|█████████▋| 141/146 [00:02<00:00, 49.91it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230:  97%|█████████▋| 142/146 [00:02<00:00, 50.03it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230:  98%|█████████▊| 143/146 [00:02<00:00, 50.15it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230:  99%|█████████▊| 144/146 [00:02<00:00, 50.19it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230:  99%|█████████▉| 145/146 [00:02<00:00, 50.29it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0138]\n",
      "Epoch 230: 100%|██████████| 146/146 [00:02<00:00, 50.29it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0153]\n",
      "Epoch 230: 100%|██████████| 146/146 [00:02<00:00, 50.19it/s, loss=0.000761, v_num=8, train_loss=0.000206, val_loss=0.0153]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 230, global step 30261: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231:  90%|████████▉ | 131/146 [00:02<00:00, 49.51it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 231:  90%|█████████ | 132/146 [00:02<00:00, 49.42it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231:  91%|█████████ | 133/146 [00:02<00:00, 49.55it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231:  92%|█████████▏| 134/146 [00:02<00:00, 49.64it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231:  92%|█████████▏| 135/146 [00:02<00:00, 49.81it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231:  93%|█████████▎| 136/146 [00:02<00:00, 49.90it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231:  94%|█████████▍| 137/146 [00:02<00:00, 50.03it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231:  95%|█████████▍| 138/146 [00:02<00:00, 50.18it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231:  95%|█████████▌| 139/146 [00:02<00:00, 50.32it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231:  96%|█████████▌| 140/146 [00:02<00:00, 50.45it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231:  97%|█████████▋| 141/146 [00:02<00:00, 50.59it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231:  97%|█████████▋| 142/146 [00:02<00:00, 50.71it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231:  98%|█████████▊| 143/146 [00:02<00:00, 50.83it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231:  99%|█████████▊| 144/146 [00:02<00:00, 50.97it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231:  99%|█████████▉| 145/146 [00:02<00:00, 51.05it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0153]\n",
      "Epoch 231: 100%|██████████| 146/146 [00:02<00:00, 51.06it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0142]\n",
      "Epoch 231: 100%|██████████| 146/146 [00:02<00:00, 50.94it/s, loss=0.000756, v_num=8, train_loss=0.00038, val_loss=0.0142]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 231, global step 30392: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232:  90%|████████▉ | 131/146 [00:02<00:00, 48.94it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 232:  90%|█████████ | 132/146 [00:02<00:00, 48.91it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232:  91%|█████████ | 133/146 [00:02<00:00, 49.06it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232:  92%|█████████▏| 134/146 [00:02<00:00, 49.25it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232:  92%|█████████▏| 135/146 [00:02<00:00, 49.40it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232:  93%|█████████▎| 136/146 [00:02<00:00, 49.59it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232:  94%|█████████▍| 137/146 [00:02<00:00, 49.79it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232:  95%|█████████▍| 138/146 [00:02<00:00, 49.95it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232:  95%|█████████▌| 139/146 [00:02<00:00, 50.13it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232:  96%|█████████▌| 140/146 [00:02<00:00, 50.29it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232:  97%|█████████▋| 141/146 [00:02<00:00, 50.47it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232:  97%|█████████▋| 142/146 [00:02<00:00, 50.61it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232:  98%|█████████▊| 143/146 [00:02<00:00, 50.75it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232:  99%|█████████▊| 144/146 [00:02<00:00, 50.92it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232:  99%|█████████▉| 145/146 [00:02<00:00, 51.10it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0142]\n",
      "Epoch 232: 100%|██████████| 146/146 [00:02<00:00, 51.18it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0139]\n",
      "Epoch 232: 100%|██████████| 146/146 [00:02<00:00, 51.11it/s, loss=0.000746, v_num=8, train_loss=0.000174, val_loss=0.0139]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 232, global step 30523: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233:  90%|████████▉ | 131/146 [00:02<00:00, 54.20it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 233:  90%|█████████ | 132/146 [00:02<00:00, 54.09it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233:  91%|█████████ | 133/146 [00:02<00:00, 54.22it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233:  92%|█████████▏| 134/146 [00:02<00:00, 54.38it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233:  92%|█████████▏| 135/146 [00:02<00:00, 54.52it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233:  93%|█████████▎| 136/146 [00:02<00:00, 54.66it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233:  94%|█████████▍| 137/146 [00:02<00:00, 54.80it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233:  95%|█████████▍| 138/146 [00:02<00:00, 54.93it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233:  95%|█████████▌| 139/146 [00:02<00:00, 55.09it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233:  96%|█████████▌| 140/146 [00:02<00:00, 55.22it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233:  97%|█████████▋| 141/146 [00:02<00:00, 55.38it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233:  97%|█████████▋| 142/146 [00:02<00:00, 55.46it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233:  98%|█████████▊| 143/146 [00:02<00:00, 55.64it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233:  99%|█████████▊| 144/146 [00:02<00:00, 55.70it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233:  99%|█████████▉| 145/146 [00:02<00:00, 55.87it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0139]\n",
      "Epoch 233: 100%|██████████| 146/146 [00:02<00:00, 55.89it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0154]\n",
      "Epoch 233: 100%|██████████| 146/146 [00:02<00:00, 55.81it/s, loss=0.000752, v_num=8, train_loss=0.000199, val_loss=0.0154]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 233, global step 30654: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234:  90%|████████▉ | 131/146 [00:02<00:00, 47.41it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 234:  90%|█████████ | 132/146 [00:02<00:00, 47.09it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234:  91%|█████████ | 133/146 [00:02<00:00, 47.01it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234:  92%|█████████▏| 134/146 [00:02<00:00, 47.06it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234:  92%|█████████▏| 135/146 [00:02<00:00, 47.17it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234:  93%|█████████▎| 136/146 [00:02<00:00, 47.27it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234:  94%|█████████▍| 137/146 [00:02<00:00, 47.35it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234:  95%|█████████▍| 138/146 [00:02<00:00, 47.45it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234:  95%|█████████▌| 139/146 [00:02<00:00, 47.55it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234:  96%|█████████▌| 140/146 [00:02<00:00, 47.68it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234:  97%|█████████▋| 141/146 [00:02<00:00, 47.79it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234:  97%|█████████▋| 142/146 [00:02<00:00, 47.92it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234:  98%|█████████▊| 143/146 [00:02<00:00, 47.97it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234:  99%|█████████▊| 144/146 [00:02<00:00, 48.09it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234:  99%|█████████▉| 145/146 [00:03<00:00, 48.25it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0154]\n",
      "Epoch 234: 100%|██████████| 146/146 [00:03<00:00, 48.32it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0135]\n",
      "Epoch 234: 100%|██████████| 146/146 [00:03<00:00, 48.23it/s, loss=0.000769, v_num=8, train_loss=0.000541, val_loss=0.0135]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 234, global step 30785: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235:  90%|████████▉ | 131/146 [00:02<00:00, 52.35it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 235:  90%|█████████ | 132/146 [00:02<00:00, 52.31it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235:  91%|█████████ | 133/146 [00:02<00:00, 52.42it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235:  92%|█████████▏| 134/146 [00:02<00:00, 52.57it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235:  92%|█████████▏| 135/146 [00:02<00:00, 52.67it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235:  93%|█████████▎| 136/146 [00:02<00:00, 52.79it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235:  94%|█████████▍| 137/146 [00:02<00:00, 52.85it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235:  95%|█████████▍| 138/146 [00:02<00:00, 52.83it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235:  95%|█████████▌| 139/146 [00:02<00:00, 52.99it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235:  96%|█████████▌| 140/146 [00:02<00:00, 53.11it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235:  97%|█████████▋| 141/146 [00:02<00:00, 53.22it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235:  97%|█████████▋| 142/146 [00:02<00:00, 53.34it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235:  98%|█████████▊| 143/146 [00:02<00:00, 53.37it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235:  99%|█████████▊| 144/146 [00:02<00:00, 53.47it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235:  99%|█████████▉| 145/146 [00:02<00:00, 53.54it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0135]\n",
      "Epoch 235: 100%|██████████| 146/146 [00:02<00:00, 53.48it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0137]\n",
      "Epoch 235: 100%|██████████| 146/146 [00:02<00:00, 53.34it/s, loss=0.000737, v_num=8, train_loss=0.000279, val_loss=0.0137]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 235, global step 30916: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236:  90%|████████▉ | 131/146 [00:02<00:00, 46.16it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 236:  90%|█████████ | 132/146 [00:02<00:00, 45.97it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236:  91%|█████████ | 133/146 [00:02<00:00, 46.10it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236:  92%|█████████▏| 134/146 [00:02<00:00, 46.20it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236:  92%|█████████▏| 135/146 [00:02<00:00, 46.32it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236:  93%|█████████▎| 136/146 [00:02<00:00, 46.44it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236:  94%|█████████▍| 137/146 [00:02<00:00, 46.58it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236:  95%|█████████▍| 138/146 [00:02<00:00, 46.68it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236:  95%|█████████▌| 139/146 [00:02<00:00, 46.81it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236:  96%|█████████▌| 140/146 [00:02<00:00, 46.93it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236:  97%|█████████▋| 141/146 [00:02<00:00, 47.06it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236:  97%|█████████▋| 142/146 [00:03<00:00, 47.17it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236:  98%|█████████▊| 143/146 [00:03<00:00, 47.27it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236:  99%|█████████▊| 144/146 [00:03<00:00, 47.41it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236:  99%|█████████▉| 145/146 [00:03<00:00, 47.54it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0137]\n",
      "Epoch 236: 100%|██████████| 146/146 [00:03<00:00, 47.58it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0167]\n",
      "Epoch 236: 100%|██████████| 146/146 [00:03<00:00, 47.51it/s, loss=0.000751, v_num=8, train_loss=0.000178, val_loss=0.0167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 236, global step 31047: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237:  90%|████████▉ | 131/146 [00:02<00:00, 48.37it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 237:  90%|█████████ | 132/146 [00:02<00:00, 48.28it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237:  91%|█████████ | 133/146 [00:02<00:00, 48.38it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237:  92%|█████████▏| 134/146 [00:02<00:00, 48.51it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237:  92%|█████████▏| 135/146 [00:02<00:00, 48.66it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237:  93%|█████████▎| 136/146 [00:02<00:00, 48.79it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237:  94%|█████████▍| 137/146 [00:02<00:00, 48.87it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237:  95%|█████████▍| 138/146 [00:02<00:00, 48.98it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237:  95%|█████████▌| 139/146 [00:02<00:00, 49.10it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237:  96%|█████████▌| 140/146 [00:02<00:00, 49.22it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237:  97%|█████████▋| 141/146 [00:02<00:00, 49.33it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237:  97%|█████████▋| 142/146 [00:02<00:00, 49.39it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237:  98%|█████████▊| 143/146 [00:02<00:00, 49.55it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237:  99%|█████████▊| 144/146 [00:02<00:00, 49.60it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237:  99%|█████████▉| 145/146 [00:02<00:00, 49.71it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0167]\n",
      "Epoch 237: 100%|██████████| 146/146 [00:02<00:00, 49.72it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0127]\n",
      "Epoch 237: 100%|██████████| 146/146 [00:02<00:00, 49.61it/s, loss=0.00082, v_num=8, train_loss=0.00075, val_loss=0.0127]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 237, global step 31178: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238:  90%|████████▉ | 131/146 [00:02<00:00, 48.70it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 238:  90%|█████████ | 132/146 [00:02<00:00, 48.65it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238:  91%|█████████ | 133/146 [00:02<00:00, 48.75it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238:  92%|█████████▏| 134/146 [00:02<00:00, 48.92it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238:  92%|█████████▏| 135/146 [00:02<00:00, 49.09it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238:  93%|█████████▎| 136/146 [00:02<00:00, 49.24it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238:  94%|█████████▍| 137/146 [00:02<00:00, 49.40it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238:  95%|█████████▍| 138/146 [00:02<00:00, 49.57it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238:  95%|█████████▌| 139/146 [00:02<00:00, 49.73it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238:  96%|█████████▌| 140/146 [00:02<00:00, 49.89it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238:  97%|█████████▋| 141/146 [00:02<00:00, 50.03it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238:  97%|█████████▋| 142/146 [00:02<00:00, 50.21it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238:  98%|█████████▊| 143/146 [00:02<00:00, 50.35it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238:  99%|█████████▊| 144/146 [00:02<00:00, 50.50it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238:  99%|█████████▉| 145/146 [00:02<00:00, 50.66it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0127]\n",
      "Epoch 238: 100%|██████████| 146/146 [00:02<00:00, 50.67it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0135]\n",
      "Epoch 238: 100%|██████████| 146/146 [00:02<00:00, 50.59it/s, loss=0.000753, v_num=8, train_loss=0.000468, val_loss=0.0135]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 238, global step 31309: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239:  90%|████████▉ | 131/146 [00:02<00:00, 51.94it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 239:  90%|█████████ | 132/146 [00:02<00:00, 51.88it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239:  91%|█████████ | 133/146 [00:02<00:00, 52.05it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239:  92%|█████████▏| 134/146 [00:02<00:00, 52.20it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239:  92%|█████████▏| 135/146 [00:02<00:00, 52.36it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239:  93%|█████████▎| 136/146 [00:02<00:00, 52.45it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239:  94%|█████████▍| 137/146 [00:02<00:00, 52.61it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239:  95%|█████████▍| 138/146 [00:02<00:00, 52.73it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239:  95%|█████████▌| 139/146 [00:02<00:00, 52.87it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239:  96%|█████████▌| 140/146 [00:02<00:00, 53.01it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239:  97%|█████████▋| 141/146 [00:02<00:00, 53.16it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239:  97%|█████████▋| 142/146 [00:02<00:00, 53.10it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239:  98%|█████████▊| 143/146 [00:02<00:00, 53.22it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239:  99%|█████████▊| 144/146 [00:02<00:00, 53.39it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239:  99%|█████████▉| 145/146 [00:02<00:00, 53.52it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0135]\n",
      "Epoch 239: 100%|██████████| 146/146 [00:02<00:00, 53.52it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0179]\n",
      "Epoch 239: 100%|██████████| 146/146 [00:02<00:00, 53.38it/s, loss=0.000754, v_num=8, train_loss=0.000205, val_loss=0.0179]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 239, global step 31440: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240:  90%|████████▉ | 131/146 [00:02<00:00, 53.25it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 240:  90%|█████████ | 132/146 [00:02<00:00, 53.18it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240:  91%|█████████ | 133/146 [00:02<00:00, 53.30it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240:  92%|█████████▏| 134/146 [00:02<00:00, 53.49it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240:  92%|█████████▏| 135/146 [00:02<00:00, 53.61it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240:  93%|█████████▎| 136/146 [00:02<00:00, 53.75it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240:  94%|█████████▍| 137/146 [00:02<00:00, 53.93it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240:  95%|█████████▍| 138/146 [00:02<00:00, 54.05it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240:  95%|█████████▌| 139/146 [00:02<00:00, 54.17it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240:  96%|█████████▌| 140/146 [00:02<00:00, 54.30it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240:  97%|█████████▋| 141/146 [00:02<00:00, 54.44it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240:  97%|█████████▋| 142/146 [00:02<00:00, 54.55it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240:  98%|█████████▊| 143/146 [00:02<00:00, 54.66it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240:  99%|█████████▊| 144/146 [00:02<00:00, 54.75it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240:  99%|█████████▉| 145/146 [00:02<00:00, 54.92it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0179]\n",
      "Epoch 240: 100%|██████████| 146/146 [00:02<00:00, 54.92it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0124]\n",
      "Epoch 240: 100%|██████████| 146/146 [00:02<00:00, 54.74it/s, loss=0.000801, v_num=8, train_loss=0.000537, val_loss=0.0124]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 240, global step 31571: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241:  60%|██████    | 88/146 [00:01<00:01, 51.58it/s, loss=0.000205, v_num=8, train_loss=0.000282, val_loss=0.0124] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hazre\\anaconda3\\envs\\yolov7\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module.train_dataloader(),data_module.val_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd98d6d5",
   "metadata": {},
   "source": [
    "# #Testing the Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2e1ea5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '~/Desktop/checkpoints2/epoch=160-step=21091.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e54babb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_model = akbankPricePredictor.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    n_features = 6   # 9 in this case\n",
    ")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d2e865ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a27e8c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "104f5fec",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [226]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [213]\u001b[0m, in \u001b[0;36makbankPricePredictor.forward\u001b[1;34m(self, x, labels)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m----> 9\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [211]\u001b[0m, in \u001b[0;36mjust_GRU.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru\u001b[38;5;241m.\u001b[39mflatten_parameters()\n\u001b[1;32m---> 19\u001b[0m     _, (hidden, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     out \u001b[38;5;241m=\u001b[39m hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor(out)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:940\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    938\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 940\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    942\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    943\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:229\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m     expected_hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:201\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    199\u001b[0m expected_input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m expected_input_dim:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    203\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
     ]
    }
   ],
   "source": [
    "trained_model(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "7e4d1260",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "both arguments to matmul need to be at least 1D, but they are 0D and 2D",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [223]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m sequence \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m label \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m _, output \u001b[38;5;241m=\u001b[39m \u001b[43mtrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m predictions\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     11\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(label\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [213]\u001b[0m, in \u001b[0;36makbankPricePredictor.forward\u001b[1;34m(self, x, labels)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m----> 9\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [211]\u001b[0m, in \u001b[0;36mjust_GRU.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m _, (hidden, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(x)\n\u001b[0;32m     20\u001b[0m out \u001b[38;5;241m=\u001b[39m hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: both arguments to matmul need to be at least 1D, but they are 0D and 2D"
     ]
    }
   ],
   "source": [
    "test_dataset = akbankDataset(test_sequences)\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for item in (test_dataset): \n",
    "  sequence = item[\"sequence\"]\n",
    "  label = item[\"label\"]\n",
    "\n",
    "  _, output = trained_model(sequence)\n",
    "  predictions.append(output.item())\n",
    "  labels.append(label.item())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "a1c2e7ce",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[         Open      High    Volume   Target1   Target2   Target3       USD  \\\n",
       " 225  0.263713  0.292056  0.657898  0.256842  0.164211  0.105263  0.218367   \n",
       " 226  0.419831  0.348131  0.718969  0.395789  0.256842  0.164211  0.176220   \n",
       " 227  0.261603  0.289719  0.266990  0.305263  0.395789  0.256842  0.418092   \n",
       " 228  0.246835  0.308411  0.400435  0.225263  0.305263  0.395789  0.000000   \n",
       " 229  0.386076  0.371495  0.298707  0.366316  0.225263  0.305263  0.305308   \n",
       " 230  0.409283  0.408878  0.473737  0.393684  0.366316  0.225263  0.596065   \n",
       " 231  0.398734  0.369159  0.369445  0.328421  0.393684  0.366316  0.461543   \n",
       " 232  0.354431  0.254673  0.204752  0.353684  0.328421  0.393684  0.496895   \n",
       " 233  0.251055  0.165888  0.446040  0.233684  0.353684  0.328421  0.471053   \n",
       " 234  0.126582  0.123832  0.681404  0.111579  0.233684  0.353684  0.388174   \n",
       " 235  0.105485  0.004673  0.507298  0.101053  0.111579  0.233684  0.415361   \n",
       " 236  0.000000  0.000000  0.418504  0.000000  0.101053  0.111579  0.420524   \n",
       " 237  0.103376  0.128505  0.324444  0.094737  0.000000  0.101053  0.577465   \n",
       " 238  0.191983  0.205608  0.751249  0.193684  0.094737  0.000000  0.537792   \n",
       " 239  0.335443  0.369159  0.621790  0.381052  0.193684  0.094737  0.297170   \n",
       " 240  0.375528  0.313084  1.000000  0.360000  0.381052  0.193684  0.454749   \n",
       " 241  0.432489  0.497664  0.857125  0.324211  0.360000  0.381052  0.159536   \n",
       " 242  0.611814  0.724299  0.000000  0.578947  0.324211  0.360000  0.659923   \n",
       " 243  0.677215  0.796729  0.825825  0.635789  0.578947  0.324211  0.365090   \n",
       " 244  1.000000  0.915888  0.583192  1.000000  0.635789  0.578947  0.668089   \n",
       " 245  0.940928  0.981308  0.696669  0.894737  1.000000  0.635789  0.552329   \n",
       " 246  0.945148  0.941589  0.424264  0.884211  0.894737  1.000000  0.798508   \n",
       " 247  0.864979  0.922897  0.292368  0.863158  0.884211  0.894737  1.000000   \n",
       " 248  0.898734  1.000000  0.169440  0.835789  0.863158  0.884211  0.372291   \n",
       " 249  0.894515  0.934579  0.223641  0.877895  0.835789  0.863158  0.825668   \n",
       " \n",
       "          EURO  last_difference  \n",
       " 225  0.000000         0.536797  \n",
       " 226  0.279520         0.064936  \n",
       " 227  0.280871         0.086580  \n",
       " 228  0.148278         0.541126  \n",
       " 229  0.057862         0.307360  \n",
       " 230  0.071989         0.116882  \n",
       " 231  0.330585         0.303031  \n",
       " 232  0.619483         0.004329  \n",
       " 233  0.666599         0.000000  \n",
       " 234  0.534283         0.229437  \n",
       " 235  0.456166         0.043290  \n",
       " 236  0.582240         0.445888  \n",
       " 237  0.719479         0.454545  \n",
       " 238  0.618946         0.636363  \n",
       " 239  0.646564         0.207792  \n",
       " 240  0.815558         0.177490  \n",
       " 241  1.000000         0.774891  \n",
       " 242  0.915799         0.367965  \n",
       " 243  0.756739         1.000000  \n",
       " 244  0.861353         0.034632  \n",
       " 245  0.898770         0.229438  \n",
       " 246  0.876773         0.207792  \n",
       " 247  0.886964         0.194805  \n",
       " 248  0.803520         0.337663  \n",
       " 249  0.921229         0.125541  ,\n",
       "         Close\n",
       " 225  0.395789\n",
       " 226  0.305263\n",
       " 227  0.225263\n",
       " 228  0.366316\n",
       " 229  0.393684\n",
       " 230  0.328421\n",
       " 231  0.353684\n",
       " 232  0.233684\n",
       " 233  0.111579\n",
       " 234  0.101053\n",
       " 235  0.000000\n",
       " 236  0.094737\n",
       " 237  0.193684\n",
       " 238  0.381052\n",
       " 239  0.360000\n",
       " 240  0.324211\n",
       " 241  0.578947\n",
       " 242  0.635789\n",
       " 243  1.000000\n",
       " 244  0.894737\n",
       " 245  0.884211\n",
       " 246  0.863158\n",
       " 247  0.835789\n",
       " 248  0.877895\n",
       " 249  0.816842]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list([test_x,test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dd7a638d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7e830b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2767255902290344,\n",
       " 0.286476731300354,\n",
       " 0.28720980882644653,\n",
       " 0.2751668691635132,\n",
       " 0.268544465303421,\n",
       " 0.274592787027359,\n",
       " 0.284201055765152,\n",
       " 0.29208076000213623,\n",
       " 0.32326269149780273,\n",
       " 0.37032073736190796,\n",
       " 0.4041759967803955,\n",
       " 0.42181074619293213,\n",
       " 0.44167831540107727,\n",
       " 0.4537658989429474,\n",
       " 0.4410260319709778,\n",
       " 0.4479372203350067,\n",
       " 0.4507511258125305,\n",
       " 0.4608563184738159,\n",
       " 0.4700332581996918,\n",
       " 0.4734700322151184,\n",
       " 0.472123920917511,\n",
       " 0.46520766615867615,\n",
       " 0.46066734194755554,\n",
       " 0.4721177816390991,\n",
       " 0.48986175656318665,\n",
       " 0.48618024587631226,\n",
       " 0.5179116129875183,\n",
       " 0.5570994019508362,\n",
       " 0.5584942102432251,\n",
       " 0.5520887970924377,\n",
       " 0.5261250734329224,\n",
       " 0.5276616811752319,\n",
       " 0.5296188592910767,\n",
       " 0.5418484807014465,\n",
       " 0.5223180651664734,\n",
       " 0.4867947995662689,\n",
       " 0.44937416911125183,\n",
       " 0.4520779252052307,\n",
       " 0.4746658205986023,\n",
       " 0.48411208391189575,\n",
       " 0.48746439814567566,\n",
       " 0.47416064143180847,\n",
       " 0.49387800693511963,\n",
       " 0.5096880197525024,\n",
       " 0.5350772738456726,\n",
       " 0.5527020692825317,\n",
       " 0.5637431740760803,\n",
       " 0.5538361072540283,\n",
       " 0.5633092522621155,\n",
       " 0.5713319182395935,\n",
       " 0.570186972618103,\n",
       " 0.5544169545173645,\n",
       " 0.5655754208564758,\n",
       " 0.5829912424087524,\n",
       " 0.5998337268829346,\n",
       " 0.6147944331169128,\n",
       " 0.6209102272987366,\n",
       " 0.6288615465164185,\n",
       " 0.6357523202896118,\n",
       " 0.6390579342842102,\n",
       " 0.6330905556678772,\n",
       " 0.6366035342216492,\n",
       " 0.6194182634353638,\n",
       " 0.6385910511016846,\n",
       " 0.6514018177986145,\n",
       " 0.6515364050865173,\n",
       " 0.6549848318099976,\n",
       " 0.6454010009765625,\n",
       " 0.6464731097221375,\n",
       " 0.6469296813011169,\n",
       " 0.6351224184036255,\n",
       " 0.6366122364997864,\n",
       " 0.653437614440918,\n",
       " 0.6735941767692566,\n",
       " 0.6914983987808228,\n",
       " 0.6843801736831665,\n",
       " 0.686600387096405,\n",
       " 0.6947394609451294,\n",
       " 0.7058476805686951,\n",
       " 0.7198845148086548,\n",
       " 0.7216918468475342,\n",
       " 0.722987711429596,\n",
       " 0.7003787755966187,\n",
       " 0.7053139805793762,\n",
       " 0.7191244959831238,\n",
       " 0.7330278158187866,\n",
       " 0.7633633017539978,\n",
       " 0.797088086605072,\n",
       " 0.7949473261833191,\n",
       " 0.792890727519989,\n",
       " 0.8112578988075256,\n",
       " 0.798123836517334,\n",
       " 0.7911215424537659,\n",
       " 0.7856543660163879,\n",
       " 0.7853944301605225,\n",
       " 0.7930915951728821,\n",
       " 0.7941154837608337,\n",
       " 0.8016909956932068,\n",
       " 0.8066632747650146,\n",
       " 0.8109099864959717,\n",
       " 0.8140037655830383,\n",
       " 0.8182874917984009,\n",
       " 0.8156057596206665,\n",
       " 0.8083009123802185,\n",
       " 0.8071649670600891,\n",
       " 0.7992905974388123,\n",
       " 0.8060439825057983,\n",
       " 0.825951874256134,\n",
       " 0.8528795838356018,\n",
       " 0.8735611438751221,\n",
       " 0.8781042695045471,\n",
       " 0.8829460740089417,\n",
       " 0.9015766978263855,\n",
       " 0.9089294672012329,\n",
       " 0.9240179657936096,\n",
       " 0.9208203554153442,\n",
       " 0.917403519153595,\n",
       " 0.9204699397087097,\n",
       " 0.9177845120429993,\n",
       " 0.9178832769393921,\n",
       " 0.8971577882766724,\n",
       " 0.8989433646202087,\n",
       " 0.9013292193412781,\n",
       " 0.9046153426170349,\n",
       " 0.9061306715011597,\n",
       " 0.871069610118866,\n",
       " 0.8528711199760437,\n",
       " 0.8583513498306274,\n",
       " 0.8595573902130127,\n",
       " 0.8676015734672546,\n",
       " 0.8710949420928955,\n",
       " 0.8658365607261658,\n",
       " 0.8219945430755615,\n",
       " 0.8029214143753052,\n",
       " 0.7760475277900696,\n",
       " 0.7783616781234741,\n",
       " 0.7635006308555603,\n",
       " 0.7736545205116272,\n",
       " 0.7859789729118347,\n",
       " 0.8112627863883972,\n",
       " 0.830493688583374,\n",
       " 0.8352851867675781,\n",
       " 0.8227335810661316,\n",
       " 0.8212403059005737,\n",
       " 0.837201714515686,\n",
       " 0.8525943160057068,\n",
       " 0.8682518601417542,\n",
       " 0.883574903011322,\n",
       " 0.8863561749458313,\n",
       " 0.8863198757171631,\n",
       " 0.851756751537323,\n",
       " 0.8373034596443176,\n",
       " 0.8371747732162476,\n",
       " 0.8562823534011841,\n",
       " 0.8949679732322693,\n",
       " 0.9245832562446594,\n",
       " 0.9273242950439453,\n",
       " 0.9357677698135376,\n",
       " 0.938915491104126,\n",
       " 0.9447040557861328,\n",
       " 0.9589424133300781,\n",
       " 0.9696348905563354,\n",
       " 0.974891185760498,\n",
       " 0.960110068321228,\n",
       " 0.9575128555297852,\n",
       " 0.9603855013847351,\n",
       " 0.9659988284111023,\n",
       " 0.9685476422309875,\n",
       " 0.9703850746154785,\n",
       " 0.9722468256950378,\n",
       " 0.980027437210083,\n",
       " 0.9722724556922913,\n",
       " 0.9623712301254272,\n",
       " 0.9518027305603027,\n",
       " 0.9251006841659546,\n",
       " 0.9189020991325378,\n",
       " 0.8973178863525391,\n",
       " 0.897573709487915,\n",
       " 0.9021368026733398,\n",
       " 0.8949059844017029,\n",
       " 0.895677924156189,\n",
       " 0.9166902899742126,\n",
       " 0.939136266708374,\n",
       " 0.9522292613983154,\n",
       " 0.9585142135620117,\n",
       " 0.9528260231018066,\n",
       " 0.932529091835022,\n",
       " 0.9262852668762207,\n",
       " 0.9241941571235657,\n",
       " 0.8998833298683167,\n",
       " 0.8929720520973206,\n",
       " 0.8968853950500488,\n",
       " 0.9120726585388184,\n",
       " 0.9309508204460144,\n",
       " 0.9441612958908081,\n",
       " 0.9438462853431702,\n",
       " 0.9413666725158691,\n",
       " 0.9399334788322449,\n",
       " 0.9291901588439941,\n",
       " 0.9168157577514648,\n",
       " 0.9135030508041382,\n",
       " 0.869678795337677,\n",
       " 0.8496684432029724,\n",
       " 0.8340963125228882,\n",
       " 0.8076981902122498,\n",
       " 0.8174970746040344,\n",
       " 0.8423199653625488,\n",
       " 0.8485168218612671,\n",
       " 0.8450796604156494,\n",
       " 0.8143128156661987,\n",
       " 0.8042277097702026,\n",
       " 0.788498044013977,\n",
       " 0.7422970533370972,\n",
       " 0.6903563141822815,\n",
       " 0.6711760759353638,\n",
       " 0.6873698830604553,\n",
       " 0.7035260200500488,\n",
       " 0.7176476120948792,\n",
       " 0.7397438883781433,\n",
       " 0.7619321346282959,\n",
       " 0.8352584838867188,\n",
       " 0.8822601437568665,\n",
       " 0.9017266631126404,\n",
       " 0.8928316235542297,\n",
       " 0.8702545762062073,\n",
       " 0.8673828840255737,\n",
       " 0.8479240536689758,\n",
       " 0.8122139573097229,\n",
       " 0.7932760119438171,\n",
       " 0.7953069806098938,\n",
       " 0.7982601523399353,\n",
       " 0.7715227007865906,\n",
       " 0.7429487109184265,\n",
       " 0.7110080718994141,\n",
       " 0.6995739936828613,\n",
       " 0.6888042688369751,\n",
       " 0.724540114402771,\n",
       " 0.7454025149345398,\n",
       " 0.757891058921814,\n",
       " 0.7644577622413635,\n",
       " 0.7707468271255493,\n",
       " 0.7494654655456543,\n",
       " 0.6984767913818359,\n",
       " 0.6773734092712402,\n",
       " 0.7134043574333191,\n",
       " 0.7281270027160645,\n",
       " 0.7266731262207031,\n",
       " 0.6945924758911133,\n",
       " 0.68569415807724,\n",
       " 0.7083489298820496,\n",
       " 0.7369630336761475,\n",
       " 0.7480890154838562,\n",
       " 0.7744871973991394,\n",
       " 0.8059282898902893,\n",
       " 0.8201165199279785,\n",
       " 0.8397414684295654,\n",
       " 0.8525780439376831,\n",
       " 0.8559072613716125,\n",
       " 0.8693174123764038,\n",
       " 0.8798601627349854,\n",
       " 0.8673698902130127,\n",
       " 0.8614887595176697,\n",
       " 0.8734591603279114,\n",
       " 0.8732636570930481,\n",
       " 0.8440223932266235,\n",
       " 0.8176685571670532,\n",
       " 0.7923610210418701,\n",
       " 0.7441767454147339,\n",
       " 0.7151180505752563,\n",
       " 0.7046353220939636,\n",
       " 0.6848593354225159,\n",
       " 0.6763077974319458,\n",
       " 0.6802989840507507,\n",
       " 0.6832706928253174,\n",
       " 0.6626699566841125,\n",
       " 0.6020712852478027,\n",
       " 0.5859018564224243,\n",
       " 0.5620242953300476,\n",
       " 0.49880826473236084,\n",
       " 0.5196152925491333,\n",
       " 0.5074489116668701,\n",
       " 0.49515339732170105,\n",
       " 0.5093125700950623,\n",
       " 0.5363947153091431,\n",
       " 0.5159792304039001,\n",
       " 0.4967581629753113,\n",
       " 0.4678976535797119,\n",
       " 0.47716328501701355,\n",
       " 0.47124165296554565,\n",
       " 0.44505575299263,\n",
       " 0.4766785800457001,\n",
       " 0.48302948474884033,\n",
       " 0.4960918426513672,\n",
       " 0.4638125002384186,\n",
       " 0.43459126353263855,\n",
       " 0.40856096148490906,\n",
       " 0.41563042998313904,\n",
       " 0.3433673083782196,\n",
       " 0.33214014768600464,\n",
       " 0.35869649052619934,\n",
       " 0.405617892742157,\n",
       " 0.43253928422927856,\n",
       " 0.46524694561958313,\n",
       " 0.48619306087493896,\n",
       " 0.47523924708366394,\n",
       " 0.5012421607971191,\n",
       " 0.5010811686515808,\n",
       " 0.505939245223999,\n",
       " 0.4944446086883545,\n",
       " 0.44728007912635803,\n",
       " 0.3810487389564514,\n",
       " 0.3688773810863495,\n",
       " 0.3948770761489868,\n",
       " 0.36719122529029846,\n",
       " 0.3559763431549072,\n",
       " 0.3991062045097351,\n",
       " 0.4259794354438782,\n",
       " 0.43210071325302124,\n",
       " 0.47030600905418396,\n",
       " 0.49804940819740295,\n",
       " 0.48042404651641846,\n",
       " 0.453913152217865,\n",
       " 0.40466782450675964,\n",
       " 0.38638627529144287,\n",
       " 0.40523916482925415,\n",
       " 0.4480409622192383,\n",
       " 0.49595895409584045,\n",
       " 0.5216261148452759,\n",
       " 0.5115085244178772,\n",
       " 0.4968947470188141,\n",
       " 0.45654788613319397,\n",
       " 0.42696765065193176,\n",
       " 0.4387829005718231,\n",
       " 0.43143486976623535,\n",
       " 0.44553595781326294,\n",
       " 0.46083635091781616,\n",
       " 0.4639793038368225,\n",
       " 0.4093615412712097,\n",
       " 0.39179176092147827,\n",
       " 0.3584596812725067,\n",
       " 0.40811601281166077,\n",
       " 0.44561100006103516,\n",
       " 0.46411919593811035,\n",
       " 0.4756246507167816,\n",
       " 0.4839087128639221,\n",
       " 0.504859447479248,\n",
       " 0.5240234136581421,\n",
       " 0.5188957452774048,\n",
       " 0.5265755653381348,\n",
       " 0.5203776359558105,\n",
       " 0.5434211492538452,\n",
       " 0.5578327775001526,\n",
       " 0.573944091796875,\n",
       " 0.580996036529541,\n",
       " 0.5825448036193848,\n",
       " 0.5664223432540894,\n",
       " 0.5618515610694885,\n",
       " 0.5390519499778748,\n",
       " 0.4990331530570984,\n",
       " 0.48230165243148804,\n",
       " 0.4760968089103699,\n",
       " 0.49446818232536316,\n",
       " 0.45581069588661194,\n",
       " 0.42632371187210083,\n",
       " 0.41246873140335083,\n",
       " 0.40809303522109985,\n",
       " 0.4037150740623474,\n",
       " 0.3972140848636627,\n",
       " 0.37494558095932007,\n",
       " 0.38566648960113525,\n",
       " 0.38740962743759155,\n",
       " 0.4081052541732788,\n",
       " 0.4218413233757019,\n",
       " 0.3655816316604614,\n",
       " 0.34124282002449036,\n",
       " 0.3249606788158417,\n",
       " 0.29828858375549316,\n",
       " 0.3001343607902527,\n",
       " 0.2846216857433319,\n",
       " 0.26790687441825867,\n",
       " 0.2559977173805237,\n",
       " 0.23714077472686768,\n",
       " 0.23121985793113708,\n",
       " 0.22904005646705627,\n",
       " 0.23644062876701355,\n",
       " 0.21678653359413147,\n",
       " 0.20317342877388,\n",
       " 0.22007504105567932,\n",
       " 0.2600971758365631,\n",
       " 0.2603065073490143,\n",
       " 0.2664908170700073,\n",
       " 0.2475133240222931,\n",
       " 0.23442548513412476,\n",
       " 0.22401395440101624,\n",
       " 0.22356733679771423,\n",
       " 0.2133379578590393,\n",
       " 0.2190534472465515,\n",
       " 0.2481403648853302,\n",
       " 0.2718918025493622,\n",
       " 0.25327473878860474,\n",
       " 0.253498375415802,\n",
       " 0.24699801206588745,\n",
       " 0.267569363117218,\n",
       " 0.296174556016922,\n",
       " 0.20599988102912903,\n",
       " 0.16095785796642303,\n",
       " 0.17732077836990356,\n",
       " 0.17662003636360168,\n",
       " 0.14595261216163635,\n",
       " 0.1018875390291214,\n",
       " 0.05755578726530075,\n",
       " 0.06133004277944565,\n",
       " 0.08070588856935501,\n",
       " 0.08686209470033646,\n",
       " 0.07479805499315262,\n",
       " 0.12926635146141052,\n",
       " 0.16802752017974854,\n",
       " 0.1691732406616211,\n",
       " 0.20297320187091827,\n",
       " 0.21766901016235352,\n",
       " 0.20928916335105896,\n",
       " 0.2092342972755432,\n",
       " 0.2003583312034607,\n",
       " 0.1927770972251892,\n",
       " 0.20899659395217896,\n",
       " 0.21266227960586548,\n",
       " 0.19440463185310364,\n",
       " 0.17758333683013916,\n",
       " 0.2100326418876648,\n",
       " 0.2503904104232788,\n",
       " 0.2446519434452057,\n",
       " 0.23740452527999878,\n",
       " 0.222148597240448,\n",
       " 0.19370236992835999,\n",
       " 0.1732790619134903,\n",
       " 0.1579166203737259,\n",
       " 0.14712411165237427,\n",
       " 0.18912646174430847,\n",
       " 0.18120360374450684,\n",
       " 0.13194778561592102,\n",
       " 0.11772660911083221,\n",
       " 0.10972049087285995,\n",
       " 0.0961349755525589,\n",
       " 0.0989258661866188,\n",
       " 0.08222588896751404,\n",
       " 0.08379516005516052,\n",
       " 0.08059835433959961,\n",
       " 0.06848403811454773,\n",
       " 0.07624757289886475,\n",
       " 0.07401645928621292,\n",
       " 0.093297578394413,\n",
       " 0.09441646933555603,\n",
       " 0.0691760778427124,\n",
       " 0.06809236854314804,\n",
       " 0.09031525999307632,\n",
       " 0.07976676523685455,\n",
       " 0.1031927764415741,\n",
       " 0.11800340563058853,\n",
       " 0.12152242660522461]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3e9c95f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.24205947]\n",
      "[0.01487675]\n"
     ]
    }
   ],
   "source": [
    "# Doing the inverse scaling now\n",
    "print(scaler1.min_)\n",
    "print(scaler1.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c04b757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descale(descaler, values):\n",
    "  # Scaler works only with 2D data\n",
    "  values_2d = np.array(values)[:, np.newaxis]\n",
    "  return descaler.inverse_transform(values_2d).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d6198c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "descaler = MinMaxScaler()\n",
    "descaler.min_, descaler.scale_ = scaler1.min_[0], scaler1.scale_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "68ee14a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_descaled = descale(descaler, predictions)\n",
    "labels_descaled = descale(descaler, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "69bbcc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.091216   102.74667797 102.79595471]\n",
      "[102.64800229 102.25299708 102.21800174]\n"
     ]
    }
   ],
   "source": [
    "print(predictions_descaled[:3])\n",
    "print(labels_descaled[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "59958174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([102.091216  , 102.74667797, 102.79595471, 101.98644033,\n",
       "       101.54128895, 101.9478511 , 102.59370934, 103.12337519,\n",
       "       105.21939351, 108.38258839, 110.65830514, 111.84369539,\n",
       "       113.17917357, 113.99168887, 113.13532773, 113.59989091,\n",
       "       113.78903883, 114.46829979, 115.08516452, 115.31618104,\n",
       "       115.22569678, 114.76079304, 114.45559698, 115.22528411,\n",
       "       116.41801639, 116.17054892, 118.30349975, 120.93766381,\n",
       "       121.03142144, 120.60085596, 118.85560036, 118.9588896 ,\n",
       "       119.09044916, 119.91251211, 118.59969707, 116.21185861,\n",
       "       113.69648117, 113.87822496, 115.39656074, 116.03152914,\n",
       "       116.25686835, 115.3626031 , 116.68798474, 117.75071802,\n",
       "       119.45735834, 120.6420795 , 121.38425155, 120.7183084 ,\n",
       "       121.35508375, 121.89435935, 121.81739725, 120.75735237,\n",
       "       121.50741335, 122.67808749, 123.81022249, 124.81586623,\n",
       "       125.22696381, 125.76144355, 126.22463449, 126.44683456,\n",
       "       126.04571333, 126.28185224, 125.12667549, 126.41545114,\n",
       "       127.27657809, 127.28562492, 127.51742472, 126.87320918,\n",
       "       126.94527526, 126.97596554, 126.18229311, 126.2824372 ,\n",
       "       127.41342231, 128.76832632, 129.97183025, 129.49335026,\n",
       "       129.64259079, 130.18969121, 130.93637465, 131.87991664,\n",
       "       132.0014037 , 132.08851042, 130.56876032, 130.90049987,\n",
       "       131.82882893, 132.76339622, 134.80251731, 137.06946369,\n",
       "       136.92556391, 136.7873214 , 138.02194432, 137.13908576,\n",
       "       136.66839854, 136.3009004 , 136.28342777, 136.80082352,\n",
       "       136.86964829, 137.37886664, 137.71309828, 137.998558  ,\n",
       "       138.20651875, 138.49446655, 138.31420319, 137.82317865,\n",
       "       137.74682154, 137.21751427, 137.67147007, 139.00965869,\n",
       "       140.81971245, 142.20990628, 142.51529065, 142.84075191,\n",
       "       144.09308385, 144.58732967, 145.60156349, 145.38662331,\n",
       "       145.15694699, 145.36306872, 145.18255695, 145.18919583,\n",
       "       143.79604916, 143.91607382, 144.0764486 , 144.29733852,\n",
       "       144.39919742, 142.04242786, 140.81914352, 141.1875191 ,\n",
       "       141.26858793, 141.8093099 , 142.04413065, 141.69066751,\n",
       "       138.74365083, 137.46157416, 135.65513832, 135.8106932 ,\n",
       "       134.81174843, 135.49428276, 136.32272015, 138.02227286,\n",
       "       139.31495491, 139.63703464, 138.79332823, 138.69295176,\n",
       "       139.76586172, 140.80053703, 141.85302152, 142.88302117,\n",
       "       143.0699755 , 143.0675355 , 140.74423679, 139.77270093,\n",
       "       139.76405076, 141.04844323, 143.64885198, 145.63956175,\n",
       "       145.82381164, 146.39137359, 146.60296027, 146.99206182,\n",
       "       147.94915   , 148.66788765, 149.02121057, 148.0276386 ,\n",
       "       147.85305655, 148.04615294, 148.42347518, 148.5948039 ,\n",
       "       148.71831427, 148.84345932, 149.36646426, 148.84518214,\n",
       "       148.17963164, 147.46922765, 145.67434274, 145.25768004,\n",
       "       143.8068108 , 143.82400697, 144.13073354, 143.64468516,\n",
       "       143.69657418, 145.10900444, 146.61780059, 147.49789863,\n",
       "       147.92036684, 147.53801235, 146.17367289, 145.75396921,\n",
       "       145.6134069 , 143.97925735, 143.51468816, 143.77773916,\n",
       "       144.79861186, 146.06758306, 146.95557804, 146.93440334,\n",
       "       146.76772624, 146.67138839, 145.94923315, 145.11743825,\n",
       "       144.8947614 , 141.94893868, 140.60386278, 139.5571197 ,\n",
       "       137.78266426, 138.44133549, 140.10990544, 140.52645195,\n",
       "       140.29540938, 138.22729278, 137.54938203, 136.4920496 ,\n",
       "       133.38646511, 129.89506046, 128.60578399, 129.69431554,\n",
       "       130.78031495, 131.72955427, 133.21484391, 134.70631568,\n",
       "       139.6352397 , 142.79464437, 144.10316437, 143.50524869,\n",
       "       141.9876421 , 141.79460982, 140.48660666, 138.08620962,\n",
       "       136.81321983, 136.94973952, 137.14824878, 135.35098397,\n",
       "       133.43026889, 131.28325101, 130.51466368, 129.79073352,\n",
       "       132.1928614 , 133.59521114, 134.43467861, 134.87608585,\n",
       "       135.29883051, 133.86831863, 130.44091084, 129.02236255,\n",
       "       131.44432693, 132.43396845, 132.33624032, 130.17981103,\n",
       "       129.58167498, 131.10450613, 133.02791763, 133.77579503,\n",
       "       135.55025447, 137.66369333, 138.61741199, 139.93658145,\n",
       "       140.79944324, 141.02322991, 141.92464688, 142.63332004,\n",
       "       141.79373638, 141.39841265, 142.20305105, 142.18990951,\n",
       "       140.22434094, 138.45286238, 136.75171506, 133.51281614,\n",
       "       131.55951967, 130.85488113, 129.52555904, 128.95073319,\n",
       "       129.21901677, 129.41877207, 128.03401114, 123.96062893,\n",
       "       122.87373606, 121.26871023, 117.01939174, 118.41801939,\n",
       "       117.60020741, 116.7737152 , 117.72548067, 119.54591544,\n",
       "       118.17360692, 116.88158595, 114.94161131, 115.56443781,\n",
       "       115.16639161, 113.40620155, 115.53185643, 115.9587579 ,\n",
       "       116.83679656, 114.66701138, 112.70278901, 110.95305809,\n",
       "       111.4282607 , 106.57080579, 105.81612725, 107.60121811,\n",
       "       110.75522794, 112.56485701, 114.76343337, 116.17141033,\n",
       "       115.4351059 , 117.18299581, 117.17217408, 117.49872914,\n",
       "       116.72607114, 113.55571853, 109.10371395, 108.28556742,\n",
       "       110.03324097, 108.17222571, 107.41837252, 110.31751876,\n",
       "       112.12391052, 112.53537671, 115.10349856, 116.96838217,\n",
       "       115.78362294, 114.00158709, 110.6913653 , 109.46249781,\n",
       "       110.72977023, 113.60686434, 116.82786393, 118.55318486,\n",
       "       117.87309053, 116.89076699, 114.17869127, 112.19033736,\n",
       "       112.98454667, 112.49061937, 113.43848044, 114.46695759,\n",
       "       114.67822375, 111.00687226, 109.82584916, 107.58530003,\n",
       "       110.92314908, 113.4435247 , 114.68762716, 115.46101235,\n",
       "       116.01785874, 117.42614621, 118.71432889, 118.36965214,\n",
       "       118.88588198, 118.46926335, 120.01822532, 120.98696059,\n",
       "       122.06994705, 122.54397174, 122.64807832, 121.56434262,\n",
       "       121.25709921, 119.7245321 , 117.03450851, 115.90983374,\n",
       "       115.49275035, 116.72765574, 114.12913808, 112.14705244,\n",
       "       111.21573448, 110.92160455, 110.62732237, 110.19033236,\n",
       "       108.69346576, 109.41411454, 109.53128652, 110.9224259 ,\n",
       "       111.84575076, 108.06403043, 106.4279998 , 105.33353052,\n",
       "       103.54065891, 103.6647302 , 102.62198366, 101.49843073,\n",
       "       100.69790908,  99.43036421,  99.03236609,  98.88584201,\n",
       "        99.3833011 ,  98.06217244,  97.14711312,  98.28322262,\n",
       "       100.97347058, 100.98754164, 101.40324477, 100.12759662,\n",
       "        99.24784515,  98.54799245,  98.51797126,  97.83036262,\n",
       "        98.21455211, 100.16974568, 101.76629362, 100.51487317,\n",
       "       100.5299058 , 100.09295787, 101.47574355, 103.39855607,\n",
       "        97.33710441,  94.30942458,  95.40932376,  95.36222058,\n",
       "        93.30078693,  90.33877669,  87.35884058,  87.61254227,\n",
       "        88.91496729,  89.32878132,  88.51784861,  92.17915314,\n",
       "        94.78464022,  94.8616544 ,  97.13365406,  98.12149163,\n",
       "        97.55820669,  97.55451865,  96.95788507,  96.44828208,\n",
       "        97.53854047,  97.7849442 ,  96.55768334,  95.42697267,\n",
       "        97.60818258, 100.3209915 ,  99.93525748,  99.44809326,\n",
       "        98.422605  ,  96.51047799,  95.13764361,  94.10499562,\n",
       "        93.37953396,  96.20289001,  95.6703234 ,  92.35939647,\n",
       "        91.40346317,  90.8652999 ,  89.95209511,  90.139696  ,\n",
       "        89.01714019,  89.12262503,  88.90773894,  88.09342669,\n",
       "        88.61528376,  88.46531052,  89.76136811,  89.83657884,\n",
       "        88.13994491,  88.06709906,  89.56089964,  88.85184035,\n",
       "        90.42651345,  91.42206916,  91.65861424])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_descaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "654b9908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"goog_max.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8516cbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = df[train_size+1:]\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "36068e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(463, 459)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences_data = test_data\n",
    "len(test_sequences_data), len(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "62d6e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(ak.shape[0]):\n",
    "    row_data= dict(\n",
    "    Date = ak.Date[i],\n",
    "    Close = ak.Close[i]\n",
    "    )\n",
    "    rows.append(row_data)\n",
    "labelsx = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "88d47321",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (labelsx[train_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictions_descaled, \"-\", label= \"Predicted\", color = \"g\")\n",
    "plt.plot(labels_descaled, \"--\", label = \"Real\", color = \"b\")\n",
    "plt.xticks(rotation =45)\n",
    "plt.legend()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "897bc4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x28573a9b130>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1400x1000 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 836,
       "width": 1148
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(predictions_descaled, \"-\", label= \"Predicted\", color = \"g\")\n",
    "plt.plot(labels_descaled, \"--\", label = \"Real\", color = \"b\")\n",
    "plt.xticks(rotation =45)\n",
    "plt.legend()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "021c4fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.38352391, 16.03539599, 15.89207569, 16.02169823, 16.70429519,\n",
       "       17.20135589, 17.57882017, 18.36808011, 18.30735099, 19.84523776,\n",
       "       19.89507659, 20.2172627 , 19.95092496, 19.57922816, 19.66921709])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_descaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2f008c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.75      , 16.200001  , 16.66999998, 17.55999906, 17.45999894,\n",
       "       17.29000098, 18.49999999, 18.76999991, 20.5       , 19.99999993,\n",
       "       19.95000114, 19.84999987, 19.719999  , 19.92000011, 19.62999903])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_descaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d11f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
